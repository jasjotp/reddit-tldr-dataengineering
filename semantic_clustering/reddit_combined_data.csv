,id,title,selftext,score,num_comments,author,created_utc,url,upvote_ratio,over_18,edited,spoiler,stickied
0,1jmsyfl,SQLFlow: DuckDB for Streaming Data,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipelineâ€”ingestion, transformation, and enrichmentâ€”as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",69,16,turbolytics,2025-03-29 18:35:07,https://www.reddit.com/r/dataengineering/comments/1jmsyfl/sqlflow_duckdb_for_streaming_data/,0,False,2025-03-29 18:44:17,False,False
1,1jmsfs6,Interactive Change Data Capture (CDC) Playground,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",51,4,dan_the_lion,2025-03-29 18:12:13,https://www.change-data-capture.com/,0,False,False,False,False
2,1jnc3zk,When to use a surrogate key instead of a primary key?,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",34,42,DataGhost404,2025-03-30 13:12:50,https://www.reddit.com/r/dataengineering/comments/1jnc3zk/when_to_use_a_surrogate_key_instead_of_a_primary/,0,False,False,False,False
3,1jn7lfp,Do I need to know software engineering to be a data engineer?,As title says ,27,61,Gloomy-Profession-19,2025-03-30 07:53:33,https://www.reddit.com/r/dataengineering/comments/1jn7lfp/do_i_need_to_know_software_engineering_to_be_a/,0,False,False,False,False
4,1jn1ko5,Should I stay in part-time role that uses Dagster or do internships in roles that use Airflow,"I am a part time data engineer/integrator who is in school at the moment. I work using Dagster, AWS, Snowflake, and Docker.

I was hoping Dagster would have roles where I lived but it seems everyone prefers Airflow.

Is it worth exploring data engineering internships that use Airflow at the expense of losing my current role? Do you guys see any growth in Dagster?",12,18,Apart-Plankton9951,2025-03-30 01:25:17,https://www.reddit.com/r/dataengineering/comments/1jn1ko5/should_i_stay_in_parttime_role_that_uses_dagster/,0,False,False,False,False
5,1jmu3u4,The classic problem of killing flies with a cannon? DW vs. LH,"I'm starting a new job (a startup that is doubling in size every year) and the IT director has already warned me that they have a lot of problems with data structure changes, both due to new implementations in internally developed software and in those developed externally.

My question is whether I should prepare the central architecture using data warehouse or lakehouse, since the current data volume is still quite small <500 GB, but as I said, constant changes in data structure have been a problem.

By the way, I will be the first data engineer on the analytics team.",9,7,EvenRelationship2110,2025-03-29 19:27:40,https://www.reddit.com/r/dataengineering/comments/1jmu3u4/the_classic_problem_of_killing_flies_with_a/,0,False,2025-03-29 19:41:38,False,False
6,1jnh7pu,A dbt column lineage visualization tool (with dynamic web visualization),"Hey dbt folks,

I'm a data engineer and use dbt on a day-to-day basis, my team and I were struggling to find a good open-source tool for user-friendly column-level lineage visualization that we could use daily, similar to what commercial solutions like dbt Cloud offer. So, I decided to start building one...

https://reddit.com/link/1jnh7pu/video/wcl9lru6zure1/player

You can find the repo [here](https://github.com/Fszta/dbt-column-lineage), and the package on [pypi](https://pypi.org/project/dbt-col-lineage/0.1.1/)

**Under the hood**

Basically, it works by combining dbt's manifest and catalog with some compiled SQL parsing magic (big shoutout to sqlglot!).

I've built it as a CLI, keeping the syntax similar to dbt-core, with upstream and downstream selectors.

    dbt-col-lineage --select stg_transactions.amount+ --format html

Right now, it supports:

* Interactive HTML visualizations
* DOT graph images
* Simple text output in the console

**What's next ?**

* Focus on compatibility with more SQL dialects
* Improve the parser to handle complex syntax specific to certain dialects
* Making the UI less... basic. It's kinda rough right now, plus some information could be added such as materialization type, col typing etc

Feel free to drop any feedback or open an issue on the [repo](https://github.com/Fszta/dbt-column-lineage/tree/main)! It's still super early, and any help for testing on other dialects would be awesome. It's only been tested on projects using Snowflake, DuckDB, and SQLite adapters so far.",5,0,Eastern-Ad-6431,2025-03-30 17:11:12,https://www.reddit.com/r/dataengineering/comments/1jnh7pu/a_dbt_column_lineage_visualization_tool_with/,0,False,2025-03-30 17:15:59,False,False
7,1jmwfwu,creating big query source node in aws glue,"i have to send data from bigquery using aws glue to rds, i need to understand how to create big query source node in glue that can access a view from big query , is it by selecting table or custom query option... also what to add in materialization dataset , i dont have that ??? i have tried using table option , added view details there but then i get an error that view is not enabled in data preview section.

",7,1,lifealtering111,2025-03-29 21:14:57,https://www.reddit.com/r/dataengineering/comments/1jmwfwu/creating_big_query_source_node_in_aws_glue/,1,False,False,False,False
8,1jni0xm,Transitioning from DE to ML Engineer in 2025?,"I am a DE with 2 years of experience, but my background is mainly in statistics. I have been offered a position as an ML Engineer (de facto Data Scientist, but also working on deployment - it is a smaller IT department, so my scope of duties will be simply quite wide). 

The position is interesting, and there are multiple pros and cons to it (that I do not want to discuss in this post). However my question is a bit more general - in 2025, with all the LLMs performing quite well with code generation and fixing, which path would you say is more stable long-term - sticking to DE and becoming better and better at it, or moving more towards ML and doing data science projects?

Furthermore, I also wonder about growth in each field - in ML/DS, my fear is that I am not PhD nor excellent mathematician. In DE, on the other hand, my fear is lack of my solid CS/SWE foundations (as my background is more in statistics). 

Ultimately, it is just an honest question, as I am very curious of your perspective on the matter - does moving towards data science projects (XGBoost and other algorithms) in 2025 from DE (PySpark and Airflow) makes sense in 2025? Which path would you say is more reasonable, and what kind of growth I can expect for each position? Personally I am a bit reluctant to switch simply since I have already dedicated 2 years to growing as an DE, but on the other hand I also see how much more and more of my tasks can be automated. Thanks for tips and honest suggestions!",5,9,absurdherowaw,2025-03-30 17:47:04,https://www.reddit.com/r/dataengineering/comments/1jni0xm/transitioning_from_de_to_ml_engineer_in_2025/,1,False,False,False,False
9,1jnfp5s,Applying for a Data Engineer Role at Docker Inc,"Hey everyone,  

I'm planning to apply for a Data Engineer position at Docker Inc. and was wondering if anyone here has experience with their application process. I have a solid background in Azure, Databricks, and data pipeline development, but Iâ€™d love to hear from others who have interviewed there or work there.

- What kind of technical questions should I expect?  
- Do they focus more on SQL, Python, or cloud architecture?  
- Any tips on system design or behavioral interviews?  

Would really appreciate any insights or advice!",5,3,Mysterious-Grape9534,2025-03-30 16:03:52,https://www.reddit.com/r/dataengineering/comments/1jnfp5s/applying_for_a_data_engineer_role_at_docker_inc/,0,False,False,False,False
10,1jn7g1f,3 years into Devops Engineering trying to move to Data Engineering,"I came to know that most of the skillset are matching 
in this 2 fields, apart from learning SQL, pyspark.

so would this be a better switching career ?",3,10,Blacktweeter,2025-03-30 07:42:11,https://www.reddit.com/r/dataengineering/comments/1jn7g1f/3_years_into_devops_engineering_trying_to_move_to/,0,False,False,False,False
11,1jn4ptx,Introducing AnuDB: A Lightweight Embedded Document Database,"**AnuDB** \- a lightweight, embedded document database.

# Key Features

* **Embedded & Serverless**: Runs directly within your application - no separate server process required
* **JSON Document Storage**: Store and query complex JSON documents with ease
* **High Performance**: Built on RocksDB's LSM-tree architecture for optimized write performance
* **C++11 Compatible**: Works with most embedded device environments that adopt C++11
* **Cross-Platform**: Supports both Windows and Linux (including embedded Linux platforms)
* **Flexible Querying**: Rich query capabilities including equality, comparison, logical operators and sorting
* **Indexing**: Create indexes on frequently accessed fields to speed up queries
* **Compression**: Optional ZSTD compression support to reduce storage footprint
* **Transactional Properties**: Inherits atomic operations and configurable durability from RocksDB
* **Import/Export**: Easy JSON import and export for data migration or integration with other systems

Checkout README for more info: [https://github.com/hash-anu/AnuDB](https://github.com/hash-anu/AnuDB)",3,0,Fine-Package-5488,2025-03-30 04:26:19,https://www.reddit.com/r/dataengineering/comments/1jn4ptx/introducing_anudb_a_lightweight_embedded_document/,1,False,False,False,False
12,1jnhk94,how to deal with azure vm nightmare?,"i am building data pipelines. i use azure vms for experimentation on sample data. when im not using them, i need to shut them off (working at bootstrapped startup).

when restarting my vm, it randomly fails. it says an allocation failure occurred due to capacity in the region (usually us-east). the only solution ive found is moving the resource to a new region, which takes 30â€“60 mins.

how do i prevent this issue in a cost-effective manner? can azure just allocate my vm to whatever region is available?

iâ€™ve tried to troubleshoot this issue for weeks with azure support, but to no avail.

thanks all! :)
",2,6,BigCountry1227,2025-03-30 17:26:28,https://www.reddit.com/r/dataengineering/comments/1jnhk94/how_to_deal_with_azure_vm_nightmare/,1,False,False,False,False
13,1jndfr9,Building a Cloud-Based Data Pipeline for Personal Use/Learning/Projects,"I am a data/functional analyst looking to get a better grasp on 'end-to-end data management and processing' (in quotations as that's the term my direct lead used when we aligned regarding my career objectives for the year, which definitely matches with my current interests).

Given this objective, my own interest in gaining a better grasp on general principles and functions of Data Engineering (and later on, Data Science), as well as a personal project I'm looking to finish for myself/my own portfolio by the end of the year, I'm looking for a cloud-based data pipeline solution for me to spend my role's training stipend on -- though I'm willing (and allowed) to use some of my personal funds on this as well since it's both for career progression and personal fun.

I started looking for small-scale subscriptions for some of the platforms we've previously used at work (AWS, Databricks), but the consensus I've gotten is that cost-wise, I'm potentially better off with multiple smaller platforms that I can just weave together, instead of going for platforms normally designed for enterprise use. And so I come here for help!

*(Though if this is not the right community for me to ask this, please let me know where instead)*

# Budget and Scope

* Base budget is a $200 stipend from our department, which refreshes at the end of the year
* I'm willing to add another $200-300/year, if necessary
* I'm from South-East Asia (converting our local currency, in case regional pricing matters)
* I prefer platforms/solutions that come with upfront costs instead of per-pull/use costs, so that's it's easier to manage the finances involved
* Languages I'm comfortable with are Python and SQL, since these are what I mainly use for work, but willing to learn basics of new languages where necessary 
* Since I will be using my personal computer, which is not that powerful currently, I am looking for a pipeline/platform that I can use completely via cloud (including running training models) 

# Learning Objectives 

*(aka things I want my platform/s to be able to do)*

**1. Data Gathering/Ingestion/Validation/Transformation/Storage**

* Obviously not too familiar with this step currently, so I ended up lumping a bunch of them together, but essentially ETL
* In terms of data I intend to use, I have three main sources I intend to work my way through, from simple to complex: **(1)** will be starting with ingesting CSV files I get from third-party free data providers like Kaggle, **(2)** then moving on to structured databases from sports stat sites such as BBREF, before  eventually work my way to **(3)** automating data collection from publicly available video games I normally play (Dota 2, Path of Exile -- if it matters)
* I want to be able to load these into SQL tables that I can regularly query form

**2. Data Querying and Extract Cloud Storage**

* I want to then be able to set it up so that I can easily query and then store-via-cloud what I need.
* To make my intention clear, most of my work experience is with AWS Athena and S3, so I hope to get platforms that function similarly.

**3. Cloud-Based Data Processing, Machine Learning, and Visualization**

* I currently do majority of my post-query data processing and modelling locally on my work laptop on Jupyter (via the Anaconda distribution), but one of my key objectives is learning to do all of these on cloud (especially since my peronal computer I'll be using would obviously not be as powerful as what I use for work)
* I definitely prefer Notebook-like environments, so perhaps something like AWS EMR
* My main experience at this level is mainly with Python (using specific packages such as Pandas, Numpy, Matplotlib, Numpy, Sklearn, etc.), but I'm looking to do more PySpark as well

**4. EXTRA/OPTIONAL: Dashboard Creation and Hosting**

* If I can get a platform/pipeline that will allow me to host interactive dashboards for me to just embed in my portfolio, that would be a plus, but I am very easily willing to drop this should it not fit my budget

#  Final Remarks

* Want to learn ETL, and cloud-based data processing on a personal data pipeline and processing platform/solution that also has SQL capabilities -- in line with my career and personal learning objectives for the year 
* Tried looking into personal subscriptions for 360 solutions like Databricks and AWS Sagemaker Unified Studio, but was told that for what I want I might be better off with patching together T2 solutions or something along those lines -- but I imagine this would be much more tricky to set up ",2,1,bacon9001,2025-03-30 14:20:16,https://www.reddit.com/r/dataengineering/comments/1jndfr9/building_a_cloudbased_data_pipeline_for_personal/,0,False,False,False,False
14,1jna701,"I am learning data engineering from a course. I am a fresher with no job experience, a commerce background, and a two-year gap.",Will any company hire me? What certificate could I obtain that would help me?,1,8,_winter_rabbit_,2025-03-30 11:12:06,https://www.reddit.com/r/dataengineering/comments/1jna701/i_am_learning_data_engineering_from_a_course_i_am/,0,False,False,False,False
15,1jnh5oy,"Struggling with Career Path â€“ Stuck in Java, Want to Return to Data Engineering (6.5 YOE)","I've been working in IT for the past 6.5 years. I started as a Java Developer for a year before transitioning into Data Engineering, where I worked with Airflow, GCP, Python, and SQL (BigQuery).

In June 2022, I joined my second company as a Data Engineer, but after just six months, the project was shelved, and I was moved to a Java-based project (Spring Boot, Kafka, etc.). This happened during a market downturn and layoffs, so I was grateful to still have a job.

Now, after two years in this role, I feel stuck. I struggle with coding, donâ€™t enjoy Java, and constantly feel like an imposter. I know for sure that I donâ€™t want to continue in Java and Spring Boot. However, Iâ€™ve stayed in this role because itâ€™s high-paying, and I have family responsibilities (supporting a family of five).

I want to transition back into Data Engineering, but now the job market expects a higher level of expertise given my experience and salary range. Iâ€™m unsure about the best way to upskill and make this switch without a major setback.

How can I strategically transition back into Data Engineering while balancing financial stability? Would love advice from those who have made similar career shifts.

Thanks in advance!",2,1,ishaheenkhan,2025-03-30 17:08:46,https://www.reddit.com/r/dataengineering/comments/1jnh5oy/struggling_with_career_path_stuck_in_java_want_to/,0,False,False,False,False
16,1jmzte8,Need help for a small website design choices,"I am working on a website whose job is to serve data from MongoDb. Just textual data in row format nothing complicated. 

This is my current setup: client sends a request to cloudfront that manages the cache and triggers a lambda for a cache miss to query from MongoDB. I also use signedurl for security purposes for each request. 

I am not an expert that but I think cloud front can handle DDoS attacks etc. Does this setup work or do I need to bring in API Gateway into the fold? I donâ€™t have any user login etc. and no form on the website (no sql injection risk I guess). I donâ€™t know much about network security etc but have heard horror stories of websites getting hacked etc. Hence am a bit paranoid before launching the website. 

Based on some reading, I came to the conclusion that I need to use AWS WAF + API Gateway for dynamic queries and AWS + cloud front for static pages. And lambda should be associated with API Gateway to connect with MongoDB and API Gateway does rate limiting and caching (user authentication is no big a problem here). I wonder if cloudfront is even needed or should just stick with the current architecture I have. 

Need your suggestions. 




",1,2,Spiritual_Piccolo793,2025-03-29 23:56:02,https://www.reddit.com/r/dataengineering/comments/1jmzte8/need_help_for_a_small_website_design_choices/,0,False,False,False,False
17,1jn7kvr,Junior vs Senior role,"What is the difference between a junior and senior in this role? How much can you really know in data engineering; get the data, clean it, dump it somewhere with a cloud service. 

But what would take someone from a junior role to a senior role? Is it just the number years of experience? 

",0,15,Gloomy-Profession-19,2025-03-30 07:52:21,https://www.reddit.com/r/dataengineering/comments/1jn7kvr/junior_vs_senior_role/,0,False,False,False,False
18,1jnc3zk,When to use a surrogate key instead of a primary key?,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",54,47,DataGhost404,2025-03-30 13:12:50,https://www.reddit.com/r/dataengineering/comments/1jnc3zk/when_to_use_a_surrogate_key_instead_of_a_primary/,0,False,False,False,False
19,1jn7lfp,Do I need to know software engineering to be a data engineer?,As title says ,46,71,Gloomy-Profession-19,2025-03-30 07:53:33,https://www.reddit.com/r/dataengineering/comments/1jn7lfp/do_i_need_to_know_software_engineering_to_be_a/,0,False,False,False,False
20,1jnh7pu,A dbt column lineage visualization tool (with dynamic web visualization),"Hey dbt folks,

I'm a data engineer and use dbt on a day-to-day basis, my team and I were struggling to find a good open-source tool for user-friendly column-level lineage visualization that we could use daily, similar to what commercial solutions like dbt Cloud offer. So, I decided to start building one...

https://reddit.com/link/1jnh7pu/video/wcl9lru6zure1/player

You can find the repo [here](https://github.com/Fszta/dbt-column-lineage), and the package on [pypi](https://pypi.org/project/dbt-col-lineage/0.1.1/)

**Under the hood**

Basically, it works by combining dbt's manifest and catalog with some compiled SQL parsing magic (big shoutout to sqlglot!).

I've built it as a CLI, keeping the syntax similar to dbt-core, with upstream and downstream selectors.

    dbt-col-lineage --select stg_transactions.amount+ --format html

Right now, it supports:

* Interactive HTML visualizations
* DOT graph images
* Simple text output in the console

**What's next ?**

* Focus on compatibility with more SQL dialects
* Improve the parser to handle complex syntax specific to certain dialects
* Making the UI less... basic. It's kinda rough right now, plus some information could be added such as materialization type, col typing etc

Feel free to drop any feedback or open an issue on the [repo](https://github.com/Fszta/dbt-column-lineage/tree/main)! It's still super early, and any help for testing on other dialects would be awesome. It's only been tested on projects using Snowflake, DuckDB, and SQLite adapters so far.",35,3,Eastern-Ad-6431,2025-03-30 17:11:12,https://www.reddit.com/r/dataengineering/comments/1jnh7pu/a_dbt_column_lineage_visualization_tool_with/,0,False,2025-03-30 17:15:59,False,False
21,1jnktzw,Passed DP-203 -- some thoughts on its retiring,"i took the Azure DP-203 last week â€” of course, itâ€™s retiring literally tomorrow. But I figured it is indeed a very broad certification and so it can give a ""grounding"" scope in Azure D.E.

Also, I think it's still super early to go full Fabric (DP-600 or even DP-700), because the job demand is still not really there. Most jobs still demand strong grounding in Azure services even in the wake of Fabric adoption (POCingâ€¦).

So of course here, itâ€™s retiring literally tomorrow unfortunately. I have passed the exam with a high score (900+). Also, I have worked (during internship) directly with MS Fabric only. So I would say some skills actually transfer quite nicely (ex: ADF ~ FDF).

---

### Some notes on resources for future exams:

I have relied primarily on [@tybulonazure](https://www.youtube.com/@tybulonazure)â€™s excellent YouTube channel (DP-203 playlist). Itâ€™s really great (watch on 1.8x â€“ 2x speed).  
Now going back to Fabric, I have seen he has pivoted to Fabric-centric content â€” also a great news!

I also used the official â€œGuideâ€ book (2024 version), which I found to be a surprisingly good way of structuring your learning. I hope equivalents for Fabric will be similar (TBSâ€¦).

---

The labs on Microsoft Learn are honestly **poorly designed** for what they offer.  
**Tip:** @tybul has video labs too â€” *use these*.  
And for the exams, always focus on **conceptual understanding**, not rote memorization.

Another **important (and mostly ignored)** tip:  
Focus on the **â€œbest practicesâ€** sections of Azure services in Microsoft Learn â€” Iâ€™ve read a lot of MS documentation, and those parts are often more helpful on the exam than the main pages.

---

**Examtopics** is obviously very helpful â€” but **read the comments**, theyâ€™re essential!

---

Finally, I do think itâ€™s a shame itâ€™s retiring â€” because the â€œtraditionalâ€ Azure environment knowledge seems to be a sort of industry standard for companies. Also, the Fabric pricing model seems quite aggressive.

So for juniors, it would have been really good to still be able to have this background knowledge as a base layer.",16,2,EducatedEarthian-99,2025-03-30 19:48:14,https://www.reddit.com/r/dataengineering/comments/1jnktzw/passed_dp203_some_thoughts_on_its_retiring/,0,False,False,False,False
22,1jn1ko5,Should I stay in part-time role that uses Dagster or do internships in roles that use Airflow,"I am a part time data engineer/integrator who is in school at the moment. I work using Dagster, AWS, Snowflake, and Docker.

I was hoping Dagster would have roles where I lived but it seems everyone prefers Airflow.

Is it worth exploring data engineering internships that use Airflow at the expense of losing my current role? Do you guys see any growth in Dagster?",11,18,Apart-Plankton9951,2025-03-30 01:25:17,https://www.reddit.com/r/dataengineering/comments/1jn1ko5/should_i_stay_in_parttime_role_that_uses_dagster/,0,False,False,False,False
23,1jnisk7,What is expected of me as a Junior Data Engineer in 2025?,"Hello all, 

I've been interviewing for a proper Junior Data Engineer position and have been doing well in the rounds so far. I've done my recruiter call, HR call and coding assessment. Waiting on the 4th.

I want to be great. I am willing to learn from those of you who are more experienced than me.  
  
Can anyone share examples from their own careers on attitude, communication, soft skills, time management, charisma, willingness to learn and other soft skills that I should keep in mind. Or maybe what I should not do instead.

How should I approach the technical side? There are 1000's of technologies to learn. So I have been learning basics with soft skills and hoping everything works out.

3 years ago I had a labour job and did well in that too. So this grind has caused me to rewire my brain to work in tech and corporate work. I am aiming for 20 years more in this field.

Any insights are appreciated.

Thanks!",9,11,turnwol7,2025-03-30 18:19:54,https://www.reddit.com/r/dataengineering/comments/1jnisk7/what_is_expected_of_me_as_a_junior_data_engineer/,0,False,False,False,False
24,1jni0xm,Transitioning from DE to ML Engineer in 2025?,"I am a DE with 2 years of experience, but my background is mainly in statistics. I have been offered a position as an ML Engineer (de facto Data Scientist, but also working on deployment - it is a smaller IT department, so my scope of duties will be simply quite wide). 

The position is interesting, and there are multiple pros and cons to it (that I do not want to discuss in this post). However my question is a bit more general - in 2025, with all the LLMs performing quite well with code generation and fixing, which path would you say is more stable long-term - sticking to DE and becoming better and better at it, or moving more towards ML and doing data science projects?

Furthermore, I also wonder about growth in each field - in ML/DS, my fear is that I am not PhD nor excellent mathematician. In DE, on the other hand, my fear is lack of my solid CS/SWE foundations (as my background is more in statistics). 

Ultimately, it is just an honest question, as I am very curious of your perspective on the matter - does moving towards data science projects (XGBoost and other algorithms) in 2025 from DE (PySpark and Airflow) makes sense in 2025? Which path would you say is more reasonable, and what kind of growth I can expect for each position? Personally I am a bit reluctant to switch simply since I have already dedicated 2 years to growing as an DE, but on the other hand I also see how much more and more of my tasks can be automated. Thanks for tips and honest suggestions!",6,11,absurdherowaw,2025-03-30 17:47:04,https://www.reddit.com/r/dataengineering/comments/1jni0xm/transitioning_from_de_to_ml_engineer_in_2025/,0,False,False,False,False
25,1jnh5oy,"Struggling with Career Path â€“ Stuck in Java, Want to Return to Data Engineering (6.5 YOE)","I've been working in IT for the past 6.5 years. I started as a Java Developer for a year before transitioning into Data Engineering, where I worked with Airflow, GCP, Python, and SQL (BigQuery).

In June 2022, I joined my second company as a Data Engineer, but after just six months, the project was shelved, and I was moved to a Java-based project (Spring Boot, Kafka, etc.). This happened during a market downturn and layoffs, so I was grateful to still have a job.

Now, after two years in this role, I feel stuck. I struggle with coding, donâ€™t enjoy Java, and constantly feel like an imposter. I know for sure that I donâ€™t want to continue in Java and Spring Boot. However, Iâ€™ve stayed in this role because itâ€™s high-paying, and I have family responsibilities (supporting a family of five).

I want to transition back into Data Engineering, but now the job market expects a higher level of expertise given my experience and salary range. Iâ€™m unsure about the best way to upskill and make this switch without a major setback.

How can I strategically transition back into Data Engineering while balancing financial stability? Would love advice from those who have made similar career shifts.

Thanks in advance!",5,9,ishaheenkhan,2025-03-30 17:08:46,https://www.reddit.com/r/dataengineering/comments/1jnh5oy/struggling_with_career_path_stuck_in_java_want_to/,0,False,False,False,False
26,1jn4ptx,Introducing AnuDB: A Lightweight Embedded Document Database,"**AnuDB** \- a lightweight, embedded document database.

# Key Features

* **Embedded & Serverless**: Runs directly within your application - no separate server process required
* **JSON Document Storage**: Store and query complex JSON documents with ease
* **High Performance**: Built on RocksDB's LSM-tree architecture for optimized write performance
* **C++11 Compatible**: Works with most embedded device environments that adopt C++11
* **Cross-Platform**: Supports both Windows and Linux (including embedded Linux platforms)
* **Flexible Querying**: Rich query capabilities including equality, comparison, logical operators and sorting
* **Indexing**: Create indexes on frequently accessed fields to speed up queries
* **Compression**: Optional ZSTD compression support to reduce storage footprint
* **Transactional Properties**: Inherits atomic operations and configurable durability from RocksDB
* **Import/Export**: Easy JSON import and export for data migration or integration with other systems

Checkout README for more info: [https://github.com/hash-anu/AnuDB](https://github.com/hash-anu/AnuDB)",3,0,Fine-Package-5488,2025-03-30 04:26:19,https://www.reddit.com/r/dataengineering/comments/1jn4ptx/introducing_anudb_a_lightweight_embedded_document/,0,False,False,False,False
27,1jnj03e,Serialisation and de-serialisation?,"I just got to know that even in today's OLAP era, but while communicating b/w the systems internally they convert it to row based storage even if the warehouses are columnar type...
This made me sickkk I never knew this at all!

So does this mean serialisation and de-serialisation?? 
I see these terms vary across many architecture ex: In spark they mention these terminologies when the data needs to searched at different instances.. they say data needs to be de-serialised which takes time...

But I am not clear how do I need to think when I hear these terminologies!!!

Source:
https://www.linkedin.com/posts/dipankar-mazumdar_dataengineering-softwareengineering-activity-7307566420828065793-LuVZ?utm_source=share&utm_medium=member_android&rcm=ACoAADeacu0BUNpPkSGeT5J-UjR35-nvjHNjhTM",4,2,Excellent-Level-9626,2025-03-30 18:29:00,https://www.reddit.com/r/dataengineering/comments/1jnj03e/serialisation_and_deserialisation/,0,False,False,False,False
28,1jnhk94,how to deal with azure vm nightmare?,"i am building data pipelines. i use azure vms for experimentation on sample data. when im not using them, i need to shut them off (working at bootstrapped startup).

when restarting my vm, it randomly fails. it says an allocation failure occurred due to capacity in the region (usually us-east). the only solution ive found is moving the resource to a new region, which takes 30â€“60 mins.

how do i prevent this issue in a cost-effective manner? can azure just allocate my vm to whatever region is available?

iâ€™ve tried to troubleshoot this issue for weeks with azure support, but to no avail.

thanks all! :)
",3,12,BigCountry1227,2025-03-30 17:26:28,https://www.reddit.com/r/dataengineering/comments/1jnhk94/how_to_deal_with_azure_vm_nightmare/,0,False,False,False,False
29,1jn7g1f,3 years into Devops Engineering trying to move to Data Engineering,"I came to know that most of the skillset are matching 
in this 2 fields, apart from learning SQL, pyspark.

so would this be a better switching career ?",1,11,Blacktweeter,2025-03-30 07:42:11,https://www.reddit.com/r/dataengineering/comments/1jn7g1f/3_years_into_devops_engineering_trying_to_move_to/,0,False,False,False,False
30,1jnrfgm,dezoomcamp project,"Real-world data engineering practice! ðŸ—ï¸ Built an end-to-end **data pipeline** using GCP, BigQuery, dbt, and Airflow to analyze Airbnb trends. Learning + hands-on = the best combo! ðŸ’¡  
\#dezoomcamp #dataengineering #learningbydoing",0,2,Impressive_Trip1382,2025-03-31 00:57:43,https://www.reddit.com/r/dataengineering/comments/1jnrfgm/dezoomcamp_project/,0,False,False,False,False
31,1jnrf68,dezoomcamp project,"How I made my Airbnb analysis efficient:  
ðŸ”¹ **Staging layer**: Standardized & cleaned raw data  
ðŸ”¹ **Core layer**: Built fact & dimension tables  
ðŸ”¹ **Analytics dataset**: Ready for insights!  
\#dezoomcamp #analyticsengineering",0,1,Impressive_Trip1382,2025-03-31 00:57:17,https://www.reddit.com/r/dataengineering/comments/1jnrf68/dezoomcamp_project/,0,False,False,False,False
32,1jnrev3,dezoomcamp project,"Orchestrating **dbt runs with Airflow** ensures transformations only happen when new data arrives. No wasted compute, no stale dashboardsâ€”just efficient workflows! ðŸ”¥  
\#dezoomcamp #dbtcore #airflow",0,0,Impressive_Trip1382,2025-03-31 00:56:49,https://www.reddit.com/r/dataengineering/comments/1jnrev3/dezoomcamp_project/,0,False,False,False,False
33,1jnrehd,dezoomcamp project,"End-to-end pipeline **deployment steps**:  
1ï¸âƒ£ Terraform: Set up GCS & BigQuery  
2ï¸âƒ£ Python: Load data to GCS  
3ï¸âƒ£ dbt: Transform data  
4ï¸âƒ£ Airflow: Orchestrate  
5ï¸âƒ£ Looker: Visualize ðŸ“Š  
\#dezoomcamp",0,0,Impressive_Trip1382,2025-03-31 00:56:16,https://www.reddit.com/r/dataengineering/comments/1jnrehd/dezoomcamp_project/,0,False,False,False,False
34,1jnre2y,dezoomcamp project,"Implemented **SCD Type 2** in dbt for tracking historical host changes. Now I can analyze how hosts evolve over time and their impact on reviews and pricing! ðŸ”„  
\#dezoomcamp #dbt #datawarehousing",0,0,Impressive_Trip1382,2025-03-31 00:55:39,https://www.reddit.com/r/dataengineering/comments/1jnre2y/dezoomcamp_project/,0,False,False,False,False
35,1jnqdug,First Major DE Project,"Hello everyone, I am working on this end-to-end process for processing Pitch-by-Pitch data with some inner workings for also enabling analytics to be done directly from the system with little set up. I began this project because I use different computers and it became an issue switching from device to device when it came to working on these projects, and I can use it as my school project to cut down on time spent. I have it posted on my GitHub here and would love for any feedback any of you could have on the overall direction of this project and ways I could improve this Thank you!

Github Link: [https://github.com/jwolfe972/mlb\_prediction\_app](https://github.com/jwolfe972/mlb_prediction_app)",1,1,scuffed12s,2025-03-31 00:02:37,https://www.reddit.com/r/dataengineering/comments/1jnqdug/first_major_de_project/,0,False,False,False,False
36,1jnlsu5,Example for complex data pipeline,"Hi community,

After working as a data analyst for several years, I've noticed a gap in tools for interactively exploring complex ETL pipeline dependencies. Many solutions handle smaller pipelines well, but struggle with 200+ tasks.

For larger pipelines, we need robust traversal features, like collapsing/expanding nodes to focus on specific sections during development or debugging. I've used `networkx` and `mermaid` for subgraph visualization, but an interactive UI would be more efficient.

I've developed a prototype and am seeking example cases to test it. I'm looking for pipelines with 60+ tasks and complex dependencies. I'm particularly interested in the challenges you face with these large pipelines. At my workplace, we have a 1500+ task pipeline, and I'm curious if this is a typical scale.

Specifically, I'd like to know:

* What challenges do you face when visualizing and managing large pipelines?
* Are pipelines with 1500+ tasks common?
* What features would you find most useful in a tool for this purpose?

If you can share sanitized examples or describe the complexity of your pipelines, it would be very helpful.

Thanks.",2,2,EliyahuRed,2025-03-30 20:29:36,https://www.reddit.com/r/dataengineering/comments/1jnlsu5/example_for_complex_data_pipeline/,0,False,False,False,False
37,1jnlm9u,"As a data analytics/data science professional, how much data engineering am I supposed to know? Any advice is greatly appreciated","I am so confused. I am looking for roles in BI/analytics/data science and it seems data engineering has just taken over the entire thing or most of it, atleast. BI and DBA is just gone and everyone now wants cloud dev ops and data engineering stack as part of a BI/analytics role? Am I now supposed to become a software engineer and learn all this stack (airflow, airtable, dbt, hadoop, pyspark, cloud, devops etc?) - this seems so overwhelming to me! How am I supposed to know all this in addition to data science, strategy, stakeholder management, program management, team leadership....so damn exhausting! Any advice on how to navigate the job market and land BI/data analytics/data science roles and how much realistic data engineering am I supposed to learn?",0,6,CreditArtistic1932,2025-03-30 20:21:44,https://www.reddit.com/r/dataengineering/comments/1jnlm9u/as_a_data_analyticsdata_science_professional_how/,0,False,False,False,False
38,1jndfr9,Building a Cloud-Based Data Pipeline for Personal Use/Learning/Projects,"I am a data/functional analyst looking to get a better grasp on 'end-to-end data management and processing' (in quotations as that's the term my direct lead used when we aligned regarding my career objectives for the year, which definitely matches with my current interests).

Given this objective, my own interest in gaining a better grasp on general principles and functions of Data Engineering (and later on, Data Science), as well as a personal project I'm looking to finish for myself/my own portfolio by the end of the year, I'm looking for a cloud-based data pipeline solution for me to spend my role's training stipend on -- though I'm willing (and allowed) to use some of my personal funds on this as well since it's both for career progression and personal fun.

I started looking for small-scale subscriptions for some of the platforms we've previously used at work (AWS, Databricks), but the consensus I've gotten is that cost-wise, I'm potentially better off with multiple smaller platforms that I can just weave together, instead of going for platforms normally designed for enterprise use. And so I come here for help!

*(Though if this is not the right community for me to ask this, please let me know where instead)*

# Budget and Scope

* Base budget is a $200 stipend from our department, which refreshes at the end of the year
* I'm willing to add another $200-300/year, if necessary
* I'm from South-East Asia (converting our local currency, in case regional pricing matters)
* I prefer platforms/solutions that come with upfront costs instead of per-pull/use costs, so that's it's easier to manage the finances involved
* Languages I'm comfortable with are Python and SQL, since these are what I mainly use for work, but willing to learn basics of new languages where necessary 
* Since I will be using my personal computer, which is not that powerful currently, I am looking for a pipeline/platform that I can use completely via cloud (including running training models) 

# Learning Objectives 

*(aka things I want my platform/s to be able to do)*

**1. Data Gathering/Ingestion/Validation/Transformation/Storage**

* Obviously not too familiar with this step currently, so I ended up lumping a bunch of them together, but essentially ETL
* In terms of data I intend to use, I have three main sources I intend to work my way through, from simple to complex: **(1)** will be starting with ingesting CSV files I get from third-party free data providers like Kaggle, **(2)** then moving on to structured databases from sports stat sites such as BBREF, before  eventually work my way to **(3)** automating data collection from publicly available video games I normally play (Dota 2, Path of Exile -- if it matters)
* I want to be able to load these into SQL tables that I can regularly query form

**2. Data Querying and Extract Cloud Storage**

* I want to then be able to set it up so that I can easily query and then store-via-cloud what I need.
* To make my intention clear, most of my work experience is with AWS Athena and S3, so I hope to get platforms that function similarly.

**3. Cloud-Based Data Processing, Machine Learning, and Visualization**

* I currently do majority of my post-query data processing and modelling locally on my work laptop on Jupyter (via the Anaconda distribution), but one of my key objectives is learning to do all of these on cloud (especially since my peronal computer I'll be using would obviously not be as powerful as what I use for work)
* I definitely prefer Notebook-like environments, so perhaps something like AWS EMR
* My main experience at this level is mainly with Python (using specific packages such as Pandas, Numpy, Matplotlib, Numpy, Sklearn, etc.), but I'm looking to do more PySpark as well

**4. EXTRA/OPTIONAL: Dashboard Creation and Hosting**

* If I can get a platform/pipeline that will allow me to host interactive dashboards for me to just embed in my portfolio, that would be a plus, but I am very easily willing to drop this should it not fit my budget

#  Final Remarks

* Want to learn ETL, and cloud-based data processing on a personal data pipeline and processing platform/solution that also has SQL capabilities -- in line with my career and personal learning objectives for the year 
* Tried looking into personal subscriptions for 360 solutions like Databricks and AWS Sagemaker Unified Studio, but was told that for what I want I might be better off with patching together T2 solutions or something along those lines -- but I imagine this would be much more tricky to set up ",1,1,bacon9001,2025-03-30 14:20:16,https://www.reddit.com/r/dataengineering/comments/1jndfr9/building_a_cloudbased_data_pipeline_for_personal/,0,False,False,False,False
39,1jnrdm2,dezoomcamp project,"Ever wondered why some Airbnb listings are way more expensive? My project explores price trends, demand drivers, and traveler-friendly insights based on NYC Airbnb data! ðŸ ðŸ“ˆ

\#dezoomcamp #dataanalysis #airbnb",0,2,Impressive_Trip1382,2025-03-31 00:54:57,https://www.reddit.com/r/dataengineering/comments/1jnrdm2/dezoomcamp_project/,0,False,False,False,False
40,1jnrd0h,DEZOOMCAMP Project,"Lessons learned from loading Airbnb data into GCP:  
âœ”ï¸ Use **autodetect schema** in BigQuery for flexibility  
âœ”ï¸ Handle CSV quirks with proper configs  
âœ”ï¸ Optimize storage with partitioning  
\#dezoomcamp #gcp #dataengineering",0,0,Impressive_Trip1382,2025-03-31 00:54:01,https://www.reddit.com/r/dataengineering/comments/1jnrd0h/dezoomcamp_project/,0,False,False,False,False
41,1jnrb6y,Data Camp Data engineering certification help,Hi Iâ€™ve been working through the data engineer in SQL track on DataCamp and decided to try the associate certification exam. There was quite a bit that didnâ€™t seem to have been covered in the courses. Can anyone recommend any other resources to help me plug the gap please? Thanks,0,1,Bodhisattva-Wannabe,2025-03-31 00:51:17,https://www.reddit.com/r/dataengineering/comments/1jnrb6y/data_camp_data_engineering_certification_help/,0,False,False,False,False
42,1jnq405,Unstructured to Structured,"Hi folks,
I know there have been some discussions on this topic; but given we had lot of development in technology and business space; would like to get your input on
1. How much is this still a problem?
2. Do agentic workflows open up some new challenges?
3. Is there any need to convert large excel files into  SQL tables?

",0,1,Fantastic-Cup-990,2025-03-30 23:48:51,https://www.reddit.com/r/dataengineering/comments/1jnq405/unstructured_to_structured/,0,False,False,False,False
43,1jnj9bw,Collect old news articles from mainstream media.,"What is the best way to collect like >10 years old news articles from the mainstream media and newspapers?
 ",0,1,SaintPellegrino4You,2025-03-30 18:39:53,https://www.reddit.com/r/dataengineering/comments/1jnj9bw/collect_old_news_articles_from_mainstream_media/,0,False,False,False,False
44,1jna701,"I am learning data engineering from a course. I am a fresher with no job experience, a commerce background, and a two-year gap.",Will any company hire me? What certificate could I obtain that would help me?,0,9,_winter_rabbit_,2025-03-30 11:12:06,https://www.reddit.com/r/dataengineering/comments/1jna701/i_am_learning_data_engineering_from_a_course_i_am/,0,False,False,False,False
45,1jnmrm5,Data Stack,"What do you think about the progress into [agentic data stack](https://goagentdata.com/)?  
",0,1,jb_nb,2025-03-30 21:11:28,https://www.reddit.com/r/dataengineering/comments/1jnmrm5/data_stack/,0,False,False,False,False
46,1jnjmkl,Why is table extraction still not solved by modern multimodal models?,"There is a lot of hype around multimodal models, such as Qwen 2.5 VL or Omni, GOT, SmolDocling, etc. I would like to know if others made a similar experience in practice: While they can do impressive things, they still struggle with table extraction, in cases which are straight-forward for humans.

Attached is a simple example, all I need is a reconstruction of the table as a flat CSV, preserving empty all empty cells correctly. Which open source model is able to do that?

https://preview.redd.it/xg8f0624jvre1.png?width=1650&format=png&auto=webp&s=4c0a22d833cb308534abf4dc38b1b12581a6e227

",0,0,Electronic-Letter592,2025-03-30 18:55:50,https://www.reddit.com/r/dataengineering/comments/1jnjmkl/why_is_table_extraction_still_not_solved_by/,0,False,False,False,False
47,1jn7kvr,Junior vs Senior role,"What is the difference between a junior and senior in this role? How much can you really know in data engineering; get the data, clean it, dump it somewhere with a cloud service. 

But what would take someone from a junior role to a senior role? Is it just the number years of experience? 

",0,18,Gloomy-Profession-19,2025-03-30 07:52:21,https://www.reddit.com/r/dataengineering/comments/1jn7kvr/junior_vs_senior_role/,0,False,False,False,False
48,1jo8l7i,Happy Monday,,368,24,Vautlo,2025-03-31 17:19:45,https://i.redd.it/l7ql2ezx62se1.png,0,False,False,False,False
49,1jnrpd7,Does your company use both Databricks & Snowflake? How does the architecture look like?,I'm just curious about this because these 2 companies have been very popular over the last few years.,78,38,NefariousnessSea5101,2025-03-31 01:12:17,https://www.reddit.com/r/dataengineering/comments/1jnrpd7/does_your_company_use_both_databricks_snowflake/,0,False,False,False,False
50,1jnuhdm,"Now, I know why am I struggling...","And why my coleagues were able to present outputs more eagerly than I do:

I am trying to deliver a 'perfect data set', which is too much to expect from a fully on-prem DW/DS filled with couple of thousands of tables with zero data documentation and governance in all 30 years of operation...

I am not even a perfectionist myself so IDK what lead me to this point. Probably I trusted myself way too much? Probably I am trying to prove I am ""one of the best data engineers they had""? (I am still on probation and this is my 4th month here)

The company is fine and has continued to prosper over the decades without much data engineering. They just looked at the big numbers and made decisions based of it intuitively. 

Then here I am, just spent hours today looking for the excess 0.4$ from a total revenue of 40Million$ from a report I broke down to a FactTable. Mathematically, this is just peanuts. I should have let it go and used my time effectively on other things.

I am letting go of this perfectionism. 

I want to get regularized in this company. I really, really want to.",42,11,noSugar-lessSalt,2025-03-31 03:47:38,https://www.reddit.com/r/dataengineering/comments/1jnuhdm/now_i_know_why_am_i_struggling/,0,False,False,False,False
51,1jo0001,Prefect - too expensive?,"Hey guys,
weâ€™re currently using self-hosted Airflow for our internal ETL and data workflows. It gets the job done, but I never really liked it. Feels too far away from actual Python, gets overly complex at times, and local development and testing is honestly a nightmare.

I recently stumbled upon Prefect and gave the self-hosted version a try. Really liked what I saw. Super Pythonic, easy to set up locally, modern UI - just felt right from the start.

But the problem is: the open-source version doesnâ€™t offer user management or logging, so weâ€™d need the Cloud version. Pricing would be around 30k USD per year, which is way above what we pay for Airflow. Even with a discount, it would still be too much for us.

Is there any way to make the community version work for a small team? Usermanagement and Audit-Logs is definitely a must for us. Or is Prefect just not realistic without going Cloud?

Would be a shame, because I really liked their approach.

If not Prefect, any tips on making Airflow easier for local dev and testing?",41,47,thsde,2025-03-31 10:26:08,https://www.reddit.com/r/dataengineering/comments/1jo0001/prefect_too_expensive/,0,False,False,False,False
52,1jo7tps,what's your opinion?,"iâ€™m designing functions to clean data for two separate pipelines: one has small string inputs, the other has medium-size pandas inputs. both pipelines require the same manipulations.

for example, which is a better design: clean\_v0 or clean\_v1?

that is, should i standardize object types inside or outside the cleaning function?

thanks all! this community has been a life saver :)

",26,36,BigCountry1227,2025-03-31 16:48:55,https://i.redd.it/woy8nd4412se1.png,0,False,False,False,False
53,1jo564b,Is using Snowflake for near real time or hourly events an overkill ?,"I've been using Snowflake for a while for just data warehousing projects (analytics) where I update the data twice per day.

  
I have now a Use Case where I need to do some reads and writes to sql tables every hour (every 10 min would be even better but not necessary). **The purpose is not only analytics but also operational.**

  
I estimate every request **costs me 0.01$,** which is quite high.

I was thinking of using **Postgresql** instead of Snowflake but I will need to invest time and resources to build it and maintain it. 

  
I was wondering if you can give me **your opinion** about building **near real time or hourly projects** in Snowflake. Does it make sense ? or is it a clear no-go ?

  
Thanks!

  
",14,12,finerius,2025-03-31 14:57:29,https://www.reddit.com/r/dataengineering/comments/1jo564b/is_using_snowflake_for_near_real_time_or_hourly/,0,False,False,False,False
54,1jnzyop,AWS Data Engineering from Azure,"Hi Folks,

  
14+ years into data engineering with Onprem for 10 and 4 years into Azure DE with mainly expertise on python and Azure databricks.

Now trying to shift job but 4 out of 5 jobs i see are asking for  AWS (i am targeting only product companies or  GCC) . Is self learning AWS for DE possible.

Has anyone shifted from Azure stack DE to AWS ?

What services to focus .

any paid courses that you have taken like udemy etc

Thanks",11,9,nifty60,2025-03-31 10:23:27,https://www.reddit.com/r/dataengineering/comments/1jnzyop/aws_data_engineering_from_azure/,0,False,False,False,False
55,1jo8u27,Asking for different tools for SQL Server + SSIS project.,"Hello guys. I work in a consultancy company and we recently got a job to set-up SQL Server as DWH and SSIS. Whole system is going to be build up from the scratch. The whole operation of the company was running on Excel spreadsheets with 20+ Excel Slave that copies and pastes some data from a source, CSV or email then presses the fancy refresh button. Company newly bought and they want to get rid of this stupid shit so SQL Server and SSIS combo is a huge improvement for them (lol).

But I want to integrate as much as fancy stuff in this project. Both of these tool will work on a Remote Desktop with no internet connection.  I want to integrate some DevOps tools into this project. I will be one of the 3 data engineers that is going to work on this project. So Git will be definitely on my list, as well as GitTea or a repo that works offline since there won't be a lot of people. But do you have any more free tools that I can use? Planning to integrate Jenkins in offline mode somehow, tsqlt for unit testing seems like a decent choice as well. dbt-core and airflow was on my list as well but my colleagues don't know any python so they are not gonna be on this list.

Do you have any other suggestions? Have you ever used a set-up like mine? I would love to hear your previous experiences as well. Thanks",7,9,Brilliant_Breath9703,2025-03-31 17:29:30,https://www.reddit.com/r/dataengineering/comments/1jo8u27/asking_for_different_tools_for_sql_server_ssis/,1,False,False,False,False
56,1jo2vxq,Date warehouse essentials guide,Check out my latest blog on data warehouses! Discover powerful insights and strategies that can transform your data management. Read it here: https://medium.com/@adityasharmah27/data-warehouse-essentials-guide-706d81eada07!,5,1,Super_Act_5816,2025-03-31 13:13:37,https://www.reddit.com/r/dataengineering/comments/1jo2vxq/date_warehouse_essentials_guide/,0,False,False,False,False
57,1jo9b0k,Operating systems and hardware available for employees in your company,"Hey guys,

I'm working as a DE in a German IT company that has about 500 employees. The company's policy regarding operating systems the employees are allowed to use is strange and unfair (IMO). All software engineers get access to Macbooks and thus, to MacOS while all other employees that have a differnt job title ""only"" get HP elite books (that are not elite at all) that run on Windows. WSL is allowed but a native Linux is not accepted because of security reasons (I don't know which security reasons).

As far as I know the company does not want other job positions to get Macbooks because the whole updating stuff for those Macbooks  is done by an external company which is quite expensive. The Windows laptops instead are maintained by an internal team.

A lot of people are very unhappy with this situation because many of them (including me) would prefer to use Linux or MacOS. Especially all DevOps are pissed because half a year ago they also got access to MacBooks but a change in the policy means that they will have to change back to Windows laptops once their MacBooks break or become too old.

My question(s): Can you choose the OS and/or hardware in your company? Do you have a clue why Linux may not be accepted? Is it really not that safe (which I cannot think of because the company has it's own data center where a lot of Linux servers run that are actually updated by an internal team)?",2,11,VeryHardToFindAName,2025-03-31 17:48:33,https://www.reddit.com/r/dataengineering/comments/1jo9b0k/operating_systems_and_hardware_available_for/,0,False,False,False,False
58,1jo4hld,How do I manage dev/test/prod when using Unity Catalog for Medallion Architecture with dbt?,"Hi everyone,

I'm in the process of setting up a dbt project on Databricks and planning to leverage Unity Catalog to implement a medallion architecture. I am not sure the correct approach.  I am considering a dev/test/prod catalog, with a bronze/silver/gold schema:

* dev.bronze
* test.bronze
* prod.bronze

However, this takes 2 of the namespaces so all of the other information has to live in a single namespace such as table type (dim/fact), department (hr/finance), and data source and table description.  It seems like a lot to cram in there.

I have used the medallion architecture as a guide, but never used it in the naming, but the current team I am on really wants it to be in the name.  Just wondering what approaches people have taken.

Thanks",5,2,jfftilton,2025-03-31 14:27:44,https://www.reddit.com/r/dataengineering/comments/1jo4hld/how_do_i_manage_devtestprod_when_using_unity/,0,False,False,False,False
59,1joeg88,My company adopted a stateful REST API solution I built that's run locally on every machine. I would like to deploy it to the cloud but I am not sure that's smart. Thoughts?,"**Context:** I joined a finance consultancy a few years ago and noticed that most people in my department are frustrated with the current ""software"" our engineering team has built over decades (yes - not years, decades). The issue is that the software consists of a bundle of Python scripts that repeatedly read large CSV files whenever a user interacts with it. The master CSV file ranges in size from 20MB to 1GB.

For example, if a user wants to select an option from a dropdown menu, clicking the dropdown triggers the reading and aggregation of a 1GB file, after which the frontend is ""returned"" 20 strings (the dropdown options). By ""returned,"" I mean that a new CSV file is created somewhere on the user's local file system, and the ""frontend"" picks it up. I tested a similar functionality using a Flask REST API, and once the CSV file is loaded into virtual memory, the process takes only 100msâ€”compared to the current design, which takes a full minute (some of it is due to scripts needing to sort out dependencies, validation, etc.). However, our engineering team refuses to adopt web-based communication, arguing that it's not worth the effort. The idea of using a cloud-based relational database is essentially taboo; it has to be either CSV files or Pythonâ€™s pickle dumps on each user's local system.

I have some experience in software engineering, so I made it my mission to redesign this legacy monsterâ€”with the blessing of a senior manager. So far, the transition has gone incredibly well. Last year, I did a soft launch of a small subset of features, and within days, every person in my department was using it.

**Question:** My current design requires users to set up a virtual environment and run an installation script that sorts out any environment variables, dependencies, etc. Each time they want to start the software, they must run a local Flask API, which interacts with a React TypeScript frontend. When the Flask API starts, it loads all necessary files into memory, does validation and other things (takes around a minute). After that, every subsequent request is easy and takes on average 100 to 200 ms. However, I dislike that each user needs a fully configured environment. Version control is also a headache since every user must manually run an update script.

Iâ€™d like to move my Flask API to the cloud so that either:

1. **A single server serves all colleagues**, or
2. **Each colleague gets a dedicated node/pod.**

The problem with a single server is that it would quickly run out of virtual memory if 100+ colleagues loaded large datasets simultaneously. The problem with one node per colleague is the complexityâ€”it would require Kubernetes (K8s) or AWS Fargate, along with an orchestrator to manage node creation and termination, which is a significant engineering effort.

I then considered making my Flask API stateless: storing large datasets in S3, using DynamoDB for file mapping, and loading data into virtual memory on every request. I converted some sample datasets to Parquet, reducing their size significantly (down to \~10MB), but I worry about added latency. Repeatedly reading the same data (given that each user makes 1â€“10 requests per minute) seems highly inefficient.

Am I missing any alternatives? Based on this, a local Flask API still seems like the best optionâ€”unless I want to pay for an expensive 64GB vRAM EC2 instance or invest significant time in building a node-per-user architecture.

Thanks!",2,6,Double_Cost4865,2025-03-31 21:17:37,https://www.reddit.com/r/dataengineering/comments/1joeg88/my_company_adopted_a_stateful_rest_api_solution_i/,1,False,False,False,False
60,1jnymqg,Need Feedback on data sharing module,"
Subject: Seeking Feedback: CrossLink - Faster Data Sharing Between Python/R/C++/Julia via Arrow & Shared Memory

Hey r/dataengineering


I've been working on a project called CrossLink aimed at tackling a common bottleneck: efficiently sharing large datasets (think multi-million row Arrow tables / Pandas DataFrames / R data.frames) between processes written in different languages (Python, R, C++, Julia) when they're running on the same machine/node.
Mainly given workflows where teams have different language expertise.

The Problem:
We often end up saving data to intermediate files (CSVs are slow, Parquet is better but still involves disk I/O and serialization/deserialization overhead) just to pass data from, say, a Python preprocessing script to an R analysis script, or a C++ simulation output to Python for plotting. This can dominate runtime for data-heavy pipelines.

CrossLink's Approach:
The idea is to create a high-performance IPC (Inter-Process Communication) layer specifically for this, leveraging:
Apache Arrow: As the common, efficient in-memory columnar format.
Shared Memory / Memory-Mapped Files: Using Arrow IPC format over these mechanisms for potential minimal-copy data transfer between processes on the same host.


DuckDB: To manage persistent metadata about the shared datasets (unique IDs, names, schemas, source language, location - shmem key or mmap path) and allow optional SQL queries across them.


Essentially, it tries to create a shared data pool where different language processes can push and pull Arrow tables with minimal overhead.


Performance:
Early benchmarks on a 100M row Python -> R pipeline are encouraging, showing CrossLink is:
Roughly 16x faster than passing data via CSV files.
Roughly 2x faster than passing data via disk-based Arrow/Parquet files.

It also now includes a streaming API with backpressure and disk-spilling capabilities for handling >RAM datasets.


Architecture:
It's built around a C++ core library (libcrosslink) handling the Arrow serialization, IPC (shmem/mmap via helper classes), and DuckDB metadata interactions. Language bindings (currently Python & R functional, Julia building) expose this functionality idiomatically.


Seeking Feedback:
I'd love to get your thoughts, especially on:
Architecture: Does using Arrow + DuckDB + (Shared Mem / MMap) seem like a reasonable approach for this problem? 

Any obvious pitfalls or complexities I might be underestimating (beyond the usual fun of shared memory management and cross-platform IPC)?


Usefulness: Is this data transfer bottleneck a significant pain point you actually encounter in your work? Would a library like CrossLink potentially fit into your workflows (e.g., local data science pipelines, multi-language services running on a single server, HPC node-local tasks)?


Alternatives: What are you currently using to handle this? (Just sticking with Parquet on shared disk? Using something like Ray's object store if you're in that ecosystem? Redis? Other IPC methods?)


Appreciate any constructive criticism or insights you might have! Happy to elaborate on any part of the design.

I built this to ease the pain of moving across different scripts and languages for a single file. Wanted to know if it useful for any of you here and would be a sensible open source project to maintain.

It is currently built only for local nodes, but looking to add support with arrow flight across nodes as well. ",2,5,pirana04,2025-03-31 08:43:18,https://www.reddit.com/r/dataengineering/comments/1jnymqg/need_feedback_on_data_sharing_module/,0,False,False,False,False
61,1joiqcm,Anyone try Semaphore?,Iâ€™ve been looking for something to unify our data and found Semaphore. Anyone have this in their company and how are they using it? Like it? Is there an alternative? Want to get some data before I engage the sales vultures ,1,0,drdacl,2025-04-01 00:29:49,https://www.progress.com/semaphore,1,False,False,False,False
62,1jnt6ir,Question about preprocessing two time-series datasets from different measurement devices,"I have a question regarding the preprocessing step in a project I'm working on. I have two different measurement devices that both collect time-series data. My goal is to analyze the similarity between these two signals.

Although both devices measure the same phenomenon and I've converted the units to be consistent, I'm unsure whether this is sufficient for meaningful comparison, given that the devices themselves are different and may have distinct ranges or variances.

From the literature, Iâ€™ve found that z-score normalization is commonly used to address such issues. However, Iâ€™m concerned that applying z-score normalization to each dataset individually might make it impossible to compare across datasets, especially when I want to analyze multiple sessions or subjects later.

Is z-score normalization the right approach in this case? Or would it be better to normalize using a common reference (e.g., using statistics from a larger dataset)? Any guidance or references would be greatly appreciated.Thank you :)",1,1,UsedExcitement5306,2025-03-31 02:33:18,https://www.reddit.com/r/dataengineering/comments/1jnt6ir/question_about_preprocessing_two_timeseries/,0,False,False,False,False
63,1jo2s2c,Cloud Pandit Azure Data Engineering course feedback or can we take !!,Had anyone taken Cloud Pandit Azure Data Engg course. just wanted to know !!,0,3,Pi_Pisces,2025-03-31 13:08:18,https://www.reddit.com/r/dataengineering/comments/1jo2s2c/cloud_pandit_azure_data_engineering_course/,0,False,False,False,False
64,1jo9ir2,~33% faster Microsoft Fabric with e6dataâ€“ Feedback Requested,"Hey folks,

I'm a data engineer at [e6data](https://www.e6data.com/), and we've been working on integrating our engine with Microsoft Fabric. We recently ran some benchmarks (TPC-DS) and observed around a **33% improvement in SQL query performance** while also significantly reducing costs compared to native Fabric compute engines.

Here's what our integration specifically enables:

* **33% faster SQL queries** directly on data stored in OneLake (TPC-DS benchmark results).
* **2-3x cost reduction** by optimizing compute efficiency.
* **Zero data movement**: direct querying of data from OneLake.
* Native vector search support for AI-driven workflows.
* Scalable to 1000+ QPS with sub-second latency and real-time autoscaling.
* Enterprise-level security measures.

We've documented our approach and benchmark results: [https://www.e6data.com/blog/e6data-fabric-increased-performance-optimized-capacity](https://www.e6data.com/blog/e6data-fabric-increased-performance-optimized-capacity)

We'd genuinely appreciate your thoughts, feedback, or questions about our approach or experiences with similar integrations.

https://preview.redd.it/vx65oc5zd2se1.jpg?width=6636&format=pjpg&auto=webp&s=57a2a75cd11a8cc616d04a042a12055bddfe5b4b

",0,0,e6data,2025-03-31 17:57:18,https://www.reddit.com/r/dataengineering/comments/1jo9ir2/33_faster_microsoft_fabric_with_e6data_feedback/,0,False,False,False,False
65,1jo89r8,Ways to quickly get total rows?,"When i am testing things often i need to run some counts  in databricks.

What is the prefered way?

I am creating a pyspark.dataframe  using spark.sql statements and later
DF.count().

Further information can be provided.",0,1,Old_Tourist_3774,2025-03-31 17:06:54,https://www.reddit.com/r/dataengineering/comments/1jo89r8/ways_to_quickly_get_total_rows/,0,False,False,False,False
66,1jo4n51,Seeking Advice from DE: Taking a Career Break to Work & Travel in Australia,"Hey DE,

Iâ€™d love to get your perspective on my situation.

# My Background

Iâ€™m a Brazilian Mechanical Engineer with 3 years of experience in the Data fieldâ€”started as a Data Analyst for 1.5 years, then transitioned into Data Engineering. Next week, Iâ€™ll be starting as a Data Architect at a multinational with 100,000+ employees, mainly working with the Azure stack.

# The Plan

My girlfriend and I are planning to move to Australia for about a year to travel and build memories together before settling down (marriage, house, etc.). This new job came unexpectedly, but it offers a good salary (\~$2,000 USD/month).

The idea is to:

* Move to Australia
* Work hard & save around $1,000 USD/month
* Travel as much as possible for \~2 years
* Return and re-enter the data field

# The Challenge

The work visa limitation allows me to stay only 6 months with the same employer, making it tough to get good Data Engineering jobs. So, I plan to work in any job that pays well (fruit picking, hospitality, etc.), and my girlfriend will do the same.

# The Concern

When I return, how hard will it be to get back into the data field after a \~2-year break?

* Iâ€™ll have enough savings to stay unemployed for about a year if needed.
* This isnâ€™t all my savingsâ€”I have the equivalent of 6 years of salary in reserve.
* I regularly get recruiter messages on LinkedIn.
* I speak Portuguese, English, and Spanish fluently.

Given your experience, how risky is this career break? is totally crazy ? Would you recommend a different approach? Any advice would be appreciated!",0,2,chongsurfer,2025-03-31 14:34:32,https://www.reddit.com/r/dataengineering/comments/1jo4n51/seeking_advice_from_de_taking_a_career_break_to/,0,False,False,False,False
67,1joo4tt,Anyone else feel like data engineering is way more stressful than expected?,"I used to work as a Tableau developer and honestly, life felt simpler. I still had deadlines, but the work was more visual, less complex, and didnâ€™t bleed into my personal time as much.

Now that I'm in data engineering, I feel like Iâ€™m constantly thinking about pipelines, bugs, unexpected data issues, or some tool update I havenâ€™t kept up with. Even on vacation, I catch myself checking Slack or thinking about the next sprint. I turned 30 recently and started wonderingâ€¦ is this normal career pressure, imposter syndrome, or am I chasing too much of management approval?

Is anyone else feeling this way? Is the stress worth it long term?",154,49,Big-Dwarf,2025-04-01 05:25:58,https://www.reddit.com/r/dataengineering/comments/1joo4tt/anyone_else_feel_like_data_engineering_is_way/,0,False,False,False,False
68,1jowmzi,Found the perfect Data Dictionary tool!,"Just launched the [Urban Data Dictionary](https://www.urbandatadictionary.com/) and to celebrate what what we actually do in data engineering. Hope you find it fun and like it too. 

Check it out and add your own definitions. What terms would you contribute?

Happy April Fools!",131,11,secodaHQ,2025-04-01 14:15:10,https://www.reddit.com/r/dataengineering/comments/1jowmzi/found_the_perfect_data_dictionary_tool/,0,False,False,False,False
69,1jowkjn,"What Python libraries, functions, methods, etc. do data engineers frequently use during the extraction and transformation steps of their ETL work?","I am currently learning and applying data engineering into my job. I am a data analyst with three years of experience. I am trying to learn ETL to construct automated data pipelines for my reports.

Using Python programming language, I am trying to extract data from Excel file and API data sources. I am then trying to manipulate that data. In essence, I am basically trying to use a more efficient and powerful form of Microsoft's Power Query.

What are the most common Python libraries, functions, methods, etc. that data engineers frequently use during the extraction and transformation steps of their ETL work?

P.S.

Please let me know if you recommend any books or YouTube channels so that I can further improve my skillset within the ETL portion of data engineering.

Thank you all for your help. I sincerely appreciate all your expertise. I am new to data engineering, so apologies if some of my terminology is wrong.



Edit:

Thank you all for the detailed responses. I highly appreciate all of this information.",89,60,Original_Chipmunk941,2025-04-01 14:12:13,https://www.reddit.com/r/dataengineering/comments/1jowkjn/what_python_libraries_functions_methods_etc_do/,0,False,2025-04-01 15:26:11,False,False
70,1jowboo,"Quack-To-SQL model : stop coding, start quacking",,26,5,TransportationOk2403,2025-04-01 14:01:45,https://motherduck.com/blog/quacktosql,0,False,False,False,False
71,1jp7anp,What is the best free BI dashboarding tool?,We have 5 developers and none of them are data scientists. We need to be able to create interactive dashboards for management.,19,25,Professional_Eye8757,2025-04-01 21:23:16,https://www.reddit.com/r/dataengineering/comments/1jp7anp/what_is_the_best_free_bi_dashboarding_tool/,0,False,False,False,False
72,1jp2zld,"A Modern Benchmark for the
Timeless Power of the Intel Pentium Pro",,14,6,ikeben,2025-04-01 18:30:47,https://www.bodo.ai/bodobench95,0,False,False,False,False
73,1joqbrl,Time-series analysis pipeline architecture,"Hi, I'm a bit outdated when it comes to all new cloud based solutions and request navigation on what architecture might be useful to start with (should be rather simple and not too much overhead to set up) while still be prepared for more data sources and more analysis requirements.

I'm using Azure

My use-case:
I have a time-series dataset coming from an API on which we perform a Python analysis. We would like to perform the Python analysis on a weekly basis, store the data and provide the output as a power bi dashboard. The dataset consists of like 500 000 rows each week, the analysis scripts processes a many to many calculation and I might be interested in adding more data sources as well as perform more KPI calculations pre-processed in data storage (i.e. not in power bi).",10,3,qiicken,2025-04-01 08:03:41,https://www.reddit.com/r/dataengineering/comments/1joqbrl/timeseries_analysis_pipeline_architecture/,1,False,False,False,False
74,1jpf97l,How the Apache Doris Compute-Storage Decoupled Mode Cuts 70% of Storage Costsâ€”in 60 Seconds,,3,0,Any_Opportunity1234,2025-04-02 03:28:07,https://v.redd.it/fuk670n3ccse1,0,False,False,False,False
75,1joz9bo,Cloud platform for dbt,"I recently started learning dbt and was using Snowflake as my database. However, my 30-day trial has ended. Are there any free cloud databases I can use to continue learning dbt and later work on projects that I can showcase on GitHub?

Which cloud database would you recommend? Most options seem quite expensive for a learning setup.

Additionally, do you have any recommendations for dbt projects that would be valuable for hands-on practice and portfolio building?

Looking forward to your suggestions!",5,13,Pro_Panda_Puppy,2025-04-01 16:02:14,https://www.reddit.com/r/dataengineering/comments/1joz9bo/cloud_platform_for_dbt/,0,False,False,False,False
76,1joz80e,Monthly General Discussion - Apr 2025,"This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.

Examples:

* What are you working on this month?
* What was something you accomplished?
* What was something you learned recently?
* What is something frustrating you currently?

As always, sub rules apply. Please be respectful and stay curious.

**Community Links:**

* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)
* [Data Engineering Events](https://dataengineering.wiki/Community/Events)
* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)
* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)",5,0,AutoModerator,2025-04-01 16:00:57,https://www.reddit.com/r/dataengineering/comments/1joz80e/monthly_general_discussion_apr_2025/,0,False,False,False,True
77,1joyxbm,Opinions on Vertex AI,"From a more technical perspective what's your opinion about Vertex AI.  
I am trying to deploy a machine learning pipeline and my data science colleges are real data scientists and I do not trust them to bring everything into production.  
What's your experience with vertex ai?",6,2,NectarineNo7098,2025-04-01 15:49:03,https://www.reddit.com/r/dataengineering/comments/1joyxbm/opinions_on_vertex_ai/,0,False,False,False,False
78,1jp25jk,"DeepSeek 3FS: non-RDMA install, faster ecosystem app dev/testing.",,3,0,HardCore_Dev,2025-04-01 17:58:27,https://blog.open3fs.com/2025/04/01/deepseek-3fs-non-rdma-install-faster-ecosystem-app-dev-testing.html,0,False,False,False,False
79,1jovty4,Making your data valuable with Data Products,https://medium.com/@smayya/decoding-data-products-more-than-just-data-89024281a781?sk=3fc692fcc8c5e356d10f4b3077a17c89,2,0,frazered,2025-04-01 13:40:02,https://www.reddit.com/r/dataengineering/comments/1jovty4/making_your_data_valuable_with_data_products/,0,False,False,False,False
80,1joxxbz,any alternatives to alteryx?,"most of our data is on prem sql server. we also have some data sources in snowflake as well (10-15% of the data). we also connect to some api's as well using the python tool. our reporting db is sql server on prem. currently we are using alteryx, and we are researching what our options are before we have to renew our contract. any suggestions that we can explore or if someone has been through a similar scenario, what did you end up with and why? please let me know if I can add more information to the context.

also,I forgot to mention that not all of my team members are familiar with python. Looking for GUI options.

Edit: thank you all. Iâ€™ll look into the mentioned options.",1,10,r0oki3r0kk,2025-04-01 15:08:23,https://www.reddit.com/r/dataengineering/comments/1joxxbz/any_alternatives_to_alteryx/,0,False,2025-04-02 00:14:33,False,False
81,1joxi3z,Databricks Compute. Thoughts and more.,,2,0,averageflatlanders,2025-04-01 14:51:20,https://dataengineeringcentral.substack.com/p/databricks-compute-thoughts-and-more,0,False,False,False,False
82,1jot1bz,Getting data from SAP HANA to snowflake,"So i have this project that will need to ingest data from SAP HANA into snowflake, it can be considered as any on-premise DB using JBDC, the big issue is, I cannot use any external ETL services as per project requirements. What is the best path to follow?  


I need to fetch the data in bulk for some tables with truncate / copy into, and some tables need to be incremental with little (10 min) delay. The tables do not contain any watermark, modified time or anything...  


There isnt much data, 20M rows tops.

If you guys can give me a hand, i'm new to snowflake and strugling to find any sources on this.",2,6,Ra-mega-bbit,2025-04-01 11:15:13,https://www.reddit.com/r/dataengineering/comments/1jot1bz/getting_data_from_sap_hana_to_snowflake/,0,False,False,False,False
83,1josncc,"Career improves, but projects don't? [discussion]","I started 6 years ago and my career has been on a growing trajectory since.

While this is very nice for me, I canâ€™t say the same about the projects I encounter. What I mean is that I was expecting the engineering soundness of the projects I encounter to grow alongside my seniority in this field.

Instead, Iâ€™ve found that regardless of where I end up (the last two companies were data consulting shops), the projects I am assigned to tend to have questionable engineering decisions (often involving an unnecessary use of Spark to move 7 rows of data).

The latest one involves ETL out of MSSQL and into object storage, using a combination of Azure synapse spark notebooks, drag and drop GUI pipelines, absolutely no tests or CICD whatsoever, and debatable modeling once data lands in the lake.

This whole thing scares me quite a lot due to the lack of guardrails, while testing and deployments are done manually. While I'd love to rewrite everything from scratch, my eng lead said since that part it's complete and there isn't a plan to change it in the future, that it's not a priority at all, and I agree with this.

What's your experience in situations like this? How do you juggle the competing priorities (client wanting new things vs. optimizing old stuff etc...)?
",1,18,wtfzambo,2025-04-01 10:51:01,https://www.reddit.com/r/dataengineering/comments/1josncc/career_improves_but_projects_dont_discussion/,0,False,False,False,False
84,1jopqvd,What is the best approach for a Bronze layer?,"Hello,

We are starting a new Big Data project in my company with Cloudera, Hive, Hadoop HDFS, and a medallion architecture, but I have some questions about ""Bronze"" layer.

Our source is a FTP and in this FTP are allocated the daily/monthly files (.txt, .csv, .xlsx...).  
We bring those files to our HDFS in separated in folders by date (E.G: xxxx/2025/4)

Here start my doubts:  
\- Our bronze layer are those files in the HDFS?  
\- For build our bronze layer, we need to load those files incrementally into a ""bronze table"" partitioned by date

Reading on internet I saw that we have to do the second option, but I saw that option like a rubbish table

Which will be the best approach?

  
For the other layers, I don't have any doubts.",2,11,Elkemao,2025-04-01 07:20:04,https://www.reddit.com/r/dataengineering/comments/1jopqvd/what_is_the_best_approach_for_a_bronze_layer/,0,False,False,False,False
85,1jpdwy1,Knime on Anaconda Nacigator,Is it possible to install Knime on Anaconda Navigator? ,1,0,Puzzleheaded_Serve39,2025-04-02 02:25:22,https://www.reddit.com/r/dataengineering/comments/1jpdwy1/knime_on_anaconda_nacigator/,0,False,False,False,False
86,1jpdpyh,Facebook Marketing API - Anyone have a successful ETL experience?,"We have a python integration set up where we pull data from Google Ads and Facebook Marketing into our data warehouse. We're pulling data about all 3 hierarchy tiers and some daily metrics:

1. Campaigns (id, name, start time, stop time)
2. Ad Groups/Ad Sets (id, name)
3. Ads (id, name, URL)
4. Metrics (clicks, impressions, spend) for the previous day

For the Google Ads API, you basically send a SQL query and the return time is like a tenth of a second.

For Facebook, we see returns times in the minutes, especially on the Ads piece. Was hoping to get an idea of how others might have successfully set up a process to get this data from Facebook in a more timely fashion, and possibly without hitting the rate limiting threshold.

Not the exact code we're using - I can get it off my work system tomorrow - but the gist:

    from facebook_business.adobjects.adaccount import AdAccount
    from facebook_business.adobjects.campaign import Campaign
    from facebook_business.adobjects.ad import AdSet
    from facebook_business.adobjects.ad import Ad
    from facebook_business.adobjects.adcreative import AdCreative
    campaigns = AdAccount('act_123456789').get_campaigns(
        params={},
        fields=[Campaign.Field.id,Campaign.Field.name,Campaign.Field.start_time,Campaign.Field.stop_time]
    )
    adsets= AdAccount('act_123456789').get_ad_sets(
        params={},
        fields=[AdSet.Field.id,AdSet.Field.name]
    )
    ads = AdAccount('act_123456789').get_ads(
        params={},
        fields=[Ad.Field.id,Ad.Field.name,Ad.Field.creative]
    )
    object_urls = AdAccount('act_123456789').get_ad_creatives(
        params={},
        fields=[AdCreative.Field.object_story_spec]
    )
    asset_urls = AdAccount('act_123456789').get_ad_creatives(
        params={},
        fields=[AdCreative.Field.asset_feed_spec]
    )

We then have to do some joining between ads/object\_urls/asset\_urls to match the Ad with the destination URL if the ad is clicked on.

The performance is so slow, that I hope we are doing it wrong. I was never able to get the batch call to work and I'm not sure how to improve things.

Sincerely a data analyst who crosses over into data engineering because our data engineers don't know python.",1,3,farm3rb0b,2025-04-02 02:17:27,https://www.reddit.com/r/dataengineering/comments/1jpdpyh/facebook_marketing_api_anyone_have_a_successful/,0,False,False,False,False
87,1jpdpnz,Resources for learning AbInitio Tool,"I tried to search the entire internet to find AbInito related tutorials/tranings. Hard luck finding anything. I came to know it's a closed source tool and everything is behind a login wall only for partner companies. 

Can anyone share me stuff they found useful?

Thanks in advance.",1,1,IdealBusiness6499,2025-04-02 02:17:07,https://www.reddit.com/r/dataengineering/comments/1jpdpnz/resources_for_learning_abinitio_tool/,0,False,False,False,False
88,1jp0vmd,Not in the field and I need help understanding how data migrations work and how they're done,"I'm an engineer in an unrelated field and want to understand how data migrations work for work (I might be put in charge of it at my job even though we're not data engineers).  Any good sources, preferably a video that would a mock walkthrough of one (maybe using an ETL too)?",1,2,BlackendLight,2025-04-01 17:07:19,https://www.reddit.com/r/dataengineering/comments/1jp0vmd/not_in_the_field_and_i_need_help_understanding/,0,False,False,False,False
89,1jp0ntu,ELI5 - High-Level Diagram of a Data Strategy,"Hello everyone!Â 

I am not a data engineer, but I am trying to help other people within my organization (as well as myself) get a better understanding of what an overall data strategy looks like.Â  So, I figured I would ask the experts.Â  Â Â 

**Do you have a go-to high-level diagram you use that simplifies the complexities of an overall data solution and helps you communicate what that should look like to non-technical people like myself?**Â 

Iâ€™m a very visual learner so seeing something that shows what the journey of data should look like from beginning to end would be extremely helpful.Â  Iâ€™ve searched online but almost everything I see is created by a vendor trying to show why their product is better.Â  Iâ€™d much rather see an unbiased explanation of what the overall process should be and then layer in vendor choices later.

I apologize if the question is phrased incorrectly or too vague.Â  If clarifying questions/answers are needed, please let me know and Iâ€™ll do my best to answer them.Â  Thanks in advance for your help.",1,0,qwopzxnm79,2025-04-01 16:58:49,https://www.reddit.com/r/dataengineering/comments/1jp0ntu/eli5_highlevel_diagram_of_a_data_strategy/,0,False,False,False,False
90,1joy98n,Dimensional modelling -> Datetime column,"Hi All,

Im learning Dimensional modelling. Im working on the NYC taxi dataset ( [here is the data dictionary](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf) ).

Im struggling to model Datetime columns: tpep\_pickup\_datetime, tpep\_dropoff\_datetime.  
Does these columns should be in Dimensions table or in Fact table? 

What I understand from the Kimball datawarehouse toolkit book is to have a DateDim table populated with dates from start\_date to end\_date with details like month, year, quarter, day of week etc. but what about timestamp?

Lets say if I want to see the data for certain time of the day like nights? In this case, do I need to split the columns: tpep\_pickup\_datetime, tpep\_dropoff\_datetime into date, hour, mins in fact table and join to a dim table with the timestamp details like hour, mins etc? ( so two dim tables - date and timestamp )

It would be great someone can help me here?",1,2,Delicious_Attempt_99,2025-04-01 15:21:53,https://www.reddit.com/r/dataengineering/comments/1joy98n/dimensional_modelling_datetime_column/,0,False,False,False,False
91,1jozvao,SQL Templating (without DBT?),"Iâ€™d like to implement jinja templated SQL for a project. But I donâ€™t want or need DBTâ€™s extra bells and whistles. I just need/want to write macros, templated .sql files, then on execution (from python application), render the SQL at runtime.

Whatâ€™s the solution here? Pure jinja? (Whatâ€™re some resources for that?) Are there OSS libraries I can use? Or, do I just use DBT, but only use it from a python driver?",0,3,boss_yaakov,2025-04-01 16:26:46,https://www.reddit.com/r/dataengineering/comments/1jozvao/sql_templating_without_dbt/,0,False,False,False,False
92,1jowcet,Lessons from operating big ClickHouse clusters for several years,"My coworker Javi Santana wrote a lengthy post about what it takes to operate large ClickHouse clusters based on his experience starting Tinybird. If you're managing any kind of OSS CH cluster, you might find this interesting.

[https://www.tinybird.co/blog-posts/what-i-learned-operating-clickhouse](https://www.tinybird.co/blog-posts/what-i-learned-operating-clickhouse)",0,1,itty-bitty-birdy-tb,2025-04-01 14:02:37,https://www.reddit.com/r/dataengineering/comments/1jowcet/lessons_from_operating_big_clickhouse_clusters/,0,False,False,False,False
93,1jossd0,Newbie to DE needs help with the approach to the architecture of a project,"So I was hired as a data analyst a few months ago and I have a background in software development. A few months ago I was moved to a smallish project with the objective of streamlining some administrative tasks that were all calculated ""manually"" with Excel.  By the time, all I had worked with were very basic, low code tools from the Microsoft enviroment: PBI for dashboards, Power Automate, Power Apps for data entry, Sharepoint lists, etc, so that's what I used to set it up. 

The cost for the client is basically nonexistent right now, apart from a couple of PBI licenses. The closest I've done to ETL work has been with power query, if you can even call it that. 

  
Now I'm at a point where it feels like that's not gonna cut it anymore. I'm going to be working with larger volumes of data, with more complex relationships between tables and transformations that need to be done earlier in the process. I could technically keep going with what I have but I want to actually build something durable and move towards actual data engineering, but I don't know where to start with a solution that's cost efficient and well structured. For example, I wanted to move the data from Sharepoint lists to a proper database but then we'd have to pay for multiple premium licenses to be able to connect to them in powerapps. Where do I even start?

I know the very basics of data engineering and I've done a couple of tutorial projects with Snowflake and Databricks as my team seems to want to focus on cloud based solutions. So I'm not starting from absolute scratch, but I feel pretty lost as I'm sure you can tell. I'd appreciate any kind of advice or input as to where to head from here, as I'm on my own right now.",0,4,Wild_Complaint_4688,2025-04-01 11:00:15,https://www.reddit.com/r/dataengineering/comments/1jossd0/newbie_to_de_needs_help_with_the_approach_to_the/,0,False,False,False,False
94,1jorny7,How do you build tests for processing data with variations,"How do you test a data pipeline which parses data having a lot of variation

I'm working on a project to parse pdfs (earnings calls), they have a common general structure, but sometimes I'll get variations in the data (very common, half of docs have some kind of variation). It's a pain to debug when things go wrong, I have to run tests on a lot of files which takes up time.

I want to build good tests, and learn to do this better in the future, then refactor the code (it's garbage right now)",0,7,Sure-Government-8423,2025-04-01 09:44:04,https://www.reddit.com/r/dataengineering/comments/1jorny7/how_do_you_build_tests_for_processing_data_with/,0,False,False,False,False
95,1jp4f74,Built a visual tool on top of Pandas that runs Python transformations row-by-row - What do you guys think?,"Hey data engineers,

For client implementations I thought it was a pain to write python scripts over and over, so I built a tool on top of Pandas to solve my own frustration and as a personal hobby. The goal was to make it so I didn't have to start from the ground up and rewrite and keep track of each script for each data source I had.

**What I Built:**  
A visual transformation tool with some features I thought might interest this community:

1. **Python execution on a row-by-row basis**Â \- Write Python once per field, save the mapping, and process. It applies each field's mapping logic to each row and returns the result without loops
2. **Visual logic builder**Â that generates Python from the drag and drop interface. It can re-parse the python so you can go back and edit form the UI again
3. **AI Co-Pilot**Â that can write Python logic based on your requirements
4. **No environment setup**Â \- just upload your data and start transforming
5. **Handles nested JSON**Â with a simple dot notation for complex structures

Here's a screenshot of the logic builder in action:

https://preview.redd.it/znh4fom8y9se1.png?width=2690&format=png&auto=webp&s=2daf229aab2f5de272c4f5668a782d8011ff3207

I'd love some feedback from people who deal with data transformations regularly. If anyone wants to give it a try feel free to shoot me a message or comment, and I can give you lifetime access if the app is of use. Not trying to sell here, just looking for some feedback and thoughts since I just built it.

**Technical Details:**

* Supports CSV, Excel, and JSON inputs/outputs, concatenating files, header & delimiter selection
* Transformations are saved as editable mapping files
* Handles large datasets by processing chunks in parallel
* Built on Pandas. Supports Pandas and re libraries

[DataFlowMapper.com](http://DataFlowMapper.com)",0,7,skrufters,2025-04-01 19:28:00,https://www.reddit.com/r/dataengineering/comments/1jp4f74/built_a_visual_tool_on_top_of_pandas_that_runs/,0,False,2025-04-01 19:56:02,False,False
96,1jpaq8v,Data Developer vs Data Engineer,"I know it varies by company blah blah blah, but also aside from a Google search, what have you guys in the field noticed to be core differences between these positions? ",0,3,diabeticspecimen,2025-04-01 23:53:39,https://www.reddit.com/r/dataengineering/comments/1jpaq8v/data_developer_vs_data_engineer/,0,False,False,False,False
97,1jp0s33,"Introducing the Knowledge Graph: things, not strings","""Fully Managed Graph Database Service | Neo4j AuraDB"" https://neo4j.com/product/auradb/?utm_source=GSearch&utm_medium=PaidSearch&utm_campaign=Evergreen&utm_content=AMS-Search-SEMCE-DSA-None-SEM-SEM-NonABM&utm_term=&utm_adgroup=DSA&gad_source=1&gclid=Cj0KCQjwna6_BhCbARIsALId2Z27LAb-nD-42tRRF5viybJfBVull8EeBvj46w_V7OCs1RdtbR7hqBQaAuObEALw_wcB",0,0,Ok_Efficiency1311,2025-04-01 17:03:16,https://blog.google/products/search/introducing-knowledge-graph-things-not/,0,False,False,False,False
98,1jp0ci2,We cut Databricks costs without sacrificing performanceâ€”hereâ€™s how,"About 6 months ago, I led a Databricks cost optimization project where we cut down costs, improved workload speed, and made life easier for engineers. I finally had time to write it all up a few days agoâ€”cluster family selection, autoscaling, serverless, EBS tweaks, and more. I also included a real example with numbers. If youâ€™re using Databricks, this might help: [https://medium.com/datadarvish/databricks-cost-optimization-practical-tips-for-performance-and-savings-7665be665f52](https://medium.com/datadarvish/databricks-cost-optimization-practical-tips-for-performance-and-savings-7665be665f52)",0,3,DataDarvesh,2025-04-01 16:45:48,https://www.reddit.com/r/dataengineering/comments/1jp0ci2/we_cut_databricks_costs_without_sacrificing/,0,False,False,False,False
99,1jphc3z,"The Struggles of Mean, Median, and Mode",,284,15,ganildata,2025-04-02 05:34:58,https://i.redd.it/dahpd85zycse1.jpeg,0,False,False,False,False
100,1jpw0uh,This is what you see all the time if you're a Data EngineerðŸ« ,,227,64,Anass-YI,2025-04-02 18:31:04,https://v.redd.it/f5lu46hgtgse1,0,False,False,False,False
101,1jpknlr,Is Databricks Becoming a Requirement for Data Engineers?,"Hey everyone,

Iâ€™m a Data Engineer with 5 years of experience, mostly working with traditional data pipelines, cloud data warehouses(AWS and Azure) and tools like Airflow, Kafka, and Spark. However, Iâ€™ve never used Databricks in a professional setting.

Lately, I see Databricks appearing more and more in job postings, and it seems like it's becoming a key player in the data world. For those of you working with Databricks, do you think it's a necessity for Data Engineers now? I see that it is mandatory requirement in job offerings but I don't have opportunity to get first experience in it.

What is your opinion, what should I do?",84,41,BigDataMax,2025-04-02 09:41:41,https://www.reddit.com/r/dataengineering/comments/1jpknlr/is_databricks_becoming_a_requirement_for_data/,0,False,False,False,False
102,1jpkgey,Does anyone feel the DE tools are chaging too fast to track,"TL;DR: a guy feeling stuck in the job and cannot figure out what skills are needed to move to a better position 

I am data engineer at a big 4 firm (may be just a etl developer) in india.

I work with Informatica Power Center, Oracle, Unix on the daily basis. Now, when I tried to switch companies for career boost, I realised nobody uses these tech anymore. 

Everyone uses pyspark for etl.
I though fair enough and started leaning pyspark dataframe api. I am so good with sql, pl/sql and python, so it was easy for me.

Then I came to know learning pyspark is not enough, you need to know databricks, snowflake, dbt kind of tools.

Even before making my mind to decide what to learn, things changed and now airflow/dagster, redshift, delta lake, duckdb. I don't what else is in trend now.

Honestly, It feels a lot, like the world is moving in the fastest pace possible and I cannot even decide what to do.

Every job has different tools, and to do the ""fake it till you make it"", I am afraid they would ask any niche question about the tool to which you can only answer if you have the experience.

My profile  is not even getting picked and I feel stuck in the job I am doing.

I am great at what I do, that is one reason the project is not letting me leave even after all the senior folks has left for better projects. The guy with 3 years of experience is the senior most developer and lead now.

But honestly, I dont think I can make it anymore.

If I was just stuck with something like SAP ABAP, frontend or core python, things might have been good. Recruiters will at least look at your profile even though you are not a perfect match as you can learn the rest to do the job. (I might be wrong in this thought)

But for DE roles, the job descriptions are becoming too specific to a tool and people are expecting complete data architect level of skills at 3 years.

I was so ambitious to get a job in a different country with big 4 experience, but now I can't even get a job in india.",42,32,venkatcg,2025-04-02 09:26:11,https://www.reddit.com/r/dataengineering/comments/1jpkgey/does_anyone_feel_the_de_tools_are_chaging_too/,0,False,2025-04-02 09:30:54,False,False
103,1jponp5,Am i doomed moving forward,"I am scared my job is a lightning strike that doesnt exist elsewhere. Im classified as a â€œdata engineerâ€ but only work in snowflake building datasets for tableau. Basically im a middle man between IT who ingests the data and then analysts who visualize in tableau. I live in fear (lol) that if i were to lose this job i would qualify for nothing else because i havent touched python or any ingesting tools or tableau and any visualizing tools in years. 
Am as as out of the norm as i feel?",18,10,GoRGoNiTe_SCuMM,2025-04-02 13:33:31,https://www.reddit.com/r/dataengineering/comments/1jponp5/am_i_doomed_moving_forward/,0,False,False,False,False
104,1jpkf42,"Lucked into a junior data engineer role, where do I go from here?","
About a month ago I was hired at a very small startup (3 employees including me) to be their ""data engineer and analyst"", replacing the previous data engineer who moved on to a grad scheme.

I recently graduated in a non-CS discipline, so my Python and SQL skills aren't exactly amazing but I'm a fast learner. It helps that the other employees are non-technical and the previous data engineer was extremely helpful while training me.

The job has been going well so far. I can see myself getting my skills up to a good standard, and it's a great role to learn the ropes BUT I can't see myself in this role for longer than a year or two. So what should I prepare for next? A more demanding data engineer job? Further education?

I'd like to have a technical job in the financial sector within the next 5-6 years e.g. data engineer for a quant firm.
",13,5,Dismal-Set-6428,2025-04-02 09:23:29,https://www.reddit.com/r/dataengineering/comments/1jpkf42/lucked_into_a_junior_data_engineer_role_where_do/,0,False,False,False,False
105,1jpf97l,How the Apache Doris Compute-Storage Decoupled Mode Cuts 70% of Storage Costsâ€”in 60 Seconds,,10,0,Any_Opportunity1234,2025-04-02 03:28:07,https://v.redd.it/fuk670n3ccse1,0,False,False,False,False
106,1jpyf3s,Skills to Stay Relevant in Data Engineering Over the Next 5-10 Years,"Hey r/dataengineering,

I've been in data engineering for about **3 years now**, and while I love what I do, I can't help but wonder: **whatâ€™s next?** With tech evolving so fast, I'm a bit concerned about what could make our current skills obsolete.

That said, Spark didnâ€™t exactly kill the demand for Hadoop, Impala, etc.â€”so maybe the fear is overblown. But still, I want to make sure I'm **learning the right things** to stay ahead and not be caught off guard by layoffs or major shifts in the industry.

My current stack: **Python, SQL, Spark, AWS (Glue, Redshift, EMR), Airflow.**

What skills/tech would you bet on for the next **5-10 years**? Is it **real-time data processing? DataOps? AI/ML integration?** Would love to hear from those whoâ€™ve been in the game longer!",14,12,Spartanno39,2025-04-02 20:07:35,https://www.reddit.com/r/dataengineering/comments/1jpyf3s/skills_to_stay_relevant_in_data_engineering_over/,0,False,False,False,False
107,1jps1g1,DBT and Snowflake,"Hello all, I am trying to implement dbt and snowflake on a personal project, most of my experience comes from databricks so I would like to know if the best approach for this would be to:
1- a server dedicated to dbt that will connect to snowflake and execute transformations.
2- snowflake of course deployed in azure .
3- azure data factory for raw ingestion and to schedule the transformation pipeline and future dbt dataquality pipelines.

What you guys think about this? ",9,12,pvic234,2025-04-02 15:54:12,https://www.reddit.com/r/dataengineering/comments/1jps1g1/dbt_and_snowflake/,0,False,False,False,False
108,1jpmna4,Iceberg catalog in gcp,"Which is your preferred way to host your data catalog inside of gcp? I know that inside of aws, glue is the preferred way?  
I know that it can make sense to use dataproc Metastore and/or big data lake Metastore.

I know that there are also a lot open source tools that you can use?

what do you prefer? what's your experience?",9,3,NectarineNo7098,2025-04-02 11:51:11,https://www.reddit.com/r/dataengineering/comments/1jpmna4/iceberg_catalog_in_gcp/,1,False,False,False,False
109,1jpl5rc,Latest Thoughtworks TechRadar - data blips,"Thoughtworks have published their latest Technology Radar: https://www.thoughtworks.com/radar

FWIW, here are a few of the 'blips' (as they call them) of note in the data space:

ðŸŸ¢ Adopt: [Data product thinking](https://www.thoughtworks.com/radar/techniques/data-product-thinking)

ðŸŸ¢ Adopt: [Trino](https://www.thoughtworks.com/radar/platforms/trino)

ðŸ‘ Trial: [Databricks Delta Live Tables](https://www.thoughtworks.com/radar/tools/databricks-delta-live-tables)

ðŸ‘ Trial: [Metabase](https://www.thoughtworks.com/radar/tools/metabase)

âœ‹ Hold: [Reverse ETL](https://www.thoughtworks.com/radar/techniques/reverse-etl)

On Reverse ETL they say: 

> we're seeing a growing trend where product vendors use Reverse ETL as an excuse to move increasing amounts of business logic into a centralized platform â€” their product. This approach exacerbates many of the issues caused by centralized data architectures, and we suggest exercising extreme caution when introducing data flows from a sprawling, central data platform to transaction processing systems.",7,2,rmoff,2025-04-02 10:17:58,https://www.reddit.com/r/dataengineering/comments/1jpl5rc/latest_thoughtworks_techradar_data_blips/,0,False,False,False,False
110,1jpxfch,"New to Data Engineering â€” Feeling a Bit Overwhelmed, Looking for Advice","Hey everyone, I could really use some advice from fellow engineers. I'm pretty new to the data world â€” I messed up uni, then did an online analytics course, and after about a year and a half of grinding, I finally landed my first role. Along the way, I found a real passion for Python and SQL.

My first job involved a ton of patchy reporting because of messy infra and data. I started automating painful tasks using basic ETL pipelines I built myself. I showed an interest in APIs and, out of nowhere, 6 months in, I was offered a data engineering role.

Fast forward to now â€” Iâ€™ve been in the new role for a month, and Iâ€™m the companyâ€™s only data engineer. Iâ€™m doing a data engineering apprenticeship at the same time, which helps, but the imposter syndrome is real. The companyâ€™s been limping along with a 25-year-old piece of software that populates our SQL Server DB, and weâ€™re now migrating to something new. Iâ€™ve been asked to learn MuleSoft for ETL and replace some existing pipelines that were built in Python.

I love the subject â€” Iâ€™m genuinely passionate about programming and networking â€” and Iâ€™m keen to take on new tech, improve the infra, and build up strong skills. But Iâ€™m not sure if Iâ€™m going too deep too fast. For example, today I was learning Docker to deploy Python scripts, just to avoid issues with hundreds of brittle batch files that break if we update Python.

My boss seems to think MuleSoft will fully replace Python, but I see it more as a tool that complements certain workflows rather than a full replacement. What worries me more is that I donâ€™t really have any technical peers. Most people in my team only know basic SQL, and itâ€™s hard to communicate strategy or get proper feedback.

My current priorities are getting comfortable with MuleSoft, Git, and Docker. Iâ€™m constantly learning, but sometimes I leave work feeling overwhelmed. Thereâ€™s so much broken or duct-taped together, I donâ€™t even know where to start. I keep telling myself I donâ€™t need to â€œsave the world,â€ but I really want to do a good job and come away with solid experience.

Long term, they want to deploy this new software, rebuild the database, and eventually use AI to help employees query the business. Thereâ€™s a shit ton to do, and Iâ€™m still figuring out basics â€” like setting up a VM just so I can run Docker.

Am I jumping the gun with how Iâ€™m feeling, or is this as wild a situation as it seems? Any advice for a new engineer navigating bad infra, limited support, and a mountain of work would be seriously appreciated.",8,3,ethg674,2025-04-02 19:27:27,https://www.reddit.com/r/dataengineering/comments/1jpxfch/new_to_data_engineering_feeling_a_bit_overwhelmed/,0,False,False,False,False
111,1jph8ei,Creating a Beginner Data Engineering Group,"Hey everyone! Iâ€™m starting a beginner-friendly Data Engineering group to learn, share resources, and stay motivated together.

If youâ€™re just starting out and want support, accountability, and useful learning materials, drop a comment or DM me! Letâ€™s grow together.

Here's the whatsapp link to join:
https://chat.whatsapp.com/GfAh5OQimLE7uKoo1y5JrH",9,13,Important_Age_552,2025-04-02 05:28:15,https://www.reddit.com/r/dataengineering/comments/1jph8ei/creating_a_beginner_data_engineering_group/,0,False,2025-04-02 05:40:31,False,False
112,1jppvzy,"For those who work in data governance but in a data engineering capacity, what are you developing?","Recruiter reached out about a role on a data governance team but the job itself is data engineering. Recruiter was sharing what was in the job post but it didn't clarify much

I'm not formally experienced with data governance but have implemented data quality tests, written documentation, etc. Is that all considered data governance? What would be data engineering responsibilities and day to day work be like on a governance team? 

Would be interested to hear especially if anyone worked in and implemented data governance from scratch, and not used 3rd party software, as this team seems to be trying to do that.",8,5,opabm,2025-04-02 14:25:44,https://www.reddit.com/r/dataengineering/comments/1jppvzy/for_those_who_work_in_data_governance_but_in_a/,0,False,False,False,False
113,1jq1ndb,"Hi, what does a data engineer do on a day-to-day basis in a company?","Right now I work as a data scientist, but I find it very, very repetitive.

That's why I'm studying Data Engineering concepts.  Right now, I'm able to create pipelines to automate ETL loads into Amazon Redshift databases (sort of) using Airflow with Dicker and Kubernetes.

I'm specialized in Python, so I'm also looking at Kafka and Apache PySpark.

Anyway, I'm just starting out in this field, so I feel overwhelmed and not sure what a company expects of me.

Help me understand your role better, thank you!",5,4,2blanck,2025-04-02 22:20:21,https://www.reddit.com/r/dataengineering/comments/1jq1ndb/hi_what_does_a_data_engineer_do_on_a_daytoday/,1,False,False,False,False
114,1jpt9yo,Transition from on-prem to cloud,"Hi everyone,

Iâ€™ve been working in data for almost three years, mainly with on-prem technologies like SQL, SSIS, and Power BI, plus some experience with SSRS, datastage, Microstrategy and pl/SQL.

Lately, Iâ€™ve been looking for new opportunities, but most roles require Spark, Python, Databricks, Snowflake, and cloud experience, which I donâ€™t have. My company wonâ€™t move me to a cloud-related project, but they do pay for some certifications (mainly related to Azure/Microsoft)â€”Iâ€™ve done Azure Data Fundamentals and I'm currently taking a Databricks course and plan to take the certification after.

Whatâ€™s the best way to gain hands-on experience with cloud and these technologies? How did you make the transition?

Would love to hear your advice!",6,6,crassus96,2025-04-02 16:44:10,https://www.reddit.com/r/dataengineering/comments/1jpt9yo/transition_from_onprem_to_cloud/,0,False,False,False,False
115,1jpyoqd,Which is easier? AWS or Azure,Data engineering on azure cloud easier or aws? which one would you say? im currently learning azure :p,4,10,Gloomy-Profession-19,2025-04-02 20:18:15,https://www.reddit.com/r/dataengineering/comments/1jpyoqd/which_is_easier_aws_or_azure/,0,False,False,False,False
116,1jq07vo,Managing 1000's of small file writes from AWS Lambda,"Hi everyone,

I have a microservices architecture where I have a lambda function that takes an ID, sends it to an API for enrichment, and then resultant response is recorded in an S3 Bucket. My issue is that over \~200 concurrent lambdas and in effort to keep memory usage low, I am getting 1000's of small 30 - 200kb compressed ndjson files that make downstream computation a little challenging.

I tried to use Firehose but quickly get throttled and getting ""Slow Down."" error. Is there a tool or architecture decision I should consider besides just a downstream process that might consolidate these files perhaps in Glue?",2,4,Dallaluce,2025-04-02 21:21:25,https://www.reddit.com/r/dataengineering/comments/1jq07vo/managing_1000s_of_small_file_writes_from_aws/,1,False,False,False,False
117,1jpwip4,DBA to Data Engineer,"Hi Everyone,
I have been working as an Oracle DBA for a while now, but I am not enjoying what am I doing. A year ago, I got interested in data engineering and tried to self-learn while juggling a full-time job, GRE prep(planning to go for masters as itâ€™s always been my dream), and everything elseâ€¦ safe to say, it wasnâ€™t easy. Since my job didnâ€™t really involve coding and I ended up with mostly theoretical knowledge. I do know Python, Azure(again theoretical knowledge) and SQL (thanks to work), but I still have a long way to go in data engineering. Now that Iâ€™m finally taking this step, I am thinking to quit my current job and put all my efforts solely on switching from DBA to data engineering. Iâ€™d really appreciate any advice on how to go about this what tech stacks I should focus on and whether transitioning within six months is realistic.",2,1,HistoricalPurchase62,2025-04-02 18:51:20,https://www.reddit.com/r/dataengineering/comments/1jpwip4/dba_to_data_engineer/,0,False,False,False,False
118,1jplvx0,Suggestions for workflow automation,"Hey there :)

  
I hope I find myself in the right subreddit for this as I am trying to **engineer** my computer to push around some **data** ;) 

I'm currently working on a project to fully automate the processing of test results for a scientific study with students. 

The workflow consists of several stages:

1. **Data Extraction:** The test data is extracted from a local SQL database.
2. **SPSS Processing:** The extracted data is then processed using SPSS with a custom-built syntax (legacy). This step generates multiple files from the data. I have been looking into how I can transition this syntax to a python script, so this step might be cut later.
3. **Python Automation:** A Python script takes over the further processing. It reads the files, splits the data per class, inserts it into pre-designed Excel reporting templates.
4. **File Upload:** The files are then automatically uploaded to a self-hosted Nextcloud instance.
5. **Notification:** Once the workflow is complete, a notification  

I have been thinking about different ways to implement this. Right now the inputs and outputs for the different steps are still done manually. 

At work I have been using Jenkins lately and I think it feels natural  to do it in Jenkins and just describe the whole workflow in a pipeline with different stages to run. Besides that I have some experience with AWS Lambda and n8n but I am not sure if they would be helpful with this task.

IÂ´m not that experienced setting up such workflows as my work background is more in Infosec, so please forgive my uneducated guesses about how I best go about this :D Just trying not to take decisions that will be problematic later.



Greetings from Germany",2,2,wowdisme,2025-04-02 11:06:05,https://www.reddit.com/r/dataengineering/comments/1jplvx0/suggestions_for_workflow_automation/,0,False,False,False,False
119,1jpis19,How are you working with your DWH ?,"I would like to understand how you manage your DWW in day-to-day basis, solution, tools, architecture, workflows, ETL, serving...",2,1,Goumari,2025-04-02 07:17:28,https://www.reddit.com/r/dataengineering/comments/1jpis19/how_are_you_working_with_your_dwh/,0,False,False,False,False
120,1jpdpyh,Facebook Marketing API - Anyone have a successful ETL experience?,"We have a python integration set up where we pull data from Google Ads and Facebook Marketing into our data warehouse. We're pulling data about all 3 hierarchy tiers and some daily metrics:

1. Campaigns (id, name, start time, stop time)
2. Ad Groups/Ad Sets (id, name)
3. Ads (id, name, URL)
4. Metrics (clicks, impressions, spend) for the previous day

For the Google Ads API, you basically send a SQL query and the return time is like a tenth of a second.

For Facebook, we see returns times in the minutes, especially on the Ads piece. Was hoping to get an idea of how others might have successfully set up a process to get this data from Facebook in a more timely fashion, and possibly without hitting the rate limiting threshold.

Not the exact code we're using - I can get it off my work system tomorrow - but the gist:

    from facebook_business.adobjects.adaccount import AdAccount
    from facebook_business.adobjects.campaign import Campaign
    from facebook_business.adobjects.ad import AdSet
    from facebook_business.adobjects.ad import Ad
    from facebook_business.adobjects.adcreative import AdCreative
    campaigns = AdAccount('act_123456789').get_campaigns(
        params={},
        fields=[Campaign.Field.id,Campaign.Field.name,Campaign.Field.start_time,Campaign.Field.stop_time]
    )
    adsets= AdAccount('act_123456789').get_ad_sets(
        params={},
        fields=[AdSet.Field.id,AdSet.Field.name]
    )
    ads = AdAccount('act_123456789').get_ads(
        params={},
        fields=[Ad.Field.id,Ad.Field.name,Ad.Field.creative]
    )
    object_urls = AdAccount('act_123456789').get_ad_creatives(
        params={},
        fields=[AdCreative.Field.object_story_spec]
    )
    asset_urls = AdAccount('act_123456789').get_ad_creatives(
        params={},
        fields=[AdCreative.Field.asset_feed_spec]
    )

We then have to do some joining between ads/object\_urls/asset\_urls to match the Ad with the destination URL if the ad is clicked on.

The performance is so slow, that I hope we are doing it wrong. I was never able to get the batch call to work and I'm not sure how to improve things.

Sincerely a data analyst who crosses over into data engineering because our data engineers don't know python.",2,6,farm3rb0b,2025-04-02 02:17:27,https://www.reddit.com/r/dataengineering/comments/1jpdpyh/facebook_marketing_api_anyone_have_a_successful/,0,False,False,False,False
121,1jpdpnz,Resources for learning AbInitio Tool,"I tried to search the entire internet to find AbInito related tutorials/tranings. Hard luck finding anything. I came to know it's a closed source tool and everything is behind a login wall only for partner companies. 

Can anyone share me stuff they found useful?

Thanks in advance.",2,2,IdealBusiness6499,2025-04-02 02:17:07,https://www.reddit.com/r/dataengineering/comments/1jpdpnz/resources_for_learning_abinitio_tool/,0,False,False,False,False
122,1jq2h0a,Feeling stuck. How to move ahead,"I have been working for a consulting firm for the past 5 years. The kind of work they assign me to is fairly basic - developing pipelines using Informatica and writing SQL queries for it. That's been majority of my experience. For the past # months, I've been assigned to a PowerBI developer role, but I just tweak the data/queries to do what the client asks. When I try to apply for data engineering/etl roles, I get asked what I think are pretty advanced questions - for example I got asked about what gaps I have noticed in Microsoft Fabric and what are best practices for data modeling etc. I tend to give general answera based on my research and theoretical answers, but I can never relate it to my actual experience because day to day I don't do anything high level. I get asked about how I optimzied queries or pipelines, the truth is I worked with small enough datasets that I never really had to do anything. Again, I give answers based on my research - like indexing or partitioning but I feel the people asking questions are always looking for more. 

I cannot leave or take a break, I'm on a visa, but how do I actually get further then. Is anyone else feeling the same? ",3,0,AppointmentFit5600,2025-04-02 22:56:33,https://www.reddit.com/r/dataengineering/comments/1jq2h0a/feeling_stuck_how_to_move_ahead/,1,False,False,False,False
123,1jq26eg,Resources to learn developing production-ready APIs?,"Books, articles, courses... what resources have been useful to you for learning how to develop production-ready APIs? Production-ready meaning robust, secure, performant, modular etc

Thanks!",1,1,JLTDE,2025-04-02 22:43:35,https://www.reddit.com/r/dataengineering/comments/1jq26eg/resources_to_learn_developing_productionready_apis/,1,False,False,False,False
124,1jpyr3w,Where next with my DE journey?,"I have completed Microsoft Azure Data Engineering (DP 203) certification which has given me a solid foundation of data engineering on Azure. 

Next, I followed along and did this project by Ansh Lamba: [https://www.youtube.com/watch?v=uc-u\_juRg-w&t=16941s&ab\_channel=AnshLamba](https://www.youtube.com/watch?v=uc-u_juRg-w&t=16941s&ab_channel=AnshLamba) 

  
What should be my next step to enhance my skills? Any recommendation? 4 weeks ago I didn't know anything about data engineering :p",2,0,Gloomy-Profession-19,2025-04-02 20:21:01,https://www.reddit.com/r/dataengineering/comments/1jpyr3w/where_next_with_my_de_journey/,0,False,False,False,False
125,1jpy7vu,Yet another iceberg catalog choice question,"We are an AWS and Databricks shop. We want to explore open source engines for cost savings and reduce vendor lock. 

We want to introduce iceberg. This interoperability with Flink, Snowflake, Trino. 

We are considering Glue,  Snowflake-version-of-Polaris or another catalog.

I appreciate any recommendations and experices from this group.

  
Databricks unity-uniform enables reading the data as a iceberg table but we cannot write a table using Flink. We use Trino and Snowflake for reads.

",1,3,SupermarketMost7089,2025-04-02 19:59:54,https://www.reddit.com/r/dataengineering/comments/1jpy7vu/yet_another_iceberg_catalog_choice_question/,0,False,False,False,False
126,1jpumat,What is a research and BI analyst?,"Hey, before this gets taken down \*I have read the wiki and it did not answer my question\*

I've just signed the contract for a Data Engineering role, but it lists me as a Research and BI Analyst without any mention of data engineering. I should note I'm gonna be an intern and I have zero corporate experience so job titles are new territory for me, sorry if it's really obvious and I'm being clueless.

Is this is a type of data engineer? Have they made a mistake on the contract? Does BI stand for Business Intelligence? What do I even do???

The Analyst bit makes me quite happy because that's what I ultimately want to do in the future but I'm kind of confused as to how this is data engineering as all my other research leading up to this contract tells me Data Analysts and Data Engineers are different lol any help appreciated, thank you!",1,2,popsicola13,2025-04-02 17:36:47,https://www.reddit.com/r/dataengineering/comments/1jpumat/what_is_a_research_and_bi_analyst/,0,False,False,False,False
127,1jpu6nc,Massively scalable collaborative text editor backend with Rama in 120 LOC,,1,0,nathanmarz,2025-04-02 17:20:03,https://blog.redplanetlabs.com/2025/04/01/massively-scalable-collaborative-text-editor-backend-with-rama-in-120-loc/,0,False,False,False,False
128,1jpu66r,Roast my simple project. STAR schema database containing London weather data,"Hey all,

I've just created my second mini-project. Again, just to practice the skill I have learnt through DataCamp's courses.

  
I imported London's weather data via OpenWeather's API, cleaned it and created a database from it (STAR Schema)

  
If I had to do it again I will probably write functions instead of doing transformations manually. I really don't know why I didn't start of using function

  
I think my next project will include multiple different data sources and will also include some form of orchestration.

Here is the link: [https://www.datacamp.com/datalab/w/6aa0a025-9fe8-4291-bafd-67e1fc0d0005/edit](https://www.datacamp.com/datalab/w/6aa0a025-9fe8-4291-bafd-67e1fc0d0005/edit)

Any and all feedback is welcome.

Thanks!",0,6,godz_ares,2025-04-02 17:19:33,https://www.reddit.com/r/dataengineering/comments/1jpu66r/roast_my_simple_project_star_schema_database/,0,False,False,False,False
129,1jpn1do,Help a noob understand whether this is feasible,"Hey all,
Iâ€™m working on a project that involves building a comprehensive overview of all therapist-related businesses in my country. Iâ€™ve found a public online source that lists approximately 16,000 such businesses, spread across many paginated result pages.

Each entry links to a detail page with information such as:

Business name
Business owner (person or legal entity)
Registration number (similar to a company ID)
Location (optional)
No consistent link to a website, but it's often listed in the details

What I need help with:

(1) Scrape all business data into a structured list (CSV, JSON or database).
This involves crawling through all paginated pages and collecting each business profileâ€™s content.

(2) Automatically search for a homepage/website for each business.
The source doesn't always list websites, so for those missing, I'd like to auto-search Google (or use a business API if necessary) to find the most likely company homepage.
(3) If a homepage is found: scrape relevant data from the website itself.

Goal:
To build a clean, filterable dataset that can be used for matching clients with therapists (via a separate platform I'm developing).

Questions Iâ€™d like help with:

Is this technically feasible using open tools or affordable APIs? What/who exactly would I be looking for? I have tried navigating Fiverr, but I am simply not sure what I need to be frank...

Thanks in advance!",1,4,SuburbNacho,2025-04-02 12:12:24,https://www.reddit.com/r/dataengineering/comments/1jpn1do/help_a_noob_understand_whether_this_is_feasible/,0,False,False,False,False
130,1jpmgvn,Feedback on Terraform Data Stack Starter,"Hi, everyone!

I'm a solo data consultant and over the past few years, Iâ€™ve been helping companies in Europe build their data stacks.

I noticed I was repeatedly performing the same tasks across my projects: setting up dbt, configuring Snowflake, and, more recently, migrating to Iceberg data lakes.

So I've been working on a solution for the past few months called [**Boring Data**](http://boringdata.io).

It's a set of Terraform templates ready to be deployed in AWS and/or Snowflake with pre-built integrations for ELT tools and orchestrators.

I think these templates are a great fit for many projects:

* Pay once, own it forever
* Get started fast
* Full control

I'd love to get feedback on this approach, which isn't very common (from what I've seen) in the data industry.

Is Terraform commonly used on your teams, or is that a barrier to using templates like these?

Is there a starter template that you'd wished you had for an implementation in the past?",1,3,Economy-Spread1955,2025-04-02 11:40:59,https://www.reddit.com/r/dataengineering/comments/1jpmgvn/feedback_on_terraform_data_stack_starter/,0,False,False,False,False
131,1jpm06j,Help with a data engineering project,"Hello guys, me and teammates want to do a project from a-z to practice what we learned in an internship we are in and we wanted to the project to be about a telecom companyâ€™s data and we have searched a lot for a dataset that mimics the datasets of real telecom companies but we never found what we are looking for so we thought about creating the data we want using AI but for some reason itâ€™s also not working out for us so i would love to hear some suggestions about what we should do and about telecom data warehouses and databases because i feel maybe we just donâ€™t still quite understand how telecom companies generally operate and perhaps thatâ€™s why we are not successful in generating the data.

I hope this post makes sense because iâ€™m just very confused and donâ€™t know what to do for this project. 

Thank you for anyone who will respond in advance!",1,0,greyishcuneyd,2025-04-02 11:13:35,https://www.reddit.com/r/dataengineering/comments/1jpm06j/help_with_a_data_engineering_project/,0,False,False,False,False
132,1jpk2oe,[BIGQUERY] How long does it take for a backfill and for the buffer resulting from that to clear?,"Hey all, 

1. I have two tables which are about 20-30 gbs and I created a backfill for them as I noticed that two days data was missing, now after an hour the backfill completed, now I am seeing some items in the streaming buffer, I need to update my seniors when the data is ready for analysis, so when can I safely say the data is present?

2. Also, one more question, if I insert a row manually into Bigquery and then create a backfill for it to fetch the data again from transactional database, will the entry I added manually (which doesn't exist in transactional database) be erased?

3. Is there a way to track the ingestion of data into BigQuery?

",1,2,Weird-Trifle-6310,2025-04-02 08:57:23,https://www.reddit.com/r/dataengineering/comments/1jpk2oe/bigquery_how_long_does_it_take_for_a_backfill_and/,0,False,False,False,False
133,1jpjdes,Beginner using API (AWS),"Hi. I work for the state and some of the tools we have are limited. Each week I go to AWS QuickSight to download a CSV file back to our NAS drive where it feeds my Power BI dashboard. I have a gateway setup for cloud to talk to my on-premise NAS drive so auto refresh works. 

Now, my next task: I want to automate the AWS data directly from Power BI so I donâ€™t have to log into their website each week but how do I accomplish this without a programming background? (I majored in Asian History so I donâ€™t know much about data engineering/setting up pipelines)

I read some articles and it seems to indicate that using API can accomplish this but I donâ€™t know Python/SDKs nor do I use CLI (I did some Powershell) and even if I do what services should I use to run CLI for me behind the scenes? Can Power BI make API calls and handle JSON? 

Thanks ðŸ™ ",1,0,RameshYandapalli,2025-04-02 08:02:21,https://www.reddit.com/r/dataengineering/comments/1jpjdes/beginner_using_api_aws/,0,False,False,False,False
134,1jpj0a0,Is my career choice taking me away from Data engineering jobs ?,"Hello everyone,

First of all English is not my first language so I apologize if there are mistakes or if everything is not clear.

I've been working for 6 years and my career path is not very consistent.  
I started in non-technical positions for 3 years and then moved on to a more technical one. 

For 3 years I had a very diversified job with software development (Php, Python), database management, Linux system administration, a bit of Cloud and a big part of â€œDataâ€ with ETL flows (Talend) and a lot of SQL. The project was quite large and the team very small, so I was working on several tasks at once.

I really enjoyed the Data part and I got it into my head that I wanted to be a 'real' Data Engineer and not just drag and drop on Talend.

I was just starting my research when a friend of mine contacted me because a software engineer position was opening up in his company. I went through the recruitment process and accepted their proposal.

  
As in my previous position, I'll be working on a lot of things (mobile development, backend, a bit of frontend, cloud, devops) and the salary offered was 20% higher than what I had in my previous job. (I'm now at 48kâ‚¬ and I don't live in a big city).  
The offer was really attractive and as the market is a bit complicated at the moment, I accepted.

But I'm wondering if this choice will take me even further away from the Data Engineer job i wanted.

Do you find my career path coherent?  
Could I switch back to Data in a few years' time?

Thank you for reading me !",0,16,Wapame92,2025-04-02 07:34:18,https://www.reddit.com/r/dataengineering/comments/1jpj0a0/is_my_career_choice_taking_me_away_from_data/,0,False,False,False,False
135,1jpiq61,Unable to copy data from mysql to azure on Mac,I am trying to load/copy data from a local mysql database in my mac into azure using Data factory. Most of the material i found online suggest to created an integration runtime which requires an installation of an app aimed at windows Os. Is there a way where i could load/copy data from my mysql on mac into azure ?,1,0,Old_Championship610,2025-04-02 07:13:52,https://www.reddit.com/r/dataengineering/comments/1jpiq61/unable_to_copy_data_from_mysql_to_azure_on_mac/,0,False,False,False,False
136,1jpdwy1,Knime on Anaconda Nacigator,Is it possible to install Knime on Anaconda Navigator? ,1,0,Puzzleheaded_Serve39,2025-04-02 02:25:22,https://www.reddit.com/r/dataengineering/comments/1jpdwy1/knime_on_anaconda_nacigator/,0,False,False,False,False
137,1jpqnx3,I Want To Improve an Internal Process At My Company,"Hey r/dataengineering,

I'm currently transitioning from a software engineering role to data engineering, and I've identified a potential project at my company that I think would be a great learning experience and a chance to introduce some data engineering best practices.

Project Overview:

We have a dashboard that displays employee utilization data, sourced from two main systems: Harvest (time tracking) and Forecast (projected utilization).

Current Process:

* Harvest Data: Currently, we're using cron jobs running on an EC2 instance to periodically pull data from Harvest.
* Forecast Data: Due to the lack of an API, we're relying on Playwright (web scraping) to extract data from their web reports, which are then saved to S3.
* Data Processing: Another cron job on EC2 processes the S3 reports and loads the data into a PostgreSQL database.
* Dashboard: A custom frontend application (using Azure OAuth) queries the PostgreSQL database to display the utilization data.

Proposed Solution:

I'm proposing a serverless architecture on AWS, using the following components:

* API Gateway + Lambda: To create a robust API for our frontend application.
* Lambda for ETL: To automate data extraction, transformation, and loading from Harvest and Forecast.
* AWS Step Functions: To orchestrate the data pipeline and manage dependencies.
* Amazon RDS PostgreSQL: To serve as our data warehouse for analytical queries.
* API Gateway Authorizer: To integrate Azure OAuth authentication.
* CI/CD with CodePipeline and CodeBuild: To automate testing and deployment.
* Docker and SAM CLI: For local development and testing.

My Goals:

* Gain hands-on experience with AWS serverless technologies.
* Implement data engineering best practices for ETL and data warehousing.
* Improve the reliability and scalability of our data pipeline.
* Potentially expand this architecture to serve as a central data warehouse for other company analytical data.

My Questions:

1. For those with experience in similar projects, what are some key considerations or potential challenges I should be aware of?
2. Any advice on best practices for designing and implementing a serverless data pipeline on AWS?
3. Are there any specific AWS services or tools that you would recommend for this project?
4. How would you recommend getting started on a project like this, what would you focus on first?
5. What would be some good ways to test this type of system?

I'm eager to learn and contribute, and I appreciate any insights or advice you can offer.

Thanks!",0,3,Tajcore,2025-04-02 14:57:52,https://www.reddit.com/r/dataengineering/comments/1jpqnx3/i_want_to_improve_an_internal_process_at_my/,0,False,False,False,False
138,1jpo030,"How would you solve a low-tech, distributed attendance tracking and service impact problem for a nonprofit with no digital infrastructure?","Iâ€™m working with a nonprofit, supporting 17 veteran communities. The communities arenâ€™t brick-and-mortar â€” they meet at churches and community spaces, and track attendance manually. Thereâ€™s very little technology â€” no computers, mostly just phones and Facebook.

They want to understand:
	â€¢	What services are being offered at the community level
	â€¢	Whoâ€™s attending (recurring vs new)
	â€¢	No-show rates
	â€¢	Cost per veteran for services

The challenge: no digital systems or staff capacity for manual data entry.

What tech-light solutions or data collection flows would you recommend to gather this info and make it analyzable? Bonus if it can integrate later with HubSpot or a simple PostgreSQL DB.",0,2,FunEstablishment77,2025-04-02 13:01:43,https://www.reddit.com/r/dataengineering/comments/1jpo030/how_would_you_solve_a_lowtech_distributed/,0,False,False,False,False
139,1jpaq8v,Data Developer vs Data Engineer,"I know it varies by company blah blah blah, but also aside from a Google search, what have you guys in the field noticed to be core differences between these positions? ",0,3,diabeticspecimen,2025-04-01 23:53:39,https://www.reddit.com/r/dataengineering/comments/1jpaq8v/data_developer_vs_data_engineer/,0,False,False,False,False
140,1jpz36k,How AI will dramatically change DE,"After some struggle with a pipeline today, Gemini 2.5 one-shotted the solution. It's superior in most software problems compared to humans (check coders eval) and we're just two and a half years in.

The capabilities are mind-bending. Data engineering as we know it will change drastically with new AI tooling and self-adjusting infrastructure.

We know this profession will evolve drastically. What do you think where things are heading and how to hedge against AI? Become more social / human I guess ðŸ˜‚

A few hypotheses:
- pipelines and infra manages itself with much higher accuracy and less misconfigurations
- the data engineer profile will shift, they become subject matter experts, they must understand the business and do product management
- technical skills do not matter since the gap from idiot to genius is much smaller than from genius to agi/asi",0,8,Ok-Sentence-8542,2025-04-02 20:34:45,https://www.reddit.com/r/dataengineering/comments/1jpz36k/how_ai_will_dramatically_change_de/,0,False,False,False,False
141,1jpjmaa,Want to know Data engineering hiring trend at present in India,"Until about a month ago hiring seemed to be freezed - lot of fake job postings, people posting google form links collecting resumes, reposting old job roles on linkedin...  Then since about three weeks ago, it seemed like hring is restarted. But now I am having my doubts again - ghosted by recruiters after first screening even told me my CV fits the role well. And not getting other shortlists too. Another thing is huge range of experience 3 yrs - 7 yrs , 2 yrs to 9 yrs experience being posted for majority of the JDs. Obviously if a 7 yrs candidate and if a 3 yrs candidate applies to the same role, they would prefer the 7 yrs exp candidate. What's going on these days? Are they not hiring anyone below 6/7 yrs work exp at all?",0,4,life_Bittersweet,2025-04-02 08:21:32,https://www.reddit.com/r/dataengineering/comments/1jpjmaa/want_to_know_data_engineering_hiring_trend_at/,0,False,False,False,False
142,1jr15ej,What's the non-technical biggest barrier you face at work?,"Whatâ€™s currently challenging for me is getting access to things.

I design a data pipeline, present it to the team that will benefit from it, and everyone gets super excited.

Then I reach out to the internal department or an external party to either grant me admin access to the platform I need, or to help me obtain an API.

A week goes byâ€”nothing. I follow up via email. Eventually, someone replies and says it's not possible to give me admin credentials. Fine. So I ask, â€œCan you help me get the API instead? Itâ€™s very straightforward.â€

Another week goes byâ€”still nothing. I send another follow-upâ€¦

Now the other person is kind of frustrated (because Iâ€™m asking them to do something slightly different, even though Iâ€™m offering guidance).

What follows is just a back-and-forth with long, frustrating waiting periods in between. Meanwhile, the team I presented the pipeline or project to starts getting frustrated with me and probably thinks Iâ€™m full of crap.

Once I finally get the damn API or whatever access I needed, I complete the project in 1â€“2 days but delayed by weeks or even months.

Aaaaaaah!",48,17,sirtuinsenolytic,2025-04-04 02:23:50,https://www.reddit.com/r/dataengineering/comments/1jr15ej/whats_the_nontechnical_biggest_barrier_you_face/,0,False,False,False,False
143,1jr68kn,Are Hyperscalers becoming more expensive in Europe due to the tariffs?,"Hi,

With the recent tariffs in mind, are cloud providers like AWS, Azure, and Google Cloud becoming more expensive for European companies? And what about other techs like Snowflake or Databricks â€“ are they affected too?

Would it be wise for European businesses to consider open-source alternatives, both for cost and strategic independence?

And from a personal perspective: should we, as employees, expand our skill sets toward open-source tech stacks to stay future-proof?",30,26,Ok-Inspection3886,2025-04-04 07:31:32,https://www.reddit.com/r/dataengineering/comments/1jr68kn/are_hyperscalers_becoming_more_expensive_in/,0,False,False,False,False
144,1jr70yg,Which tool do you use to move data from the cloud to Snowflake?,"Hey, r/dataengineering 

Iâ€™m working on a project where I need to move data from our cloud-hosted databases into Snowflake, and Iâ€™m trying to figure out the best tool for the job. Ideally, Iâ€™d like something thatâ€™s cost-effective and scales well. 

If youâ€™ve done this before, what did you use?
Would love to hear about your experienceâ€”how reliable it is, how much it roughly costs, and any pros/cons youâ€™ve noticed. Appreciate any insights!

[View Poll](https://www.reddit.com/poll/1jr70yg)",7,12,Many-Tart-7661,2025-04-04 08:30:40,https://www.reddit.com/r/dataengineering/comments/1jr70yg/which_tool_do_you_use_to_move_data_from_the_cloud/,0,False,False,False,False
145,1jrfp85,Data Engineer Consulting Rate?,"I currently work as a mid-level DE (3y) and Iâ€™ve recently been offered an opportunity in Consulting. Iâ€™m clueless what rate I should ask for. Should it be 25% more than what I currently earn? 50% more? Double!? 

I know that leaping into consulting means compromising job stability and higher expectations for deliveries, so I want to ask for a much higher rate without high or low balling a ridiculous offer. Does someone have experience going from DE to consultant DE? Thanks!",5,16,ActRepresentative378,2025-04-04 16:11:28,https://www.reddit.com/r/dataengineering/comments/1jrfp85/data_engineer_consulting_rate/,0,False,False,False,False
146,1jre0v2,Logging in Spark applications.,"Hi guys, i am moving to on-prem managed Spark applications with Kuberenetes. I am wondering what do u use for logging? I am talking about Python and PySpark. Do u setup log4j? Or just use Python's logging library for application? What is the standard here? I have not seen much about log4j within PySpark.",5,2,Hot_While_6471,2025-04-04 15:01:43,https://www.reddit.com/r/dataengineering/comments/1jre0v2/logging_in_spark_applications/,0,False,False,False,False
147,1jrdue4,Anyone know of any vscode linter for sql that can accommodate pyspark sql?,"In pyspark 3.4 you can write sql as 

spark.sql(SELECT * FROM {df_input}, df_input = df_input) 

The popular sql linters I tried SQL Formatter and and Prettier SQL Vscode currently does not accommodate{}. Does anyone know of any linters that does? Thank you",5,0,AUGcodon,2025-04-04 14:54:19,https://www.reddit.com/r/dataengineering/comments/1jrdue4/anyone_know_of_any_vscode_linter_for_sql_that_can/,1,False,False,False,False
148,1jr3z1u,Faster way to view + debug data,"Hi r/dataengineering!

  
I wanted to share a project that I have been working on.Â It's an intuitive data editor where you can interact with local and remote data (e.g. Athena & BigQuery). For several important tasks, it can speed you up by 10x or more. (see website for more)

  
For data engineering specifically, this would be really useful in debugging pipelines, cleaning local or remote data, and being able to easy create new tables within data warehouses etc.

I know this could be a lot faster than having to type everything out, especially if you're just poking around. I personally find myself using this before trying any manual work.

Also, for those doing complex queries, you can split them up and work with the frame visually and add queries when needed. Super useful for when you want to iteratively build an analysis or new frameÂ ***without writing a super long query.***

  
As for data size, it can handle local data up to around 1B rows, and remote data is only limited by your data warehouse.

  
You don't have to migrate *anything* either.

  
If you're interested, you can check it out here: [https://www.cocoalemana.com](https://www.cocoalemana.com)

  
I'd love to hear about your workflow, and see what we can change to make it cover more data engineering use cases.

  
Cheers!

[Coco Alemana](https://preview.redd.it/02wogjj72rse1.jpg?width=3820&format=pjpg&auto=webp&s=0905bd40927b4dd7e80521568982ebe82994a5fe)

",4,3,Impressive_Run8512,2025-04-04 05:00:10,https://www.reddit.com/r/dataengineering/comments/1jr3z1u/faster_way_to_view_debug_data/,0,False,False,False,False
149,1jr6a6h,How to stream results of a complex SQL query,"Hello,

I'm writing you because I have a problem with a side project and maybe here somebody can help me. I have to run a complex query with a potentially high number of results and it takes a lot of time. However, for my project I don't need all the results to be showed together, perhaps after some hours/days. It would be much more useful to get a stream of the partial results in real time. How can I achieve this? I would prefer to use free software, however please suggest me any solution you have in mind.

Thank you in advance!",3,12,forevernevermore_,2025-04-04 07:35:07,https://www.reddit.com/r/dataengineering/comments/1jr6a6h/how_to_stream_results_of_a_complex_sql_query/,0,False,False,False,False
150,1jr23mk,How do I get out of this rut,"Iâ€™m currently about the finish an early career rotational program with a top 10 bank. The rotation I am currently on and where the company is placing me post program (I tried to get placed somewhere else) is as a data engineer on a data delivery team. When I was advertised this rotation and the team I was told pretty specifically we would be using all the relevant technologies and I would be very hands on keyboard building pipelines with python , configuring cloud services and snowflake, being a part of data modeling. Mind you Iâ€™m not completely new I have experience with all this in personal projects and previous work experience as a SWE and researcher in college. 

Turns out all of that was a lie. I later learned there is an army of contractors that do the actual work. I was stuck with analyzing .egp files and other SAS files documenting it and handing off to consultants to rebuild in Talend to ingest into snowflake. The only tech that I use is Visio and Word.

I coped with that by saying after Iâ€™m out of the program Iâ€™ll get to do the actual work. But I had a conversation with my manager today about what my role will be post program. He basically said there are a lot more of these SAS procedures they are porting over to talend and snowflake and Iâ€™ll be documenting them and handing over to contractors so they can implement the new process. Honestly that is all really quick and easy to do because there isnâ€™t that much complicated business logic for the LOBs we support just joins and the occasional aggregation so most days Iâ€™m not doing anything.

When I told him I would really like to be involved in the technical work or the data modeling , he said that is not my job anymore and that is what we pay the contractors to do so I canâ€™t do it. Almost made it seem like I should be grateful and he is doing me a favor somehow.

It just feels like I was misled or even outright lied to about the position. We donâ€™t use any of the technologies that were advertised (Drag and drop/low code tools seem like fake engineering), I donâ€™t get to be hands on keyboard at all. Just seems like there really I no growth or opportunity in this role. I would leave but I took relocation and a signing bonus for this and if I leave too early I owe it back. I also canâ€™t internally transfer anywhere for a year after starting my new role.

I guess my rant is just to ask what should I be doing in this situation? I work on personal projects and open source and I have gotten a few certs in the downtime at work but I donâ€™t know if itâ€™s enough to make sure my skills donâ€™t atrophy while I wait out my repayment period. I consider myself a somewhat technical guy but I have been boxed into a non technical role.

",3,6,anonymous_0618615740,2025-04-04 03:13:53,https://www.reddit.com/r/dataengineering/comments/1jr23mk/how_do_i_get_out_of_this_rut/,0,False,2025-04-04 09:08:09,False,False
151,1jr1r2t,"Built a real-time e-commerce data pipeline with Kinesis, Spark, Redshift & QuickSight â€” looking for feedback","I recently completed a real-time ETL pipeline project as part of my data engineering portfolio, and Iâ€™d love to share it here and get some feedback from the community.

# What it does:

* Streams transactional data using **Amazon Kinesis**
* Backs up raw data in **S3** (Parquet format)
* Processes and transforms data with **Apache Spark**
* Loads the transformed data into **Redshift Serverless**
* Orchestrates the pipeline with **Apache Airflow (Docker)**
* Visualizes insights through a **QuickSight dashboard**

# Key Metrics Visualized:

* Total Revenue
* Orders Over Time
* Average Order Value
* Top Products
* Revenue by Category (donut chart)

I built this to practice real-time ingestion, transformation, and visualization in a scalable, production-like setup using AWS-native services.

# GitHub Repo:

[https://github.com/amanuel496/real-time-ecommerce-etl-pipeline](https://github.com/amanuel496/real-time-ecommerce-etl-pipeline)

If you have any thoughts on how to improve the architecture, scale it better, or handle ops/monitoring more effectively, Iâ€™d love to hear your input.

Thanks!",5,6,MysteriousRide5284,2025-04-04 02:55:36,https://www.reddit.com/r/dataengineering/comments/1jr1r2t/built_a_realtime_ecommerce_data_pipeline_with/,0,False,False,False,False
152,1jramqt,"Airbyte Connector Builder now supports GraphQL, Async Requests and Custom Components","Hello, Marcos from the Airbyte Team.

For those who may not be familiar, Airbyte is an open-source data integration (EL) platform with over 500 connectors for APIs, databases, and file storage.

In our last release we added several new features to our no-code Connector Builder:

* [GraphQL Support](https://docs.airbyte.com/connector-development/config-based/understanding-the-yaml-file/request-options#graphql-request-injection): In addition to REST, you can now make requests to GraphQL APIs (and properly handle pagination!)
* [Async Data Requests](https://docs.airbyte.com/connector-development/connector-builder-ui/async-streams): There are some reporting APIs that do not return responses immediately. For instance, with Google Ads.Â  You can now request a custom report from these sources and wait for the report to be processed and downloaded.
* [Custom Python Code Components](https://docs.airbyte.com/connector-development/connector-builder-ui/custom-components): We recognize that some APIs behave uniquelyâ€”for example, by returning records as key-value pairs instead of arrays or by not ordering data correctly. To address these cases, our open-source platform now supports custom Python components that extend the capabilities of the no-code framework without blocking you from building your connector.

We believe these updates will make connector development faster and more accessible, helping you get the most out of your data integration projects.

We understand there are discussions about the trade-offs between no-code and low-code solutions. At Airbyte, transitioning from fully coded connectors to a low-code approach allowed us to maintain a large connector catalog using standard components.Â  We were also able to create a better build and test process directly in the UI. Users frequently give us the feedback that the no-code connector Builder enables less technical users to create and ship connectors. This reduces the workload on senior data engineers allowing them to focus on critical data pipelines.

Something else that has been top of mind is speed and performance. With a robust and stable connector framework, the engineering team has been dedicating significant resources to introduce concurrency to enhance sync speed. You can read this[ blog post](https://airbyte.com/blog/improving-connector-sync-speed-up-to-10x-faster) about how the team implemented concurrency in the Klaviyo connector, resulting in a speed increase of about 10x for syncs.

I hope you like the news! Let me know if you want to discuss any missing features or provide feedback about Airbyte.",3,2,marcos_airbyte,2025-04-04 12:26:58,https://www.reddit.com/r/dataengineering/comments/1jramqt/airbyte_connector_builder_now_supports_graphql/,0,False,False,False,False
153,1jrm80y,Marketing Report & Fivetran,"Fishing for advice as I'm sure many have been here before. I came from DE at a SaaS company where I was more focused on the infra but now I'm in a role much close to the business and currently working with marketing. I'm sure this could make the Top-5 all time repeated DE tasks. A daily marketing report showing metrics like Spend, cost-per-click, engagement rate, cost-add-to-cart, cost-per-traffic... etc. These are per campaign based on various data sources like GA4, Google Ads, Facebook Ads, TikTok etc. Data updates once a day.

It should be obvious I'm not writing API connectors for a dozen different services. I'm just one person doing this and have many other things to do. I have Fivetran up and running getting the data I need but MY GOD is it ever expensive for something that seems like it should be simple, infrequent & low volume. It comes with a ton of build in reports that I don't even need sucking rows and bloating the bill. I can't seem to get what I need without pulling millions of event rows which costs a fortune to do.

Are there other similar but (way) cheaper solutions are out there? I know of others but any recommendations for this specific purpose?",2,5,bcsamsquanch,2025-04-04 20:45:27,https://www.reddit.com/r/dataengineering/comments/1jrm80y/marketing_report_fivetran/,0,False,False,False,False
154,1jrd8po,PII Obfuscation in Databricks,"Hi Data Champs,

I have been recently given chance to explore PII obfuscation technique in databricks.

I proposed using sql aes_encryption or python fernet for PII column level encryption before landing to bronze.

And use column masking on delta tables which has built in logic for group membership check and decryption so to avoid the overhead of a new view per table.

My HDE was more interested in sql approach than the fernet but fernet offers built in key rotation out of the box.

Has anyone used aes_encryption 
Is it secure, easy to work with and relatively more robust.

From my experience for data type other than binary like long, int, double it needs to be first converted to binary (donâ€™t like it)

Apart from that usual error here and there for padding and generic error when decrypting sometimes.

So given the choice what will be your architecture 

What you will prefer, what you donâ€™t and why

I am open to DM if you wanna ðŸ’¬ ",2,1,Intelligent-Mind8510,2025-04-04 14:28:31,https://www.reddit.com/r/dataengineering/comments/1jrd8po/pii_obfuscation_in_databricks/,0,False,2025-04-04 14:33:56,False,False
155,1jqzz3y,General question about data consulting,"Let's say there's a data consulting company working within a certain industry (e.g., utilities or energy). How do they gain access to their clients' databases if they want to perform ETL or other services? How about working with their data in a cloud setting (e.g., AWS)? What is the usual process for that? Is the consulting company responsible for setting and managing AWS costs, etc.?",2,9,No7-Francesco88,2025-04-04 01:23:44,https://www.reddit.com/r/dataengineering/comments/1jqzz3y/general_question_about_data_consulting/,0,False,False,False,False
156,1jrohrl,Question about file sync,"Pardon the noob question. I'm building a simple ETL process using Airflow on a remote Linux server and need a way for users to upload input files and download processed files.

I would prefer a method that is easy to use for users like a shared drive (like Google Drive).

I've considered Syncthing, and in the worst case, SFTP access. What solutions do you typically use or recommend for this? Thanks!",3,1,CraftedLove,2025-04-04 22:25:11,https://www.reddit.com/r/dataengineering/comments/1jrohrl/question_about_file_sync/,1,False,False,False,False
157,1jrd0k2,Great Expectations Implementation,"Our company is implementing data quality testing and we are interested in borrowing from the Great Expectations suite of open source tests. I've read mostly negative reviews of the initial implementation of Great Expectations, but am curious if anyone else set up a much more lightweight configuration?

Ultimately, we plan to use the GX python code to run tests on data in Snowflake and then make the results available in Snowflake. Has anyone done something similar to this?",1,2,HAKOC534,2025-04-04 14:18:53,https://www.reddit.com/r/dataengineering/comments/1jrd0k2/great_expectations_implementation/,0,False,False,False,False
158,1jrc62b,Can you call an aimless star schema a data mart?,"So,

  
as always that's for the insight from other people, I find a lot of these discussions around points very entertaining and very helpful!

I'm having an argument with someone who is several levels above me. This might sound petty so I apologise in advance. It centres around the definition of a Mart. Our Mart is a single Fact with around 20 dimensions. The Fact is extremely wide and deep. Indeed we usually put it into a de normalised table for reporting. To me this isn't a MART as it isn't based on requirements but rather a star schema that supposedly servers multiple purposed or potential purposes. When engaged on requirements the person leans on there experience in the domain and says a user probable wants to do X, Y and Z. I've never seen anything written down. Constantly that report also defers to Kimball methodology and how this follows them closely. My take on the book is that these things need to be based of requirement, business requirements. 

My questions is, is it fair to say that a data mart needs to have requirements and ideally a business domain in mind or else its just a star schema?

Yes this  is  very theoretical... yes I probable need a hobby but look there hasn't been a decent RTS game in years and its friday!!!

Have a good weekend everyone",1,3,ObjectiveAssist7177,2025-04-04 13:41:31,https://www.reddit.com/r/dataengineering/comments/1jrc62b/can_you_call_an_aimless_star_schema_a_data_mart/,0,False,False,False,False
159,1jrbgqt,Data Engineering Performance -  Authors,I having worked in BI and transitioned to DE have followed best practices reading books by authors like Ralph Kimball in BI. Is there someone in DE with a similar level of reputation. I am not looking for specific technologies but rather want to pick up DE fundamentals especially in the performance and optimization space.,1,1,Amar_K1,2025-04-04 13:08:48,https://www.reddit.com/r/dataengineering/comments/1jrbgqt/data_engineering_performance_authors/,0,False,False,False,False
160,1jravos,Unstructured Data,"I see this has been asked prior but I didn't see a clear answer. We have a smallish database (glorified spreadsheet) where one field contains text. It houses details regarding customers, etc calling in for various issues. For various reasons (in-house) they want to keep using the simple app (it's a SharePoint List). I can easily download the data to a CSV file, for example, but is there a fairly simple method (AI?) to make sense of this data and correlate it? Maybe a creative prompt? Or is there a tool for this? (I'm not a software engineer). Thanks!",1,5,Top_Sink9871,2025-04-04 12:39:42,https://www.reddit.com/r/dataengineering/comments/1jravos/unstructured_data/,0,False,False,False,False
161,1jr6oc9,Do you need statistics to land a DE job?,"As the title suggests. Even if stats are not used on the job, will having stats qualifications give me an edge in the hiring process?",1,20,Normal-Bandicoot-180,2025-04-04 08:03:59,https://www.reddit.com/r/dataengineering/comments/1jr6oc9/do_you_need_statistics_to_land_a_de_job/,0,False,False,False,False
162,1jr05id,[Seeking Guidance] Aspiring GCP Data Engineer â€“ Will Work Pro Bono for Hands-On Experience!,"Hey r/dataengineering community,  

Iâ€™m deep into prepping for the Google Cloud Professional Data Engineer cert and want to transition from theory to real-world projects. To ace the exam and build job-ready skills, Iâ€™m looking for:  

- Hands-on opportunities (pro bono!) to work with GCP tools like BigQuery, Dataflow, Pub/Sub, Cloud Composer, etc.  
- Mentorship or collaboration on data pipelines, workflow optimization, or cloud architecture projects.  
- Open-source/community projects needing an extra pair of hands.  

Why me? Iâ€™m motivated, detail-oriented, and eager to learn. Iâ€™ll treat your project like my own!  

If youâ€™re working on anything data-related in GCP - or know someone who is - Iâ€™d hugely appreciate a chance to contribute (or even just advice on where to start). Comment/DM me, and thanks for being an awesome community!  

P.S. Upvotes for visibility help a ton! ðŸ™",1,3,aiqdec,2025-04-04 01:33:00,https://www.reddit.com/r/dataengineering/comments/1jr05id/seeking_guidance_aspiring_gcp_data_engineer_will/,0,False,False,False,False
163,1jrmmcn,AI agent for complex query,"https://www.reddit.com/r/AI_Agents/s/iKnUXMLoxZ

",0,1,Future_Scar_7875,2025-04-04 21:02:34,https://www.reddit.com/r/dataengineering/comments/1jrmmcn/ai_agent_for_complex_query/,0,False,False,False,False
164,1jrd286,Just wanted to share a recent win that made our whole team feel pretty good.,"We worked with this e-commerce client last month (kitchen products company, can't name names) who was dealing with data chaos.

When they came to us, their situation was rough. Dashboards taking forever to load, some poor analyst manually combining data from 5 different sources, and their CEO breathing down everyone's neck for daily conversion reports. Classic spreadsheet hell that we've all seen before.

We spent about two weeks redesigning their entire data architecture. Built them a proper [**data warehouse solution** ](https://datafortune.com/services/enterprise-data-management/data-warehouse/)with automated ETL pipelines that consolidated everything into one central location. Created some logical data models and connected it all to their existing BI tools.

The transformation was honestly pretty incredible to watch. Reports that used to take hours now run in seconds. Their analyst actually took a vacation for the first time in a year. And we got this really nice email from their CTO saying we'd ""changed how they make decisions"" which gave us all the warm fuzzies.

It's projects like these that remind us why we got into this field in the first place. There's something so satisfying about taking a messy data situation and turning it into something clean and efficient that actually helps people do their jobs better.",0,9,DataMaster2025,2025-04-04 14:20:44,https://www.reddit.com/r/dataengineering/comments/1jrd286/just_wanted_to_share_a_recent_win_that_made_our/,0,False,False,False,False
165,1ju81cr,Jira: Is it still helping teams... or just slowing them down?,"Iâ€™ve been part of (and led) a teams over the last decade â€” in enterprises

And one tool keeps showing up everywhere: **Jira**.

Itâ€™s the ""default"" for a lot of engineering orgs. Everyone knows it. Everyone uses it.  
But **I donâ€™t seen anyone who actually likes it.**

Not in the *""ugh it's corporate but fine""* way â€” I mean people who are actively frustrated by it but still use it daily.

Here are some of the most common friction points Iâ€™ve either experienced or heard from other devs/product folks:

1. **Custom workflows spiral out of control** â€” What starts as ""just a few tweaks"" becomes an unmanageable mess.
2. **Slow performance** â€” Large projects? Boards crawling? Yup.
3. **Search that requires sorcery** â€” Good luck finding an old ticket without a detailed Jira PhD.
4. **New team members struggle to onboard** â€” Itâ€™s not exactly intuitive.
5. **The â€œtool taxâ€** â€” Teams spend hours updating Jira instead of moving work forward.

And yet... most teams stick with it. Because switching is painful. Because â€œat least everyone knows Jira.â€ Because the alternative is more uncertainty.  
What's your take on this?",64,47,IllWasabi8734,2025-04-08 07:40:50,https://www.reddit.com/r/dataengineering/comments/1ju81cr/jira_is_it_still_helping_teams_or_just_slowing/,0,False,False,False,False
166,1jukwsu,Why do you dislike MS Fabric?,"Title.  I've only tested it. It seems like not a good solution for us (at least currently) for various reasons, but beyond that...

It seems people generally don't feel it's production ready - how specifically?  What issues have you found?",40,41,cdigioia,2025-04-08 18:33:12,https://www.reddit.com/r/dataengineering/comments/1jukwsu/why_do_you_dislike_ms_fabric/,0,False,2025-04-08 23:46:51,False,False
167,1ju9kqo,How did you start your data engineering journey?,"I am getting into this role, I wondered how other people became data engineers? Most didn't start as a junior data engineer; some came from an analyst(business or data), software engineers, or database administrators. 

What helped you become one or motivated you to become one?",16,39,FuzzyCraft68,2025-04-08 09:39:06,https://www.reddit.com/r/dataengineering/comments/1ju9kqo/how_did_you_start_your_data_engineering_journey/,0,False,False,False,False
168,1ju6uoo,Ingesting a billion small .csv files from blob?,"Currently, we're ""streaming"" data by having an Azure Function write event grid messages to csv in blob storage, and then by having snowpipe ingest them. There's about a million csv's generated daily. The blob is not partitioned at all.

What's the best way to ingest/delete everything? Snowpipe has a configuration error, and a portion of the data hasn't been loaded, ever. ADF was pretty slow when I tested it out.

This was all done by consultants before I was in house btw.


edit: I was a bit unclear in my message. I mean, that we've had snowpipe ingesting these files. However, now we need to re-ingest the billion or so small .csv's that are in the blob, to compare the data to the already ingested data.

What further complicates this is:

- some files have two additional columns
- we also need to parse the filename to a column
- there is absolutely no partitioning at all",16,4,hi_top_please,2025-04-08 06:14:25,https://www.reddit.com/r/dataengineering/comments/1ju6uoo/ingesting_a_billion_small_csv_files_from_blob/,0,False,2025-04-08 21:44:57,False,False
169,1jumngl,Hung DBT jobs,"According to the DBT Cloud [api](https://docs.getdbt.com/dbt-cloud/api-v2#/operations/Retrieve%20Run%20Failure%20Details), I can only tell that a job has failed and retrieve the failure details. 

There's no way for me to know when a job is hung.

Yesterday, an issue with our Fivetran replication and several of our DBT jobs hung for several hours.

Any idea how to monitor for hung DBT jobs?",13,3,CrabEnvironmental864,2025-04-08 19:45:39,https://www.reddit.com/r/dataengineering/comments/1jumngl/hung_dbt_jobs/,0,False,False,False,False
170,1jugab3,What are the Python Data Engineering approaches every data scientist should know?,"Is it building data pipelines to connect to a DB?
Is it automatically downloading data from a DB and creating reports or is it something else? 
I am a data scientist who would like to polish his Data Engineering skills with Python because my company is beginning to incorporate more and more Python and I think I can be helpful. ",13,5,Pineapple_throw_105,2025-04-08 15:25:54,https://www.reddit.com/r/dataengineering/comments/1jugab3/what_are_the_python_data_engineering_approaches/,0,False,False,False,False
171,1jukena,Clean architecture for Data Engineering,"Hi Guys,

Do anyone use or tried to use clean architecture for data engineering projects? If yes, May I know, how did it go and any comments on it or any references on github if you have?

Please don't give negative comments/responses without reasons.

Best regards",9,5,Harshadeep21,2025-04-08 18:13:04,https://www.reddit.com/r/dataengineering/comments/1jukena/clean_architecture_for_data_engineering/,0,False,False,False,False
172,1ju714r,"reflect-cpp - a C++20 library for fast serialization, deserialization and validation using reflection, like Python's Pydantic or Rust's serde.","[https://github.com/getml/reflect-cpp](https://github.com/getml/reflect-cpp)

I am a data engineer, ML engineer and software developer with strong background in functional programming. As such, I am a strong proponent of the ""Parse, Don't Validate"" principle (https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/).  
  
Unfortunately, C++ does not yet support reflection, which is necessary to do something apply these principles. However, after some discussions on the topic over on r/cpp, we figured out a way to do this anyway. This library emerged out of these discussions.

I have personally used this library in real-world projects and it has been very useful. I hope other people in data engineering can benefit from it as well.

And before you ask: Yes, I use C++ for data engineering. It is quite common in finance and energy or other fields where you really care about speed. ",6,0,liuzicheng1987,2025-04-08 06:26:36,https://www.reddit.com/r/dataengineering/comments/1ju714r/reflectcpp_a_c20_library_for_fast_serialization/,0,False,False,False,False
173,1jusby0,Best way to handle loading JSON API data into database in pipelines,"Greetings, this is my first post here. I've been working in DE for the last 5 years now doing various things with Airflow and Dagster.  I have a question regarding design of data flow from APIs to our database.

I am using Dagster/Python to perform the API pulls and loads into Snowflake.

My team lead insists that we load JSON data into our Snowflake RAW\_DATA in the following way:

ID (should be a surrogate/non-native PK)  
PAYLOAD (raw JSON payload, either as a VARCHAR or VARIANT type)  
CREATED\_DATE (timestamp this row was created in Snowflake)  
UPDATE\_DATE (timestamp this row was updated in Snowflake)

Flattening of the payload then happens in SQL as a plain View, which we currently autogenerate using Python and manually edit and add to Snowflake.

He does not want us (DE team) to use DBT to do any transforming of RAW\_DATA. DBT is only for the Data Analyst team to use for creating models.

The main advantage I see to this approach is flexibility if the JSON schema changes. You can freely append/drop/insert/reorder/rename columns. whereas a normal table you can only drop, append, and rename.

On the downside, it is slow and clunky to parse with SQL and access the data as a view. It just seems inefficient to have to recompute the view and parse all those JSON payloads whenever you want to access the table.  
  
I'd much rather do the flattening in Python, either manually or using dlt. Some JSON payloads I 'pre-flatten' in Python to make them easier to parse in SQL.

Is there a better way, or is this how you all handle this as well?",6,1,fetus-flipper,2025-04-08 23:56:12,https://www.reddit.com/r/dataengineering/comments/1jusby0/best_way_to_handle_loading_json_api_data_into/,0,False,2025-04-09 00:01:00,False,False
174,1juvgjf,Azure Course for Beginners | Learn Azure & Data Bricks in 1 Hour,"# FREE Azure Course for Beginners | Learn Azure & Data Bricks in 1 Hour

[https://www.youtube.com/watch?v=8XH2vTyzL7c](https://www.youtube.com/watch?v=8XH2vTyzL7c)",4,1,chrisgarzon19,2025-04-09 02:36:56,https://www.reddit.com/r/dataengineering/comments/1juvgjf/azure_course_for_beginners_learn_azure_data/,0,False,False,False,False
175,1juu00b,Azure vs Microsoft Fabric?,"As a data engineer, I really like the control and customization that Azure offers.
At the same time, I can see how Fabric is more business-friendly and leans toward a low/no-code experience.

But with all the content and comparisons floating around the internet, why is no one talking about how insanely expensive Fabric is?! Seriouslyâ€”am I missing something here?


-
",4,7,Dharneeshkar,2025-04-09 01:20:51,https://www.reddit.com/r/dataengineering/comments/1juu00b/azure_vs_microsoft_fabric/,0,False,False,False,False
176,1jujn9j,Lessons from optimizing dashboard performance on Looker Studio with BigQuery data,"Weâ€™ve been using Looker Studio (formerly Data Studio) to build reporting dashboards for digital marketing and SEO data. At first, things worked fineâ€”but as datasets grew, dashboard performance dropped significantly.



The biggest bottlenecks were:

â€¢ Overuse of blended data sources

â€¢ Direct querying of large GA4 datasets

â€¢ Too many calculated fields applied in the visualization layer



To fix this, we adjusted our approach on the data engineering side:

â€¢ Moved most calculations (e.g., conversion rates, ROAS) to the query layer in BigQuery

â€¢ Created materialized views for campaign-level summaries

â€¢ Used scheduled queries to pre-aggregate weekly and monthly data

â€¢ Limited Looker Studio to one direct connector per dashboard and cached data where possible



Result: dashboards now load in \~3 seconds instead of 15â€“20, and we can scale them across accounts with minimal changes.



Just sharing this in case others are using BI tools on top of large datasetsâ€”interested to hear how others here are managing dashboard performance from a data pipeline perspective.",3,1,kodalogic,2025-04-08 17:43:22,https://www.reddit.com/r/dataengineering/comments/1jujn9j/lessons_from_optimizing_dashboard_performance_on/,0,False,False,False,False
177,1juhrrs,Help: Looking to set up a decent data architecture (data lake and/or warehouse),"Hi, I need help. I need a proper architecture for a department, and I am trying to get a data lake/warehouse.

Why: We have a lot of data sources from SaaS to manually created documents. We use a lot of SaaS products, but we have no centralised repository to store and stage the data, so we end up with a lot of workaround such as using SharePoint and csv stored in folders for reporting. We also change SaaS products quite frequently, so sources can change often. It is difficult to do advanced analytics. 

I prefer a lake & warehouse approach because (1) for SaaS users, they can can just drop the data to the lake and (2) transformation and processing can be done for reporting, and we could combine the datasets even when we change the SaaS software. 

My huge considerations are that (1) the data is to be accessible within the department only and (2) it has to be decent cost. Currently considered Azure Data Lake Storage Gen2 & DataBricks, or Snowflake (to have both the lake and warehouse). My previous experience was only with Data Lake Storage Gen2.

I'm willing to work my way up for my technical limitations, but at this stage I am exploring the software solutions to get the buy in to kickstart this project. 

Any sharing is much appreciated, and if you worked with such an environment, I appreciate your guidance and learnings as well. Thank you in advance.",3,1,thehotdawning,2025-04-08 16:27:08,https://www.reddit.com/r/dataengineering/comments/1juhrrs/help_looking_to_set_up_a_decent_data_architecture/,0,False,False,False,False
178,1juo1uo,How are entry level data engineering roles at Amazon?,"If anyone on this sub has worked for Amazon as a Data engineer, preferably entry level or early careers, how has your experience been working at amazon at Amazon? 

Iâ€™ve heard their work culture is very startup like, and their is an abundance of poor managers. The company just cars about share holder value, instead of caring for their customers and employees.

I wanted to hear on this sub, how has your experience been? How was the hiring process like? What all skills I should develop to work for Amazon?",2,2,gta35,2025-04-08 20:43:31,https://www.reddit.com/r/dataengineering/comments/1juo1uo/how_are_entry_level_data_engineering_roles_at/,0,False,False,False,False
179,1jufvpe,Question around migrating to dbt,"We're considering moving from a dated ETL system to dbt with data being ingested via AWS Glue.

We have a data warehouse which uses a Kimball dimensional model, and I am wondering how we would migrate the dimension load processes.

We don't have access to all historic data, so it's not a case of being able to look across all files and then pull out the dimensions. Would it make sense fur the dimension table to be bothered a source and a dimension?

I'm still trying to pivot my way of thinking away from the traditional ETL approach so might be missing something obvious.",2,2,receding_bareline,2025-04-08 15:09:02,https://www.reddit.com/r/dataengineering/comments/1jufvpe/question_around_migrating_to_dbt/,0,False,False,False,False
180,1juvakz,Beginner Predictive Model Feedback/Guidance,"My predictive modeling folks, beginner here could use some feedback guidance. Go easy on me, this is my first machine learning/predictive model project and I had very basic python experience before this.


Iâ€™ve been working on a personal project building a model that predicts NFL player performance using full career, game-by-game data for any offensive player who logged a snap between 2017â€“2024. 

I trained the model using data through 2023 with XGBoost Regressor, and then used actual 2024 matchups â€” including player demographics (age, team, position, depth chart) and opponent defensive stats (Pass YPG, Rush YPG, Points Allowed, etc.) â€” as inputs to predict game-level performance in 2024.

The model performs really well for some stats (e.g., RÂ² > 0.875 for Completions, Pass Attempts, CMP%, Pass Yards, and Passer Rating), but others â€” like Touchdowns, Fumbles, or Yards per Target â€” arenâ€™t as strong.

Hereâ€™s where I need input:

-Whatâ€™s a solid baseline RÂ², RMSE, and MAE to aim for â€” and does that benchmark shift depending on the industry?

-Could trying other models/a combination of models improve the weaker stats? Should I use different models for different stat categories (e.g., XGBoost for high-RÂ² ones, something else for low-RÂ²)?

-How do you typically decide which model is the best fit? Trial and error? Is there a structured way to choose based on the stat being predicted?

-I used XGBRegressor based on common recommendations â€” are there variants of XGBoost or alternatives you'd suggest trying? Any others you like better?

-Are these considered â€œgoodâ€ model results for sports data?

-Are sports models generally harder to predict than industries like retail, finance, or real estate?

-What should my next step be if I want to make this model more complete and reliable (more accurate) across all stat types?

-How do people generally feel about manually adding in more intangible stats to tweak data and model performance? Example: Adding an injury index/strength multiplier for a Defense that has a lot of injuries, or more playerâ€™s coming back from injury, etc.? Is this a generally accepted method or not really utilized?

Any advice, criticism, resources, or just general direction is welcomed.",1,0,ynwFreddyKrueger,2025-04-09 02:27:57,https://www.reddit.com/gallery/1juvakz,0,False,False,False,False
181,1jugyx7,Cornerstone data,"Hi all,

Has anybody pulled cornerstone training data using their APIs or used anyother method to pull the data?",1,0,arunrajan96,2025-04-08 15:54:25,https://www.reddit.com/r/dataengineering/comments/1jugyx7/cornerstone_data/,0,False,False,False,False
182,1judm9f,GizmoSQL: Power your Enterprise analytics with Arrow Flight SQL and DuckDB,"Hi! This is Phil - Founder ofÂ [GizmoData](https://gizmodata.com). We have a new commercial database engine product called:Â [GizmoSQL](https://gizmodata.com/gizmosql)Â \- built with Apache Arrow Flight SQL (for remote connectivity) and DuckDB (or optionally: SQLite) as a back-end execution engine.

This product allows you to run DuckDB or SQLite as a server (remotely) - harnessing the power of computers in the cloud - which typically have more CPUs, more memory, and faster storage (NVMe) than your laptop. In fact, running GizmoSQL on a modern arm64-based VM in Azure, GCP, or AWS allows you to run at terabyte scale - with equivalent (or better) performance - for a fraction of the cost of other popular platforms such as Snowflake, BigQuery, or Databricks SQL.

**GizmoSQL**Â is self-hosted (for now) - with a possible SaaS offering in the near future. It has these features to differentiate it from ""base"" DuckDB:

* Run DuckDB or SQLite as a server (remote connectivity)
* Concurrency - allows multiple users to work simultaneously - with independent, ACID-compliant sessions
* Security
   * Authentication
   * TLS for encryption of traffic to/from the database
* Static executable with Arrow Flight SQL, DuckDB, SQLite, and JWT-CPP built-in. There are no dependencies to install - just a single executable file to run
* Free for use in development, evaluation, and testing
* Easily containerized for running in the Cloud - especially in Kubernetes
* Easy to talk to - with ADBC, JDBC, and ODBC drivers, and now a Websocket proxy server (created by GizmoData) - so it is easy to use with javascript frameworks
   * Use it with Tableau, PowerBI, Apache Superset dashboards, and more
* Easy to work with in Python - use ADBC, or the new experimental Ibis back-end - details here:Â [https://github.com/gizmodata/ibis-gizmosql](https://github.com/gizmodata/ibis-gizmosql)

Because it is powered by DuckDB - GizmoSQL can work with the popular open-source data formats - such as Iceberg, Delta Lake, Parquet, and more.

GizmoSQL performs very well (when running DuckDB as its back-end execution engine) - check out our graph comparing popular SQL engines for TPC-H at scale-factor 1 Terabyte - on the homepage at:Â [https://gizmodata.com/gizmosql](https://gizmodata.com/gizmosql)Â \- there you will find it also costs far less than other options.

We would love to get your feedback on the software - it is easy to get started for free in two different ways:

* For a limited time - try GizmoSQL online on our dime - with the SQL Query Navigator - it just requires a quick registration and sign-in to get going - at:Â [https://app.gizmodata.com](https://app.gizmodata.com)Â \- where we have a read-only 1TB TPC-H database mounted for you to query in real-time. It is running on an Azure Cobalt 100 VM - with local NVMe SSD's - so it should be quite zippy.
* Download and self-host GizmoSQL - using our Docker image or executables for Linux and macOS for both x86-64 and arm64 architectures. See our README at:Â [https://github.com/gizmodata/gizmosql-public](https://github.com/gizmodata/gizmosql-public)Â for details on how to easily and quickly get started that way

Thank you for taking a look at GizmoSQL. We are excited and are glad to answer any questions you may have!

* **Public facing repo (README):**Â [https://github.com/gizmodata/gizmosql-public?tab=readme-ov-file](https://github.com/gizmodata/gizmosql-public?tab=readme-ov-file)
* **HomePage**:Â [https://gizmodata.com/gizmosql](https://gizmodata.com/gizmosql)
* **ProductHunt:**Â [https://www.producthunt.com/posts/gizmosql?embed=true&utm\_source=badge-featured&utm\_medium=badge&utm\_souce=badge-gizmosql](https://www.producthunt.com/posts/gizmosql?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-gizmosql)
* **Try GizmoSQL online:**Â [https://app.gizmodata.com](https://app.gizmodata.com)
* **GizmoSQL in action video:**Â [https://youtu.be/QSlE6FWlAaM](https://youtu.be/QSlE6FWlAaM)",1,1,Adventurous-Visit161,2025-04-08 13:30:54,https://www.reddit.com/r/dataengineering/comments/1judm9f/gizmosql_power_your_enterprise_analytics_with/,0,False,False,False,False
183,1jucavs,Is there any tool you use to keep track on the dates you need to reset API keys?,"I currently use teams events where I set a day on my calendar to update keys, but there has to be a better way. How do you guys do it?

Edit: The idea is to renew keys before they expire and there are no errors in the pipelines",1,7,dataguydream,2025-04-08 12:26:25,https://www.reddit.com/r/dataengineering/comments/1jucavs/is_there_any_tool_you_use_to_keep_track_on_the/,0,False,2025-04-08 19:04:12,False,False
184,1jub08u,How do you group your tables into pipelines?,"I was wondering how do data engineers in different company group their pipelines together ?

Usually tables need to be refreshed at some specific refresh rates. This means that some table upstream might require 1h refresh while downstream table might require daily.

I can see people grouping things by domain and running domain one after each other sequentially, but then this break the concept of having different refresh rate per table or domain. I can see table configure with multiple corn but then I see issues with needing to schedule offset in cron jobs. 

Like most of the domain are very close to each other so when creating them I might be mixing a lot of stuff together which would impact downstream.

Whatâ€™s your experience in structuring pipeline? Or any good reference I can read ?
",1,9,Commercial_Dig2401,2025-04-08 11:13:44,https://www.reddit.com/r/dataengineering/comments/1jub08u/how_do_you_group_your_tables_into_pipelines/,0,False,False,False,False
185,1ju7cmf,What is the best way to reflect data in clickhouse from MySQL other than the MySQL engine?,"Hi everyone, I am working on a project currently where we have a MySQL database. We are using clickhouse as our warehouse. 

What we need to achieve is to reflect the data from MySQL to clickhouse for certain tables. For this, I found a few ways and am looking to get some insights on which method has the most potential and if there are other methods as welp:

1. Use the MySQL engine in clickhouse. 

Pros: No need to store data in clickhouse as it can just proxy it directly from MySQL.

Cons: This however puts extra reads on MySQL and doesn't help us if MySQL ever goes down. 

2. Use signals to send the data to clickhouse whenever there is a change in MySQL.

Pros: We don't have a lot of tables currently so it's the quickest to setup. 

Cons: Extremely inefficient and not scalable. 

3. Use some sort of third party sink to achieve this. I have found this https://github.com/Altinity/clickhouse-sink-connector which seems to do the job but it has way too many open issues and not sure if it is reliable enough. Plus, it complicates our tech stack which we are looking not to do. 

I'm open to any other ideas. We would ideally not want to duplicate this data in clickhouse but if that's the last resort we would go for it. 

Thanks in advance. 

P.S, I am a beginner in data engineering so feel free to correct me if I've used some wrong jargons or if I am seriously deviating from the right path. ",1,9,Danyboi16,2025-04-08 06:49:07,https://www.reddit.com/r/dataengineering/comments/1ju7cmf/what_is_the_best_way_to_reflect_data_in/,0,False,False,False,False
186,1jun8gx,Designing a database ERP from scratch.,"My goal is to re create something like Oracle's Net-suite, are there any help full resources on how i can go about it. i have previously worked on simple Finance management systems but this one is more complicated. i need sample ERD's books or anything helpfull atp

",0,2,Specific_Bad8942,2025-04-08 20:09:25,https://www.reddit.com/r/dataengineering/comments/1jun8gx/designing_a_database_erp_from_scratch/,0,False,False,False,False
187,1juj0x6,Beginning Data Scientist in Azure needing some help (iot),"Hi all,

I currently am working on a new structure to save sensor data coming from Azure Iot Hub in Azure to store it into Azure Blob Storage for historical data, and Clickhouse for hot data with TTL (around half year). The sensor data is coming from different entities (e.g building1, boat1, boat2) and should be partioned by entity. The data weâ€™re processing daily is around 300-2 million records per day.

I know Azure Iot Hub is essentially a built-in Azure Hub. I had a few questions since Iâ€™ve tried multiple solutions. 

1. Normal message routing to Azure Blob
Issue: no custom partitioning on file structure (e.g entityid/timestamp_sensor/) it requires you to use the enqueued time. And there is no dead letter queue for fallback

2. IoT hub -> Azure Functions -> Blob Storage & Clickhouse
Issue: this should work correctly but I have not that much experience in Azure Functions, I tried creating a function with the IoT Hub template but it seems I need to also have an Event Hubs namespace which is not what I want. HTTP trigger is also not what I want. I donâ€™t find any good documentation on it aswell. I know I can maybe use Event Hubs trigger and use the Iot Hub connection string but I didnâ€™t manage to do this yet.

3. IoT hub -> Event Grid 
Someone suggested using Event Grid, however to my knowledge Event Grid is not used for telemetry data despite there being an option for. Is this beneficial? I donâ€™t really know what the flow would be since you canâ€™t use Event Grid to send data to Clickhouse. You would still need an Azure Functions.

4. IoT Hub -> Event Grid -> Event Hubs -> Azure Functions -> Azure Blob & Clickhouse
This one seemed the most appealing to me but I donâ€™t know if itâ€™s the smartest, it can get expensive (maybe).
But the idea here is that we use Event Grid for batching the data and to have a dead letter queue.
Arrived in Event Hubs we use an Azure Function to send the data to blob storage and clickhouse.

The only problem is I might need some delay to sending to Clickhouse & Blob Storage (around maybe every 15 minutes) to reduce the risks of memory usage in Clickhouse and to reduce costs.

Can someone help me out? Am I forgetting something crucial? I am a graduated data scientist, however I have no in depth experience with Azure.


",0,8,PaqS18,2025-04-08 17:18:34,https://www.reddit.com/r/dataengineering/comments/1juj0x6/beginning_data_scientist_in_azure_needing_some/,0,False,2025-04-08 18:22:26,False,False
188,1juibws,From Data Tyranny to Data Democratization,,0,0,growth_man,2025-04-08 16:50:26,https://moderndata101.substack.com/p/from-data-tyranny-to-data-democratization,0,False,False,False,False
189,1ju693k,Experienced data engineer looking to expand to devops,"Hey everyone, I've been a working a few years as a data engineer, I'd say I'm very comfortable in python (databricks), sql and git and have mostly worked in Azure. I would like to get comfortable with devops, setting up proper ci/cd, iac etc.

What resources would you recommend?

Where I work we 2 repos set up, an infratsructure repo that I am totally clueless about that is mostly terraform and another repo where we make changes to notebooks and pipelines etc whose structure makes more sense to me.

The whole thing was initially set up by consultants. My goal is really to understand how it was set up, why 2 different repos, how to change the ci/cd pipeline to add testing etc.

Thanks!",0,5,Lamyya,2025-04-08 05:33:41,https://www.reddit.com/r/dataengineering/comments/1ju693k/experienced_data_engineer_looking_to_expand_to/,0,False,False,False,False
190,1jui1dg,Mirror snowflake to PG,"Hi everyone,
Once per day, my team needs to mirror a lot of tables from snowflake to postgres. 
Currently, we are copying data with script written with GO.
do you familiar with tools, or any idea what is the best way to mirror the tables?",0,6,gal_12345,2025-04-08 16:38:21,https://www.reddit.com/r/dataengineering/comments/1jui1dg/mirror_snowflake_to_pg/,0,False,2025-04-08 17:13:11,False,False
191,1jukctt,Hot Take: You shouldn't be a data engineer if you've never been a data analyst,"You're better able to understand the needs and goals of what you're actually working towards when you being as an analyst. Not to mention the other skills that you develop whist being an analyst. Understanding downstream requirements helps build DE pipelines carefully keeping in mind the end goals.

What are you thoughts on this?",0,6,_areebpasha,2025-04-08 18:11:04,https://www.reddit.com/r/dataengineering/comments/1jukctt/hot_take_you_shouldnt_be_a_data_engineer_if_youve/,0,False,False,False,False

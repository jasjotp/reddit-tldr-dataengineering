,id,title,body,score,num_comments,author,created_utc,url,upvote_ratio,over_18,edited,spoiler,stickied,created_dt,hour,weekday,word_count,char_count,flesch_reading_ease,sentence_count,syllable_count,smog_index,has_url,has_code
0,1jmsyfl,SQLFlow: DuckDB for Streaming Data,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipelineâ€”ingestion, transformation, and enrichmentâ€”as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",69,16,turbolytics,2025-03-29 18:35:07,https://www.reddit.com/r/dataengineering/comments/1jmsyfl/sqlflow_duckdb_for_streaming_data/,0,False,2025-03-29 18:44:17,False,False,2025-03-29 18:35:07,18,Saturday,121.0,1106,9.55,11,262,12.2,1,0
1,1jmsfs6,Interactive Change Data Capture (CDC) Playground,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",51,4,dan_the_lion,2025-03-29 18:12:13,https://www.change-data-capture.com/,0,False,False,False,False,2025-03-29 18:12:13,18,Saturday,146.0,975,17.68,5,263,17.6,0,1
2,1jnc3zk,When to use a surrogate key instead of a primary key?,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",34,42,DataGhost404,2025-03-30 13:12:50,https://www.reddit.com/r/dataengineering/comments/1jnc3zk/when_to_use_a_surrogate_key_instead_of_a_primary/,0,False,False,False,False,2025-03-30 13:12:50,13,Sunday,165.0,980,50.57,8,264,14.4,0,0
3,1jn7lfp,Do I need to know software engineering to be a data engineer?,As title says ,27,61,Gloomy-Profession-19,2025-03-30 07:53:33,https://www.reddit.com/r/dataengineering/comments/1jn7lfp/do_i_need_to_know_software_engineering_to_be_a/,0,False,False,False,False,2025-03-30 07:53:33,7,Sunday,3.0,14,93.81,1,4,0.0,0,0
4,1jn1ko5,Should I stay in part-time role that uses Dagster or do internships in roles that use Airflow,"I am a part time data engineer/integrator who is in school at the moment. I work using Dagster, AWS, Snowflake, and Docker.

I was hoping Dagster would have roles where I lived but it seems everyone prefers Airflow.

Is it worth exploring data engineering internships that use Airflow at the expense of losing my current role? Do you guys see any growth in Dagster?",12,18,Apart-Plankton9951,2025-03-30 01:25:17,https://www.reddit.com/r/dataengineering/comments/1jn1ko5/should_i_stay_in_parttime_role_that_uses_dagster/,0,False,False,False,False,2025-03-30 01:25:17,1,Sunday,64.0,365,66.94,5,96,8.8,0,0
5,1jmu3u4,The classic problem of killing flies with a cannon? DW vs. LH,"I'm starting a new job (a startup that is doubling in size every year) and the IT director has already warned me that they have a lot of problems with data structure changes, both due to new implementations in internally developed software and in those developed externally.

My question is whether I should prepare the central architecture using data warehouse or lakehouse, since the current data volume is still quite small <500 GB, but as I said, constant changes in data structure have been a problem.

By the way, I will be the first data engineer on the analytics team.",9,7,EvenRelationship2110,2025-03-29 19:27:40,https://www.reddit.com/r/dataengineering/comments/1jmu3u4/the_classic_problem_of_killing_flies_with_a/,0,False,2025-03-29 19:41:38,False,False,2025-03-29 19:27:40,19,Saturday,100.0,576,46.14,3,154,14.6,0,0
6,1jnh7pu,A dbt column lineage visualization tool (with dynamic web visualization),"Hey dbt folks,

I'm a data engineer and use dbt on a day-to-day basis, my team and I were struggling to find a good open-source tool for user-friendly column-level lineage visualization that we could use daily, similar to what commercial solutions like dbt Cloud offer. So, I decided to start building one...

https://reddit.com/link/1jnh7pu/video/wcl9lru6zure1/player

You can find the repo [here](https://github.com/Fszta/dbt-column-lineage), and the package on [pypi](https://pypi.org/project/dbt-col-lineage/0.1.1/)

**Under the hood**

Basically, it works by combining dbt's manifest and catalog with some compiled SQL parsing magic (big shoutout to sqlglot!).

I've built it as a CLI, keeping the syntax similar to dbt-core, with upstream and downstream selectors.

    dbt-col-lineage --select stg_transactions.amount+ --format html

Right now, it supports:

* Interactive HTML visualizations
* DOT graph images
* Simple text output in the console

**What's next ?**

* Focus on compatibility with more SQL dialects
* Improve the parser to handle complex syntax specific to certain dialects
* Making the UI less... basic. It's kinda rough right now, plus some information could be added such as materialization type, col typing etc

Feel free to drop any feedback or open an issue on the [repo](https://github.com/Fszta/dbt-column-lineage/tree/main)! It's still super early, and any help for testing on other dialects would be awesome. It's only been tested on projects using Snowflake, DuckDB, and SQLite adapters so far.",5,0,Eastern-Ad-6431,2025-03-30 17:11:12,https://www.reddit.com/r/dataengineering/comments/1jnh7pu/a_dbt_column_lineage_visualization_tool_with/,0,False,2025-03-30 17:15:59,False,False,2025-03-30 17:11:12,17,Sunday,216.0,1529,28.43,12,387,13.3,1,1
7,1jmwfwu,creating big query source node in aws glue,"i have to send data from bigquery using aws glue to rds, i need to understand how to create big query source node in glue that can access a view from big query , is it by selecting table or custom query option... also what to add in materialization dataset , i dont have that ??? i have tried using table option , added view details there but then i get an error that view is not enabled in data preview section.

",7,1,lifealtering111,2025-03-29 21:14:57,https://www.reddit.com/r/dataengineering/comments/1jmwfwu/creating_big_query_source_node_in_aws_glue/,1,False,False,False,False,2025-03-29 21:14:57,21,Saturday,82.0,414,62.01,3,112,10.5,0,0
8,1jni0xm,Transitioning from DE to ML Engineer in 2025?,"I am a DE with 2 years of experience, but my background is mainly in statistics. I have been offered a position as an ML Engineer (de facto Data Scientist, but also working on deployment - it is a smaller IT department, so my scope of duties will be simply quite wide). 

The position is interesting, and there are multiple pros and cons to it (that I do not want to discuss in this post). However my question is a bit more general - in 2025, with all the LLMs performing quite well with code generation and fixing, which path would you say is more stable long-term - sticking to DE and becoming better and better at it, or moving more towards ML and doing data science projects?

Furthermore, I also wonder about growth in each field - in ML/DS, my fear is that I am not PhD nor excellent mathematician. In DE, on the other hand, my fear is lack of my solid CS/SWE foundations (as my background is more in statistics). 

Ultimately, it is just an honest question, as I am very curious of your perspective on the matter - does moving towards data science projects (XGBoost and other algorithms) in 2025 from DE (PySpark and Airflow) makes sense in 2025? Which path would you say is more reasonable, and what kind of growth I can expect for each position? Personally I am a bit reluctant to switch simply since I have already dedicated 2 years to growing as an DE, but on the other hand I also see how much more and more of my tasks can be automated. Thanks for tips and honest suggestions!",5,9,absurdherowaw,2025-03-30 17:47:04,https://www.reddit.com/r/dataengineering/comments/1jni0xm/transitioning_from_de_to_ml_engineer_in_2025/,1,False,False,False,False,2025-03-30 17:47:04,17,Sunday,278.0,1489,52.23,10,399,13.5,0,0
9,1jnfp5s,Applying for a Data Engineer Role at Docker Inc,"Hey everyone,  

I'm planning to apply for a Data Engineer position at Docker Inc. and was wondering if anyone here has experience with their application process. I have a solid background in Azure, Databricks, and data pipeline development, but Iâ€™d love to hear from others who have interviewed there or work there.

- What kind of technical questions should I expect?  
- Do they focus more on SQL, Python, or cloud architecture?  
- Any tips on system design or behavioral interviews?  

Would really appreciate any insights or advice!",5,3,Mysterious-Grape9534,2025-03-30 16:03:52,https://www.reddit.com/r/dataengineering/comments/1jnfp5s/applying_for_a_data_engineer_role_at_docker_inc/,0,False,False,False,False,2025-03-30 16:03:52,16,Sunday,88.0,538,50.73,7,143,11.5,0,0
10,1jn7g1f,3 years into Devops Engineering trying to move to Data Engineering,"I came to know that most of the skillset are matching 
in this 2 fields, apart from learning SQL, pyspark.

so would this be a better switching career ?",3,10,Blacktweeter,2025-03-30 07:42:11,https://www.reddit.com/r/dataengineering/comments/1jn7g1f/3_years_into_devops_engineering_trying_to_move_to/,0,False,False,False,False,2025-03-30 07:42:11,7,Sunday,29.0,152,82.65,2,37,0.0,0,0
11,1jn4ptx,Introducing AnuDB: A Lightweight Embedded Document Database,"**AnuDB** \- a lightweight, embedded document database.

# Key Features

* **Embedded & Serverless**: Runs directly within your application - no separate server process required
* **JSON Document Storage**: Store and query complex JSON documents with ease
* **High Performance**: Built on RocksDB's LSM-tree architecture for optimized write performance
* **C++11 Compatible**: Works with most embedded device environments that adopt C++11
* **Cross-Platform**: Supports both Windows and Linux (including embedded Linux platforms)
* **Flexible Querying**: Rich query capabilities including equality, comparison, logical operators and sorting
* **Indexing**: Create indexes on frequently accessed fields to speed up queries
* **Compression**: Optional ZSTD compression support to reduce storage footprint
* **Transactional Properties**: Inherits atomic operations and configurable durability from RocksDB
* **Import/Export**: Easy JSON import and export for data migration or integration with other systems

Checkout README for more info: [https://github.com/hash-anu/AnuDB](https://github.com/hash-anu/AnuDB)",3,0,Fine-Package-5488,2025-03-30 04:26:19,https://www.reddit.com/r/dataengineering/comments/1jn4ptx/introducing_anudb_a_lightweight_embedded_document/,1,False,False,False,False,2025-03-30 04:26:19,4,Sunday,139.0,1107,-42.73,2,272,0.0,1,1
12,1jnhk94,how to deal with azure vm nightmare?,"i am building data pipelines. i use azure vms for experimentation on sample data. when im not using them, i need to shut them off (working at bootstrapped startup).

when restarting my vm, it randomly fails. it says an allocation failure occurred due to capacity in the region (usually us-east). the only solution ive found is moving the resource to a new region, which takes 30â€“60 mins.

how do i prevent this issue in a cost-effective manner? can azure just allocate my vm to whatever region is available?

iâ€™ve tried to troubleshoot this issue for weeks with azure support, but to no avail.

thanks all! :)
",2,6,BigCountry1227,2025-03-30 17:26:28,https://www.reddit.com/r/dataengineering/comments/1jnhk94/how_to_deal_with_azure_vm_nightmare/,1,False,False,False,False,2025-03-30 17:26:28,17,Sunday,106.0,610,59.6,9,164,9.7,0,0
13,1jndfr9,Building a Cloud-Based Data Pipeline for Personal Use/Learning/Projects,"I am a data/functional analyst looking to get a better grasp on 'end-to-end data management and processing' (in quotations as that's the term my direct lead used when we aligned regarding my career objectives for the year, which definitely matches with my current interests).

Given this objective, my own interest in gaining a better grasp on general principles and functions of Data Engineering (and later on, Data Science), as well as a personal project I'm looking to finish for myself/my own portfolio by the end of the year, I'm looking for a cloud-based data pipeline solution for me to spend my role's training stipend on -- though I'm willing (and allowed) to use some of my personal funds on this as well since it's both for career progression and personal fun.

I started looking for small-scale subscriptions for some of the platforms we've previously used at work (AWS, Databricks), but the consensus I've gotten is that cost-wise, I'm potentially better off with multiple smaller platforms that I can just weave together, instead of going for platforms normally designed for enterprise use. And so I come here for help!

*(Though if this is not the right community for me to ask this, please let me know where instead)*

# Budget and Scope

* Base budget is a $200 stipend from our department, which refreshes at the end of the year
* I'm willing to add another $200-300/year, if necessary
* I'm from South-East Asia (converting our local currency, in case regional pricing matters)
* I prefer platforms/solutions that come with upfront costs instead of per-pull/use costs, so that's it's easier to manage the finances involved
* Languages I'm comfortable with are Python and SQL, since these are what I mainly use for work, but willing to learn basics of new languages where necessary 
* Since I will be using my personal computer, which is not that powerful currently, I am looking for a pipeline/platform that I can use completely via cloud (including running training models) 

# Learning Objectives 

*(aka things I want my platform/s to be able to do)*

**1. Data Gathering/Ingestion/Validation/Transformation/Storage**

* Obviously not too familiar with this step currently, so I ended up lumping a bunch of them together, but essentially ETL
* In terms of data I intend to use, I have three main sources I intend to work my way through, from simple to complex: **(1)** will be starting with ingesting CSV files I get from third-party free data providers like Kaggle, **(2)** then moving on to structured databases from sports stat sites such as BBREF, before  eventually work my way to **(3)** automating data collection from publicly available video games I normally play (Dota 2, Path of Exile -- if it matters)
* I want to be able to load these into SQL tables that I can regularly query form

**2. Data Querying and Extract Cloud Storage**

* I want to then be able to set it up so that I can easily query and then store-via-cloud what I need.
* To make my intention clear, most of my work experience is with AWS Athena and S3, so I hope to get platforms that function similarly.

**3. Cloud-Based Data Processing, Machine Learning, and Visualization**

* I currently do majority of my post-query data processing and modelling locally on my work laptop on Jupyter (via the Anaconda distribution), but one of my key objectives is learning to do all of these on cloud (especially since my peronal computer I'll be using would obviously not be as powerful as what I use for work)
* I definitely prefer Notebook-like environments, so perhaps something like AWS EMR
* My main experience at this level is mainly with Python (using specific packages such as Pandas, Numpy, Matplotlib, Numpy, Sklearn, etc.), but I'm looking to do more PySpark as well

**4. EXTRA/OPTIONAL: Dashboard Creation and Hosting**

* If I can get a platform/pipeline that will allow me to host interactive dashboards for me to just embed in my portfolio, that would be a plus, but I am very easily willing to drop this should it not fit my budget

#  Final Remarks

* Want to learn ETL, and cloud-based data processing on a personal data pipeline and processing platform/solution that also has SQL capabilities -- in line with my career and personal learning objectives for the year 
* Tried looking into personal subscriptions for 360 solutions like Databricks and AWS Sagemaker Unified Studio, but was told that for what I want I might be better off with patching together T2 solutions or something along those lines -- but I imagine this would be much more tricky to set up ",2,1,bacon9001,2025-03-30 14:20:16,https://www.reddit.com/r/dataengineering/comments/1jndfr9/building_a_cloudbased_data_pipeline_for_personal/,0,False,False,False,False,2025-03-30 14:20:16,14,Sunday,778.0,4571,1.95,11,1197,22.0,0,1
14,1jna701,"I am learning data engineering from a course. I am a fresher with no job experience, a commerce background, and a two-year gap.",Will any company hire me? What certificate could I obtain that would help me?,1,8,_winter_rabbit_,2025-03-30 11:12:06,https://www.reddit.com/r/dataengineering/comments/1jna701/i_am_learning_data_engineering_from_a_course_i_am/,0,False,False,False,False,2025-03-30 11:12:06,11,Sunday,14.0,77,64.37,2,22,0.0,0,0
15,1jnh5oy,"Struggling with Career Path â€“ Stuck in Java, Want to Return to Data Engineering (6.5 YOE)","I've been working in IT for the past 6.5 years. I started as a Java Developer for a year before transitioning into Data Engineering, where I worked with Airflow, GCP, Python, and SQL (BigQuery).

In June 2022, I joined my second company as a Data Engineer, but after just six months, the project was shelved, and I was moved to a Java-based project (Spring Boot, Kafka, etc.). This happened during a market downturn and layoffs, so I was grateful to still have a job.

Now, after two years in this role, I feel stuck. I struggle with coding, donâ€™t enjoy Java, and constantly feel like an imposter. I know for sure that I donâ€™t want to continue in Java and Spring Boot. However, Iâ€™ve stayed in this role because itâ€™s high-paying, and I have family responsibilities (supporting a family of five).

I want to transition back into Data Engineering, but now the job market expects a higher level of expertise given my experience and salary range. Iâ€™m unsure about the best way to upskill and make this switch without a major setback.

How can I strategically transition back into Data Engineering while balancing financial stability? Would love advice from those who have made similar career shifts.

Thanks in advance!",2,1,ishaheenkhan,2025-03-30 17:08:46,https://www.reddit.com/r/dataengineering/comments/1jnh5oy/struggling_with_career_path_stuck_in_java_want_to/,0,False,False,False,False,2025-03-30 17:08:46,17,Sunday,209.0,1214,55.13,13,324,11.5,0,0
16,1jmzte8,Need help for a small website design choices,"I am working on a website whose job is to serve data from MongoDb. Just textual data in row format nothing complicated. 

This is my current setup: client sends a request to cloudfront that manages the cache and triggers a lambda for a cache miss to query from MongoDB. I also use signedurl for security purposes for each request. 

I am not an expert that but I think cloud front can handle DDoS attacks etc. Does this setup work or do I need to bring in API Gateway into the fold? I donâ€™t have any user login etc. and no form on the website (no sql injection risk I guess). I donâ€™t know much about network security etc but have heard horror stories of websites getting hacked etc. Hence am a bit paranoid before launching the website. 

Based on some reading, I came to the conclusion that I need to use AWS WAF + API Gateway for dynamic queries and AWS + cloud front for static pages. And lambda should be associated with API Gateway to connect with MongoDB and API Gateway does rate limiting and caching (user authentication is no big a problem here). I wonder if cloudfront is even needed or should just stick with the current architecture I have. 

Need your suggestions. 




",1,2,Spiritual_Piccolo793,2025-03-29 23:56:02,https://www.reddit.com/r/dataengineering/comments/1jmzte8/need_help_for_a_small_website_design_choices/,0,False,False,False,False,2025-03-29 23:56:02,23,Saturday,215.0,1183,64.51,14,329,10.6,0,0
17,1jn7kvr,Junior vs Senior role,"What is the difference between a junior and senior in this role? How much can you really know in data engineering; get the data, clean it, dump it somewhere with a cloud service. 

But what would take someone from a junior role to a senior role? Is it just the number years of experience? 

",0,15,Gloomy-Profession-19,2025-03-30 07:52:21,https://www.reddit.com/r/dataengineering/comments/1jn7kvr/junior_vs_senior_role/,0,False,False,False,False,2025-03-30 07:52:21,7,Sunday,54.0,291,74.69,4,74,8.1,0,0
18,1jnc3zk,When to use a surrogate key instead of a primary key?,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",54,47,DataGhost404,2025-03-30 13:12:50,https://www.reddit.com/r/dataengineering/comments/1jnc3zk/when_to_use_a_surrogate_key_instead_of_a_primary/,0,False,False,False,False,2025-03-30 13:12:50,13,Sunday,165.0,980,50.57,8,264,14.4,0,0
19,1jn7lfp,Do I need to know software engineering to be a data engineer?,As title says ,46,71,Gloomy-Profession-19,2025-03-30 07:53:33,https://www.reddit.com/r/dataengineering/comments/1jn7lfp/do_i_need_to_know_software_engineering_to_be_a/,0,False,False,False,False,2025-03-30 07:53:33,7,Sunday,3.0,14,93.81,1,4,0.0,0,0
20,1jnh7pu,A dbt column lineage visualization tool (with dynamic web visualization),"Hey dbt folks,

I'm a data engineer and use dbt on a day-to-day basis, my team and I were struggling to find a good open-source tool for user-friendly column-level lineage visualization that we could use daily, similar to what commercial solutions like dbt Cloud offer. So, I decided to start building one...

https://reddit.com/link/1jnh7pu/video/wcl9lru6zure1/player

You can find the repo [here](https://github.com/Fszta/dbt-column-lineage), and the package on [pypi](https://pypi.org/project/dbt-col-lineage/0.1.1/)

**Under the hood**

Basically, it works by combining dbt's manifest and catalog with some compiled SQL parsing magic (big shoutout to sqlglot!).

I've built it as a CLI, keeping the syntax similar to dbt-core, with upstream and downstream selectors.

    dbt-col-lineage --select stg_transactions.amount+ --format html

Right now, it supports:

* Interactive HTML visualizations
* DOT graph images
* Simple text output in the console

**What's next ?**

* Focus on compatibility with more SQL dialects
* Improve the parser to handle complex syntax specific to certain dialects
* Making the UI less... basic. It's kinda rough right now, plus some information could be added such as materialization type, col typing etc

Feel free to drop any feedback or open an issue on the [repo](https://github.com/Fszta/dbt-column-lineage/tree/main)! It's still super early, and any help for testing on other dialects would be awesome. It's only been tested on projects using Snowflake, DuckDB, and SQLite adapters so far.",35,3,Eastern-Ad-6431,2025-03-30 17:11:12,https://www.reddit.com/r/dataengineering/comments/1jnh7pu/a_dbt_column_lineage_visualization_tool_with/,0,False,2025-03-30 17:15:59,False,False,2025-03-30 17:11:12,17,Sunday,216.0,1529,28.43,12,387,13.3,1,1
21,1jnktzw,Passed DP-203 -- some thoughts on its retiring,"i took the Azure DP-203 last week â€” of course, itâ€™s retiring literally tomorrow. But I figured it is indeed a very broad certification and so it can give a ""grounding"" scope in Azure D.E.

Also, I think it's still super early to go full Fabric (DP-600 or even DP-700), because the job demand is still not really there. Most jobs still demand strong grounding in Azure services even in the wake of Fabric adoption (POCingâ€¦).

So of course here, itâ€™s retiring literally tomorrow unfortunately. I have passed the exam with a high score (900+). Also, I have worked (during internship) directly with MS Fabric only. So I would say some skills actually transfer quite nicely (ex: ADF ~ FDF).

---

### Some notes on resources for future exams:

I have relied primarily on [@tybulonazure](https://www.youtube.com/@tybulonazure)â€™s excellent YouTube channel (DP-203 playlist). Itâ€™s really great (watch on 1.8x â€“ 2x speed).  
Now going back to Fabric, I have seen he has pivoted to Fabric-centric content â€” also a great news!

I also used the official â€œGuideâ€ book (2024 version), which I found to be a surprisingly good way of structuring your learning. I hope equivalents for Fabric will be similar (TBSâ€¦).

---

The labs on Microsoft Learn are honestly **poorly designed** for what they offer.  
**Tip:** @tybul has video labs too â€” *use these*.  
And for the exams, always focus on **conceptual understanding**, not rote memorization.

Another **important (and mostly ignored)** tip:  
Focus on the **â€œbest practicesâ€** sections of Azure services in Microsoft Learn â€” Iâ€™ve read a lot of MS documentation, and those parts are often more helpful on the exam than the main pages.

---

**Examtopics** is obviously very helpful â€” but **read the comments**, theyâ€™re essential!

---

Finally, I do think itâ€™s a shame itâ€™s retiring â€” because the â€œtraditionalâ€ Azure environment knowledge seems to be a sort of industry standard for companies. Also, the Fabric pricing model seems quite aggressive.

So for juniors, it would have been really good to still be able to have this background knowledge as a base layer.",16,2,EducatedEarthian-99,2025-03-30 19:48:14,https://www.reddit.com/r/dataengineering/comments/1jnktzw/passed_dp203_some_thoughts_on_its_retiring/,0,False,False,False,False,2025-03-30 19:48:14,19,Sunday,344.0,2100,56.86,23,525,11.2,1,0
22,1jn1ko5,Should I stay in part-time role that uses Dagster or do internships in roles that use Airflow,"I am a part time data engineer/integrator who is in school at the moment. I work using Dagster, AWS, Snowflake, and Docker.

I was hoping Dagster would have roles where I lived but it seems everyone prefers Airflow.

Is it worth exploring data engineering internships that use Airflow at the expense of losing my current role? Do you guys see any growth in Dagster?",11,18,Apart-Plankton9951,2025-03-30 01:25:17,https://www.reddit.com/r/dataengineering/comments/1jn1ko5/should_i_stay_in_parttime_role_that_uses_dagster/,0,False,False,False,False,2025-03-30 01:25:17,1,Sunday,64.0,365,66.94,5,96,8.8,0,0
23,1jnisk7,What is expected of me as a Junior Data Engineer in 2025?,"Hello all, 

I've been interviewing for a proper Junior Data Engineer position and have been doing well in the rounds so far. I've done my recruiter call, HR call and coding assessment. Waiting on the 4th.

I want to be great. I am willing to learn from those of you who are more experienced than me.  
  
Can anyone share examples from their own careers on attitude, communication, soft skills, time management, charisma, willingness to learn and other soft skills that I should keep in mind. Or maybe what I should not do instead.

How should I approach the technical side? There are 1000's of technologies to learn. So I have been learning basics with soft skills and hoping everything works out.

3 years ago I had a labour job and did well in that too. So this grind has caused me to rewire my brain to work in tech and corporate work. I am aiming for 20 years more in this field.

Any insights are appreciated.

Thanks!",9,11,turnwol7,2025-03-30 18:19:54,https://www.reddit.com/r/dataengineering/comments/1jnisk7/what_is_expected_of_me_as_a_junior_data_engineer/,0,False,False,False,False,2025-03-30 18:19:54,18,Sunday,167.0,925,76.32,14,234,9.8,0,0
24,1jni0xm,Transitioning from DE to ML Engineer in 2025?,"I am a DE with 2 years of experience, but my background is mainly in statistics. I have been offered a position as an ML Engineer (de facto Data Scientist, but also working on deployment - it is a smaller IT department, so my scope of duties will be simply quite wide). 

The position is interesting, and there are multiple pros and cons to it (that I do not want to discuss in this post). However my question is a bit more general - in 2025, with all the LLMs performing quite well with code generation and fixing, which path would you say is more stable long-term - sticking to DE and becoming better and better at it, or moving more towards ML and doing data science projects?

Furthermore, I also wonder about growth in each field - in ML/DS, my fear is that I am not PhD nor excellent mathematician. In DE, on the other hand, my fear is lack of my solid CS/SWE foundations (as my background is more in statistics). 

Ultimately, it is just an honest question, as I am very curious of your perspective on the matter - does moving towards data science projects (XGBoost and other algorithms) in 2025 from DE (PySpark and Airflow) makes sense in 2025? Which path would you say is more reasonable, and what kind of growth I can expect for each position? Personally I am a bit reluctant to switch simply since I have already dedicated 2 years to growing as an DE, but on the other hand I also see how much more and more of my tasks can be automated. Thanks for tips and honest suggestions!",6,11,absurdherowaw,2025-03-30 17:47:04,https://www.reddit.com/r/dataengineering/comments/1jni0xm/transitioning_from_de_to_ml_engineer_in_2025/,0,False,False,False,False,2025-03-30 17:47:04,17,Sunday,278.0,1489,52.23,10,399,13.5,0,0
25,1jnh5oy,"Struggling with Career Path â€“ Stuck in Java, Want to Return to Data Engineering (6.5 YOE)","I've been working in IT for the past 6.5 years. I started as a Java Developer for a year before transitioning into Data Engineering, where I worked with Airflow, GCP, Python, and SQL (BigQuery).

In June 2022, I joined my second company as a Data Engineer, but after just six months, the project was shelved, and I was moved to a Java-based project (Spring Boot, Kafka, etc.). This happened during a market downturn and layoffs, so I was grateful to still have a job.

Now, after two years in this role, I feel stuck. I struggle with coding, donâ€™t enjoy Java, and constantly feel like an imposter. I know for sure that I donâ€™t want to continue in Java and Spring Boot. However, Iâ€™ve stayed in this role because itâ€™s high-paying, and I have family responsibilities (supporting a family of five).

I want to transition back into Data Engineering, but now the job market expects a higher level of expertise given my experience and salary range. Iâ€™m unsure about the best way to upskill and make this switch without a major setback.

How can I strategically transition back into Data Engineering while balancing financial stability? Would love advice from those who have made similar career shifts.

Thanks in advance!",5,9,ishaheenkhan,2025-03-30 17:08:46,https://www.reddit.com/r/dataengineering/comments/1jnh5oy/struggling_with_career_path_stuck_in_java_want_to/,0,False,False,False,False,2025-03-30 17:08:46,17,Sunday,209.0,1214,55.13,13,324,11.5,0,0
26,1jn4ptx,Introducing AnuDB: A Lightweight Embedded Document Database,"**AnuDB** \- a lightweight, embedded document database.

# Key Features

* **Embedded & Serverless**: Runs directly within your application - no separate server process required
* **JSON Document Storage**: Store and query complex JSON documents with ease
* **High Performance**: Built on RocksDB's LSM-tree architecture for optimized write performance
* **C++11 Compatible**: Works with most embedded device environments that adopt C++11
* **Cross-Platform**: Supports both Windows and Linux (including embedded Linux platforms)
* **Flexible Querying**: Rich query capabilities including equality, comparison, logical operators and sorting
* **Indexing**: Create indexes on frequently accessed fields to speed up queries
* **Compression**: Optional ZSTD compression support to reduce storage footprint
* **Transactional Properties**: Inherits atomic operations and configurable durability from RocksDB
* **Import/Export**: Easy JSON import and export for data migration or integration with other systems

Checkout README for more info: [https://github.com/hash-anu/AnuDB](https://github.com/hash-anu/AnuDB)",3,0,Fine-Package-5488,2025-03-30 04:26:19,https://www.reddit.com/r/dataengineering/comments/1jn4ptx/introducing_anudb_a_lightweight_embedded_document/,0,False,False,False,False,2025-03-30 04:26:19,4,Sunday,139.0,1107,-42.73,2,272,0.0,1,1
27,1jnj03e,Serialisation and de-serialisation?,"I just got to know that even in today's OLAP era, but while communicating b/w the systems internally they convert it to row based storage even if the warehouses are columnar type...
This made me sickkk I never knew this at all!

So does this mean serialisation and de-serialisation?? 
I see these terms vary across many architecture ex: In spark they mention these terminologies when the data needs to searched at different instances.. they say data needs to be de-serialised which takes time...

But I am not clear how do I need to think when I hear these terminologies!!!

Source:
https://www.linkedin.com/posts/dipankar-mazumdar_dataengineering-softwareengineering-activity-7307566420828065793-LuVZ?utm_source=share&utm_medium=member_android&rcm=ACoAADeacu0BUNpPkSGeT5J-UjR35-nvjHNjhTM",4,2,Excellent-Level-9626,2025-03-30 18:29:00,https://www.reddit.com/r/dataengineering/comments/1jnj03e/serialisation_and_deserialisation/,0,False,False,False,False,2025-03-30 18:29:00,18,Sunday,101.0,788,29.04,6,188,11.2,1,0
28,1jnhk94,how to deal with azure vm nightmare?,"i am building data pipelines. i use azure vms for experimentation on sample data. when im not using them, i need to shut them off (working at bootstrapped startup).

when restarting my vm, it randomly fails. it says an allocation failure occurred due to capacity in the region (usually us-east). the only solution ive found is moving the resource to a new region, which takes 30â€“60 mins.

how do i prevent this issue in a cost-effective manner? can azure just allocate my vm to whatever region is available?

iâ€™ve tried to troubleshoot this issue for weeks with azure support, but to no avail.

thanks all! :)
",3,12,BigCountry1227,2025-03-30 17:26:28,https://www.reddit.com/r/dataengineering/comments/1jnhk94/how_to_deal_with_azure_vm_nightmare/,0,False,False,False,False,2025-03-30 17:26:28,17,Sunday,106.0,610,59.6,9,164,9.7,0,0
29,1jn7g1f,3 years into Devops Engineering trying to move to Data Engineering,"I came to know that most of the skillset are matching 
in this 2 fields, apart from learning SQL, pyspark.

so would this be a better switching career ?",1,11,Blacktweeter,2025-03-30 07:42:11,https://www.reddit.com/r/dataengineering/comments/1jn7g1f/3_years_into_devops_engineering_trying_to_move_to/,0,False,False,False,False,2025-03-30 07:42:11,7,Sunday,29.0,152,82.65,2,37,0.0,0,0
30,1jnrfgm,dezoomcamp project,"Real-world data engineering practice! ðŸ—ï¸ Built an end-to-end **data pipeline** using GCP, BigQuery, dbt, and Airflow to analyze Airbnb trends. Learning + hands-on = the best combo! ðŸ’¡  
\#dezoomcamp #dataengineering #learningbydoing",0,2,Impressive_Trip1382,2025-03-31 00:57:43,https://www.reddit.com/r/dataengineering/comments/1jnrfgm/dezoomcamp_project/,0,False,False,False,False,2025-03-31 00:57:43,0,Monday,31.0,231,30.73,4,53,10.1,0,0
31,1jnrf68,dezoomcamp project,"How I made my Airbnb analysis efficient:  
ðŸ”¹ **Staging layer**: Standardized & cleaned raw data  
ðŸ”¹ **Core layer**: Built fact & dimension tables  
ðŸ”¹ **Analytics dataset**: Ready for insights!  
\#dezoomcamp #analyticsengineering",0,1,Impressive_Trip1382,2025-03-31 00:57:17,https://www.reddit.com/r/dataengineering/comments/1jnrf68/dezoomcamp_project/,0,False,False,False,False,2025-03-31 00:57:17,0,Monday,31.0,229,-5.68,1,56,0.0,0,0
32,1jnrev3,dezoomcamp project,"Orchestrating **dbt runs with Airflow** ensures transformations only happen when new data arrives. No wasted compute, no stale dashboardsâ€”just efficient workflows! ðŸ”¥  
\#dezoomcamp #dbtcore #airflow",0,0,Impressive_Trip1382,2025-03-31 00:56:49,https://www.reddit.com/r/dataengineering/comments/1jnrev3/dezoomcamp_project/,0,False,False,False,False,2025-03-31 00:56:49,0,Monday,25.0,198,29.52,3,47,10.5,0,0
33,1jnrehd,dezoomcamp project,"End-to-end pipeline **deployment steps**:  
1ï¸âƒ£ Terraform: Set up GCS & BigQuery  
2ï¸âƒ£ Python: Load data to GCS  
3ï¸âƒ£ dbt: Transform data  
4ï¸âƒ£ Airflow: Orchestrate  
5ï¸âƒ£ Looker: Visualize ðŸ“Š  
\#dezoomcamp",0,0,Impressive_Trip1382,2025-03-31 00:56:16,https://www.reddit.com/r/dataengineering/comments/1jnrehd/dezoomcamp_project/,0,False,False,False,False,2025-03-31 00:56:16,0,Monday,29.0,205,35.61,1,45,0.0,0,0
34,1jnre2y,dezoomcamp project,"Implemented **SCD Type 2** in dbt for tracking historical host changes. Now I can analyze how hosts evolve over time and their impact on reviews and pricing! ðŸ”„  
\#dezoomcamp #dbt #datawarehousing",0,0,Impressive_Trip1382,2025-03-31 00:55:39,https://www.reddit.com/r/dataengineering/comments/1jnre2y/dezoomcamp_project/,0,False,False,False,False,2025-03-31 00:55:39,0,Monday,31.0,196,61.33,3,49,10.5,0,0
35,1jnqdug,First Major DE Project,"Hello everyone, I am working on this end-to-end process for processing Pitch-by-Pitch data with some inner workings for also enabling analytics to be done directly from the system with little set up. I began this project because I use different computers and it became an issue switching from device to device when it came to working on these projects, and I can use it as my school project to cut down on time spent. I have it posted on my GitHub here and would love for any feedback any of you could have on the overall direction of this project and ways I could improve this Thank you!

Github Link: [https://github.com/jwolfe972/mlb\_prediction\_app](https://github.com/jwolfe972/mlb_prediction_app)",1,1,scuffed12s,2025-03-31 00:02:37,https://www.reddit.com/r/dataengineering/comments/1jnqdug/first_major_de_project/,0,False,False,False,False,2025-03-31 00:02:37,0,Monday,111.0,703,43.26,4,175,12.6,1,0
36,1jnlsu5,Example for complex data pipeline,"Hi community,

After working as a data analyst for several years, I've noticed a gap in tools for interactively exploring complex ETL pipeline dependencies. Many solutions handle smaller pipelines well, but struggle with 200+ tasks.

For larger pipelines, we need robust traversal features, like collapsing/expanding nodes to focus on specific sections during development or debugging. I've used `networkx` and `mermaid` for subgraph visualization, but an interactive UI would be more efficient.

I've developed a prototype and am seeking example cases to test it. I'm looking for pipelines with 60+ tasks and complex dependencies. I'm particularly interested in the challenges you face with these large pipelines. At my workplace, we have a 1500+ task pipeline, and I'm curious if this is a typical scale.

Specifically, I'd like to know:

* What challenges do you face when visualizing and managing large pipelines?
* Are pipelines with 1500+ tasks common?
* What features would you find most useful in a tool for this purpose?

If you can share sanitized examples or describe the complexity of your pipelines, it would be very helpful.

Thanks.",2,2,EliyahuRed,2025-03-30 20:29:36,https://www.reddit.com/r/dataengineering/comments/1jnlsu5/example_for_complex_data_pipeline/,0,False,False,False,False,2025-03-30 20:29:36,20,Sunday,180.0,1147,47.99,12,294,12.2,0,0
37,1jnlm9u,"As a data analytics/data science professional, how much data engineering am I supposed to know? Any advice is greatly appreciated","I am so confused. I am looking for roles in BI/analytics/data science and it seems data engineering has just taken over the entire thing or most of it, atleast. BI and DBA is just gone and everyone now wants cloud dev ops and data engineering stack as part of a BI/analytics role? Am I now supposed to become a software engineer and learn all this stack (airflow, airtable, dbt, hadoop, pyspark, cloud, devops etc?) - this seems so overwhelming to me! How am I supposed to know all this in addition to data science, strategy, stakeholder management, program management, team leadership....so damn exhausting! Any advice on how to navigate the job market and land BI/data analytics/data science roles and how much realistic data engineering am I supposed to learn?",0,6,CreditArtistic1932,2025-03-30 20:21:44,https://www.reddit.com/r/dataengineering/comments/1jnlm9u/as_a_data_analyticsdata_science_professional_how/,0,False,False,False,False,2025-03-30 20:21:44,20,Sunday,129.0,763,55.24,8,209,12.2,0,0
38,1jndfr9,Building a Cloud-Based Data Pipeline for Personal Use/Learning/Projects,"I am a data/functional analyst looking to get a better grasp on 'end-to-end data management and processing' (in quotations as that's the term my direct lead used when we aligned regarding my career objectives for the year, which definitely matches with my current interests).

Given this objective, my own interest in gaining a better grasp on general principles and functions of Data Engineering (and later on, Data Science), as well as a personal project I'm looking to finish for myself/my own portfolio by the end of the year, I'm looking for a cloud-based data pipeline solution for me to spend my role's training stipend on -- though I'm willing (and allowed) to use some of my personal funds on this as well since it's both for career progression and personal fun.

I started looking for small-scale subscriptions for some of the platforms we've previously used at work (AWS, Databricks), but the consensus I've gotten is that cost-wise, I'm potentially better off with multiple smaller platforms that I can just weave together, instead of going for platforms normally designed for enterprise use. And so I come here for help!

*(Though if this is not the right community for me to ask this, please let me know where instead)*

# Budget and Scope

* Base budget is a $200 stipend from our department, which refreshes at the end of the year
* I'm willing to add another $200-300/year, if necessary
* I'm from South-East Asia (converting our local currency, in case regional pricing matters)
* I prefer platforms/solutions that come with upfront costs instead of per-pull/use costs, so that's it's easier to manage the finances involved
* Languages I'm comfortable with are Python and SQL, since these are what I mainly use for work, but willing to learn basics of new languages where necessary 
* Since I will be using my personal computer, which is not that powerful currently, I am looking for a pipeline/platform that I can use completely via cloud (including running training models) 

# Learning Objectives 

*(aka things I want my platform/s to be able to do)*

**1. Data Gathering/Ingestion/Validation/Transformation/Storage**

* Obviously not too familiar with this step currently, so I ended up lumping a bunch of them together, but essentially ETL
* In terms of data I intend to use, I have three main sources I intend to work my way through, from simple to complex: **(1)** will be starting with ingesting CSV files I get from third-party free data providers like Kaggle, **(2)** then moving on to structured databases from sports stat sites such as BBREF, before  eventually work my way to **(3)** automating data collection from publicly available video games I normally play (Dota 2, Path of Exile -- if it matters)
* I want to be able to load these into SQL tables that I can regularly query form

**2. Data Querying and Extract Cloud Storage**

* I want to then be able to set it up so that I can easily query and then store-via-cloud what I need.
* To make my intention clear, most of my work experience is with AWS Athena and S3, so I hope to get platforms that function similarly.

**3. Cloud-Based Data Processing, Machine Learning, and Visualization**

* I currently do majority of my post-query data processing and modelling locally on my work laptop on Jupyter (via the Anaconda distribution), but one of my key objectives is learning to do all of these on cloud (especially since my peronal computer I'll be using would obviously not be as powerful as what I use for work)
* I definitely prefer Notebook-like environments, so perhaps something like AWS EMR
* My main experience at this level is mainly with Python (using specific packages such as Pandas, Numpy, Matplotlib, Numpy, Sklearn, etc.), but I'm looking to do more PySpark as well

**4. EXTRA/OPTIONAL: Dashboard Creation and Hosting**

* If I can get a platform/pipeline that will allow me to host interactive dashboards for me to just embed in my portfolio, that would be a plus, but I am very easily willing to drop this should it not fit my budget

#  Final Remarks

* Want to learn ETL, and cloud-based data processing on a personal data pipeline and processing platform/solution that also has SQL capabilities -- in line with my career and personal learning objectives for the year 
* Tried looking into personal subscriptions for 360 solutions like Databricks and AWS Sagemaker Unified Studio, but was told that for what I want I might be better off with patching together T2 solutions or something along those lines -- but I imagine this would be much more tricky to set up ",1,1,bacon9001,2025-03-30 14:20:16,https://www.reddit.com/r/dataengineering/comments/1jndfr9/building_a_cloudbased_data_pipeline_for_personal/,0,False,False,False,False,2025-03-30 14:20:16,14,Sunday,778.0,4571,1.95,11,1197,22.0,0,1
39,1jnrdm2,dezoomcamp project,"Ever wondered why some Airbnb listings are way more expensive? My project explores price trends, demand drivers, and traveler-friendly insights based on NYC Airbnb data! ðŸ ðŸ“ˆ

\#dezoomcamp #dataanalysis #airbnb",0,2,Impressive_Trip1382,2025-03-31 00:54:57,https://www.reddit.com/r/dataengineering/comments/1jnrdm2/dezoomcamp_project/,0,False,False,False,False,2025-03-31 00:54:57,0,Monday,29.0,208,53.58,3,48,9.7,0,0
40,1jnrd0h,DEZOOMCAMP Project,"Lessons learned from loading Airbnb data into GCP:  
âœ”ï¸ Use **autodetect schema** in BigQuery for flexibility  
âœ”ï¸ Handle CSV quirks with proper configs  
âœ”ï¸ Optimize storage with partitioning  
\#dezoomcamp #gcp #dataengineering",0,0,Impressive_Trip1382,2025-03-31 00:54:01,https://www.reddit.com/r/dataengineering/comments/1jnrd0h/dezoomcamp_project/,0,False,False,False,False,2025-03-31 00:54:01,0,Monday,31.0,229,9.22,1,55,0.0,0,0
41,1jnrb6y,Data Camp Data engineering certification help,Hi Iâ€™ve been working through the data engineer in SQL track on DataCamp and decided to try the associate certification exam. There was quite a bit that didnâ€™t seem to have been covered in the courses. Can anyone recommend any other resources to help me plug the gap please? Thanks,0,1,Bodhisattva-Wannabe,2025-03-31 00:51:17,https://www.reddit.com/r/dataengineering/comments/1jnrb6y/data_camp_data_engineering_certification_help/,0,False,False,False,False,2025-03-31 00:51:17,0,Monday,50.0,280,54.52,3,79,13.0,0,0
42,1jnq405,Unstructured to Structured,"Hi folks,
I know there have been some discussions on this topic; but given we had lot of development in technology and business space; would like to get your input on
1. How much is this still a problem?
2. Do agentic workflows open up some new challenges?
3. Is there any need to convert large excel files into  SQL tables?

",0,1,Fantastic-Cup-990,2025-03-30 23:48:51,https://www.reddit.com/r/dataengineering/comments/1jnq405/unstructured_to_structured/,0,False,False,False,False,2025-03-30 23:48:51,23,Sunday,61.0,326,72.87,4,86,9.5,0,0
43,1jnj9bw,Collect old news articles from mainstream media.,"What is the best way to collect like >10 years old news articles from the mainstream media and newspapers?
 ",0,1,SaintPellegrino4You,2025-03-30 18:39:53,https://www.reddit.com/r/dataengineering/comments/1jnj9bw/collect_old_news_articles_from_mainstream_media/,0,False,False,False,False,2025-03-30 18:39:53,18,Sunday,19.0,108,69.11,1,27,0.0,0,0
44,1jna701,"I am learning data engineering from a course. I am a fresher with no job experience, a commerce background, and a two-year gap.",Will any company hire me? What certificate could I obtain that would help me?,0,9,_winter_rabbit_,2025-03-30 11:12:06,https://www.reddit.com/r/dataengineering/comments/1jna701/i_am_learning_data_engineering_from_a_course_i_am/,0,False,False,False,False,2025-03-30 11:12:06,11,Sunday,14.0,77,64.37,2,22,0.0,0,0
45,1jnmrm5,Data Stack,"What do you think about the progress into [agentic data stack](https://goagentdata.com/)?  
",0,1,jb_nb,2025-03-30 21:11:28,https://www.reddit.com/r/dataengineering/comments/1jnmrm5/data_stack/,0,False,False,False,False,2025-03-30 21:11:28,21,Sunday,11.0,92,26.47,1,22,0.0,1,0
46,1jnjmkl,Why is table extraction still not solved by modern multimodal models?,"There is a lot of hype around multimodal models, such as Qwen 2.5 VL or Omni, GOT, SmolDocling, etc. I would like to know if others made a similar experience in practice: While they can do impressive things, they still struggle with table extraction, in cases which are straight-forward for humans.

Attached is a simple example, all I need is a reconstruction of the table as a flat CSV, preserving empty all empty cells correctly. Which open source model is able to do that?

https://preview.redd.it/xg8f0624jvre1.png?width=1650&format=png&auto=webp&s=4c0a22d833cb308534abf4dc38b1b12581a6e227

",0,0,Electronic-Letter592,2025-03-30 18:55:50,https://www.reddit.com/r/dataengineering/comments/1jnjmkl/why_is_table_extraction_still_not_solved_by/,0,False,False,False,False,2025-03-30 18:55:50,18,Sunday,85.0,596,45.76,5,143,12.7,1,0
47,1jn7kvr,Junior vs Senior role,"What is the difference between a junior and senior in this role? How much can you really know in data engineering; get the data, clean it, dump it somewhere with a cloud service. 

But what would take someone from a junior role to a senior role? Is it just the number years of experience? 

",0,18,Gloomy-Profession-19,2025-03-30 07:52:21,https://www.reddit.com/r/dataengineering/comments/1jn7kvr/junior_vs_senior_role/,0,False,False,False,False,2025-03-30 07:52:21,7,Sunday,54.0,291,74.69,4,74,8.1,0,0
48,1jo8l7i,Happy Monday,,368,24,Vautlo,2025-03-31 17:19:45,https://i.redd.it/l7ql2ezx62se1.png,0,False,False,False,False,2025-03-31 17:19:45,17,Monday,,0,206.84,1,0,0.0,0,0
49,1jnrpd7,Does your company use both Databricks & Snowflake? How does the architecture look like?,I'm just curious about this because these 2 companies have been very popular over the last few years.,78,38,NefariousnessSea5101,2025-03-31 01:12:17,https://www.reddit.com/r/dataengineering/comments/1jnrpd7/does_your_company_use_both_databricks_snowflake/,0,False,False,False,False,2025-03-31 01:12:17,1,Monday,18.0,101,53.21,1,28,0.0,0,0
50,1jnuhdm,"Now, I know why am I struggling...","And why my coleagues were able to present outputs more eagerly than I do:

I am trying to deliver a 'perfect data set', which is too much to expect from a fully on-prem DW/DS filled with couple of thousands of tables with zero data documentation and governance in all 30 years of operation...

I am not even a perfectionist myself so IDK what lead me to this point. Probably I trusted myself way too much? Probably I am trying to prove I am ""one of the best data engineers they had""? (I am still on probation and this is my 4th month here)

The company is fine and has continued to prosper over the decades without much data engineering. They just looked at the big numbers and made decisions based of it intuitively. 

Then here I am, just spent hours today looking for the excess 0.4$ from a total revenue of 40Million$ from a report I broke down to a FactTable. Mathematically, this is just peanuts. I should have let it go and used my time effectively on other things.

I am letting go of this perfectionism. 

I want to get regularized in this company. I really, really want to.",42,11,noSugar-lessSalt,2025-03-31 03:47:38,https://www.reddit.com/r/dataengineering/comments/1jnuhdm/now_i_know_why_am_i_struggling/,0,False,False,False,False,2025-03-31 03:47:38,3,Monday,200.0,1083,64.3,13,295,10.4,0,1
51,1jo0001,Prefect - too expensive?,"Hey guys,
weâ€™re currently using self-hosted Airflow for our internal ETL and data workflows. It gets the job done, but I never really liked it. Feels too far away from actual Python, gets overly complex at times, and local development and testing is honestly a nightmare.

I recently stumbled upon Prefect and gave the self-hosted version a try. Really liked what I saw. Super Pythonic, easy to set up locally, modern UI - just felt right from the start.

But the problem is: the open-source version doesnâ€™t offer user management or logging, so weâ€™d need the Cloud version. Pricing would be around 30k USD per year, which is way above what we pay for Airflow. Even with a discount, it would still be too much for us.

Is there any way to make the community version work for a small team? Usermanagement and Audit-Logs is definitely a must for us. Or is Prefect just not realistic without going Cloud?

Would be a shame, because I really liked their approach.

If not Prefect, any tips on making Airflow easier for local dev and testing?",41,47,thsde,2025-03-31 10:26:08,https://www.reddit.com/r/dataengineering/comments/1jo0001/prefect_too_expensive/,0,False,False,False,False,2025-03-31 10:26:08,10,Monday,183.0,1036,66.74,14,272,9.4,0,0
52,1jo7tps,what's your opinion?,"iâ€™m designing functions to clean data for two separate pipelines: one has small string inputs, the other has medium-size pandas inputs. both pipelines require the same manipulations.

for example, which is a better design: clean\_v0 or clean\_v1?

that is, should i standardize object types inside or outside the cleaning function?

thanks all! this community has been a life saver :)

",26,36,BigCountry1227,2025-03-31 16:48:55,https://i.redd.it/woy8nd4412se1.png,0,False,False,False,False,2025-03-31 16:48:55,16,Monday,60.0,386,59.5,5,94,10.4,0,1
53,1jo564b,Is using Snowflake for near real time or hourly events an overkill ?,"I've been using Snowflake for a while for just data warehousing projects (analytics) where I update the data twice per day.

  
I have now a Use Case where I need to do some reads and writes to sql tables every hour (every 10 min would be even better but not necessary). **The purpose is not only analytics but also operational.**

  
I estimate every request **costs me 0.01$,** which is quite high.

I was thinking of using **Postgresql** instead of Snowflake but I will need to invest time and resources to build it and maintain it. 

  
I was wondering if you can give me **your opinion** about building **near real time or hourly projects** in Snowflake. Does it make sense ? or is it a clear no-go ?

  
Thanks!

  
",14,12,finerius,2025-03-31 14:57:29,https://www.reddit.com/r/dataengineering/comments/1jo564b/is_using_snowflake_for_near_real_time_or_hourly/,0,False,False,False,False,2025-03-31 14:57:29,14,Monday,128.0,722,65.73,9,184,10.0,0,0
54,1jnzyop,AWS Data Engineering from Azure,"Hi Folks,

  
14+ years into data engineering with Onprem for 10 and 4 years into Azure DE with mainly expertise on python and Azure databricks.

Now trying to shift job but 4 out of 5 jobs i see are asking for  AWS (i am targeting only product companies or  GCC) . Is self learning AWS for DE possible.

Has anyone shifted from Azure stack DE to AWS ?

What services to focus .

any paid courses that you have taken like udemy etc

Thanks",11,9,nifty60,2025-03-31 10:23:27,https://www.reddit.com/r/dataengineering/comments/1jnzyop/aws_data_engineering_from_azure/,0,False,False,False,False,2025-03-31 10:23:27,10,Monday,84.0,439,66.23,6,119,9.7,0,0
55,1jo8u27,Asking for different tools for SQL Server + SSIS project.,"Hello guys. I work in a consultancy company and we recently got a job to set-up SQL Server as DWH and SSIS. Whole system is going to be build up from the scratch. The whole operation of the company was running on Excel spreadsheets with 20+ Excel Slave that copies and pastes some data from a source, CSV or email then presses the fancy refresh button. Company newly bought and they want to get rid of this stupid shit so SQL Server and SSIS combo is a huge improvement for them (lol).

But I want to integrate as much as fancy stuff in this project. Both of these tool will work on a Remote Desktop with no internet connection.  I want to integrate some DevOps tools into this project. I will be one of the 3 data engineers that is going to work on this project. So Git will be definitely on my list, as well as GitTea or a repo that works offline since there won't be a lot of people. But do you have any more free tools that I can use? Planning to integrate Jenkins in offline mode somehow, tsqlt for unit testing seems like a decent choice as well. dbt-core and airflow was on my list as well but my colleagues don't know any python so they are not gonna be on this list.

Do you have any other suggestions? Have you ever used a set-up like mine? I would love to hear your previous experiences as well. Thanks",7,9,Brilliant_Breath9703,2025-03-31 17:29:30,https://www.reddit.com/r/dataengineering/comments/1jo8u27/asking_for_different_tools_for_sql_server_ssis/,1,False,False,False,False,2025-03-31 17:29:30,17,Monday,253.0,1313,71.24,15,350,9.7,0,0
56,1jo2vxq,Date warehouse essentials guide,Check out my latest blog on data warehouses! Discover powerful insights and strategies that can transform your data management. Read it here: https://medium.com/@adityasharmah27/data-warehouse-essentials-guide-706d81eada07!,5,1,Super_Act_5816,2025-03-31 13:13:37,https://www.reddit.com/r/dataengineering/comments/1jo2vxq/date_warehouse_essentials_guide/,0,False,False,False,False,2025-03-31 13:13:37,13,Monday,23.0,223,29.82,3,47,11.2,1,0
57,1jo9b0k,Operating systems and hardware available for employees in your company,"Hey guys,

I'm working as a DE in a German IT company that has about 500 employees. The company's policy regarding operating systems the employees are allowed to use is strange and unfair (IMO). All software engineers get access to Macbooks and thus, to MacOS while all other employees that have a differnt job title ""only"" get HP elite books (that are not elite at all) that run on Windows. WSL is allowed but a native Linux is not accepted because of security reasons (I don't know which security reasons).

As far as I know the company does not want other job positions to get Macbooks because the whole updating stuff for those Macbooks  is done by an external company which is quite expensive. The Windows laptops instead are maintained by an internal team.

A lot of people are very unhappy with this situation because many of them (including me) would prefer to use Linux or MacOS. Especially all DevOps are pissed because half a year ago they also got access to MacBooks but a change in the policy means that they will have to change back to Windows laptops once their MacBooks break or become too old.

My question(s): Can you choose the OS and/or hardware in your company? Do you have a clue why Linux may not be accepted? Is it really not that safe (which I cannot think of because the company has it's own data center where a lot of Linux servers run that are actually updated by an internal team)?",2,11,VeryHardToFindAName,2025-03-31 17:48:33,https://www.reddit.com/r/dataengineering/comments/1jo9b0k/operating_systems_and_hardware_available_for/,0,False,False,False,False,2025-03-31 17:48:33,17,Monday,255.0,1410,56.39,11,382,12.7,0,0
58,1jo4hld,How do I manage dev/test/prod when using Unity Catalog for Medallion Architecture with dbt?,"Hi everyone,

I'm in the process of setting up a dbt project on Databricks and planning to leverage Unity Catalog to implement a medallion architecture. I am not sure the correct approach.  I am considering a dev/test/prod catalog, with a bronze/silver/gold schema:

* dev.bronze
* test.bronze
* prod.bronze

However, this takes 2 of the namespaces so all of the other information has to live in a single namespace such as table type (dim/fact), department (hr/finance), and data source and table description.  It seems like a lot to cram in there.

I have used the medallion architecture as a guide, but never used it in the naming, but the current team I am on really wants it to be in the name.  Just wondering what approaches people have taken.

Thanks",5,2,jfftilton,2025-03-31 14:27:44,https://www.reddit.com/r/dataengineering/comments/1jo4hld/how_do_i_manage_devtestprod_when_using_unity/,0,False,False,False,False,2025-03-31 14:27:44,14,Monday,129.0,756,61.67,7,195,12.8,0,0
59,1joeg88,My company adopted a stateful REST API solution I built that's run locally on every machine. I would like to deploy it to the cloud but I am not sure that's smart. Thoughts?,"**Context:** I joined a finance consultancy a few years ago and noticed that most people in my department are frustrated with the current ""software"" our engineering team has built over decades (yes - not years, decades). The issue is that the software consists of a bundle of Python scripts that repeatedly read large CSV files whenever a user interacts with it. The master CSV file ranges in size from 20MB to 1GB.

For example, if a user wants to select an option from a dropdown menu, clicking the dropdown triggers the reading and aggregation of a 1GB file, after which the frontend is ""returned"" 20 strings (the dropdown options). By ""returned,"" I mean that a new CSV file is created somewhere on the user's local file system, and the ""frontend"" picks it up. I tested a similar functionality using a Flask REST API, and once the CSV file is loaded into virtual memory, the process takes only 100msâ€”compared to the current design, which takes a full minute (some of it is due to scripts needing to sort out dependencies, validation, etc.). However, our engineering team refuses to adopt web-based communication, arguing that it's not worth the effort. The idea of using a cloud-based relational database is essentially taboo; it has to be either CSV files or Pythonâ€™s pickle dumps on each user's local system.

I have some experience in software engineering, so I made it my mission to redesign this legacy monsterâ€”with the blessing of a senior manager. So far, the transition has gone incredibly well. Last year, I did a soft launch of a small subset of features, and within days, every person in my department was using it.

**Question:** My current design requires users to set up a virtual environment and run an installation script that sorts out any environment variables, dependencies, etc. Each time they want to start the software, they must run a local Flask API, which interacts with a React TypeScript frontend. When the Flask API starts, it loads all necessary files into memory, does validation and other things (takes around a minute). After that, every subsequent request is easy and takes on average 100 to 200 ms. However, I dislike that each user needs a fully configured environment. Version control is also a headache since every user must manually run an update script.

Iâ€™d like to move my Flask API to the cloud so that either:

1. **A single server serves all colleagues**, or
2. **Each colleague gets a dedicated node/pod.**

The problem with a single server is that it would quickly run out of virtual memory if 100+ colleagues loaded large datasets simultaneously. The problem with one node per colleague is the complexityâ€”it would require Kubernetes (K8s) or AWS Fargate, along with an orchestrator to manage node creation and termination, which is a significant engineering effort.

I then considered making my Flask API stateless: storing large datasets in S3, using DynamoDB for file mapping, and loading data into virtual memory on every request. I converted some sample datasets to Parquet, reducing their size significantly (down to \~10MB), but I worry about added latency. Repeatedly reading the same data (given that each user makes 1â€“10 requests per minute) seems highly inefficient.

Am I missing any alternatives? Based on this, a local Flask API still seems like the best optionâ€”unless I want to pay for an expensive 64GB vRAM EC2 instance or invest significant time in building a node-per-user architecture.

Thanks!",2,6,Double_Cost4865,2025-03-31 21:17:37,https://www.reddit.com/r/dataengineering/comments/1joeg88/my_company_adopted_a_stateful_rest_api_solution_i/,1,False,False,False,False,2025-03-31 21:17:37,21,Monday,576.0,3463,41.4,27,961,14.1,0,1
60,1jnymqg,Need Feedback on data sharing module,"
Subject: Seeking Feedback: CrossLink - Faster Data Sharing Between Python/R/C++/Julia via Arrow & Shared Memory

Hey r/dataengineering


I've been working on a project called CrossLink aimed at tackling a common bottleneck: efficiently sharing large datasets (think multi-million row Arrow tables / Pandas DataFrames / R data.frames) between processes written in different languages (Python, R, C++, Julia) when they're running on the same machine/node.
Mainly given workflows where teams have different language expertise.

The Problem:
We often end up saving data to intermediate files (CSVs are slow, Parquet is better but still involves disk I/O and serialization/deserialization overhead) just to pass data from, say, a Python preprocessing script to an R analysis script, or a C++ simulation output to Python for plotting. This can dominate runtime for data-heavy pipelines.

CrossLink's Approach:
The idea is to create a high-performance IPC (Inter-Process Communication) layer specifically for this, leveraging:
Apache Arrow: As the common, efficient in-memory columnar format.
Shared Memory / Memory-Mapped Files: Using Arrow IPC format over these mechanisms for potential minimal-copy data transfer between processes on the same host.


DuckDB: To manage persistent metadata about the shared datasets (unique IDs, names, schemas, source language, location - shmem key or mmap path) and allow optional SQL queries across them.


Essentially, it tries to create a shared data pool where different language processes can push and pull Arrow tables with minimal overhead.


Performance:
Early benchmarks on a 100M row Python -> R pipeline are encouraging, showing CrossLink is:
Roughly 16x faster than passing data via CSV files.
Roughly 2x faster than passing data via disk-based Arrow/Parquet files.

It also now includes a streaming API with backpressure and disk-spilling capabilities for handling >RAM datasets.


Architecture:
It's built around a C++ core library (libcrosslink) handling the Arrow serialization, IPC (shmem/mmap via helper classes), and DuckDB metadata interactions. Language bindings (currently Python & R functional, Julia building) expose this functionality idiomatically.


Seeking Feedback:
I'd love to get your thoughts, especially on:
Architecture: Does using Arrow + DuckDB + (Shared Mem / MMap) seem like a reasonable approach for this problem? 

Any obvious pitfalls or complexities I might be underestimating (beyond the usual fun of shared memory management and cross-platform IPC)?


Usefulness: Is this data transfer bottleneck a significant pain point you actually encounter in your work? Would a library like CrossLink potentially fit into your workflows (e.g., local data science pipelines, multi-language services running on a single server, HPC node-local tasks)?


Alternatives: What are you currently using to handle this? (Just sticking with Parquet on shared disk? Using something like Ray's object store if you're in that ecosystem? Redis? Other IPC methods?)


Appreciate any constructive criticism or insights you might have! Happy to elaborate on any part of the design.

I built this to ease the pain of moving across different scripts and languages for a single file. Wanted to know if it useful for any of you here and would be a sensible open source project to maintain.

It is currently built only for local nodes, but looking to add support with arrow flight across nodes as well. ",2,5,pirana04,2025-03-31 08:43:18,https://www.reddit.com/r/dataengineering/comments/1jnymqg/need_feedback_on_data_sharing_module/,0,False,False,False,False,2025-03-31 08:43:18,8,Monday,512.0,3444,36.39,28,896,13.6,0,1
61,1joiqcm,Anyone try Semaphore?,Iâ€™ve been looking for something to unify our data and found Semaphore. Anyone have this in their company and how are they using it? Like it? Is there an alternative? Want to get some data before I engage the sales vultures ,1,0,drdacl,2025-04-01 00:29:49,https://www.progress.com/semaphore,1,False,False,False,False,2025-04-01 00:29:49,0,Tuesday,41.0,223,69.48,4,61,9.5,0,0
62,1jnt6ir,Question about preprocessing two time-series datasets from different measurement devices,"I have a question regarding the preprocessing step in a project I'm working on. I have two different measurement devices that both collect time-series data. My goal is to analyze the similarity between these two signals.

Although both devices measure the same phenomenon and I've converted the units to be consistent, I'm unsure whether this is sufficient for meaningful comparison, given that the devices themselves are different and may have distinct ranges or variances.

From the literature, Iâ€™ve found that z-score normalization is commonly used to address such issues. However, Iâ€™m concerned that applying z-score normalization to each dataset individually might make it impossible to compare across datasets, especially when I want to analyze multiple sessions or subjects later.

Is z-score normalization the right approach in this case? Or would it be better to normalize using a common reference (e.g., using statistics from a larger dataset)? Any guidance or references would be greatly appreciated.Thank you :)",1,1,UsedExcitement5306,2025-03-31 02:33:18,https://www.reddit.com/r/dataengineering/comments/1jnt6ir/question_about_preprocessing_two_timeseries/,0,False,False,False,False,2025-03-31 02:33:18,2,Monday,156.0,1023,38.82,10,286,14.1,0,0
63,1jo2s2c,Cloud Pandit Azure Data Engineering course feedback or can we take !!,Had anyone taken Cloud Pandit Azure Data Engg course. just wanted to know !!,0,3,Pi_Pisces,2025-03-31 13:08:18,https://www.reddit.com/r/dataengineering/comments/1jo2s2c/cloud_pandit_azure_data_engineering_course/,0,False,False,False,False,2025-03-31 13:08:18,13,Monday,14.0,76,73.34,2,20,0.0,0,0
64,1jo9ir2,~33% faster Microsoft Fabric with e6dataâ€“ Feedback Requested,"Hey folks,

I'm a data engineer at [e6data](https://www.e6data.com/), and we've been working on integrating our engine with Microsoft Fabric. We recently ran some benchmarks (TPC-DS) and observed around a **33% improvement in SQL query performance** while also significantly reducing costs compared to native Fabric compute engines.

Here's what our integration specifically enables:

* **33% faster SQL queries** directly on data stored in OneLake (TPC-DS benchmark results).
* **2-3x cost reduction** by optimizing compute efficiency.
* **Zero data movement**: direct querying of data from OneLake.
* Native vector search support for AI-driven workflows.
* Scalable to 1000+ QPS with sub-second latency and real-time autoscaling.
* Enterprise-level security measures.

We've documented our approach and benchmark results: [https://www.e6data.com/blog/e6data-fabric-increased-performance-optimized-capacity](https://www.e6data.com/blog/e6data-fabric-increased-performance-optimized-capacity)

We'd genuinely appreciate your thoughts, feedback, or questions about our approach or experiences with similar integrations.

https://preview.redd.it/vx65oc5zd2se1.jpg?width=6636&format=pjpg&auto=webp&s=57a2a75cd11a8cc616d04a042a12055bddfe5b4b

",0,0,e6data,2025-03-31 17:57:18,https://www.reddit.com/r/dataengineering/comments/1jo9ir2/33_faster_microsoft_fabric_with_e6data_feedback/,0,False,False,False,False,2025-03-31 17:57:18,17,Monday,132.0,1239,0.58,11,295,13.0,1,0
65,1jo89r8,Ways to quickly get total rows?,"When i am testing things often i need to run some counts  in databricks.

What is the prefered way?

I am creating a pyspark.dataframe  using spark.sql statements and later
DF.count().

Further information can be provided.",0,1,Old_Tourist_3774,2025-03-31 17:06:54,https://www.reddit.com/r/dataengineering/comments/1jo89r8/ways_to_quickly_get_total_rows/,0,False,False,False,False,2025-03-31 17:06:54,17,Monday,35.0,222,74.05,6,53,7.8,0,0
66,1jo4n51,Seeking Advice from DE: Taking a Career Break to Work & Travel in Australia,"Hey DE,

Iâ€™d love to get your perspective on my situation.

# My Background

Iâ€™m a Brazilian Mechanical Engineer with 3 years of experience in the Data fieldâ€”started as a Data Analyst for 1.5 years, then transitioned into Data Engineering. Next week, Iâ€™ll be starting as a Data Architect at a multinational with 100,000+ employees, mainly working with the Azure stack.

# The Plan

My girlfriend and I are planning to move to Australia for about a year to travel and build memories together before settling down (marriage, house, etc.). This new job came unexpectedly, but it offers a good salary (\~$2,000 USD/month).

The idea is to:

* Move to Australia
* Work hard & save around $1,000 USD/month
* Travel as much as possible for \~2 years
* Return and re-enter the data field

# The Challenge

The work visa limitation allows me to stay only 6 months with the same employer, making it tough to get good Data Engineering jobs. So, I plan to work in any job that pays well (fruit picking, hospitality, etc.), and my girlfriend will do the same.

# The Concern

When I return, how hard will it be to get back into the data field after a \~2-year break?

* Iâ€™ll have enough savings to stay unemployed for about a year if needed.
* This isnâ€™t all my savingsâ€”I have the equivalent of 6 years of salary in reserve.
* I regularly get recruiter messages on LinkedIn.
* I speak Portuguese, English, and Spanish fluently.

Given your experience, how risky is this career break? is totally crazy ? Would you recommend a different approach? Any advice would be appreciated!",0,2,chongsurfer,2025-03-31 14:34:32,https://www.reddit.com/r/dataengineering/comments/1jo4n51/seeking_advice_from_de_taking_a_career_break_to/,0,False,False,False,False,2025-03-31 14:34:32,14,Monday,276.0,1564,56.66,18,421,12.1,0,0
67,1joo4tt,Anyone else feel like data engineering is way more stressful than expected?,"I used to work as a Tableau developer and honestly, life felt simpler. I still had deadlines, but the work was more visual, less complex, and didnâ€™t bleed into my personal time as much.

Now that I'm in data engineering, I feel like Iâ€™m constantly thinking about pipelines, bugs, unexpected data issues, or some tool update I havenâ€™t kept up with. Even on vacation, I catch myself checking Slack or thinking about the next sprint. I turned 30 recently and started wonderingâ€¦ is this normal career pressure, imposter syndrome, or am I chasing too much of management approval?

Is anyone else feeling this way? Is the stress worth it long term?",154,49,Big-Dwarf,2025-04-01 05:25:58,https://www.reddit.com/r/dataengineering/comments/1joo4tt/anyone_else_feel_like_data_engineering_is_way/,0,False,False,False,False,2025-04-01 05:25:58,5,Tuesday,111.0,642,63.8,7,168,11.5,0,0
68,1jowmzi,Found the perfect Data Dictionary tool!,"Just launched the [Urban Data Dictionary](https://www.urbandatadictionary.com/) and to celebrate what what we actually do in data engineering. Hope you find it fun and like it too. 

Check it out and add your own definitions. What terms would you contribute?

Happy April Fools!",131,11,secodaHQ,2025-04-01 14:15:10,https://www.reddit.com/r/dataengineering/comments/1jowmzi/found_the_perfect_data_dictionary_tool/,0,False,False,False,False,2025-04-01 14:15:10,14,Tuesday,42.0,278,64.37,6,68,8.8,1,0
69,1jowkjn,"What Python libraries, functions, methods, etc. do data engineers frequently use during the extraction and transformation steps of their ETL work?","I am currently learning and applying data engineering into my job. I am a data analyst with three years of experience. I am trying to learn ETL to construct automated data pipelines for my reports.

Using Python programming language, I am trying to extract data from Excel file and API data sources. I am then trying to manipulate that data. In essence, I am basically trying to use a more efficient and powerful form of Microsoft's Power Query.

What are the most common Python libraries, functions, methods, etc. that data engineers frequently use during the extraction and transformation steps of their ETL work?

P.S.

Please let me know if you recommend any books or YouTube channels so that I can further improve my skillset within the ETL portion of data engineering.

Thank you all for your help. I sincerely appreciate all your expertise. I am new to data engineering, so apologies if some of my terminology is wrong.



Edit:

Thank you all for the detailed responses. I highly appreciate all of this information.",89,60,Original_Chipmunk941,2025-04-01 14:12:13,https://www.reddit.com/r/dataengineering/comments/1jowkjn/what_python_libraries_functions_methods_etc_do/,0,False,2025-04-01 15:26:11,False,False,2025-04-01 14:12:13,14,Tuesday,173.0,1023,50.43,14,288,11.4,0,1
70,1jowboo,"Quack-To-SQL model : stop coding, start quacking",,26,5,TransportationOk2403,2025-04-01 14:01:45,https://motherduck.com/blog/quacktosql,0,False,False,False,False,2025-04-01 14:01:45,14,Tuesday,,0,206.84,1,0,0.0,0,0
71,1jp7anp,What is the best free BI dashboarding tool?,We have 5 developers and none of them are data scientists. We need to be able to create interactive dashboards for management.,19,25,Professional_Eye8757,2025-04-01 21:23:16,https://www.reddit.com/r/dataengineering/comments/1jp7anp/what_is_the_best_free_bi_dashboarding_tool/,0,False,False,False,False,2025-04-01 21:23:16,21,Tuesday,22.0,126,60.31,2,36,0.0,0,0
72,1jp2zld,"A Modern Benchmark for the
Timeless Power of the Intel Pentium Pro",,14,6,ikeben,2025-04-01 18:30:47,https://www.bodo.ai/bodobench95,0,False,False,False,False,2025-04-01 18:30:47,18,Tuesday,,0,206.84,1,0,0.0,0,0
73,1joqbrl,Time-series analysis pipeline architecture,"Hi, I'm a bit outdated when it comes to all new cloud based solutions and request navigation on what architecture might be useful to start with (should be rather simple and not too much overhead to set up) while still be prepared for more data sources and more analysis requirements.

I'm using Azure

My use-case:
I have a time-series dataset coming from an API on which we perform a Python analysis. We would like to perform the Python analysis on a weekly basis, store the data and provide the output as a power bi dashboard. The dataset consists of like 500 000 rows each week, the analysis scripts processes a many to many calculation and I might be interested in adding more data sources as well as perform more KPI calculations pre-processed in data storage (i.e. not in power bi).",10,3,qiicken,2025-04-01 08:03:41,https://www.reddit.com/r/dataengineering/comments/1joqbrl/timeseries_analysis_pipeline_architecture/,1,False,False,False,False,2025-04-01 08:03:41,8,Tuesday,140.0,788,43.06,5,217,14.0,0,0
74,1jpf97l,How the Apache Doris Compute-Storage Decoupled Mode Cuts 70% of Storage Costsâ€”in 60 Seconds,,3,0,Any_Opportunity1234,2025-04-02 03:28:07,https://v.redd.it/fuk670n3ccse1,0,False,False,False,False,2025-04-02 03:28:07,3,Wednesday,,0,206.84,1,0,0.0,0,0
75,1joz9bo,Cloud platform for dbt,"I recently started learning dbt and was using Snowflake as my database. However, my 30-day trial has ended. Are there any free cloud databases I can use to continue learning dbt and later work on projects that I can showcase on GitHub?

Which cloud database would you recommend? Most options seem quite expensive for a learning setup.

Additionally, do you have any recommendations for dbt projects that would be valuable for hands-on practice and portfolio building?

Looking forward to your suggestions!",5,13,Pro_Panda_Puppy,2025-04-01 16:02:14,https://www.reddit.com/r/dataengineering/comments/1joz9bo/cloud_platform_for_dbt/,0,False,False,False,False,2025-04-01 16:02:14,16,Tuesday,81.0,505,51.24,7,136,10.9,0,0
76,1joz80e,Monthly General Discussion - Apr 2025,"This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.

Examples:

* What are you working on this month?
* What was something you accomplished?
* What was something you learned recently?
* What is something frustrating you currently?

As always, sub rules apply. Please be respectful and stay curious.

**Community Links:**

* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)
* [Data Engineering Events](https://dataengineering.wiki/Community/Events)
* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)
* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)",5,0,AutoModerator,2025-04-01 16:00:57,https://www.reddit.com/r/dataengineering/comments/1joz80e/monthly_general_discussion_apr_2025/,0,False,False,False,True,2025-04-01 16:00:57,16,Tuesday,91.0,761,22.17,12,173,9.9,1,0
77,1joyxbm,Opinions on Vertex AI,"From a more technical perspective what's your opinion about Vertex AI.  
I am trying to deploy a machine learning pipeline and my data science colleges are real data scientists and I do not trust them to bring everything into production.  
What's your experience with vertex ai?",6,2,NectarineNo7098,2025-04-01 15:49:03,https://www.reddit.com/r/dataengineering/comments/1joyxbm/opinions_on_vertex_ai/,0,False,False,False,False,2025-04-01 15:49:03,15,Tuesday,46.0,278,55.95,3,75,12.5,0,0
78,1jp25jk,"DeepSeek 3FS: non-RDMA install, faster ecosystem app dev/testing.",,3,0,HardCore_Dev,2025-04-01 17:58:27,https://blog.open3fs.com/2025/04/01/deepseek-3fs-non-rdma-install-faster-ecosystem-app-dev-testing.html,0,False,False,False,False,2025-04-01 17:58:27,17,Tuesday,,0,206.84,1,0,0.0,0,0
79,1jovty4,Making your data valuable with Data Products,https://medium.com/@smayya/decoding-data-products-more-than-just-data-89024281a781?sk=3fc692fcc8c5e356d10f4b3077a17c89,2,0,frazered,2025-04-01 13:40:02,https://www.reddit.com/r/dataengineering/comments/1jovty4/making_your_data_valuable_with_data_products/,0,False,False,False,False,2025-04-01 13:40:02,13,Tuesday,1.0,118,-724.79,1,11,0.0,1,0
80,1joxxbz,any alternatives to alteryx?,"most of our data is on prem sql server. we also have some data sources in snowflake as well (10-15% of the data). we also connect to some api's as well using the python tool. our reporting db is sql server on prem. currently we are using alteryx, and we are researching what our options are before we have to renew our contract. any suggestions that we can explore or if someone has been through a similar scenario, what did you end up with and why? please let me know if I can add more information to the context.

also,I forgot to mention that not all of my team members are familiar with python. Looking for GUI options.

Edit: thank you all. Iâ€™ll look into the mentioned options.",1,10,r0oki3r0kk,2025-04-01 15:08:23,https://www.reddit.com/r/dataengineering/comments/1joxxbz/any_alternatives_to_alteryx/,0,False,2025-04-02 00:14:33,False,False,2025-04-01 15:08:23,15,Tuesday,128.0,683,68.16,11,187,8.6,0,1
81,1joxi3z,Databricks Compute. Thoughts and more.,,2,0,averageflatlanders,2025-04-01 14:51:20,https://dataengineeringcentral.substack.com/p/databricks-compute-thoughts-and-more,0,False,False,False,False,2025-04-01 14:51:20,14,Tuesday,,0,206.84,1,0,0.0,0,0
82,1jot1bz,Getting data from SAP HANA to snowflake,"So i have this project that will need to ingest data from SAP HANA into snowflake, it can be considered as any on-premise DB using JBDC, the big issue is, I cannot use any external ETL services as per project requirements. What is the best path to follow?  


I need to fetch the data in bulk for some tables with truncate / copy into, and some tables need to be incremental with little (10 min) delay. The tables do not contain any watermark, modified time or anything...  


There isnt much data, 20M rows tops.

If you guys can give me a hand, i'm new to snowflake and strugling to find any sources on this.",2,6,Ra-mega-bbit,2025-04-01 11:15:13,https://www.reddit.com/r/dataengineering/comments/1jot1bz/getting_data_from_sap_hana_to_snowflake/,0,False,False,False,False,2025-04-01 11:15:13,11,Tuesday,114.0,610,69.31,6,161,9.7,0,0
83,1josncc,"Career improves, but projects don't? [discussion]","I started 6 years ago and my career has been on a growing trajectory since.

While this is very nice for me, I canâ€™t say the same about the projects I encounter. What I mean is that I was expecting the engineering soundness of the projects I encounter to grow alongside my seniority in this field.

Instead, Iâ€™ve found that regardless of where I end up (the last two companies were data consulting shops), the projects I am assigned to tend to have questionable engineering decisions (often involving an unnecessary use of Spark to move 7 rows of data).

The latest one involves ETL out of MSSQL and into object storage, using a combination of Azure synapse spark notebooks, drag and drop GUI pipelines, absolutely no tests or CICD whatsoever, and debatable modeling once data lands in the lake.

This whole thing scares me quite a lot due to the lack of guardrails, while testing and deployments are done manually. While I'd love to rewrite everything from scratch, my eng lead said since that part it's complete and there isn't a plan to change it in the future, that it's not a priority at all, and I agree with this.

What's your experience in situations like this? How do you juggle the competing priorities (client wanting new things vs. optimizing old stuff etc...)?
",1,18,wtfzambo,2025-04-01 10:51:01,https://www.reddit.com/r/dataengineering/comments/1josncc/career_improves_but_projects_dont_discussion/,0,False,False,False,False,2025-04-01 10:51:01,10,Tuesday,223.0,1274,57.3,10,339,13.0,0,0
84,1jopqvd,What is the best approach for a Bronze layer?,"Hello,

We are starting a new Big Data project in my company with Cloudera, Hive, Hadoop HDFS, and a medallion architecture, but I have some questions about ""Bronze"" layer.

Our source is a FTP and in this FTP are allocated the daily/monthly files (.txt, .csv, .xlsx...).  
We bring those files to our HDFS in separated in folders by date (E.G: xxxx/2025/4)

Here start my doubts:  
\- Our bronze layer are those files in the HDFS?  
\- For build our bronze layer, we need to load those files incrementally into a ""bronze table"" partitioned by date

Reading on internet I saw that we have to do the second option, but I saw that option like a rubbish table

Which will be the best approach?

  
For the other layers, I don't have any doubts.",2,11,Elkemao,2025-04-01 07:20:04,https://www.reddit.com/r/dataengineering/comments/1jopqvd/what_is_the_best_approach_for_a_bronze_layer/,0,False,False,False,False,2025-04-01 07:20:04,7,Tuesday,132.0,741,57.91,6,189,11.2,0,0
85,1jpdwy1,Knime on Anaconda Nacigator,Is it possible to install Knime on Anaconda Navigator? ,1,0,Puzzleheaded_Serve39,2025-04-02 02:25:22,https://www.reddit.com/r/dataengineering/comments/1jpdwy1/knime_on_anaconda_nacigator/,0,False,False,False,False,2025-04-02 02:25:22,2,Wednesday,9.0,55,20.04,1,19,0.0,0,0
86,1jpdpyh,Facebook Marketing API - Anyone have a successful ETL experience?,"We have a python integration set up where we pull data from Google Ads and Facebook Marketing into our data warehouse. We're pulling data about all 3 hierarchy tiers and some daily metrics:

1. Campaigns (id, name, start time, stop time)
2. Ad Groups/Ad Sets (id, name)
3. Ads (id, name, URL)
4. Metrics (clicks, impressions, spend) for the previous day

For the Google Ads API, you basically send a SQL query and the return time is like a tenth of a second.

For Facebook, we see returns times in the minutes, especially on the Ads piece. Was hoping to get an idea of how others might have successfully set up a process to get this data from Facebook in a more timely fashion, and possibly without hitting the rate limiting threshold.

Not the exact code we're using - I can get it off my work system tomorrow - but the gist:

    from facebook_business.adobjects.adaccount import AdAccount
    from facebook_business.adobjects.campaign import Campaign
    from facebook_business.adobjects.ad import AdSet
    from facebook_business.adobjects.ad import Ad
    from facebook_business.adobjects.adcreative import AdCreative
    campaigns = AdAccount('act_123456789').get_campaigns(
        params={},
        fields=[Campaign.Field.id,Campaign.Field.name,Campaign.Field.start_time,Campaign.Field.stop_time]
    )
    adsets= AdAccount('act_123456789').get_ad_sets(
        params={},
        fields=[AdSet.Field.id,AdSet.Field.name]
    )
    ads = AdAccount('act_123456789').get_ads(
        params={},
        fields=[Ad.Field.id,Ad.Field.name,Ad.Field.creative]
    )
    object_urls = AdAccount('act_123456789').get_ad_creatives(
        params={},
        fields=[AdCreative.Field.object_story_spec]
    )
    asset_urls = AdAccount('act_123456789').get_ad_creatives(
        params={},
        fields=[AdCreative.Field.asset_feed_spec]
    )

We then have to do some joining between ads/object\_urls/asset\_urls to match the Ad with the destination URL if the ad is clicked on.

The performance is so slow, that I hope we are doing it wrong. I was never able to get the batch call to work and I'm not sure how to improve things.

Sincerely a data analyst who crosses over into data engineering because our data engineers don't know python.",1,3,farm3rb0b,2025-04-02 02:17:27,https://www.reddit.com/r/dataengineering/comments/1jpdpyh/facebook_marketing_api_anyone_have_a_successful/,0,False,False,False,False,2025-04-02 02:17:27,2,Wednesday,273.0,2244,44.71,27,482,10.1,0,1
87,1jpdpnz,Resources for learning AbInitio Tool,"I tried to search the entire internet to find AbInito related tutorials/tranings. Hard luck finding anything. I came to know it's a closed source tool and everything is behind a login wall only for partner companies. 

Can anyone share me stuff they found useful?

Thanks in advance.",1,1,IdealBusiness6499,2025-04-02 02:17:07,https://www.reddit.com/r/dataengineering/comments/1jpdpnz/resources_for_learning_abinitio_tool/,0,False,False,False,False,2025-04-02 02:17:07,2,Wednesday,47.0,283,61.93,5,74,10.4,0,0
88,1jp0vmd,Not in the field and I need help understanding how data migrations work and how they're done,"I'm an engineer in an unrelated field and want to understand how data migrations work for work (I might be put in charge of it at my job even though we're not data engineers).  Any good sources, preferably a video that would a mock walkthrough of one (maybe using an ETL too)?",1,2,BlackendLight,2025-04-01 17:07:19,https://www.reddit.com/r/dataengineering/comments/1jp0vmd/not_in_the_field_and_i_need_help_understanding/,0,False,False,False,False,2025-04-01 17:07:19,17,Tuesday,52.0,276,53.55,2,76,0.0,0,0
89,1jp0ntu,ELI5 - High-Level Diagram of a Data Strategy,"Hello everyone!Â 

I am not a data engineer, but I am trying to help other people within my organization (as well as myself) get a better understanding of what an overall data strategy looks like.Â  So, I figured I would ask the experts.Â  Â Â 

**Do you have a go-to high-level diagram you use that simplifies the complexities of an overall data solution and helps you communicate what that should look like to non-technical people like myself?**Â 

Iâ€™m a very visual learner so seeing something that shows what the journey of data should look like from beginning to end would be extremely helpful.Â  Iâ€™ve searched online but almost everything I see is created by a vendor trying to show why their product is better.Â  Iâ€™d much rather see an unbiased explanation of what the overall process should be and then layer in vendor choices later.

I apologize if the question is phrased incorrectly or too vague.Â  If clarifying questions/answers are needed, please let me know and Iâ€™ll do my best to answer them.Â  Thanks in advance for your help.",1,0,qwopzxnm79,2025-04-01 16:58:49,https://www.reddit.com/r/dataengineering/comments/1jp0ntu/eli5_highlevel_diagram_of_a_data_strategy/,0,False,False,False,False,2025-04-01 16:58:49,16,Tuesday,177.0,1033,51.48,9,278,12.8,0,1
90,1joy98n,Dimensional modelling -> Datetime column,"Hi All,

Im learning Dimensional modelling. Im working on the NYC taxi dataset ( [here is the data dictionary](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf) ).

Im struggling to model Datetime columns: tpep\_pickup\_datetime, tpep\_dropoff\_datetime.  
Does these columns should be in Dimensions table or in Fact table? 

What I understand from the Kimball datawarehouse toolkit book is to have a DateDim table populated with dates from start\_date to end\_date with details like month, year, quarter, day of week etc. but what about timestamp?

Lets say if I want to see the data for certain time of the day like nights? In this case, do I need to split the columns: tpep\_pickup\_datetime, tpep\_dropoff\_datetime into date, hour, mins in fact table and join to a dim table with the timestamp details like hour, mins etc? ( so two dim tables - date and timestamp )

It would be great someone can help me here?",1,2,Delicious_Attempt_99,2025-04-01 15:21:53,https://www.reddit.com/r/dataengineering/comments/1joy98n/dimensional_modelling_datetime_column/,0,False,False,False,False,2025-04-01 15:21:53,15,Tuesday,145.0,951,55.64,9,230,10.5,1,0
91,1jozvao,SQL Templating (without DBT?),"Iâ€™d like to implement jinja templated SQL for a project. But I donâ€™t want or need DBTâ€™s extra bells and whistles. I just need/want to write macros, templated .sql files, then on execution (from python application), render the SQL at runtime.

Whatâ€™s the solution here? Pure jinja? (Whatâ€™re some resources for that?) Are there OSS libraries I can use? Or, do I just use DBT, but only use it from a python driver?",0,3,boss_yaakov,2025-04-01 16:26:46,https://www.reddit.com/r/dataengineering/comments/1jozvao/sql_templating_without_dbt/,0,False,False,False,False,2025-04-01 16:26:46,16,Tuesday,73.0,411,70.7,8,110,9.8,0,0
92,1jowcet,Lessons from operating big ClickHouse clusters for several years,"My coworker Javi Santana wrote a lengthy post about what it takes to operate large ClickHouse clusters based on his experience starting Tinybird. If you're managing any kind of OSS CH cluster, you might find this interesting.

[https://www.tinybird.co/blog-posts/what-i-learned-operating-clickhouse](https://www.tinybird.co/blog-posts/what-i-learned-operating-clickhouse)",0,1,itty-bitty-birdy-tb,2025-04-01 14:02:37,https://www.reddit.com/r/dataengineering/comments/1jowcet/lessons_from_operating_big_clickhouse_clusters/,0,False,False,False,False,2025-04-01 14:02:37,14,Tuesday,38.0,371,9.89,2,80,0.0,1,0
93,1jossd0,Newbie to DE needs help with the approach to the architecture of a project,"So I was hired as a data analyst a few months ago and I have a background in software development. A few months ago I was moved to a smallish project with the objective of streamlining some administrative tasks that were all calculated ""manually"" with Excel.  By the time, all I had worked with were very basic, low code tools from the Microsoft enviroment: PBI for dashboards, Power Automate, Power Apps for data entry, Sharepoint lists, etc, so that's what I used to set it up. 

The cost for the client is basically nonexistent right now, apart from a couple of PBI licenses. The closest I've done to ETL work has been with power query, if you can even call it that. 

  
Now I'm at a point where it feels like that's not gonna cut it anymore. I'm going to be working with larger volumes of data, with more complex relationships between tables and transformations that need to be done earlier in the process. I could technically keep going with what I have but I want to actually build something durable and move towards actual data engineering, but I don't know where to start with a solution that's cost efficient and well structured. For example, I wanted to move the data from Sharepoint lists to a proper database but then we'd have to pay for multiple premium licenses to be able to connect to them in powerapps. Where do I even start?

I know the very basics of data engineering and I've done a couple of tutorial projects with Snowflake and Databricks as my team seems to want to focus on cloud based solutions. So I'm not starting from absolute scratch, but I feel pretty lost as I'm sure you can tell. I'd appreciate any kind of advice or input as to where to head from here, as I'm on my own right now.",0,4,Wild_Complaint_4688,2025-04-01 11:00:15,https://www.reddit.com/r/dataengineering/comments/1jossd0/newbie_to_de_needs_help_with_the_approach_to_the/,0,False,False,False,False,2025-04-01 11:00:15,11,Tuesday,314.0,1716,55.37,13,460,12.6,0,0
94,1jorny7,How do you build tests for processing data with variations,"How do you test a data pipeline which parses data having a lot of variation

I'm working on a project to parse pdfs (earnings calls), they have a common general structure, but sometimes I'll get variations in the data (very common, half of docs have some kind of variation). It's a pain to debug when things go wrong, I have to run tests on a lot of files which takes up time.

I want to build good tests, and learn to do this better in the future, then refactor the code (it's garbage right now)",0,7,Sure-Government-8423,2025-04-01 09:44:04,https://www.reddit.com/r/dataengineering/comments/1jorny7/how_do_you_build_tests_for_processing_data_with/,0,False,False,False,False,2025-04-01 09:44:04,9,Tuesday,95.0,496,64.68,3,125,9.7,0,0
95,1jp4f74,Built a visual tool on top of Pandas that runs Python transformations row-by-row - What do you guys think?,"Hey data engineers,

For client implementations I thought it was a pain to write python scripts over and over, so I built a tool on top of Pandas to solve my own frustration and as a personal hobby. The goal was to make it so I didn't have to start from the ground up and rewrite and keep track of each script for each data source I had.

**What I Built:**  
A visual transformation tool with some features I thought might interest this community:

1. **Python execution on a row-by-row basis**Â \- Write Python once per field, save the mapping, and process. It applies each field's mapping logic to each row and returns the result without loops
2. **Visual logic builder**Â that generates Python from the drag and drop interface. It can re-parse the python so you can go back and edit form the UI again
3. **AI Co-Pilot**Â that can write Python logic based on your requirements
4. **No environment setup**Â \- just upload your data and start transforming
5. **Handles nested JSON**Â with a simple dot notation for complex structures

Here's a screenshot of the logic builder in action:

https://preview.redd.it/znh4fom8y9se1.png?width=2690&format=png&auto=webp&s=2daf229aab2f5de272c4f5668a782d8011ff3207

I'd love some feedback from people who deal with data transformations regularly. If anyone wants to give it a try feel free to shoot me a message or comment, and I can give you lifetime access if the app is of use. Not trying to sell here, just looking for some feedback and thoughts since I just built it.

**Technical Details:**

* Supports CSV, Excel, and JSON inputs/outputs, concatenating files, header & delimiter selection
* Transformations are saved as editable mapping files
* Handles large datasets by processing chunks in parallel
* Built on Pandas. Supports Pandas and re libraries

[DataFlowMapper.com](http://DataFlowMapper.com)",0,7,skrufters,2025-04-01 19:28:00,https://www.reddit.com/r/dataengineering/comments/1jp4f74/built_a_visual_tool_on_top_of_pandas_that_runs/,0,False,2025-04-01 19:56:02,False,False,2025-04-01 19:28:00,19,Tuesday,293.0,1843,60.55,15,443,11.6,1,0
96,1jpaq8v,Data Developer vs Data Engineer,"I know it varies by company blah blah blah, but also aside from a Google search, what have you guys in the field noticed to be core differences between these positions? ",0,3,diabeticspecimen,2025-04-01 23:53:39,https://www.reddit.com/r/dataengineering/comments/1jpaq8v/data_developer_vs_data_engineer/,0,False,False,False,False,2025-04-01 23:53:39,23,Tuesday,31.0,169,56.93,1,44,0.0,0,0
97,1jp0s33,"Introducing the Knowledge Graph: things, not strings","""Fully Managed Graph Database Service | Neo4j AuraDB"" https://neo4j.com/product/auradb/?utm_source=GSearch&utm_medium=PaidSearch&utm_campaign=Evergreen&utm_content=AMS-Search-SEMCE-DSA-None-SEM-SEM-NonABM&utm_term=&utm_adgroup=DSA&gad_source=1&gclid=Cj0KCQjwna6_BhCbARIsALId2Z27LAb-nD-42tRRF5viybJfBVull8EeBvj46w_V7OCs1RdtbR7hqBQaAuObEALw_wcB",0,0,Ok_Efficiency1311,2025-04-01 17:03:16,https://blog.google/products/search/introducing-knowledge-graph-things-not/,0,False,False,False,False,2025-04-01 17:03:16,17,Tuesday,9.0,342,-503.47,1,66,0.0,1,0
98,1jp0ci2,We cut Databricks costs without sacrificing performanceâ€”hereâ€™s how,"About 6 months ago, I led a Databricks cost optimization project where we cut down costs, improved workload speed, and made life easier for engineers. I finally had time to write it all up a few days agoâ€”cluster family selection, autoscaling, serverless, EBS tweaks, and more. I also included a real example with numbers. If youâ€™re using Databricks, this might help: [https://medium.com/datadarvish/databricks-cost-optimization-practical-tips-for-performance-and-savings-7665be665f52](https://medium.com/datadarvish/databricks-cost-optimization-practical-tips-for-performance-and-savings-7665be665f52)",0,3,DataDarvesh,2025-04-01 16:45:48,https://www.reddit.com/r/dataengineering/comments/1jp0ci2/we_cut_databricks_costs_without_sacrificing/,0,False,False,False,False,2025-04-01 16:45:48,16,Tuesday,62.0,601,21.9,4,127,12.6,1,0
99,1jphc3z,"The Struggles of Mean, Median, and Mode",,284,15,ganildata,2025-04-02 05:34:58,https://i.redd.it/dahpd85zycse1.jpeg,0,False,False,False,False,2025-04-02 05:34:58,5,Wednesday,,0,206.84,1,0,0.0,0,0
100,1jpw0uh,This is what you see all the time if you're a Data EngineerðŸ« ,,227,64,Anass-YI,2025-04-02 18:31:04,https://v.redd.it/f5lu46hgtgse1,0,False,False,False,False,2025-04-02 18:31:04,18,Wednesday,,0,206.84,1,0,0.0,0,0
101,1jpknlr,Is Databricks Becoming a Requirement for Data Engineers?,"Hey everyone,

Iâ€™m a Data Engineer with 5 years of experience, mostly working with traditional data pipelines, cloud data warehouses(AWS and Azure) and tools like Airflow, Kafka, and Spark. However, Iâ€™ve never used Databricks in a professional setting.

Lately, I see Databricks appearing more and more in job postings, and it seems like it's becoming a key player in the data world. For those of you working with Databricks, do you think it's a necessity for Data Engineers now? I see that it is mandatory requirement in job offerings but I don't have opportunity to get first experience in it.

What is your opinion, what should I do?",84,41,BigDataMax,2025-04-02 09:41:41,https://www.reddit.com/r/dataengineering/comments/1jpknlr/is_databricks_becoming_a_requirement_for_data/,0,False,False,False,False,2025-04-02 09:41:41,9,Wednesday,108.0,636,53.21,6,168,12.7,0,0
102,1jpkgey,Does anyone feel the DE tools are chaging too fast to track,"TL;DR: a guy feeling stuck in the job and cannot figure out what skills are needed to move to a better position 

I am data engineer at a big 4 firm (may be just a etl developer) in india.

I work with Informatica Power Center, Oracle, Unix on the daily basis. Now, when I tried to switch companies for career boost, I realised nobody uses these tech anymore. 

Everyone uses pyspark for etl.
I though fair enough and started leaning pyspark dataframe api. I am so good with sql, pl/sql and python, so it was easy for me.

Then I came to know learning pyspark is not enough, you need to know databricks, snowflake, dbt kind of tools.

Even before making my mind to decide what to learn, things changed and now airflow/dagster, redshift, delta lake, duckdb. I don't what else is in trend now.

Honestly, It feels a lot, like the world is moving in the fastest pace possible and I cannot even decide what to do.

Every job has different tools, and to do the ""fake it till you make it"", I am afraid they would ask any niche question about the tool to which you can only answer if you have the experience.

My profile  is not even getting picked and I feel stuck in the job I am doing.

I am great at what I do, that is one reason the project is not letting me leave even after all the senior folks has left for better projects. The guy with 3 years of experience is the senior most developer and lead now.

But honestly, I dont think I can make it anymore.

If I was just stuck with something like SAP ABAP, frontend or core python, things might have been good. Recruiters will at least look at your profile even though you are not a perfect match as you can learn the rest to do the job. (I might be wrong in this thought)

But for DE roles, the job descriptions are becoming too specific to a tool and people are expecting complete data architect level of skills at 3 years.

I was so ambitious to get a job in a different country with big 4 experience, but now I can't even get a job in india.",42,32,venkatcg,2025-04-02 09:26:11,https://www.reddit.com/r/dataengineering/comments/1jpkgey/does_anyone_feel_the_de_tools_are_chaging_too/,0,False,2025-04-02 09:30:54,False,False,2025-04-02 09:26:11,9,Wednesday,382.0,1993,67.99,19,524,10.7,0,0
103,1jponp5,Am i doomed moving forward,"I am scared my job is a lightning strike that doesnt exist elsewhere. Im classified as a â€œdata engineerâ€ but only work in snowflake building datasets for tableau. Basically im a middle man between IT who ingests the data and then analysts who visualize in tableau. I live in fear (lol) that if i were to lose this job i would qualify for nothing else because i havent touched python or any ingesting tools or tableau and any visualizing tools in years. 
Am as as out of the norm as i feel?",18,10,GoRGoNiTe_SCuMM,2025-04-02 13:33:31,https://www.reddit.com/r/dataengineering/comments/1jponp5/am_i_doomed_moving_forward/,0,False,False,False,False,2025-04-02 13:33:31,13,Wednesday,92.0,489,69.72,5,131,10.8,0,0
104,1jpkf42,"Lucked into a junior data engineer role, where do I go from here?","
About a month ago I was hired at a very small startup (3 employees including me) to be their ""data engineer and analyst"", replacing the previous data engineer who moved on to a grad scheme.

I recently graduated in a non-CS discipline, so my Python and SQL skills aren't exactly amazing but I'm a fast learner. It helps that the other employees are non-technical and the previous data engineer was extremely helpful while training me.

The job has been going well so far. I can see myself getting my skills up to a good standard, and it's a great role to learn the ropes BUT I can't see myself in this role for longer than a year or two. So what should I prepare for next? A more demanding data engineer job? Further education?

I'd like to have a technical job in the financial sector within the next 5-6 years e.g. data engineer for a quant firm.
",13,5,Dismal-Set-6428,2025-04-02 09:23:29,https://www.reddit.com/r/dataengineering/comments/1jpkf42/lucked_into_a_junior_data_engineer_role_where_do/,0,False,False,False,False,2025-04-02 09:23:29,9,Wednesday,157.0,850,62.27,9,234,12.5,0,0
105,1jpf97l,How the Apache Doris Compute-Storage Decoupled Mode Cuts 70% of Storage Costsâ€”in 60 Seconds,,10,0,Any_Opportunity1234,2025-04-02 03:28:07,https://v.redd.it/fuk670n3ccse1,0,False,False,False,False,2025-04-02 03:28:07,3,Wednesday,,0,206.84,1,0,0.0,0,0
106,1jpyf3s,Skills to Stay Relevant in Data Engineering Over the Next 5-10 Years,"Hey r/dataengineering,

I've been in data engineering for about **3 years now**, and while I love what I do, I can't help but wonder: **whatâ€™s next?** With tech evolving so fast, I'm a bit concerned about what could make our current skills obsolete.

That said, Spark didnâ€™t exactly kill the demand for Hadoop, Impala, etc.â€”so maybe the fear is overblown. But still, I want to make sure I'm **learning the right things** to stay ahead and not be caught off guard by layoffs or major shifts in the industry.

My current stack: **Python, SQL, Spark, AWS (Glue, Redshift, EMR), Airflow.**

What skills/tech would you bet on for the next **5-10 years**? Is it **real-time data processing? DataOps? AI/ML integration?** Would love to hear from those whoâ€™ve been in the game longer!",14,12,Spartanno39,2025-04-02 20:07:35,https://www.reddit.com/r/dataengineering/comments/1jpyf3s/skills_to_stay_relevant_in_data_engineering_over/,0,False,False,False,False,2025-04-02 20:07:35,20,Wednesday,131.0,776,73.58,9,180,9.7,0,0
107,1jps1g1,DBT and Snowflake,"Hello all, I am trying to implement dbt and snowflake on a personal project, most of my experience comes from databricks so I would like to know if the best approach for this would be to:
1- a server dedicated to dbt that will connect to snowflake and execute transformations.
2- snowflake of course deployed in azure .
3- azure data factory for raw ingestion and to schedule the transformation pipeline and future dbt dataquality pipelines.

What you guys think about this? ",9,12,pvic234,2025-04-02 15:54:12,https://www.reddit.com/r/dataengineering/comments/1jps1g1/dbt_and_snowflake/,0,False,False,False,False,2025-04-02 15:54:12,15,Wednesday,82.0,475,59.33,4,123,12.2,0,0
108,1jpmna4,Iceberg catalog in gcp,"Which is your preferred way to host your data catalog inside of gcp? I know that inside of aws, glue is the preferred way?  
I know that it can make sense to use dataproc Metastore and/or big data lake Metastore.

I know that there are also a lot open source tools that you can use?

what do you prefer? what's your experience?",9,3,NectarineNo7098,2025-04-02 11:51:11,https://www.reddit.com/r/dataengineering/comments/1jpmna4/iceberg_catalog_in_gcp/,1,False,False,False,False,2025-04-02 11:51:11,11,Wednesday,62.0,327,86.4,6,82,7.8,0,0
109,1jpl5rc,Latest Thoughtworks TechRadar - data blips,"Thoughtworks have published their latest Technology Radar: https://www.thoughtworks.com/radar

FWIW, here are a few of the 'blips' (as they call them) of note in the data space:

ðŸŸ¢ Adopt: [Data product thinking](https://www.thoughtworks.com/radar/techniques/data-product-thinking)

ðŸŸ¢ Adopt: [Trino](https://www.thoughtworks.com/radar/platforms/trino)

ðŸ‘ Trial: [Databricks Delta Live Tables](https://www.thoughtworks.com/radar/tools/databricks-delta-live-tables)

ðŸ‘ Trial: [Metabase](https://www.thoughtworks.com/radar/tools/metabase)

âœ‹ Hold: [Reverse ETL](https://www.thoughtworks.com/radar/techniques/reverse-etl)

On Reverse ETL they say: 

> we're seeing a growing trend where product vendors use Reverse ETL as an excuse to move increasing amounts of business logic into a centralized platform â€” their product. This approach exacerbates many of the issues caused by centralized data architectures, and we suggest exercising extreme caution when introducing data flows from a sprawling, central data platform to transaction processing systems.",7,2,rmoff,2025-04-02 10:17:58,https://www.reddit.com/r/dataengineering/comments/1jpl5rc/latest_thoughtworks_techradar_data_blips/,0,False,False,False,False,2025-04-02 10:17:58,10,Wednesday,113.0,1048,24.14,8,212,11.2,1,0
110,1jpxfch,"New to Data Engineering â€” Feeling a Bit Overwhelmed, Looking for Advice","Hey everyone, I could really use some advice from fellow engineers. I'm pretty new to the data world â€” I messed up uni, then did an online analytics course, and after about a year and a half of grinding, I finally landed my first role. Along the way, I found a real passion for Python and SQL.

My first job involved a ton of patchy reporting because of messy infra and data. I started automating painful tasks using basic ETL pipelines I built myself. I showed an interest in APIs and, out of nowhere, 6 months in, I was offered a data engineering role.

Fast forward to now â€” Iâ€™ve been in the new role for a month, and Iâ€™m the companyâ€™s only data engineer. Iâ€™m doing a data engineering apprenticeship at the same time, which helps, but the imposter syndrome is real. The companyâ€™s been limping along with a 25-year-old piece of software that populates our SQL Server DB, and weâ€™re now migrating to something new. Iâ€™ve been asked to learn MuleSoft for ETL and replace some existing pipelines that were built in Python.

I love the subject â€” Iâ€™m genuinely passionate about programming and networking â€” and Iâ€™m keen to take on new tech, improve the infra, and build up strong skills. But Iâ€™m not sure if Iâ€™m going too deep too fast. For example, today I was learning Docker to deploy Python scripts, just to avoid issues with hundreds of brittle batch files that break if we update Python.

My boss seems to think MuleSoft will fully replace Python, but I see it more as a tool that complements certain workflows rather than a full replacement. What worries me more is that I donâ€™t really have any technical peers. Most people in my team only know basic SQL, and itâ€™s hard to communicate strategy or get proper feedback.

My current priorities are getting comfortable with MuleSoft, Git, and Docker. Iâ€™m constantly learning, but sometimes I leave work feeling overwhelmed. Thereâ€™s so much broken or duct-taped together, I donâ€™t even know where to start. I keep telling myself I donâ€™t need to â€œsave the world,â€ but I really want to do a good job and come away with solid experience.

Long term, they want to deploy this new software, rebuild the database, and eventually use AI to help employees query the business. Thereâ€™s a shit ton to do, and Iâ€™m still figuring out basics â€” like setting up a VM just so I can run Docker.

Am I jumping the gun with how Iâ€™m feeling, or is this as wild a situation as it seems? Any advice for a new engineer navigating bad infra, limited support, and a mountain of work would be seriously appreciated.",8,3,ethg674,2025-04-02 19:27:27,https://www.reddit.com/r/dataengineering/comments/1jpxfch/new_to_data_engineering_feeling_a_bit_overwhelmed/,0,False,False,False,False,2025-04-02 19:27:27,19,Wednesday,457.0,2534,60.85,24,666,11.0,0,0
111,1jph8ei,Creating a Beginner Data Engineering Group,"Hey everyone! Iâ€™m starting a beginner-friendly Data Engineering group to learn, share resources, and stay motivated together.

If youâ€™re just starting out and want support, accountability, and useful learning materials, drop a comment or DM me! Letâ€™s grow together.

Here's the whatsapp link to join:
https://chat.whatsapp.com/GfAh5OQimLE7uKoo1y5JrH",9,13,Important_Age_552,2025-04-02 05:28:15,https://www.reddit.com/r/dataengineering/comments/1jph8ei/creating_a_beginner_data_engineering_group/,0,False,2025-04-02 05:40:31,False,False,2025-04-02 05:28:15,5,Wednesday,46.0,349,34.42,4,87,12.2,1,0
112,1jppvzy,"For those who work in data governance but in a data engineering capacity, what are you developing?","Recruiter reached out about a role on a data governance team but the job itself is data engineering. Recruiter was sharing what was in the job post but it didn't clarify much

I'm not formally experienced with data governance but have implemented data quality tests, written documentation, etc. Is that all considered data governance? What would be data engineering responsibilities and day to day work be like on a governance team? 

Would be interested to hear especially if anyone worked in and implemented data governance from scratch, and not used 3rd party software, as this team seems to be trying to do that.",8,5,opabm,2025-04-02 14:25:44,https://www.reddit.com/r/dataengineering/comments/1jppvzy/for_those_who_work_in_data_governance_but_in_a/,0,False,False,False,False,2025-04-02 14:25:44,14,Wednesday,103.0,616,42.11,5,173,15.1,0,0
113,1jq1ndb,"Hi, what does a data engineer do on a day-to-day basis in a company?","Right now I work as a data scientist, but I find it very, very repetitive.

That's why I'm studying Data Engineering concepts.  Right now, I'm able to create pipelines to automate ETL loads into Amazon Redshift databases (sort of) using Airflow with Dicker and Kubernetes.

I'm specialized in Python, so I'm also looking at Kafka and Apache PySpark.

Anyway, I'm just starting out in this field, so I feel overwhelmed and not sure what a company expects of me.

Help me understand your role better, thank you!",5,4,2blanck,2025-04-02 22:20:21,https://www.reddit.com/r/dataengineering/comments/1jq1ndb/hi_what_does_a_data_engineer_do_on_a_daytoday/,1,False,False,False,False,2025-04-02 22:20:21,22,Wednesday,87.0,509,56.76,6,139,11.9,0,0
114,1jpt9yo,Transition from on-prem to cloud,"Hi everyone,

Iâ€™ve been working in data for almost three years, mainly with on-prem technologies like SQL, SSIS, and Power BI, plus some experience with SSRS, datastage, Microstrategy and pl/SQL.

Lately, Iâ€™ve been looking for new opportunities, but most roles require Spark, Python, Databricks, Snowflake, and cloud experience, which I donâ€™t have. My company wonâ€™t move me to a cloud-related project, but they do pay for some certifications (mainly related to Azure/Microsoft)â€”Iâ€™ve done Azure Data Fundamentals and I'm currently taking a Databricks course and plan to take the certification after.

Whatâ€™s the best way to gain hands-on experience with cloud and these technologies? How did you make the transition?

Would love to hear your advice!",6,6,crassus96,2025-04-02 16:44:10,https://www.reddit.com/r/dataengineering/comments/1jpt9yo/transition_from_onprem_to_cloud/,0,False,False,False,False,2025-04-02 16:44:10,16,Wednesday,115.0,748,51.99,6,185,13.0,0,0
115,1jpyoqd,Which is easier? AWS or Azure,Data engineering on azure cloud easier or aws? which one would you say? im currently learning azure :p,4,10,Gloomy-Profession-19,2025-04-02 20:18:15,https://www.reddit.com/r/dataengineering/comments/1jpyoqd/which_is_easier_aws_or_azure/,0,False,False,False,False,2025-04-02 20:18:15,20,Wednesday,18.0,102,65.39,3,29,8.8,0,0
116,1jq07vo,Managing 1000's of small file writes from AWS Lambda,"Hi everyone,

I have a microservices architecture where I have a lambda function that takes an ID, sends it to an API for enrichment, and then resultant response is recorded in an S3 Bucket. My issue is that over \~200 concurrent lambdas and in effort to keep memory usage low, I am getting 1000's of small 30 - 200kb compressed ndjson files that make downstream computation a little challenging.

I tried to use Firehose but quickly get throttled and getting ""Slow Down."" error. Is there a tool or architecture decision I should consider besides just a downstream process that might consolidate these files perhaps in Glue?",2,4,Dallaluce,2025-04-02 21:21:25,https://www.reddit.com/r/dataengineering/comments/1jq07vo/managing_1000s_of_small_file_writes_from_aws/,1,False,False,False,False,2025-04-02 21:21:25,21,Wednesday,106.0,624,44.78,4,163,14.6,0,1
117,1jpwip4,DBA to Data Engineer,"Hi Everyone,
I have been working as an Oracle DBA for a while now, but I am not enjoying what am I doing. A year ago, I got interested in data engineering and tried to self-learn while juggling a full-time job, GRE prep(planning to go for masters as itâ€™s always been my dream), and everything elseâ€¦ safe to say, it wasnâ€™t easy. Since my job didnâ€™t really involve coding and I ended up with mostly theoretical knowledge. I do know Python, Azure(again theoretical knowledge) and SQL (thanks to work), but I still have a long way to go in data engineering. Now that Iâ€™m finally taking this step, I am thinking to quit my current job and put all my efforts solely on switching from DBA to data engineering. Iâ€™d really appreciate any advice on how to go about this what tech stacks I should focus on and whether transitioning within six months is realistic.",2,1,HistoricalPurchase62,2025-04-02 18:51:20,https://www.reddit.com/r/dataengineering/comments/1jpwip4/dba_to_data_engineer/,0,False,False,False,False,2025-04-02 18:51:20,18,Wednesday,155.0,852,53.75,6,235,13.0,0,0
118,1jplvx0,Suggestions for workflow automation,"Hey there :)

  
I hope I find myself in the right subreddit for this as I am trying to **engineer** my computer to push around some **data** ;) 

I'm currently working on a project to fully automate the processing of test results for a scientific study with students. 

The workflow consists of several stages:

1. **Data Extraction:** The test data is extracted from a local SQL database.
2. **SPSS Processing:** The extracted data is then processed using SPSS with a custom-built syntax (legacy). This step generates multiple files from the data. I have been looking into how I can transition this syntax to a python script, so this step might be cut later.
3. **Python Automation:** A Python script takes over the further processing. It reads the files, splits the data per class, inserts it into pre-designed Excel reporting templates.
4. **File Upload:** The files are then automatically uploaded to a self-hosted Nextcloud instance.
5. **Notification:** Once the workflow is complete, a notification  

I have been thinking about different ways to implement this. Right now the inputs and outputs for the different steps are still done manually. 

At work I have been using Jenkins lately and I think it feels natural  to do it in Jenkins and just describe the whole workflow in a pipeline with different stages to run. Besides that I have some experience with AWS Lambda and n8n but I am not sure if they would be helpful with this task.

IÂ´m not that experienced setting up such workflows as my work background is more in Infosec, so please forgive my uneducated guesses about how I best go about this :D Just trying not to take decisions that will be problematic later.



Greetings from Germany",2,2,wowdisme,2025-04-02 11:06:05,https://www.reddit.com/r/dataengineering/comments/1jplvx0/suggestions_for_workflow_automation/,0,False,False,False,False,2025-04-02 11:06:05,11,Wednesday,288.0,1705,60.55,15,443,12.2,0,0
119,1jpis19,How are you working with your DWH ?,"I would like to understand how you manage your DWW in day-to-day basis, solution, tools, architecture, workflows, ETL, serving...",2,1,Goumari,2025-04-02 07:17:28,https://www.reddit.com/r/dataengineering/comments/1jpis19/how_are_you_working_with_your_dwh/,0,False,False,False,False,2025-04-02 07:17:28,7,Wednesday,19.0,129,43.73,1,32,0.0,0,0
120,1jpdpyh,Facebook Marketing API - Anyone have a successful ETL experience?,"We have a python integration set up where we pull data from Google Ads and Facebook Marketing into our data warehouse. We're pulling data about all 3 hierarchy tiers and some daily metrics:

1. Campaigns (id, name, start time, stop time)
2. Ad Groups/Ad Sets (id, name)
3. Ads (id, name, URL)
4. Metrics (clicks, impressions, spend) for the previous day

For the Google Ads API, you basically send a SQL query and the return time is like a tenth of a second.

For Facebook, we see returns times in the minutes, especially on the Ads piece. Was hoping to get an idea of how others might have successfully set up a process to get this data from Facebook in a more timely fashion, and possibly without hitting the rate limiting threshold.

Not the exact code we're using - I can get it off my work system tomorrow - but the gist:

    from facebook_business.adobjects.adaccount import AdAccount
    from facebook_business.adobjects.campaign import Campaign
    from facebook_business.adobjects.ad import AdSet
    from facebook_business.adobjects.ad import Ad
    from facebook_business.adobjects.adcreative import AdCreative
    campaigns = AdAccount('act_123456789').get_campaigns(
        params={},
        fields=[Campaign.Field.id,Campaign.Field.name,Campaign.Field.start_time,Campaign.Field.stop_time]
    )
    adsets= AdAccount('act_123456789').get_ad_sets(
        params={},
        fields=[AdSet.Field.id,AdSet.Field.name]
    )
    ads = AdAccount('act_123456789').get_ads(
        params={},
        fields=[Ad.Field.id,Ad.Field.name,Ad.Field.creative]
    )
    object_urls = AdAccount('act_123456789').get_ad_creatives(
        params={},
        fields=[AdCreative.Field.object_story_spec]
    )
    asset_urls = AdAccount('act_123456789').get_ad_creatives(
        params={},
        fields=[AdCreative.Field.asset_feed_spec]
    )

We then have to do some joining between ads/object\_urls/asset\_urls to match the Ad with the destination URL if the ad is clicked on.

The performance is so slow, that I hope we are doing it wrong. I was never able to get the batch call to work and I'm not sure how to improve things.

Sincerely a data analyst who crosses over into data engineering because our data engineers don't know python.",2,6,farm3rb0b,2025-04-02 02:17:27,https://www.reddit.com/r/dataengineering/comments/1jpdpyh/facebook_marketing_api_anyone_have_a_successful/,0,False,False,False,False,2025-04-02 02:17:27,2,Wednesday,273.0,2244,44.71,27,482,10.1,0,1
121,1jpdpnz,Resources for learning AbInitio Tool,"I tried to search the entire internet to find AbInito related tutorials/tranings. Hard luck finding anything. I came to know it's a closed source tool and everything is behind a login wall only for partner companies. 

Can anyone share me stuff they found useful?

Thanks in advance.",2,2,IdealBusiness6499,2025-04-02 02:17:07,https://www.reddit.com/r/dataengineering/comments/1jpdpnz/resources_for_learning_abinitio_tool/,0,False,False,False,False,2025-04-02 02:17:07,2,Wednesday,47.0,283,61.93,5,74,10.4,0,0
122,1jq2h0a,Feeling stuck. How to move ahead,"I have been working for a consulting firm for the past 5 years. The kind of work they assign me to is fairly basic - developing pipelines using Informatica and writing SQL queries for it. That's been majority of my experience. For the past # months, I've been assigned to a PowerBI developer role, but I just tweak the data/queries to do what the client asks. When I try to apply for data engineering/etl roles, I get asked what I think are pretty advanced questions - for example I got asked about what gaps I have noticed in Microsoft Fabric and what are best practices for data modeling etc. I tend to give general answera based on my research and theoretical answers, but I can never relate it to my actual experience because day to day I don't do anything high level. I get asked about how I optimzied queries or pipelines, the truth is I worked with small enough datasets that I never really had to do anything. Again, I give answers based on my research - like indexing or partitioning but I feel the people asking questions are always looking for more. 

I cannot leave or take a break, I'm on a visa, but how do I actually get further then. Is anyone else feeling the same? ",3,0,AppointmentFit5600,2025-04-02 22:56:33,https://www.reddit.com/r/dataengineering/comments/1jq2h0a/feeling_stuck_how_to_move_ahead/,1,False,False,False,False,2025-04-02 22:56:33,22,Wednesday,219.0,1183,58.11,10,324,12.3,0,0
123,1jq26eg,Resources to learn developing production-ready APIs?,"Books, articles, courses... what resources have been useful to you for learning how to develop production-ready APIs? Production-ready meaning robust, secure, performant, modular etc

Thanks!",1,1,JLTDE,2025-04-02 22:43:35,https://www.reddit.com/r/dataengineering/comments/1jq26eg/resources_to_learn_developing_productionready_apis/,1,False,False,False,False,2025-04-02 22:43:35,22,Wednesday,25.0,191,29.21,3,50,12.5,0,0
124,1jpyr3w,Where next with my DE journey?,"I have completed Microsoft Azure Data Engineering (DP 203) certification which has given me a solid foundation of data engineering on Azure. 

Next, I followed along and did this project by Ansh Lamba: [https://www.youtube.com/watch?v=uc-u\_juRg-w&t=16941s&ab\_channel=AnshLamba](https://www.youtube.com/watch?v=uc-u_juRg-w&t=16941s&ab_channel=AnshLamba) 

  
What should be my next step to enhance my skills? Any recommendation? 4 weeks ago I didn't know anything about data engineering :p",2,0,Gloomy-Profession-19,2025-04-02 20:21:01,https://www.reddit.com/r/dataengineering/comments/1jpyr3w/where_next_with_my_de_journey/,0,False,False,False,False,2025-04-02 20:21:01,20,Wednesday,57.0,490,23.12,4,114,12.2,1,0
125,1jpy7vu,Yet another iceberg catalog choice question,"We are an AWS and Databricks shop. We want to explore open source engines for cost savings and reduce vendor lock. 

We want to introduce iceberg. This interoperability with Flink, Snowflake, Trino. 

We are considering Glue,  Snowflake-version-of-Polaris or another catalog.

I appreciate any recommendations and experices from this group.

  
Databricks unity-uniform enables reading the data as a iceberg table but we cannot write a table using Flink. We use Trino and Snowflake for reads.

",1,3,SupermarketMost7089,2025-04-02 19:59:54,https://www.reddit.com/r/dataengineering/comments/1jpy7vu/yet_another_iceberg_catalog_choice_question/,0,False,False,False,False,2025-04-02 19:59:54,19,Wednesday,74.0,494,53.58,8,125,9.5,0,0
126,1jpumat,What is a research and BI analyst?,"Hey, before this gets taken down \*I have read the wiki and it did not answer my question\*

I've just signed the contract for a Data Engineering role, but it lists me as a Research and BI Analyst without any mention of data engineering. I should note I'm gonna be an intern and I have zero corporate experience so job titles are new territory for me, sorry if it's really obvious and I'm being clueless.

Is this is a type of data engineer? Have they made a mistake on the contract? Does BI stand for Business Intelligence? What do I even do???

The Analyst bit makes me quite happy because that's what I ultimately want to do in the future but I'm kind of confused as to how this is data engineering as all my other research leading up to this contract tells me Data Analysts and Data Engineers are different lol any help appreciated, thank you!",1,2,popsicola13,2025-04-02 17:36:47,https://www.reddit.com/r/dataengineering/comments/1jpumat/what_is_a_research_and_bi_analyst/,0,False,False,False,False,2025-04-02 17:36:47,17,Wednesday,157.0,847,57.2,7,235,11.8,0,0
127,1jpu6nc,Massively scalable collaborative text editor backend with Rama in 120 LOC,,1,0,nathanmarz,2025-04-02 17:20:03,https://blog.redplanetlabs.com/2025/04/01/massively-scalable-collaborative-text-editor-backend-with-rama-in-120-loc/,0,False,False,False,False,2025-04-02 17:20:03,17,Wednesday,,0,206.84,1,0,0.0,0,0
128,1jpu66r,Roast my simple project. STAR schema database containing London weather data,"Hey all,

I've just created my second mini-project. Again, just to practice the skill I have learnt through DataCamp's courses.

  
I imported London's weather data via OpenWeather's API, cleaned it and created a database from it (STAR Schema)

  
If I had to do it again I will probably write functions instead of doing transformations manually. I really don't know why I didn't start of using function

  
I think my next project will include multiple different data sources and will also include some form of orchestration.

Here is the link: [https://www.datacamp.com/datalab/w/6aa0a025-9fe8-4291-bafd-67e1fc0d0005/edit](https://www.datacamp.com/datalab/w/6aa0a025-9fe8-4291-bafd-67e1fc0d0005/edit)

Any and all feedback is welcome.

Thanks!",0,6,godz_ares,2025-04-02 17:19:33,https://www.reddit.com/r/dataengineering/comments/1jpu66r/roast_my_simple_project_star_schema_database/,0,False,False,False,False,2025-04-02 17:19:33,17,Wednesday,97.0,745,38.11,6,174,11.9,1,1
129,1jpn1do,Help a noob understand whether this is feasible,"Hey all,
Iâ€™m working on a project that involves building a comprehensive overview of all therapist-related businesses in my country. Iâ€™ve found a public online source that lists approximately 16,000 such businesses, spread across many paginated result pages.

Each entry links to a detail page with information such as:

Business name
Business owner (person or legal entity)
Registration number (similar to a company ID)
Location (optional)
No consistent link to a website, but it's often listed in the details

What I need help with:

(1) Scrape all business data into a structured list (CSV, JSON or database).
This involves crawling through all paginated pages and collecting each business profileâ€™s content.

(2) Automatically search for a homepage/website for each business.
The source doesn't always list websites, so for those missing, I'd like to auto-search Google (or use a business API if necessary) to find the most likely company homepage.
(3) If a homepage is found: scrape relevant data from the website itself.

Goal:
To build a clean, filterable dataset that can be used for matching clients with therapists (via a separate platform I'm developing).

Questions Iâ€™d like help with:

Is this technically feasible using open tools or affordable APIs? What/who exactly would I be looking for? I have tried navigating Fiverr, but I am simply not sure what I need to be frank...

Thanks in advance!",1,4,SuburbNacho,2025-04-02 12:12:24,https://www.reddit.com/r/dataengineering/comments/1jpn1do/help_a_noob_understand_whether_this_is_feasible/,0,False,False,False,False,2025-04-02 12:12:24,12,Wednesday,225.0,1409,52.39,12,368,12.9,0,0
130,1jpmgvn,Feedback on Terraform Data Stack Starter,"Hi, everyone!

I'm a solo data consultant and over the past few years, Iâ€™ve been helping companies in Europe build their data stacks.

I noticed I was repeatedly performing the same tasks across my projects: setting up dbt, configuring Snowflake, and, more recently, migrating to Iceberg data lakes.

So I've been working on a solution for the past few months called [**Boring Data**](http://boringdata.io).

It's a set of Terraform templates ready to be deployed in AWS and/or Snowflake with pre-built integrations for ELT tools and orchestrators.

I think these templates are a great fit for many projects:

* Pay once, own it forever
* Get started fast
* Full control

I'd love to get feedback on this approach, which isn't very common (from what I've seen) in the data industry.

Is Terraform commonly used on your teams, or is that a barrier to using templates like these?

Is there a starter template that you'd wished you had for an implementation in the past?",1,3,Economy-Spread1955,2025-04-02 11:40:59,https://www.reddit.com/r/dataengineering/comments/1jpmgvn/feedback_on_terraform_data_stack_starter/,0,False,False,False,False,2025-04-02 11:40:59,11,Wednesday,162.0,967,56.89,7,238,12.0,1,0
131,1jpm06j,Help with a data engineering project,"Hello guys, me and teammates want to do a project from a-z to practice what we learned in an internship we are in and we wanted to the project to be about a telecom companyâ€™s data and we have searched a lot for a dataset that mimics the datasets of real telecom companies but we never found what we are looking for so we thought about creating the data we want using AI but for some reason itâ€™s also not working out for us so i would love to hear some suggestions about what we should do and about telecom data warehouses and databases because i feel maybe we just donâ€™t still quite understand how telecom companies generally operate and perhaps thatâ€™s why we are not successful in generating the data.

I hope this post makes sense because iâ€™m just very confused and donâ€™t know what to do for this project. 

Thank you for anyone who will respond in advance!",1,0,greyishcuneyd,2025-04-02 11:13:35,https://www.reddit.com/r/dataengineering/comments/1jpm06j/help_with_a_data_engineering_project/,0,False,False,False,False,2025-04-02 11:13:35,11,Wednesday,160.0,859,25.84,3,233,17.9,0,0
132,1jpk2oe,[BIGQUERY] How long does it take for a backfill and for the buffer resulting from that to clear?,"Hey all, 

1. I have two tables which are about 20-30 gbs and I created a backfill for them as I noticed that two days data was missing, now after an hour the backfill completed, now I am seeing some items in the streaming buffer, I need to update my seniors when the data is ready for analysis, so when can I safely say the data is present?

2. Also, one more question, if I insert a row manually into Bigquery and then create a backfill for it to fetch the data again from transactional database, will the entry I added manually (which doesn't exist in transactional database) be erased?

3. Is there a way to track the ingestion of data into BigQuery?

",1,2,Weird-Trifle-6310,2025-04-02 08:57:23,https://www.reddit.com/r/dataengineering/comments/1jpk2oe/bigquery_how_long_does_it_take_for_a_backfill_and/,0,False,False,False,False,2025-04-02 08:57:23,8,Wednesday,123.0,656,48.67,4,185,12.2,0,0
133,1jpjdes,Beginner using API (AWS),"Hi. I work for the state and some of the tools we have are limited. Each week I go to AWS QuickSight to download a CSV file back to our NAS drive where it feeds my Power BI dashboard. I have a gateway setup for cloud to talk to my on-premise NAS drive so auto refresh works. 

Now, my next task: I want to automate the AWS data directly from Power BI so I donâ€™t have to log into their website each week but how do I accomplish this without a programming background? (I majored in Asian History so I donâ€™t know much about data engineering/setting up pipelines)

I read some articles and it seems to indicate that using API can accomplish this but I donâ€™t know Python/SDKs nor do I use CLI (I did some Powershell) and even if I do what services should I use to run CLI for me behind the scenes? Can Power BI make API calls and handle JSON? 

Thanks ðŸ™ ",1,0,RameshYandapalli,2025-04-02 08:02:21,https://www.reddit.com/r/dataengineering/comments/1jpjdes/beginner_using_api_aws/,0,False,False,False,False,2025-04-02 08:02:21,8,Wednesday,166.0,849,60.48,6,225,12.5,0,0
134,1jpj0a0,Is my career choice taking me away from Data engineering jobs ?,"Hello everyone,

First of all English is not my first language so I apologize if there are mistakes or if everything is not clear.

I've been working for 6 years and my career path is not very consistent.  
I started in non-technical positions for 3 years and then moved on to a more technical one. 

For 3 years I had a very diversified job with software development (Php, Python), database management, Linux system administration, a bit of Cloud and a big part of â€œDataâ€ with ETL flows (Talend) and a lot of SQL. The project was quite large and the team very small, so I was working on several tasks at once.

I really enjoyed the Data part and I got it into my head that I wanted to be a 'real' Data Engineer and not just drag and drop on Talend.

I was just starting my research when a friend of mine contacted me because a software engineer position was opening up in his company. I went through the recruitment process and accepted their proposal.

  
As in my previous position, I'll be working on a lot of things (mobile development, backend, a bit of frontend, cloud, devops) and the salary offered was 20% higher than what I had in my previous job. (I'm now at 48kâ‚¬ and I don't live in a big city).  
The offer was really attractive and as the market is a bit complicated at the moment, I accepted.

But I'm wondering if this choice will take me even further away from the Data Engineer job i wanted.

Do you find my career path coherent?  
Could I switch back to Data in a few years' time?

Thank you for reading me !",0,16,Wapame92,2025-04-02 07:34:18,https://www.reddit.com/r/dataengineering/comments/1jpj0a0/is_my_career_choice_taking_me_away_from_data/,0,False,False,False,False,2025-04-02 07:34:18,7,Wednesday,286.0,1528,69.11,15,412,11.7,0,0
135,1jpiq61,Unable to copy data from mysql to azure on Mac,I am trying to load/copy data from a local mysql database in my mac into azure using Data factory. Most of the material i found online suggest to created an integration runtime which requires an installation of an app aimed at windows Os. Is there a way where i could load/copy data from my mysql on mac into azure ?,1,0,Old_Championship610,2025-04-02 07:13:52,https://www.reddit.com/r/dataengineering/comments/1jpiq61/unable_to_copy_data_from_mysql_to_azure_on_mac/,0,False,False,False,False,2025-04-02 07:13:52,7,Wednesday,60.0,316,51.48,3,92,11.9,0,0
136,1jpdwy1,Knime on Anaconda Nacigator,Is it possible to install Knime on Anaconda Navigator? ,1,0,Puzzleheaded_Serve39,2025-04-02 02:25:22,https://www.reddit.com/r/dataengineering/comments/1jpdwy1/knime_on_anaconda_nacigator/,0,False,False,False,False,2025-04-02 02:25:22,2,Wednesday,9.0,55,20.04,1,19,0.0,0,0
137,1jpqnx3,I Want To Improve an Internal Process At My Company,"Hey r/dataengineering,

I'm currently transitioning from a software engineering role to data engineering, and I've identified a potential project at my company that I think would be a great learning experience and a chance to introduce some data engineering best practices.

Project Overview:

We have a dashboard that displays employee utilization data, sourced from two main systems: Harvest (time tracking) and Forecast (projected utilization).

Current Process:

* Harvest Data: Currently, we're using cron jobs running on an EC2 instance to periodically pull data from Harvest.
* Forecast Data: Due to the lack of an API, we're relying on Playwright (web scraping) to extract data from their web reports, which are then saved to S3.
* Data Processing: Another cron job on EC2 processes the S3 reports and loads the data into a PostgreSQL database.
* Dashboard: A custom frontend application (using Azure OAuth) queries the PostgreSQL database to display the utilization data.

Proposed Solution:

I'm proposing a serverless architecture on AWS, using the following components:

* API Gateway + Lambda: To create a robust API for our frontend application.
* Lambda for ETL: To automate data extraction, transformation, and loading from Harvest and Forecast.
* AWS Step Functions: To orchestrate the data pipeline and manage dependencies.
* Amazon RDS PostgreSQL: To serve as our data warehouse for analytical queries.
* API Gateway Authorizer: To integrate Azure OAuth authentication.
* CI/CD with CodePipeline and CodeBuild: To automate testing and deployment.
* Docker and SAM CLI: For local development and testing.

My Goals:

* Gain hands-on experience with AWS serverless technologies.
* Implement data engineering best practices for ETL and data warehousing.
* Improve the reliability and scalability of our data pipeline.
* Potentially expand this architecture to serve as a central data warehouse for other company analytical data.

My Questions:

1. For those with experience in similar projects, what are some key considerations or potential challenges I should be aware of?
2. Any advice on best practices for designing and implementing a serverless data pipeline on AWS?
3. Are there any specific AWS services or tools that you would recommend for this project?
4. How would you recommend getting started on a project like this, what would you focus on first?
5. What would be some good ways to test this type of system?

I'm eager to learn and contribute, and I appreciate any insights or advice you can offer.

Thanks!",0,3,Tajcore,2025-04-02 14:57:52,https://www.reddit.com/r/dataengineering/comments/1jpqnx3/i_want_to_improve_an_internal_process_at_my/,0,False,False,False,False,2025-04-02 14:57:52,14,Wednesday,400.0,2537,38.32,24,688,13.3,0,1
138,1jpo030,"How would you solve a low-tech, distributed attendance tracking and service impact problem for a nonprofit with no digital infrastructure?","Iâ€™m working with a nonprofit, supporting 17 veteran communities. The communities arenâ€™t brick-and-mortar â€” they meet at churches and community spaces, and track attendance manually. Thereâ€™s very little technology â€” no computers, mostly just phones and Facebook.

They want to understand:
	â€¢	What services are being offered at the community level
	â€¢	Whoâ€™s attending (recurring vs new)
	â€¢	No-show rates
	â€¢	Cost per veteran for services

The challenge: no digital systems or staff capacity for manual data entry.

What tech-light solutions or data collection flows would you recommend to gather this info and make it analyzable? Bonus if it can integrate later with HubSpot or a simple PostgreSQL DB.",0,2,FunEstablishment77,2025-04-02 13:01:43,https://www.reddit.com/r/dataengineering/comments/1jpo030/how_would_you_solve_a_lowtech_distributed/,0,False,False,False,False,2025-04-02 13:01:43,13,Wednesday,109.0,697,37.1,6,189,15.0,0,1
139,1jpaq8v,Data Developer vs Data Engineer,"I know it varies by company blah blah blah, but also aside from a Google search, what have you guys in the field noticed to be core differences between these positions? ",0,3,diabeticspecimen,2025-04-01 23:53:39,https://www.reddit.com/r/dataengineering/comments/1jpaq8v/data_developer_vs_data_engineer/,0,False,False,False,False,2025-04-01 23:53:39,23,Tuesday,31.0,169,56.93,1,44,0.0,0,0
140,1jpz36k,How AI will dramatically change DE,"After some struggle with a pipeline today, Gemini 2.5 one-shotted the solution. It's superior in most software problems compared to humans (check coders eval) and we're just two and a half years in.

The capabilities are mind-bending. Data engineering as we know it will change drastically with new AI tooling and self-adjusting infrastructure.

We know this profession will evolve drastically. What do you think where things are heading and how to hedge against AI? Become more social / human I guess ðŸ˜‚

A few hypotheses:
- pipelines and infra manages itself with much higher accuracy and less misconfigurations
- the data engineer profile will shift, they become subject matter experts, they must understand the business and do product management
- technical skills do not matter since the gap from idiot to genius is much smaller than from genius to agi/asi",0,8,Ok-Sentence-8542,2025-04-02 20:34:45,https://www.reddit.com/r/dataengineering/comments/1jpz36k/how_ai_will_dramatically_change_de/,0,False,False,False,False,2025-04-02 20:34:45,20,Wednesday,140.0,860,54.32,8,219,12.2,0,0
141,1jpjmaa,Want to know Data engineering hiring trend at present in India,"Until about a month ago hiring seemed to be freezed - lot of fake job postings, people posting google form links collecting resumes, reposting old job roles on linkedin...  Then since about three weeks ago, it seemed like hring is restarted. But now I am having my doubts again - ghosted by recruiters after first screening even told me my CV fits the role well. And not getting other shortlists too. Another thing is huge range of experience 3 yrs - 7 yrs , 2 yrs to 9 yrs experience being posted for majority of the JDs. Obviously if a 7 yrs candidate and if a 3 yrs candidate applies to the same role, they would prefer the 7 yrs exp candidate. What's going on these days? Are they not hiring anyone below 6/7 yrs work exp at all?",0,4,life_Bittersweet,2025-04-02 08:21:32,https://www.reddit.com/r/dataengineering/comments/1jpjmaa/want_to_know_data_engineering_hiring_trend_at/,0,False,False,False,False,2025-04-02 08:21:32,8,Wednesday,139.0,733,71.24,8,194,10.4,0,0
142,1jr15ej,What's the non-technical biggest barrier you face at work?,"Whatâ€™s currently challenging for me is getting access to things.

I design a data pipeline, present it to the team that will benefit from it, and everyone gets super excited.

Then I reach out to the internal department or an external party to either grant me admin access to the platform I need, or to help me obtain an API.

A week goes byâ€”nothing. I follow up via email. Eventually, someone replies and says it's not possible to give me admin credentials. Fine. So I ask, â€œCan you help me get the API instead? Itâ€™s very straightforward.â€

Another week goes byâ€”still nothing. I send another follow-upâ€¦

Now the other person is kind of frustrated (because Iâ€™m asking them to do something slightly different, even though Iâ€™m offering guidance).

What follows is just a back-and-forth with long, frustrating waiting periods in between. Meanwhile, the team I presented the pipeline or project to starts getting frustrated with me and probably thinks Iâ€™m full of crap.

Once I finally get the damn API or whatever access I needed, I complete the project in 1â€“2 days but delayed by weeks or even months.

Aaaaaaah!",48,17,sirtuinsenolytic,2025-04-04 02:23:50,https://www.reddit.com/r/dataengineering/comments/1jr15ej/whats_the_nontechnical_biggest_barrier_you_face/,0,False,False,False,False,2025-04-04 02:23:50,2,Friday,190.0,1110,65.12,13,293,11.5,0,0
143,1jr68kn,Are Hyperscalers becoming more expensive in Europe due to the tariffs?,"Hi,

With the recent tariffs in mind, are cloud providers like AWS, Azure, and Google Cloud becoming more expensive for European companies? And what about other techs like Snowflake or Databricks â€“ are they affected too?

Would it be wise for European businesses to consider open-source alternatives, both for cost and strategic independence?

And from a personal perspective: should we, as employees, expand our skill sets toward open-source tech stacks to stay future-proof?",30,26,Ok-Inspection3886,2025-04-04 07:31:32,https://www.reddit.com/r/dataengineering/comments/1jr68kn/are_hyperscalers_becoming_more_expensive_in/,0,False,False,False,False,2025-04-04 07:31:32,7,Friday,73.0,476,44.75,4,120,14.6,0,0
144,1jr70yg,Which tool do you use to move data from the cloud to Snowflake?,"Hey, r/dataengineering 

Iâ€™m working on a project where I need to move data from our cloud-hosted databases into Snowflake, and Iâ€™m trying to figure out the best tool for the job. Ideally, Iâ€™d like something thatâ€™s cost-effective and scales well. 

If youâ€™ve done this before, what did you use?
Would love to hear about your experienceâ€”how reliable it is, how much it roughly costs, and any pros/cons youâ€™ve noticed. Appreciate any insights!

[View Poll](https://www.reddit.com/poll/1jr70yg)",7,12,Many-Tart-7661,2025-04-04 08:30:40,https://www.reddit.com/r/dataengineering/comments/1jr70yg/which_tool_do_you_use_to_move_data_from_the_cloud/,0,False,False,False,False,2025-04-04 08:30:40,8,Friday,74.0,491,56.45,5,120,10.8,1,0
145,1jrfp85,Data Engineer Consulting Rate?,"I currently work as a mid-level DE (3y) and Iâ€™ve recently been offered an opportunity in Consulting. Iâ€™m clueless what rate I should ask for. Should it be 25% more than what I currently earn? 50% more? Double!? 

I know that leaping into consulting means compromising job stability and higher expectations for deliveries, so I want to ask for a much higher rate without high or low balling a ridiculous offer. Does someone have experience going from DE to consultant DE? Thanks!",5,16,ActRepresentative378,2025-04-04 16:11:28,https://www.reddit.com/r/dataengineering/comments/1jrfp85/data_engineer_consulting_rate/,0,False,False,False,False,2025-04-04 16:11:28,16,Friday,82.0,478,54.83,5,130,12.7,0,0
146,1jre0v2,Logging in Spark applications.,"Hi guys, i am moving to on-prem managed Spark applications with Kuberenetes. I am wondering what do u use for logging? I am talking about Python and PySpark. Do u setup log4j? Or just use Python's logging library for application? What is the standard here? I have not seen much about log4j within PySpark.",5,2,Hot_While_6471,2025-04-04 15:01:43,https://www.reddit.com/r/dataengineering/comments/1jre0v2/logging_in_spark_applications/,0,False,False,False,False,2025-04-04 15:01:43,15,Friday,54.0,305,72.12,7,79,7.4,0,0
147,1jrdue4,Anyone know of any vscode linter for sql that can accommodate pyspark sql?,"In pyspark 3.4 you can write sql as 

spark.sql(SELECT * FROM {df_input}, df_input = df_input) 

The popular sql linters I tried SQL Formatter and and Prettier SQL Vscode currently does not accommodate{}. Does anyone know of any linters that does? Thank you",5,0,AUGcodon,2025-04-04 14:54:19,https://www.reddit.com/r/dataengineering/comments/1jrdue4/anyone_know_of_any_vscode_linter_for_sql_that_can/,1,False,False,False,False,2025-04-04 14:54:19,14,Friday,42.0,257,44.41,4,72,12.6,0,1
148,1jr3z1u,Faster way to view + debug data,"Hi r/dataengineering!

  
I wanted to share a project that I have been working on.Â It's an intuitive data editor where you can interact with local and remote data (e.g. Athena & BigQuery). For several important tasks, it can speed you up by 10x or more. (see website for more)

  
For data engineering specifically, this would be really useful in debugging pipelines, cleaning local or remote data, and being able to easy create new tables within data warehouses etc.

I know this could be a lot faster than having to type everything out, especially if you're just poking around. I personally find myself using this before trying any manual work.

Also, for those doing complex queries, you can split them up and work with the frame visually and add queries when needed. Super useful for when you want to iteratively build an analysis or new frameÂ ***without writing a super long query.***

  
As for data size, it can handle local data up to around 1B rows, and remote data is only limited by your data warehouse.

  
You don't have to migrate *anything* either.

  
If you're interested, you can check it out here: [https://www.cocoalemana.com](https://www.cocoalemana.com)

  
I'd love to hear about your workflow, and see what we can change to make it cover more data engineering use cases.

  
Cheers!

[Coco Alemana](https://preview.redd.it/02wogjj72rse1.jpg?width=3820&format=pjpg&auto=webp&s=0905bd40927b4dd7e80521568982ebe82994a5fe)

",4,3,Impressive_Run8512,2025-04-04 05:00:10,https://www.reddit.com/r/dataengineering/comments/1jr3z1u/faster_way_to_view_debug_data/,0,False,False,False,False,2025-04-04 05:00:10,5,Friday,217.0,1443,44.75,12,361,11.2,1,0
149,1jr6a6h,How to stream results of a complex SQL query,"Hello,

I'm writing you because I have a problem with a side project and maybe here somebody can help me. I have to run a complex query with a potentially high number of results and it takes a lot of time. However, for my project I don't need all the results to be showed together, perhaps after some hours/days. It would be much more useful to get a stream of the partial results in real time. How can I achieve this? I would prefer to use free software, however please suggest me any solution you have in mind.

Thank you in advance!",3,12,forevernevermore_,2025-04-04 07:35:07,https://www.reddit.com/r/dataengineering/comments/1jr6a6h/how_to_stream_results_of_a_complex_sql_query/,0,False,False,False,False,2025-04-04 07:35:07,7,Friday,102.0,535,73.58,7,139,8.4,0,0
150,1jr23mk,How do I get out of this rut,"Iâ€™m currently about the finish an early career rotational program with a top 10 bank. The rotation I am currently on and where the company is placing me post program (I tried to get placed somewhere else) is as a data engineer on a data delivery team. When I was advertised this rotation and the team I was told pretty specifically we would be using all the relevant technologies and I would be very hands on keyboard building pipelines with python , configuring cloud services and snowflake, being a part of data modeling. Mind you Iâ€™m not completely new I have experience with all this in personal projects and previous work experience as a SWE and researcher in college. 

Turns out all of that was a lie. I later learned there is an army of contractors that do the actual work. I was stuck with analyzing .egp files and other SAS files documenting it and handing off to consultants to rebuild in Talend to ingest into snowflake. The only tech that I use is Visio and Word.

I coped with that by saying after Iâ€™m out of the program Iâ€™ll get to do the actual work. But I had a conversation with my manager today about what my role will be post program. He basically said there are a lot more of these SAS procedures they are porting over to talend and snowflake and Iâ€™ll be documenting them and handing over to contractors so they can implement the new process. Honestly that is all really quick and easy to do because there isnâ€™t that much complicated business logic for the LOBs we support just joins and the occasional aggregation so most days Iâ€™m not doing anything.

When I told him I would really like to be involved in the technical work or the data modeling , he said that is not my job anymore and that is what we pay the contractors to do so I canâ€™t do it. Almost made it seem like I should be grateful and he is doing me a favor somehow.

It just feels like I was misled or even outright lied to about the position. We donâ€™t use any of the technologies that were advertised (Drag and drop/low code tools seem like fake engineering), I donâ€™t get to be hands on keyboard at all. Just seems like there really I no growth or opportunity in this role. I would leave but I took relocation and a signing bonus for this and if I leave too early I owe it back. I also canâ€™t internally transfer anywhere for a year after starting my new role.

I guess my rant is just to ask what should I be doing in this situation? I work on personal projects and open source and I have gotten a few certs in the downtime at work but I donâ€™t know if itâ€™s enough to make sure my skills donâ€™t atrophy while I wait out my repayment period. I consider myself a somewhat technical guy but I have been boxed into a non technical role.

",3,6,anonymous_0618615740,2025-04-04 03:13:53,https://www.reddit.com/r/dataengineering/comments/1jr23mk/how_do_i_get_out_of_this_rut/,0,False,2025-04-04 09:08:09,False,False,2025-04-04 03:13:53,3,Friday,514.0,2718,65.76,23,736,12.3,0,0
151,1jr1r2t,"Built a real-time e-commerce data pipeline with Kinesis, Spark, Redshift & QuickSight â€” looking for feedback","I recently completed a real-time ETL pipeline project as part of my data engineering portfolio, and Iâ€™d love to share it here and get some feedback from the community.

# What it does:

* Streams transactional data using **Amazon Kinesis**
* Backs up raw data in **S3** (Parquet format)
* Processes and transforms data with **Apache Spark**
* Loads the transformed data into **Redshift Serverless**
* Orchestrates the pipeline with **Apache Airflow (Docker)**
* Visualizes insights through a **QuickSight dashboard**

# Key Metrics Visualized:

* Total Revenue
* Orders Over Time
* Average Order Value
* Top Products
* Revenue by Category (donut chart)

I built this to practice real-time ingestion, transformation, and visualization in a scalable, production-like setup using AWS-native services.

# GitHub Repo:

[https://github.com/amanuel496/real-time-ecommerce-etl-pipeline](https://github.com/amanuel496/real-time-ecommerce-etl-pipeline)

If you have any thoughts on how to improve the architecture, scale it better, or handle ops/monitoring more effectively, Iâ€™d love to hear your input.

Thanks!",5,6,MysteriousRide5284,2025-04-04 02:55:36,https://www.reddit.com/r/dataengineering/comments/1jr1r2t/built_a_realtime_ecommerce_data_pipeline_with/,0,False,False,False,False,2025-04-04 02:55:36,2,Friday,152.0,1103,11.08,4,265,18.8,1,0
152,1jramqt,"Airbyte Connector Builder now supports GraphQL, Async Requests and Custom Components","Hello, Marcos from the Airbyte Team.

For those who may not be familiar, Airbyte is an open-source data integration (EL) platform with over 500 connectors for APIs, databases, and file storage.

In our last release we added several new features to our no-code Connector Builder:

* [GraphQL Support](https://docs.airbyte.com/connector-development/config-based/understanding-the-yaml-file/request-options#graphql-request-injection): In addition to REST, you can now make requests to GraphQL APIs (and properly handle pagination!)
* [Async Data Requests](https://docs.airbyte.com/connector-development/connector-builder-ui/async-streams): There are some reporting APIs that do not return responses immediately. For instance, with Google Ads.Â  You can now request a custom report from these sources and wait for the report to be processed and downloaded.
* [Custom Python Code Components](https://docs.airbyte.com/connector-development/connector-builder-ui/custom-components): We recognize that some APIs behave uniquelyâ€”for example, by returning records as key-value pairs instead of arrays or by not ordering data correctly. To address these cases, our open-source platform now supports custom Python components that extend the capabilities of the no-code framework without blocking you from building your connector.

We believe these updates will make connector development faster and more accessible, helping you get the most out of your data integration projects.

We understand there are discussions about the trade-offs between no-code and low-code solutions. At Airbyte, transitioning from fully coded connectors to a low-code approach allowed us to maintain a large connector catalog using standard components.Â  We were also able to create a better build and test process directly in the UI. Users frequently give us the feedback that the no-code connector Builder enables less technical users to create and ship connectors. This reduces the workload on senior data engineers allowing them to focus on critical data pipelines.

Something else that has been top of mind is speed and performance. With a robust and stable connector framework, the engineering team has been dedicating significant resources to introduce concurrency to enhance sync speed. You can read this[ blog post](https://airbyte.com/blog/improving-connector-sync-speed-up-to-10x-faster) about how the team implemented concurrency in the Klaviyo connector, resulting in a speed increase of about 10x for syncs.

I hope you like the news! Let me know if you want to discuss any missing features or provide feedback about Airbyte.",3,2,marcos_airbyte,2025-04-04 12:26:58,https://www.reddit.com/r/dataengineering/comments/1jramqt/airbyte_connector_builder_now_supports_graphql/,0,False,False,False,False,2025-04-04 12:26:58,12,Friday,351.0,2602,30.77,23,654,12.4,1,1
153,1jrm80y,Marketing Report & Fivetran,"Fishing for advice as I'm sure many have been here before. I came from DE at a SaaS company where I was more focused on the infra but now I'm in a role much close to the business and currently working with marketing. I'm sure this could make the Top-5 all time repeated DE tasks. A daily marketing report showing metrics like Spend, cost-per-click, engagement rate, cost-add-to-cart, cost-per-traffic... etc. These are per campaign based on various data sources like GA4, Google Ads, Facebook Ads, TikTok etc. Data updates once a day.

It should be obvious I'm not writing API connectors for a dozen different services. I'm just one person doing this and have many other things to do. I have Fivetran up and running getting the data I need but MY GOD is it ever expensive for something that seems like it should be simple, infrequent & low volume. It comes with a ton of build in reports that I don't even need sucking rows and bloating the bill. I can't seem to get what I need without pulling millions of event rows which costs a fortune to do.

Are there other similar but (way) cheaper solutions are out there? I know of others but any recommendations for this specific purpose?",2,5,bcsamsquanch,2025-04-04 20:45:27,https://www.reddit.com/r/dataengineering/comments/1jrm80y/marketing_report_fivetran/,0,False,False,False,False,2025-04-04 20:45:27,20,Friday,211.0,1182,63.49,13,308,10.6,0,0
154,1jrd8po,PII Obfuscation in Databricks,"Hi Data Champs,

I have been recently given chance to explore PII obfuscation technique in databricks.

I proposed using sql aes_encryption or python fernet for PII column level encryption before landing to bronze.

And use column masking on delta tables which has built in logic for group membership check and decryption so to avoid the overhead of a new view per table.

My HDE was more interested in sql approach than the fernet but fernet offers built in key rotation out of the box.

Has anyone used aes_encryption 
Is it secure, easy to work with and relatively more robust.

From my experience for data type other than binary like long, int, double it needs to be first converted to binary (donâ€™t like it)

Apart from that usual error here and there for padding and generic error when decrypting sometimes.

So given the choice what will be your architecture 

What you will prefer, what you donâ€™t and why

I am open to DM if you wanna ðŸ’¬ ",2,1,Intelligent-Mind8510,2025-04-04 14:28:31,https://www.reddit.com/r/dataengineering/comments/1jrd8po/pii_obfuscation_in_databricks/,0,False,2025-04-04 14:33:56,False,False,2025-04-04 14:28:31,14,Friday,166.0,945,55.98,7,251,13.3,0,0
155,1jqzz3y,General question about data consulting,"Let's say there's a data consulting company working within a certain industry (e.g., utilities or energy). How do they gain access to their clients' databases if they want to perform ETL or other services? How about working with their data in a cloud setting (e.g., AWS)? What is the usual process for that? Is the consulting company responsible for setting and managing AWS costs, etc.?",2,9,No7-Francesco88,2025-04-04 01:23:44,https://www.reddit.com/r/dataengineering/comments/1jqzz3y/general_question_about_data_consulting/,0,False,False,False,False,2025-04-04 01:23:44,1,Friday,65.0,387,52.05,6,109,11.5,0,0
156,1jrohrl,Question about file sync,"Pardon the noob question. I'm building a simple ETL process using Airflow on a remote Linux server and need a way for users to upload input files and download processed files.

I would prefer a method that is easy to use for users like a shared drive (like Google Drive).

I've considered Syncthing, and in the worst case, SFTP access. What solutions do you typically use or recommend for this? Thanks!",3,1,CraftedLove,2025-04-04 22:25:11,https://www.reddit.com/r/dataengineering/comments/1jrohrl/question_about_file_sync/,1,False,False,False,False,2025-04-04 22:25:11,22,Friday,71.0,402,73.98,5,101,8.2,0,0
157,1jrd0k2,Great Expectations Implementation,"Our company is implementing data quality testing and we are interested in borrowing from the Great Expectations suite of open source tests. I've read mostly negative reviews of the initial implementation of Great Expectations, but am curious if anyone else set up a much more lightweight configuration?

Ultimately, we plan to use the GX python code to run tests on data in Snowflake and then make the results available in Snowflake. Has anyone done something similar to this?",1,2,HAKOC534,2025-04-04 14:18:53,https://www.reddit.com/r/dataengineering/comments/1jrd0k2/great_expectations_implementation/,0,False,False,False,False,2025-04-04 14:18:53,14,Friday,78.0,476,43.22,4,134,14.9,0,0
158,1jrc62b,Can you call an aimless star schema a data mart?,"So,

  
as always that's for the insight from other people, I find a lot of these discussions around points very entertaining and very helpful!

I'm having an argument with someone who is several levels above me. This might sound petty so I apologise in advance. It centres around the definition of a Mart. Our Mart is a single Fact with around 20 dimensions. The Fact is extremely wide and deep. Indeed we usually put it into a de normalised table for reporting. To me this isn't a MART as it isn't based on requirements but rather a star schema that supposedly servers multiple purposed or potential purposes. When engaged on requirements the person leans on there experience in the domain and says a user probable wants to do X, Y and Z. I've never seen anything written down. Constantly that report also defers to Kimball methodology and how this follows them closely. My take on the book is that these things need to be based of requirement, business requirements. 

My questions is, is it fair to say that a data mart needs to have requirements and ideally a business domain in mind or else its just a star schema?

Yes this  is  very theoretical... yes I probable need a hobby but look there hasn't been a decent RTS game in years and its friday!!!

Have a good weekend everyone",1,3,ObjectiveAssist7177,2025-04-04 13:41:31,https://www.reddit.com/r/dataengineering/comments/1jrc62b/can_you_call_an_aimless_star_schema_a_data_mart/,0,False,False,False,False,2025-04-04 13:41:31,13,Friday,229.0,1285,65.42,16,347,10.6,0,0
159,1jrbgqt,Data Engineering Performance -  Authors,I having worked in BI and transitioned to DE have followed best practices reading books by authors like Ralph Kimball in BI. Is there someone in DE with a similar level of reputation. I am not looking for specific technologies but rather want to pick up DE fundamentals especially in the performance and optimization space.,1,1,Amar_K1,2025-04-04 13:08:48,https://www.reddit.com/r/dataengineering/comments/1jrbgqt/data_engineering_performance_authors/,0,False,False,False,False,2025-04-04 13:08:48,13,Friday,55.0,323,52.9,3,89,13.6,0,0
160,1jravos,Unstructured Data,"I see this has been asked prior but I didn't see a clear answer. We have a smallish database (glorified spreadsheet) where one field contains text. It houses details regarding customers, etc calling in for various issues. For various reasons (in-house) they want to keep using the simple app (it's a SharePoint List). I can easily download the data to a CSV file, for example, but is there a fairly simple method (AI?) to make sense of this data and correlate it? Maybe a creative prompt? Or is there a tool for this? (I'm not a software engineer). Thanks!",1,5,Top_Sink9871,2025-04-04 12:39:42,https://www.reddit.com/r/dataengineering/comments/1jravos/unstructured_data/,0,False,False,False,False,2025-04-04 12:39:42,12,Friday,99.0,556,68.77,9,149,10.0,0,0
161,1jr6oc9,Do you need statistics to land a DE job?,"As the title suggests. Even if stats are not used on the job, will having stats qualifications give me an edge in the hiring process?",1,20,Normal-Bandicoot-180,2025-04-04 08:03:59,https://www.reddit.com/r/dataengineering/comments/1jr6oc9/do_you_need_statistics_to_land_a_de_job/,0,False,False,False,False,2025-04-04 08:03:59,8,Friday,25.0,133,75.71,2,35,0.0,0,0
162,1jr05id,[Seeking Guidance] Aspiring GCP Data Engineer â€“ Will Work Pro Bono for Hands-On Experience!,"Hey r/dataengineering community,  

Iâ€™m deep into prepping for the Google Cloud Professional Data Engineer cert and want to transition from theory to real-world projects. To ace the exam and build job-ready skills, Iâ€™m looking for:  

- Hands-on opportunities (pro bono!) to work with GCP tools like BigQuery, Dataflow, Pub/Sub, Cloud Composer, etc.  
- Mentorship or collaboration on data pipelines, workflow optimization, or cloud architecture projects.  
- Open-source/community projects needing an extra pair of hands.  

Why me? Iâ€™m motivated, detail-oriented, and eager to learn. Iâ€™ll treat your project like my own!  

If youâ€™re working on anything data-related in GCP - or know someone who is - Iâ€™d hugely appreciate a chance to contribute (or even just advice on where to start). Comment/DM me, and thanks for being an awesome community!  

P.S. Upvotes for visibility help a ton! ðŸ™",1,3,aiqdec,2025-04-04 01:33:00,https://www.reddit.com/r/dataengineering/comments/1jr05id/seeking_guidance_aspiring_gcp_data_engineer_will/,0,False,False,False,False,2025-04-04 01:33:00,1,Friday,137.0,891,49.72,10,227,11.8,0,0
163,1jrmmcn,AI agent for complex query,"https://www.reddit.com/r/AI_Agents/s/iKnUXMLoxZ

",0,1,Future_Scar_7875,2025-04-04 21:02:34,https://www.reddit.com/r/dataengineering/comments/1jrmmcn/ai_agent_for_complex_query/,0,False,False,False,False,2025-04-04 21:02:34,21,Friday,1.0,49,-386.39,1,7,0.0,1,0
164,1jrd286,Just wanted to share a recent win that made our whole team feel pretty good.,"We worked with this e-commerce client last month (kitchen products company, can't name names) who was dealing with data chaos.

When they came to us, their situation was rough. Dashboards taking forever to load, some poor analyst manually combining data from 5 different sources, and their CEO breathing down everyone's neck for daily conversion reports. Classic spreadsheet hell that we've all seen before.

We spent about two weeks redesigning their entire data architecture. Built them a proper [**data warehouse solution** ](https://datafortune.com/services/enterprise-data-management/data-warehouse/)with automated ETL pipelines that consolidated everything into one central location. Created some logical data models and connected it all to their existing BI tools.

The transformation was honestly pretty incredible to watch. Reports that used to take hours now run in seconds. Their analyst actually took a vacation for the first time in a year. And we got this really nice email from their CTO saying we'd ""changed how they make decisions"" which gave us all the warm fuzzies.

It's projects like these that remind us why we got into this field in the first place. There's something so satisfying about taking a messy data situation and turning it into something clean and efficient that actually helps people do their jobs better.",0,9,DataMaster2025,2025-04-04 14:20:44,https://www.reddit.com/r/dataengineering/comments/1jrd286/just_wanted_to_share_a_recent_win_that_made_our/,0,False,False,False,False,2025-04-04 14:20:44,14,Friday,203.0,1339,48.3,14,343,12.0,1,0
165,1ju81cr,Jira: Is it still helping teams... or just slowing them down?,"Iâ€™ve been part of (and led) a teams over the last decade â€” in enterprises

And one tool keeps showing up everywhere: **Jira**.

Itâ€™s the ""default"" for a lot of engineering orgs. Everyone knows it. Everyone uses it.  
But **I donâ€™t seen anyone who actually likes it.**

Not in the *""ugh it's corporate but fine""* way â€” I mean people who are actively frustrated by it but still use it daily.

Here are some of the most common friction points Iâ€™ve either experienced or heard from other devs/product folks:

1. **Custom workflows spiral out of control** â€” What starts as ""just a few tweaks"" becomes an unmanageable mess.
2. **Slow performance** â€” Large projects? Boards crawling? Yup.
3. **Search that requires sorcery** â€” Good luck finding an old ticket without a detailed Jira PhD.
4. **New team members struggle to onboard** â€” Itâ€™s not exactly intuitive.
5. **The â€œtool taxâ€** â€” Teams spend hours updating Jira instead of moving work forward.

And yet... most teams stick with it. Because switching is painful. Because â€œat least everyone knows Jira.â€ Because the alternative is more uncertainty.  
What's your take on this?",64,47,IllWasabi8734,2025-04-08 07:40:50,https://www.reddit.com/r/dataengineering/comments/1ju81cr/jira_is_it_still_helping_teams_or_just_slowing/,0,False,False,False,False,2025-04-08 07:40:50,7,Tuesday,189.0,1123,69.07,17,275,9.8,0,0
166,1jukwsu,Why do you dislike MS Fabric?,"Title.  I've only tested it. It seems like not a good solution for us (at least currently) for various reasons, but beyond that...

It seems people generally don't feel it's production ready - how specifically?  What issues have you found?",40,41,cdigioia,2025-04-08 18:33:12,https://www.reddit.com/r/dataengineering/comments/1jukwsu/why_do_you_dislike_ms_fabric/,0,False,2025-04-08 23:46:51,False,False,2025-04-08 18:33:12,18,Tuesday,40.0,239,61.53,4,61,10.1,0,0
167,1ju9kqo,How did you start your data engineering journey?,"I am getting into this role, I wondered how other people became data engineers? Most didn't start as a junior data engineer; some came from an analyst(business or data), software engineers, or database administrators. 

What helped you become one or motivated you to become one?",16,39,FuzzyCraft68,2025-04-08 09:39:06,https://www.reddit.com/r/dataengineering/comments/1ju9kqo/how_did_you_start_your_data_engineering_journey/,0,False,False,False,False,2025-04-08 09:39:06,9,Tuesday,45.0,278,47.79,3,78,11.9,0,0
168,1ju6uoo,Ingesting a billion small .csv files from blob?,"Currently, we're ""streaming"" data by having an Azure Function write event grid messages to csv in blob storage, and then by having snowpipe ingest them. There's about a million csv's generated daily. The blob is not partitioned at all.

What's the best way to ingest/delete everything? Snowpipe has a configuration error, and a portion of the data hasn't been loaded, ever. ADF was pretty slow when I tested it out.

This was all done by consultants before I was in house btw.


edit: I was a bit unclear in my message. I mean, that we've had snowpipe ingesting these files. However, now we need to re-ingest the billion or so small .csv's that are in the blob, to compare the data to the already ingested data.

What further complicates this is:

- some files have two additional columns
- we also need to parse the filename to a column
- there is absolutely no partitioning at all",16,4,hi_top_please,2025-04-08 06:14:25,https://www.reddit.com/r/dataengineering/comments/1ju6uoo/ingesting_a_billion_small_csv_files_from_blob/,0,False,2025-04-08 21:44:57,False,False,2025-04-08 06:14:25,6,Tuesday,157.0,882,66.94,12,230,9.9,0,1
169,1jumngl,Hung DBT jobs,"According to the DBT Cloud [api](https://docs.getdbt.com/dbt-cloud/api-v2#/operations/Retrieve%20Run%20Failure%20Details), I can only tell that a job has failed and retrieve the failure details. 

There's no way for me to know when a job is hung.

Yesterday, an issue with our Fivetran replication and several of our DBT jobs hung for several hours.

Any idea how to monitor for hung DBT jobs?",13,3,CrabEnvironmental864,2025-04-08 19:45:39,https://www.reddit.com/r/dataengineering/comments/1jumngl/hung_dbt_jobs/,0,False,False,False,False,2025-04-08 19:45:39,19,Tuesday,58.0,393,51.24,5,96,9.4,1,0
170,1jugab3,What are the Python Data Engineering approaches every data scientist should know?,"Is it building data pipelines to connect to a DB?
Is it automatically downloading data from a DB and creating reports or is it something else? 
I am a data scientist who would like to polish his Data Engineering skills with Python because my company is beginning to incorporate more and more Python and I think I can be helpful. ",13,5,Pineapple_throw_105,2025-04-08 15:25:54,https://www.reddit.com/r/dataengineering/comments/1jugab3/what_are_the_python_data_engineering_approaches/,0,False,False,False,False,2025-04-08 15:25:54,15,Tuesday,60.0,329,51.18,3,94,12.5,0,0
171,1jukena,Clean architecture for Data Engineering,"Hi Guys,

Do anyone use or tried to use clean architecture for data engineering projects? If yes, May I know, how did it go and any comments on it or any references on github if you have?

Please don't give negative comments/responses without reasons.

Best regards",9,5,Harshadeep21,2025-04-08 18:13:04,https://www.reddit.com/r/dataengineering/comments/1jukena/clean_architecture_for_data_engineering/,0,False,False,False,False,2025-04-08 18:13:04,18,Tuesday,46.0,265,64.41,3,71,11.2,0,0
172,1ju714r,"reflect-cpp - a C++20 library for fast serialization, deserialization and validation using reflection, like Python's Pydantic or Rust's serde.","[https://github.com/getml/reflect-cpp](https://github.com/getml/reflect-cpp)

I am a data engineer, ML engineer and software developer with strong background in functional programming. As such, I am a strong proponent of the ""Parse, Don't Validate"" principle (https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/).  
  
Unfortunately, C++ does not yet support reflection, which is necessary to do something apply these principles. However, after some discussions on the topic over on r/cpp, we figured out a way to do this anyway. This library emerged out of these discussions.

I have personally used this library in real-world projects and it has been very useful. I hope other people in data engineering can benefit from it as well.

And before you ask: Yes, I use C++ for data engineering. It is quite common in finance and energy or other fields where you really care about speed. ",6,0,liuzicheng1987,2025-04-08 06:26:36,https://www.reddit.com/r/dataengineering/comments/1ju714r/reflectcpp_a_c20_library_for_fast_serialization/,0,False,False,False,False,2025-04-08 06:26:36,6,Tuesday,129.0,901,40.04,9,226,12.8,1,1
173,1jusby0,Best way to handle loading JSON API data into database in pipelines,"Greetings, this is my first post here. I've been working in DE for the last 5 years now doing various things with Airflow and Dagster.  I have a question regarding design of data flow from APIs to our database.

I am using Dagster/Python to perform the API pulls and loads into Snowflake.

My team lead insists that we load JSON data into our Snowflake RAW\_DATA in the following way:

ID (should be a surrogate/non-native PK)  
PAYLOAD (raw JSON payload, either as a VARCHAR or VARIANT type)  
CREATED\_DATE (timestamp this row was created in Snowflake)  
UPDATE\_DATE (timestamp this row was updated in Snowflake)

Flattening of the payload then happens in SQL as a plain View, which we currently autogenerate using Python and manually edit and add to Snowflake.

He does not want us (DE team) to use DBT to do any transforming of RAW\_DATA. DBT is only for the Data Analyst team to use for creating models.

The main advantage I see to this approach is flexibility if the JSON schema changes. You can freely append/drop/insert/reorder/rename columns. whereas a normal table you can only drop, append, and rename.

On the downside, it is slow and clunky to parse with SQL and access the data as a view. It just seems inefficient to have to recompute the view and parse all those JSON payloads whenever you want to access the table.  
  
I'd much rather do the flattening in Python, either manually or using dlt. Some JSON payloads I 'pre-flatten' in Python to make them easier to parse in SQL.

Is there a better way, or is this how you all handle this as well?",6,1,fetus-flipper,2025-04-08 23:56:12,https://www.reddit.com/r/dataengineering/comments/1jusby0/best_way_to_handle_loading_json_api_data_into/,0,False,2025-04-09 00:01:00,False,False,2025-04-08 23:56:12,23,Tuesday,272.0,1563,61.56,15,416,11.7,0,0
174,1juvgjf,Azure Course for Beginners | Learn Azure & Data Bricks in 1 Hour,"# FREE Azure Course for Beginners | Learn Azure & Data Bricks in 1 Hour

[https://www.youtube.com/watch?v=8XH2vTyzL7c](https://www.youtube.com/watch?v=8XH2vTyzL7c)",4,1,chrisgarzon19,2025-04-09 02:36:56,https://www.reddit.com/r/dataengineering/comments/1juvgjf/azure_course_for_beginners_learn_azure_data/,0,False,False,False,False,2025-04-09 02:36:56,2,Wednesday,16.0,163,7.52,1,29,0.0,1,0
175,1juu00b,Azure vs Microsoft Fabric?,"As a data engineer, I really like the control and customization that Azure offers.
At the same time, I can see how Fabric is more business-friendly and leans toward a low/no-code experience.

But with all the content and comparisons floating around the internet, why is no one talking about how insanely expensive Fabric is?! Seriouslyâ€”am I missing something here?


-
",4,7,Dharneeshkar,2025-04-09 01:20:51,https://www.reddit.com/r/dataengineering/comments/1juu00b/azure_vs_microsoft_fabric/,0,False,False,False,False,2025-04-09 01:20:51,1,Wednesday,60.0,369,56.45,4,95,11.7,0,0
176,1jujn9j,Lessons from optimizing dashboard performance on Looker Studio with BigQuery data,"Weâ€™ve been using Looker Studio (formerly Data Studio) to build reporting dashboards for digital marketing and SEO data. At first, things worked fineâ€”but as datasets grew, dashboard performance dropped significantly.



The biggest bottlenecks were:

â€¢ Overuse of blended data sources

â€¢ Direct querying of large GA4 datasets

â€¢ Too many calculated fields applied in the visualization layer



To fix this, we adjusted our approach on the data engineering side:

â€¢ Moved most calculations (e.g., conversion rates, ROAS) to the query layer in BigQuery

â€¢ Created materialized views for campaign-level summaries

â€¢ Used scheduled queries to pre-aggregate weekly and monthly data

â€¢ Limited Looker Studio to one direct connector per dashboard and cached data where possible



Result: dashboards now load in \~3 seconds instead of 15â€“20, and we can scale them across accounts with minimal changes.



Just sharing this in case others are using BI tools on top of large datasetsâ€”interested to hear how others here are managing dashboard performance from a data pipeline perspective.",3,1,kodalogic,2025-04-08 17:43:22,https://www.reddit.com/r/dataengineering/comments/1jujn9j/lessons_from_optimizing_dashboard_performance_on/,0,False,False,False,False,2025-04-08 17:43:22,17,Tuesday,164.0,1077,22.68,5,278,17.6,0,0
177,1juhrrs,Help: Looking to set up a decent data architecture (data lake and/or warehouse),"Hi, I need help. I need a proper architecture for a department, and I am trying to get a data lake/warehouse.

Why: We have a lot of data sources from SaaS to manually created documents. We use a lot of SaaS products, but we have no centralised repository to store and stage the data, so we end up with a lot of workaround such as using SharePoint and csv stored in folders for reporting. We also change SaaS products quite frequently, so sources can change often. It is difficult to do advanced analytics. 

I prefer a lake & warehouse approach because (1) for SaaS users, they can can just drop the data to the lake and (2) transformation and processing can be done for reporting, and we could combine the datasets even when we change the SaaS software. 

My huge considerations are that (1) the data is to be accessible within the department only and (2) it has to be decent cost. Currently considered Azure Data Lake Storage Gen2 & DataBricks, or Snowflake (to have both the lake and warehouse). My previous experience was only with Data Lake Storage Gen2.

I'm willing to work my way up for my technical limitations, but at this stage I am exploring the software solutions to get the buy in to kickstart this project. 

Any sharing is much appreciated, and if you worked with such an environment, I appreciate your guidance and learnings as well. Thank you in advance.",3,1,thehotdawning,2025-04-08 16:27:08,https://www.reddit.com/r/dataengineering/comments/1juhrrs/help_looking_to_set_up_a_decent_data_architecture/,0,False,False,False,False,2025-04-08 16:27:08,16,Tuesday,245.0,1373,60.95,13,366,12.0,0,0
178,1juo1uo,How are entry level data engineering roles at Amazon?,"If anyone on this sub has worked for Amazon as a Data engineer, preferably entry level or early careers, how has your experience been working at amazon at Amazon? 

Iâ€™ve heard their work culture is very startup like, and their is an abundance of poor managers. The company just cars about share holder value, instead of caring for their customers and employees.

I wanted to hear on this sub, how has your experience been? How was the hiring process like? What all skills I should develop to work for Amazon?",2,2,gta35,2025-04-08 20:43:31,https://www.reddit.com/r/dataengineering/comments/1juo1uo/how_are_entry_level_data_engineering_roles_at/,0,False,False,False,False,2025-04-08 20:43:31,20,Tuesday,90.0,508,56.25,6,140,12.2,0,0
179,1jufvpe,Question around migrating to dbt,"We're considering moving from a dated ETL system to dbt with data being ingested via AWS Glue.

We have a data warehouse which uses a Kimball dimensional model, and I am wondering how we would migrate the dimension load processes.

We don't have access to all historic data, so it's not a case of being able to look across all files and then pull out the dimensions. Would it make sense fur the dimension table to be bothered a source and a dimension?

I'm still trying to pivot my way of thinking away from the traditional ETL approach so might be missing something obvious.",2,2,receding_bareline,2025-04-08 15:09:02,https://www.reddit.com/r/dataengineering/comments/1jufvpe/question_around_migrating_to_dbt/,0,False,False,False,False,2025-04-08 15:09:02,15,Tuesday,104.0,575,58.82,5,157,12.0,0,0
180,1juvakz,Beginner Predictive Model Feedback/Guidance,"My predictive modeling folks, beginner here could use some feedback guidance. Go easy on me, this is my first machine learning/predictive model project and I had very basic python experience before this.


Iâ€™ve been working on a personal project building a model that predicts NFL player performance using full career, game-by-game data for any offensive player who logged a snap between 2017â€“2024. 

I trained the model using data through 2023 with XGBoost Regressor, and then used actual 2024 matchups â€” including player demographics (age, team, position, depth chart) and opponent defensive stats (Pass YPG, Rush YPG, Points Allowed, etc.) â€” as inputs to predict game-level performance in 2024.

The model performs really well for some stats (e.g., RÂ² > 0.875 for Completions, Pass Attempts, CMP%, Pass Yards, and Passer Rating), but others â€” like Touchdowns, Fumbles, or Yards per Target â€” arenâ€™t as strong.

Hereâ€™s where I need input:

-Whatâ€™s a solid baseline RÂ², RMSE, and MAE to aim for â€” and does that benchmark shift depending on the industry?

-Could trying other models/a combination of models improve the weaker stats? Should I use different models for different stat categories (e.g., XGBoost for high-RÂ² ones, something else for low-RÂ²)?

-How do you typically decide which model is the best fit? Trial and error? Is there a structured way to choose based on the stat being predicted?

-I used XGBRegressor based on common recommendations â€” are there variants of XGBoost or alternatives you'd suggest trying? Any others you like better?

-Are these considered â€œgoodâ€ model results for sports data?

-Are sports models generally harder to predict than industries like retail, finance, or real estate?

-What should my next step be if I want to make this model more complete and reliable (more accurate) across all stat types?

-How do people generally feel about manually adding in more intangible stats to tweak data and model performance? Example: Adding an injury index/strength multiplier for a Defense that has a lot of injuries, or more playerâ€™s coming back from injury, etc.? Is this a generally accepted method or not really utilized?

Any advice, criticism, resources, or just general direction is welcomed.",1,0,ynwFreddyKrueger,2025-04-09 02:27:57,https://www.reddit.com/gallery/1juvakz,0,False,False,False,False,2025-04-09 02:27:57,2,Wednesday,357.0,2230,56.05,23,575,11.9,0,0
181,1jugyx7,Cornerstone data,"Hi all,

Has anybody pulled cornerstone training data using their APIs or used anyother method to pull the data?",1,0,arunrajan96,2025-04-08 15:54:25,https://www.reddit.com/r/dataengineering/comments/1jugyx7/cornerstone_data/,0,False,False,False,False,2025-04-08 15:54:25,15,Tuesday,19.0,112,60.65,1,29,0.0,0,0
182,1judm9f,GizmoSQL: Power your Enterprise analytics with Arrow Flight SQL and DuckDB,"Hi! This is Phil - Founder ofÂ [GizmoData](https://gizmodata.com). We have a new commercial database engine product called:Â [GizmoSQL](https://gizmodata.com/gizmosql)Â \- built with Apache Arrow Flight SQL (for remote connectivity) and DuckDB (or optionally: SQLite) as a back-end execution engine.

This product allows you to run DuckDB or SQLite as a server (remotely) - harnessing the power of computers in the cloud - which typically have more CPUs, more memory, and faster storage (NVMe) than your laptop. In fact, running GizmoSQL on a modern arm64-based VM in Azure, GCP, or AWS allows you to run at terabyte scale - with equivalent (or better) performance - for a fraction of the cost of other popular platforms such as Snowflake, BigQuery, or Databricks SQL.

**GizmoSQL**Â is self-hosted (for now) - with a possible SaaS offering in the near future. It has these features to differentiate it from ""base"" DuckDB:

* Run DuckDB or SQLite as a server (remote connectivity)
* Concurrency - allows multiple users to work simultaneously - with independent, ACID-compliant sessions
* Security
   * Authentication
   * TLS for encryption of traffic to/from the database
* Static executable with Arrow Flight SQL, DuckDB, SQLite, and JWT-CPP built-in. There are no dependencies to install - just a single executable file to run
* Free for use in development, evaluation, and testing
* Easily containerized for running in the Cloud - especially in Kubernetes
* Easy to talk to - with ADBC, JDBC, and ODBC drivers, and now a Websocket proxy server (created by GizmoData) - so it is easy to use with javascript frameworks
   * Use it with Tableau, PowerBI, Apache Superset dashboards, and more
* Easy to work with in Python - use ADBC, or the new experimental Ibis back-end - details here:Â [https://github.com/gizmodata/ibis-gizmosql](https://github.com/gizmodata/ibis-gizmosql)

Because it is powered by DuckDB - GizmoSQL can work with the popular open-source data formats - such as Iceberg, Delta Lake, Parquet, and more.

GizmoSQL performs very well (when running DuckDB as its back-end execution engine) - check out our graph comparing popular SQL engines for TPC-H at scale-factor 1 Terabyte - on the homepage at:Â [https://gizmodata.com/gizmosql](https://gizmodata.com/gizmosql)Â \- there you will find it also costs far less than other options.

We would love to get your feedback on the software - it is easy to get started for free in two different ways:

* For a limited time - try GizmoSQL online on our dime - with the SQL Query Navigator - it just requires a quick registration and sign-in to get going - at:Â [https://app.gizmodata.com](https://app.gizmodata.com)Â \- where we have a read-only 1TB TPC-H database mounted for you to query in real-time. It is running on an Azure Cobalt 100 VM - with local NVMe SSD's - so it should be quite zippy.
* Download and self-host GizmoSQL - using our Docker image or executables for Linux and macOS for both x86-64 and arm64 architectures. See our README at:Â [https://github.com/gizmodata/gizmosql-public](https://github.com/gizmodata/gizmosql-public)Â for details on how to easily and quickly get started that way

Thank you for taking a look at GizmoSQL. We are excited and are glad to answer any questions you may have!

* **Public facing repo (README):**Â [https://github.com/gizmodata/gizmosql-public?tab=readme-ov-file](https://github.com/gizmodata/gizmosql-public?tab=readme-ov-file)
* **HomePage**:Â [https://gizmodata.com/gizmosql](https://gizmodata.com/gizmosql)
* **ProductHunt:**Â [https://www.producthunt.com/posts/gizmosql?embed=true&utm\_source=badge-featured&utm\_medium=badge&utm\_souce=badge-gizmosql](https://www.producthunt.com/posts/gizmosql?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-gizmosql)
* **Try GizmoSQL online:**Â [https://app.gizmodata.com](https://app.gizmodata.com)
* **GizmoSQL in action video:**Â [https://youtu.be/QSlE6FWlAaM](https://youtu.be/QSlE6FWlAaM)",1,1,Adventurous-Visit161,2025-04-08 13:30:54,https://www.reddit.com/r/dataengineering/comments/1judm9f/gizmosql_power_your_enterprise_analytics_with/,0,False,False,False,False,2025-04-08 13:30:54,13,Tuesday,538.0,3960,16.02,23,972,13.8,1,0
183,1jucavs,Is there any tool you use to keep track on the dates you need to reset API keys?,"I currently use teams events where I set a day on my calendar to update keys, but there has to be a better way. How do you guys do it?

Edit: The idea is to renew keys before they expire and there are no errors in the pipelines",1,7,dataguydream,2025-04-08 12:26:25,https://www.reddit.com/r/dataengineering/comments/1jucavs/is_there_any_tool_you_use_to_keep_track_on_the/,0,False,2025-04-08 19:04:12,False,False,2025-04-08 12:26:25,12,Tuesday,48.0,227,80.62,3,63,8.8,0,0
184,1jub08u,How do you group your tables into pipelines?,"I was wondering how do data engineers in different company group their pipelines together ?

Usually tables need to be refreshed at some specific refresh rates. This means that some table upstream might require 1h refresh while downstream table might require daily.

I can see people grouping things by domain and running domain one after each other sequentially, but then this break the concept of having different refresh rate per table or domain. I can see table configure with multiple corn but then I see issues with needing to schedule offset in cron jobs. 

Like most of the domain are very close to each other so when creating them I might be mixing a lot of stuff together which would impact downstream.

Whatâ€™s your experience in structuring pipeline? Or any good reference I can read ?
",1,9,Commercial_Dig2401,2025-04-08 11:13:44,https://www.reddit.com/r/dataengineering/comments/1jub08u/how_do_you_group_your_tables_into_pipelines/,0,False,False,False,False,2025-04-08 11:13:44,11,Tuesday,136.0,797,54.42,8,209,11.7,0,0
185,1ju7cmf,What is the best way to reflect data in clickhouse from MySQL other than the MySQL engine?,"Hi everyone, I am working on a project currently where we have a MySQL database. We are using clickhouse as our warehouse. 

What we need to achieve is to reflect the data from MySQL to clickhouse for certain tables. For this, I found a few ways and am looking to get some insights on which method has the most potential and if there are other methods as welp:

1. Use the MySQL engine in clickhouse. 

Pros: No need to store data in clickhouse as it can just proxy it directly from MySQL.

Cons: This however puts extra reads on MySQL and doesn't help us if MySQL ever goes down. 

2. Use signals to send the data to clickhouse whenever there is a change in MySQL.

Pros: We don't have a lot of tables currently so it's the quickest to setup. 

Cons: Extremely inefficient and not scalable. 

3. Use some sort of third party sink to achieve this. I have found this https://github.com/Altinity/clickhouse-sink-connector which seems to do the job but it has way too many open issues and not sure if it is reliable enough. Plus, it complicates our tech stack which we are looking not to do. 

I'm open to any other ideas. We would ideally not want to duplicate this data in clickhouse but if that's the last resort we would go for it. 

Thanks in advance. 

P.S, I am a beginner in data engineering so feel free to correct me if I've used some wrong jargons or if I am seriously deviating from the right path. ",1,9,Danyboi16,2025-04-08 06:49:07,https://www.reddit.com/r/dataengineering/comments/1ju7cmf/what_is_the_best_way_to_reflect_data_in/,0,False,False,False,False,2025-04-08 06:49:07,6,Tuesday,258.0,1408,73.88,18,363,9.3,1,0
186,1jun8gx,Designing a database ERP from scratch.,"My goal is to re create something like Oracle's Net-suite, are there any help full resources on how i can go about it. i have previously worked on simple Finance management systems but this one is more complicated. i need sample ERD's books or anything helpfull atp

",0,2,Specific_Bad8942,2025-04-08 20:09:25,https://www.reddit.com/r/dataengineering/comments/1jun8gx/designing_a_database_erp_from_scratch/,0,False,False,False,False,2025-04-08 20:09:25,20,Tuesday,47.0,267,55.54,3,73,11.9,0,0
187,1juj0x6,Beginning Data Scientist in Azure needing some help (iot),"Hi all,

I currently am working on a new structure to save sensor data coming from Azure Iot Hub in Azure to store it into Azure Blob Storage for historical data, and Clickhouse for hot data with TTL (around half year). The sensor data is coming from different entities (e.g building1, boat1, boat2) and should be partioned by entity. The data weâ€™re processing daily is around 300-2 million records per day.

I know Azure Iot Hub is essentially a built-in Azure Hub. I had a few questions since Iâ€™ve tried multiple solutions. 

1. Normal message routing to Azure Blob
Issue: no custom partitioning on file structure (e.g entityid/timestamp_sensor/) it requires you to use the enqueued time. And there is no dead letter queue for fallback

2. IoT hub -> Azure Functions -> Blob Storage & Clickhouse
Issue: this should work correctly but I have not that much experience in Azure Functions, I tried creating a function with the IoT Hub template but it seems I need to also have an Event Hubs namespace which is not what I want. HTTP trigger is also not what I want. I donâ€™t find any good documentation on it aswell. I know I can maybe use Event Hubs trigger and use the Iot Hub connection string but I didnâ€™t manage to do this yet.

3. IoT hub -> Event Grid 
Someone suggested using Event Grid, however to my knowledge Event Grid is not used for telemetry data despite there being an option for. Is this beneficial? I donâ€™t really know what the flow would be since you canâ€™t use Event Grid to send data to Clickhouse. You would still need an Azure Functions.

4. IoT Hub -> Event Grid -> Event Hubs -> Azure Functions -> Azure Blob & Clickhouse
This one seemed the most appealing to me but I donâ€™t know if itâ€™s the smartest, it can get expensive (maybe).
But the idea here is that we use Event Grid for batching the data and to have a dead letter queue.
Arrived in Event Hubs we use an Azure Function to send the data to blob storage and clickhouse.

The only problem is I might need some delay to sending to Clickhouse & Blob Storage (around maybe every 15 minutes) to reduce the risks of memory usage in Clickhouse and to reduce costs.

Can someone help me out? Am I forgetting something crucial? I am a graduated data scientist, however I have no in depth experience with Azure.


",0,8,PaqS18,2025-04-08 17:18:34,https://www.reddit.com/r/dataengineering/comments/1juj0x6/beginning_data_scientist_in_azure_needing_some/,0,False,2025-04-08 18:22:26,False,False,2025-04-08 17:18:34,17,Tuesday,415.0,2281,62.78,24,595,9.7,1,1
188,1juibws,From Data Tyranny to Data Democratization,,0,0,growth_man,2025-04-08 16:50:26,https://moderndata101.substack.com/p/from-data-tyranny-to-data-democratization,0,False,False,False,False,2025-04-08 16:50:26,16,Tuesday,,0,206.84,1,0,0.0,0,0
189,1ju693k,Experienced data engineer looking to expand to devops,"Hey everyone, I've been a working a few years as a data engineer, I'd say I'm very comfortable in python (databricks), sql and git and have mostly worked in Azure. I would like to get comfortable with devops, setting up proper ci/cd, iac etc.

What resources would you recommend?

Where I work we 2 repos set up, an infratsructure repo that I am totally clueless about that is mostly terraform and another repo where we make changes to notebooks and pipelines etc whose structure makes more sense to me.

The whole thing was initially set up by consultants. My goal is really to understand how it was set up, why 2 different repos, how to change the ci/cd pipeline to add testing etc.

Thanks!",0,5,Lamyya,2025-04-08 05:33:41,https://www.reddit.com/r/dataengineering/comments/1ju693k/experienced_data_engineer_looking_to_expand_to/,0,False,False,False,False,2025-04-08 05:33:41,5,Tuesday,124.0,693,58.92,6,191,12.7,0,0
190,1jui1dg,Mirror snowflake to PG,"Hi everyone,
Once per day, my team needs to mirror a lot of tables from snowflake to postgres. 
Currently, we are copying data with script written with GO.
do you familiar with tools, or any idea what is the best way to mirror the tables?",0,6,gal_12345,2025-04-08 16:38:21,https://www.reddit.com/r/dataengineering/comments/1jui1dg/mirror_snowflake_to_pg/,0,False,2025-04-08 17:13:11,False,False,2025-04-08 16:38:21,16,Tuesday,45.0,238,73.17,3,64,10.5,0,0
191,1jukctt,Hot Take: You shouldn't be a data engineer if you've never been a data analyst,"You're better able to understand the needs and goals of what you're actually working towards when you being as an analyst. Not to mention the other skills that you develop whist being an analyst. Understanding downstream requirements helps build DE pipelines carefully keeping in mind the end goals.

What are you thoughts on this?",0,6,_areebpasha,2025-04-08 18:11:04,https://www.reddit.com/r/dataengineering/comments/1jukctt/hot_take_you_shouldnt_be_a_data_engineer_if_youve/,0,False,False,False,False,2025-04-08 18:11:04,18,Tuesday,54.0,331,66.23,4,83,11.2,0,0
192,1jxb4zd,Understand basics of Snowflake â„ï¸â„ï¸,"Exciting news, a new blog post about Snowflake architecture. Dive in and explore all the amazing features! 

https://medium.com/@adityasharmah27/understanding-snowflake-architecture-a-beginners-guide-to-cloud-data-warehousing-22a6f4e3a6be?sk=40c0128a3f07d30ba0cd92ab710112ae",21,0,Super_Act_5816,2025-04-12 06:38:55,https://www.reddit.com/r/dataengineering/comments/1jxb4zd/understand_basics_of_snowflake/,0,False,False,False,False,2025-04-12 06:38:55,6,Saturday,18.0,274,-22.27,2,46,0.0,1,0
193,1jxtxqo,How do my fellow on-prem DEs keep their sanity...,"...the joys of memory and compute resources seems to be a neverending suck ðŸ˜­

We're building ETL pipelines, using Airflow in one K8s namespace and Spark in another (the latter having dedicated hardware). Most data workloads aren't really Spark-worthy as files are typically <20GB, and we keep hitting pain points where processes struggle in Airflow's memory (workers are 6Gi and 6 CPU, with a limit of 10GI; no KEDA or HPA). We are looking into more efficient data structures like DuckDB, Polars, etc or running ""mid-tier"" processes as separate K8s jobs but then we hit constraints like tools/libraries relying on Pandas use so we seem stuck with eager processes.

Case in point, I just learned that our teams are having to split files into smaller files of 125k records so Pydantic schema validation won't fail on memory. I looked into GX Core and see the main source options there again appear to be Pandas or Spark dataframes (yes, I'm going to try DuckDB through SQLAlchemy). I could bite the bullet and just say to go with Spark, but then our pipelines will be using Spark for QA and not for ETL which will be fun to keep clarifying. 

Sisyphus is the patron saint of Data Engineering... just sayin'

[Make it stoooooooooop!](https://preview.redd.it/qwikfhcpihue1.png?width=503&format=png&auto=webp&s=6565d874d8d2213835c172a8ed449b14cff8214a)

(there may be some internal sobbing/laughing whenever I see posts asking ""should I get into DE..."")",15,6,Nightwyrm,2025-04-12 23:08:19,https://www.reddit.com/r/dataengineering/comments/1jxtxqo/how_do_my_fellow_onprem_des_keep_their_sanity/,0,False,False,False,False,2025-04-12 23:08:19,23,Saturday,227.0,1448,54.46,9,348,12.8,1,1
194,1jxasop,Need course advice on building ETL Piplines in Databricks using Python.,Please suggest Courses/YT Channels on building ETL Pipelines in Databricks using Python. I have good knowledge on Pandas and NumPy and also used Databricks for my personal projects but never build ETL Piplines.,13,5,Sweet-Expert-6356,2025-04-12 06:15:15,https://www.reddit.com/r/dataengineering/comments/1jxasop/need_course_advice_on_building_etl_piplines_in/,0,False,False,False,False,2025-04-12 06:15:15,6,Saturday,33.0,210,63.19,2,48,0.0,0,0
195,1jxun0q,Data Inserts best practices with Iceberg,"I receive various files at different intervals which are not defined. Can be every seconds, hour, daily, etc.

I donâ€™t have any indication also of when something is finished. For example, itâ€™s highly possible to have 100 files that would end up being 100% of my daily table, but I receive them scattered over 15min-30 when the data become available and my ingestion process ingest it. Can be 1 to 12 hours after the day is over.

Not thatâ€™s itâ€™s also possible to have 10000 very small files per day.

Iâ€™m wondering how is this solves with Iceberg tables. Very newbie Iceberg guy here. Like I donâ€™t see throughput write benchmark anywhere but I figure that rewriting the metadata files must be a big overhead if thereâ€™s a very large amount of files so inserting every times thereâ€™s a new one must not be the ideal solution.

Iâ€™ve read some medium post saying that there was a snapshot feature which track new files so you donâ€™t have to do some fancy things to load them incrementally. But again if every insert is a query that change the metadata files it must be bad at some point.

Do you wait and usually build a process to store a list of files before inserting them or is this a feature build somewhere already in a doc I canâ€™t find ?

Any help would be appreciated.

",9,0,Commercial_Dig2401,2025-04-12 23:44:32,https://www.reddit.com/r/dataengineering/comments/1jxun0q/data_inserts_best_practices_with_iceberg/,1,False,False,False,False,2025-04-12 23:44:32,23,Saturday,233.0,1272,61.87,13,342,11.4,0,0
196,1jxffmp,"Any ETL, Data Quality, Data Governance professionals ?","Hi everyone,

Iâ€™m currently working as an IDQ and CDQ developer for a US-based project, with about 2 years of overall experience

Iâ€™m really passionate about growing in this space and want to deepen my knowledge, especially in data quality and data governance . 

Iâ€™ve recently started reading the DAMA DMBOK2 to build a strong foundation.

Iâ€™m here to connect with experienced professionals and like-minded individuals to learn, share insights, and get guidance on how to navigate and grow in this domain.

Any tips, resources, or advice would be truly appreciated. Looking forward to learning from all of you!

Thank you!
",8,1,Physical_Bad_2945,2025-04-12 11:45:32,https://www.reddit.com/r/dataengineering/comments/1jxffmp/any_etl_data_quality_data_governance_professionals/,0,False,False,False,False,2025-04-12 11:45:32,11,Saturday,101.0,624,42.72,5,169,14.0,0,0
197,1jxhtte,Non IT background,"After a year of self teaching I managed to secure an internal career move to data engineering from finance 

What I am wondering is long term will my non IT background matter/discount me against other candidates? I have a degree in accountancy and I am a qualified accountant but I am considering doing a masters in data or computing if it will be beneficial longer term

Thanks",7,8,Fancy_Arugula5173,2025-04-12 13:54:29,https://www.reddit.com/r/dataengineering/comments/1jxhtte/non_it_background/,0,False,False,False,False,2025-04-12 13:54:29,13,Saturday,67.0,378,37.47,2,109,0.0,0,0
198,1jxswyj,Dilemma: SWE vs DE @ Big Tech,"I currently work at a Big Tech and have 3 YoE. My role is a mix of Full-Stack + Data Engineering. 

I want to keep preparing for interviews on the side, and to do that I need to know which role to aim for. 

Pros of SWE:
- more jobs positions 
- I have already invested 300 hours into DSA Leetcode. Donâ€™t have to start DE prep from scratch
-Maybe better quality of work/pay(?)

Pros of DE:
- targeting a niche has always given me more callbacks
- if I practice a lot of sql, the interviews at FAANG could be gamed. FAANG do ask DSA but they barely scratch the surface

My thoughts:
Ideally I want to crack the SWE role at a FAANG as I like both roles equally but SWE pays 20% more. If I donâ€™t get callbacks for SWE, then securing a similar pay through a DE role at FAANG is lucrative too. 
Iâ€™d be completely fine with doing DE, but I feel uneasy wasting the 100s of hours I spent on DSA. 

Applying for both jobs is sub optimal as I can only sink my time into SQL or DSA | system design or data modelling. 

What do you folks suggest? 
",3,6,marioagario123,2025-04-12 22:17:20,https://www.reddit.com/r/dataengineering/comments/1jxswyj/dilemma_swe_vs_de_big_tech/,0,False,False,False,False,2025-04-12 22:17:20,22,Saturday,206.0,1036,78.38,11,269,11.2,0,0
199,1jxp3m5,Question about HDFS,"The course I'm taking is 10 years old so some information I'm finding is irrelevant, which prompted the following questions from me:

  
I'm learning about replication factors/rack awareness in HDFS and I'm curious about the current state of the world. How big are replication factors for massive companies today like, let's say, Uber? What about Amazon?

  
Moreover, do these tech giants even use Hadoop anymore or are they using a modernized version of it in 2025? Thank you for any insights.",3,10,undercoverlife,2025-04-12 19:20:05,https://www.reddit.com/r/dataengineering/comments/1jxp3m5/question_about_hdfs/,0,False,False,False,False,2025-04-12 19:20:05,19,Saturday,81.0,495,55.03,5,130,12.0,0,0
200,1jxk6l2,Mastering Spark Structured Streaming Integration with Azure Event Hubs,"Are you curious about building real-time streaming pipelines from popular streaming platforms like Azure Event Hubs? In this tutorial, I explain key Event Hubs concepts and demonstrate how to build Spark Structured Streaming pipelines interacting with Event Hubs. Check it out here:Â [https://youtu.be/wo9vhVBUKXI](https://youtu.be/wo9vhVBUKXI)",2,0,Nice_Substance_6594,2025-04-12 15:43:22,https://www.reddit.com/r/dataengineering/comments/1jxk6l2/mastering_spark_structured_streaming_integration/,0,False,False,False,False,2025-04-12 15:43:22,15,Saturday,43.0,343,31.58,3,80,11.9,1,0
201,1jxdch4,Which API system for my Postgres DWH?,"Hi everyone,

I am building a data warehouse for my company and because we have to process mostly spatial data I went with a postgres materialization. My stack is currently:

- dlt
- dbt
- dagster
- postgres

Now I have the use case that our developers at our company need some of the data for our software solutions to be integrated. And I would like to provide an API for easy access to the data. 

So I am wondering which solution is best for me. I have some experience in a private project with postgREST and found it pretty cool to directly use DB views and functions as endpoints for the API. But tools like FastAPI might be more mature for a production system. What would you recommend?



[View Poll](https://www.reddit.com/poll/1jxdch4)",3,0,rick854,2025-04-12 09:20:05,https://www.reddit.com/r/dataengineering/comments/1jxdch4/which_api_system_for_my_postgres_dwh/,0,False,False,False,False,2025-04-12 09:20:05,9,Saturday,131.0,745,53.1,7,200,12.0,1,1
202,1jxsce8,I'm struggling to evaluate job offer and would appreciate outside opinions,"I've been searching for a new opportunity over the last few years (500+ applications) and have finally received an offer I'm strongly considering. I would really like to hear some outside opinions.

## Current position
- Analytics Lead
- $126k base, 10% bonus
- Tool stack: on-prem SQL Server, SSIS, Power BI, some Python/R
- Downsides: 
	- Incoherent/non-existent corporate data strategy
	- 3 days required in-office (~20-minute commute)
	- Lack of executive support for data and analytics
	- Data Scientist and Data Engineer roles have recently been eliminated
	- No clear path for additional growth or progression 
	- A significant part of the job involves training/mentoring several inexperienced analysts, which I don't enjoy
- Upsides: 
	- Very stable company (no risk of layoffs)
	- Very good relationship with direct manager

## New offer
- Senior Data Analyst
- $130k base, 10% bonus
- Tool stack: BigQuery, FiveTran, dbt / SQLMesh, Looker Studio, GSheets
- Downsides:
	- High-growth company, potentially volatile industry
- Upsides:
	- Fully remote
	- Working alongside experienced data engineers

Other info/significant factors:
- My current company paid for my MSDS degree, and they are within their right to claw back the entire ~$37k tuition if I leave.  I'm prepared to pay this, but it's a big factor in the decision.
- At this stage in my career, I'm putting a very high value on growth/development opportunities

Am I crazy to consider a lateral move that involves a significant amount of uncompensated risk, just for a potentially better learning and growth opportunity?",4,21,Dozer11,2025-04-12 21:49:54,https://www.reddit.com/r/dataengineering/comments/1jxsce8/im_struggling_to_evaluate_job_offer_and_would/,0,False,False,False,False,2025-04-12 21:49:54,21,Saturday,254.0,1589,8.27,5,421,21.2,0,1
203,1jxs473,Struggling to resolve tickets - DE Course Recommendations?,"Iâ€™m looking for recommendations for a solid online course to learn Data Engineering. Less than a year ago, I started a new role as a BI developer. Most of my work involves creating data models and reports in Power BI using T-SQL and DAX, but lately Iâ€™ve been tasked with handling tickets related to reports showing incorrect data on the ETL side.

We use Wherescape for our ETL processes, but Iâ€™ve struggled to find good learning material for this tool. There's no formal training and everyone learns on the job. Thereâ€™s so much to analyze during investigations, especially when reverse-engineering the problem.

Iâ€™m a visual learner, so Iâ€™d love recommendations for courses with videos and hands-on practice. Any suggestions? Thanks!

Edit: Most posts asking for course recommendations are 1 year older or more. Some links doesn't work anymore or are not found when I look it up. ",2,1,evilsemantics,2025-04-12 21:39:00,https://www.reddit.com/r/dataengineering/comments/1jxs473/struggling_to_resolve_tickets_de_course/,1,False,2025-04-12 23:13:53,False,False,2025-04-12 21:39:00,21,Saturday,147.0,881,54.93,9,231,11.4,0,0
204,1jxiy3h,Debezium connector Sql server 2016,"Iâ€™m trying to get the Debezium SQL Server connector working with a SQL Server 2016 instance, but not having much luck. The official docs mention compatibility with 2017, 2019, and 2022â€”but nothing about 2016.

Is 2016 just not supported, or has anyone managed to get it working regardless?
Would love to hear if there are known limitations, workarounds, or specific gotchas for this version.",2,0,hulioshort,2025-04-12 14:47:33,https://www.reddit.com/r/dataengineering/comments/1jxiy3h/debezium_connector_sql_server_2016/,1,False,False,False,False,2025-04-12 14:47:33,14,Saturday,64.0,391,55.24,4,104,12.6,0,0
205,1jxcryb,Discovering data dependencies / lineage from excel workbooks,Hi r/dataengineering community. Trying to replace excel based reports that connect to databases and have in-built data transformation logic across worksheets. Is there a utility or platform you have used to help decipher and document the data dependencies / data lineage from excel?,2,0,collab_inc,2025-04-12 08:37:34,https://www.reddit.com/r/dataengineering/comments/1jxcryb/discovering_data_dependencies_lineage_from_excel/,0,False,False,False,False,2025-04-12 08:37:34,8,Saturday,43.0,282,31.89,3,79,13.0,0,0
206,1jxv28g,Help,"I'm using Airbyte Cloud because my PC doesn't have enough resources to install it. I have a Docker container running PostgreSQL on Airbyte Cloud. I want to set the PostgreSQL destination. Can anyone give me some guidance on how to do this? Should I create an SSH tunnel?

",1,1,Imaginary_Pirate_267,2025-04-13 00:06:47,https://www.reddit.com/r/dataengineering/comments/1jxv28g/help/,1,False,False,False,False,2025-04-13 00:06:47,0,Sunday,48.0,272,70.19,5,72,8.2,0,0
207,1jxtvbu,Thoughts on Acryl vs other metadata platforms,"Hi all, I'm evaluating metadata management solutions for our data platform and would appreciate any thoughts from folks who've actually implemented these tools in production.

We're currently running into scaling issues with our in-house data catalog and I think we need something more robust for governance and lineage tracking.

I've narrowed it down to Acryl (DataHub) and Collate (openmetadata) as the main contenders. I know I should look at Collibra and Alation and maybe Unity Catalog?

For context, we're a mid-sized fintech (\~500 employees) with about 30 data engineers and scientists. We're AWS with Snowflake, Airflow for orchestration, and a growing number of ML models in production.

My question list is: 

1. How these tools handle machine-scale operations 
2. How painful was it to get set up?
3. For DataHub and openmetadata specifically - is the open source version viable or is the cloud version necessary?
4. Any unexpected limitations you've hit with any of these platforms?
5. Do you feel like these grow with you as we increasingly head into AI governance? 
6. How well they integrate with existing tools (Snowflake, dbt, Looker, etc.)

If anyone has switched from one solution to another, I'd love to hear why you made the change and whether it was worth it.

Sorry for the pick list of questions - the last post on this was years ago and I was hoping for some more insights. Thanks in advance for anyone's thoughts. ",0,0,arronsky,2025-04-12 23:05:00,https://www.reddit.com/r/dataengineering/comments/1jxtvbu/thoughts_on_acryl_vs_other_metadata_platforms/,0,False,False,False,False,2025-04-12 23:05:00,23,Saturday,239.0,1442,56.45,16,377,11.9,0,0
208,1jxr3uh,How to create changeStreams pipeline to bigquery,"I am building a streaming pipeline in GCP for work that works like this:

Cloud Run Service --> PubSub --> Dataflow --> BigQuery


My Cloud Run Service when it starts, it watches a collections with changeStreams and then published all changes into a PubSub topic. Dataflow then streams that messages into BQ.

The service runs in VPC connector where the linked IP is whitelisted in mongodb.

My issue is with my service! It keeps failing die to timeouts when trying to publish to pubsub after a few hours running.

Ive tried batching the publishing, extending the timeout, retries.


Any suggestion? Have you done something similar?",0,0,Realistic_Salary_942,2025-04-12 20:51:58,https://www.reddit.com/r/dataengineering/comments/1jxr3uh/how_to_create_changestreams_pipeline_to_bigquery/,0,False,False,False,False,2025-04-12 20:51:58,20,Saturday,105.0,632,65.12,7,149,8.8,0,0
209,1jxlz1d,"I've been working on a query engine over semi-structured logs (think trino but for JSONs), would like to get feedback / feature ideas","[https://github.com/tontinton/miso](https://github.com/tontinton/miso)

Other than the obvious stuff like:

* Make it faster (benchmarking + improving implementation)
* Make it spool to disk to handle queries larger than memory
* Make it distributed to handle queries larger than memory / disk
* Implement a simple query language frontend for faster onboarding, something like KQL

Currently I only support [quickwit](https://quickwit.io/), and can pretty easily add elasticsearch support, but what other JSON databases would you think are the best fit? Datadog logs? MongoDB? Clickhouse jsons? Snowflake VARIANTs?

What features can a query engine that treats semi-structured data as a first class citizen have, that trino cannot?",0,1,TonTinTon,2025-04-12 17:02:24,https://www.reddit.com/r/dataengineering/comments/1jxlz1d/ive_been_working_on_a_query_engine_over/,0,False,False,False,False,2025-04-12 17:02:24,17,Saturday,102.0,731,22.08,3,176,16.7,1,1
210,1jxq9r1,Data Engineering Employment,"I'm an Engineer with an MBA. I've spent 5 years at a steelplant and 5 years working in finance for the government.

In the past five years have been building data pipelines in Synapse off D365 data models that I have built with a vendor in SQL/Power BI. I have gained quite a bit of experience in this timeframe, but would actually like more data engineering experience.

Should I try to land a role in the data engineering department where I would get first hand experience in data engineering tools and frameworks or just keep doing what I am doing in Finance and learning as I go.

I make decent money for the city I live in, but I feel like the end to end would definitely help me land other roles in the future that would branch out from just financial reporting and data.

Especially in the capacity for remote work if for some reason company or job gets moved to another city.",0,8,wheels_656,2025-04-12 20:13:03,https://www.reddit.com/r/dataengineering/comments/1jxq9r1/data_engineering_employment/,0,False,2025-04-12 20:30:53,False,False,2025-04-12 20:13:03,20,Saturday,164.0,883,64.64,7,237,12.3,0,0
211,1jxo6ga,help with a research survey that im doing regarding big data please.,"Hi everyone! I'm conducting a university research survey on commonly used Big Data tools among students and professionals. If you work in data or tech, Iâ€™d really appreciate your input â€” it only takes 3 minutes! Thank you

[https://docs.google.com/forms/d/e/1FAIpQLScXK6CnNUHGR9UIEHUhX83kHoZGYuSunRE0foZgnew81nxxLg/viewform?usp=header](https://docs.google.com/forms/d/e/1FAIpQLScXK6CnNUHGR9UIEHUhX83kHoZGYuSunRE0foZgnew81nxxLg/viewform?usp=header)",0,0,chiki_rukis,2025-04-12 18:39:21,https://www.reddit.com/r/dataengineering/comments/1jxo6ga/help_with_a_research_survey_that_im_doing/,0,False,False,False,False,2025-04-12 18:39:21,18,Saturday,39.0,447,-26.03,3,100,11.9,1,0
212,1jy09o8,Is this take-home assignment too large and complex ?,"I was given the following assignment as part of a job application. Would love to hear if people think this is reasonable or overkill for a take-home test:

**Assignment Summary:**

* Build a **Python data pipeline** and expose it via an **API**.
* The API must:
   * Accept a **venue ID**, **start date**, and **end date**.
   * Use Open-Meteo's historical weather API to fetch **hourly weather data** for the specified range and location.
   * Extract 10+ parameters (e.g., temperature, precipitation, snowfall, etc.).
   * Store the data in a **cloud-hosted database**.
   * Return success or error responses accordingly.
* Design the database schema for storing the weather data.
* Use **OpenAPI 3.0** to document the API.
* Deploy on **any cloud provider** (AWS, Azure, or GCP), including:
   * Database
   * API runtime
   * API Gateway or equivalent
* Set up **CI/CD pipeline** for the solution.
* Include a **README** with setup and testing instructions (Postman or Curl).
* Implement **QA checks in SQL** for data consistency.

Does this feel like a reasonable assignment for a take-home? How much time would you expect this to take?",87,105,hopesandfearss,2025-04-13 05:13:59,https://www.reddit.com/r/dataengineering/comments/1jy09o8/is_this_takehome_assignment_too_large_and_complex/,0,False,False,False,False,2025-04-13 05:13:59,5,Sunday,184.0,1141,43.9,16,294,11.8,0,0
213,1jxtxqo,How do my fellow on-prem DEs keep their sanity...,"...the joys of memory and compute resources seems to be a neverending suck ðŸ˜­

We're building ETL pipelines, using Airflow in one K8s namespace and Spark in another (the latter having dedicated hardware). Most data workloads aren't really Spark-worthy as files are typically <20GB, and we keep hitting pain points where processes struggle in Airflow's memory (workers are 6Gi and 6 CPU, with a limit of 10GI; no KEDA or HPA). We are looking into more efficient data structures like DuckDB, Polars, etc or running ""mid-tier"" processes as separate K8s jobs but then we hit constraints like tools/libraries relying on Pandas use so we seem stuck with eager processes.

Case in point, I just learned that our teams are having to split files into smaller files of 125k records so Pydantic schema validation won't fail on memory. I looked into GX Core and see the main source options there again appear to be Pandas or Spark dataframes (yes, I'm going to try DuckDB through SQLAlchemy). I could bite the bullet and just say to go with Spark, but then our pipelines will be using Spark for QA and not for ETL which will be fun to keep clarifying. 

Sisyphus is the patron saint of Data Engineering... just sayin'

[Make it stoooooooooop!](https://preview.redd.it/qwikfhcpihue1.png?width=503&format=png&auto=webp&s=6565d874d8d2213835c172a8ed449b14cff8214a)

(there may be some internal sobbing/laughing whenever I see posts asking ""should I get into DE..."")",49,14,Nightwyrm,2025-04-12 23:08:19,https://www.reddit.com/r/dataengineering/comments/1jxtxqo/how_do_my_fellow_onprem_des_keep_their_sanity/,0,False,False,False,False,2025-04-12 23:08:19,23,Saturday,227.0,1448,54.46,9,348,12.8,1,1
214,1jxun0q,Data Inserts best practices with Iceberg,"I receive various files at different intervals which are not defined. Can be every seconds, hour, daily, etc.

I donâ€™t have any indication also of when something is finished. For example, itâ€™s highly possible to have 100 files that would end up being 100% of my daily table, but I receive them scattered over 15min-30 when the data become available and my ingestion process ingest it. Can be 1 to 12 hours after the day is over.

Not thatâ€™s itâ€™s also possible to have 10000 very small files per day.

Iâ€™m wondering how is this solves with Iceberg tables. Very newbie Iceberg guy here. Like I donâ€™t see throughput write benchmark anywhere but I figure that rewriting the metadata files must be a big overhead if thereâ€™s a very large amount of files so inserting every times thereâ€™s a new one must not be the ideal solution.

Iâ€™ve read some medium post saying that there was a snapshot feature which track new files so you donâ€™t have to do some fancy things to load them incrementally. But again if every insert is a query that change the metadata files it must be bad at some point.

Do you wait and usually build a process to store a list of files before inserting them or is this a feature build somewhere already in a doc I canâ€™t find ?

Any help would be appreciated.

",18,2,Commercial_Dig2401,2025-04-12 23:44:32,https://www.reddit.com/r/dataengineering/comments/1jxun0q/data_inserts_best_practices_with_iceberg/,0,False,False,False,False,2025-04-12 23:44:32,23,Saturday,233.0,1272,61.87,13,342,11.4,0,0
215,1jy1n56,is Microsoft fabric the right shortcut for a data analyst moving to data engineer ?,"I'm currently on my data engineering journey using AWS as my cloud platform. However, Iâ€™ve come across the Microsoft Fabric data engineering challenge. Should I pause my AWS learning to take the Fabric challenge? Is it worth switching focus?
",17,40,LinkWray0101,2025-04-13 06:48:20,https://www.reddit.com/r/dataengineering/comments/1jy1n56/is_microsoft_fabric_the_right_shortcut_for_a_data/,0,False,False,False,False,2025-04-13 06:48:20,6,Sunday,39.0,242,61.53,4,64,9.5,0,0
216,1jyg4ps,Self-Healing Data Quality in DBT â€” Without Any Extra Tools,"I just published a practical breakdown of a method I call **Observe & Fix** â€” a simple way to manage data quality in DBT without breaking your pipelines or relying on external tools.

Itâ€™s a self-healing pattern that works entirely within DBT using native tests, macros, and logic â€” and itâ€™s ideal for fixable issues like duplicates or nulls.

Includes examples, YAML configs, macros, and even when to alert via Elementary.

Would love feedback or to hear how others are handling this kind of pattern.

ðŸ‘‰[Read the full post here ](https://medium.com/@baruchjacob/self-healing-pipelines-with-dbt-the-observe-fix-method-9d6b2da4eae3)",18,2,jb_nb,2025-04-13 19:41:47,https://www.reddit.com/r/dataengineering/comments/1jyg4ps/selfhealing_data_quality_in_dbt_without_any_extra/,0,False,False,False,False,2025-04-13 19:41:47,19,Sunday,91.0,631,45.15,5,150,11.6,1,0
217,1jydmqv,We built a natural language search tool for finding U.S. government datasets,"Hey everyone! My friend and I built [Crystal](https://askcrystal.info/search), a tool to help you search through 300,000+ datasets from [data.gov](http://data.gov) using plain English.

Example queries:

* *""Air quality in NYC after 2015""*
* *""Unemployment trends in Texas""*
* *""Obesity rates in Alabama""*

It finds and ranks the most relevant datasets, with clean summaries and download links.

We made it because searching [data.gov](http://data.gov) can be frustrating â€” we wanted something that feels more like asking a smart assistant than guessing keywords.

Itâ€™s in early alpha, but very usable. Weâ€™d love feedback on how useful it is for everyone's data analysis, and what features might make your work easier.

Try it out: [askcrystal.info/search](https://askcrystal.info/search)",13,2,xmrslittlehelper,2025-04-13 17:55:33,https://www.reddit.com/r/dataengineering/comments/1jydmqv/we_built_a_natural_language_search_tool_for/,0,False,False,False,False,2025-04-13 17:55:33,17,Sunday,110.0,788,51.04,9,183,11.6,1,0
218,1jyi735,"Landed a Role with SQL/dbt, But Clueless About Data Modeling â€” Advice?","Hi everyone,  
Iâ€™m a 2025 new grad starting this May, and Iâ€™ll be working at a small start-up as an Analytics Engineer. Iâ€™ve gotten pretty solid at SQL as Iâ€™ve been grinding Leetcode questions for fun, and it really helped me land the job. During my internships, I also worked a lot with dbt, Snowflake, and Airflow, so Iâ€™m fairly comfortable with the tooling side of things. 

Where Iâ€™m struggling is data modelingâ€”specifically the Kimball methodology, Star Schemas, and different types of dimensions and fact tables. I tried reading the Kimball book, but honestly, it felt super abstract without any hands-on practice. I get that real data modeling often involves trade-offs, business context, and actual stakeholder input, which isnâ€™t easy to simulate on your own.

So my question isâ€”how can a college student or new grad start building intuition and skills in data modeling? Are there any practical resources or projects I can work through to better understand this area? And if you have any general advice for someone entering the industry in this kind of role, Iâ€™d love to hear it.

Thanks a lot!",9,6,SmartPersonality1862,2025-04-13 21:12:25,https://www.reddit.com/r/dataengineering/comments/1jyi735/landed_a_role_with_sqldbt_but_clueless_about_data/,0,False,False,False,False,2025-04-13 21:12:25,21,Sunday,185.0,1102,52.7,10,291,12.2,0,0
219,1jy9eh1,Data modeling for analytics with legacy Schema-on-Read data lake?,"Most guides on data modeling and data pipelines seem to focus on greenfield projects.

But how do you deal with a legacy data lake where there's been years of data written into tables with no changes to original source-defined schemas?

I have hundreds of table schemas which analysts want to use but can't because they have to manually go through the data catalogue and find every column containing 'x' data or simply not bothering with some tables.

How do you tackle such a legacy mess of data? Say I want to create a Kimball model that models a persons fact table as the grain, and dimensions tables for biographical and employment data. Is my only choice to just manually inspect all the different tables to find which have the kind of column I need? Note here that there wasn't even a basic normalisation of column names enforced (""phone_num"", ""phone"", ""tel"", ""phone_number"" etc) and some of this data is already in OBT form with some containing up to a hundred sparsely populated columns.

Do I apply fuzzy matching to identify source columns? Use an LLM to build massive mapping dictionaries? What are some approaches or methods I should consider when tackling this so I'm not stuck scrolling through infinite print outs? There is a metadata catalogue with some columns having been given tags to identify its content, but these aren't consistent and also have high cardinality.

From the business perspective, they want completeness, so I can't strategically pick which tables to use and ignore the rest. Is there a way I should prioritize based on integrating the largest datasets first?

The tables are a mix of both static imports and a few daily pipelines. I'm primarily working in pyspark and spark SQL ",7,0,No_Poem_1136,2025-04-13 14:51:05,https://www.reddit.com/r/dataengineering/comments/1jy9eh1/data_modeling_for_analytics_with_legacy/,1,False,False,False,False,2025-04-13 14:51:05,14,Sunday,291.0,1716,51.78,15,458,12.2,0,0
220,1jy7sii,Building a Real-Time Analytics Pipeline: Balancing Throughput and Latency,"Hey everyone,

I'm designing a system to process and analyze a continuous stream of data with a focus on both high throughput and low latency. I wanted to share my proposed architecture and get your insights.

1. The core components are: **Kafka:** Serving as the central nervous system for ingesting a massive amount of data reliably.
2. **Go Processor:** A consumer application written in Go, placed directly after Kafka, to perform initial, low-latency processing and filtering of the incoming data.
3. **Intermediate Queue (Redis Streams/NATS JetStream):** To decouple the low-latency processing from the potentially slower analytics and to provide buffering for data that needs further analysis.
4. **Analytics Consumer:** Responsible for the more intensive analytical tasks on the filtered data from the queue.
5. **WebSockets:** For pushing the processed insights to a frontend in real-time.

The idea is to leverage Kafka's throughput capabilities while using Go for quick initial processing. The queue acts as a buffer and allows us to be selective about the data sent for deeper analytics. Finally, WebSockets provide the real-time link to the user.

I built this keeping in mind these three principles

* **Separation of Concerns:** Each component has a specific responsibility.
* **Scalability:** Kafka handles ingestion, and individual consumers can be scaled independently.
* **Resilience:** The queue helps decouple processing stages.

Has anyone implemented a similar architecture? What were some of the challenges and lessons learned? Any recommendations for improvements or alternative approaches?

Looking forward to your feedback!",7,14,Opposite_Confusion96,2025-04-13 13:35:41,https://www.reddit.com/r/dataengineering/comments/1jy7sii/building_a_realtime_analytics_pipeline_balancing/,1,False,False,False,False,2025-04-13 13:35:41,13,Sunday,244.0,1650,31.68,17,456,14.3,0,0
221,1jyj4wc,Freelancing - Real Talk,"
Hey folks,
Iâ€™m a data/software engineer trying to break into freelancing, and honestly, I could use some advice. Iâ€™ve been focusing on niches like building ETL pipelines, automation tools, web scraping, data mining, and even playing around with RAG bots.

Iâ€™ve been on Upwork for about 2 months now and only landed one small scraping gig so far. My portfolio is pretty solid (or at least I think it is), but Iâ€™m not getting much traction, barely any invites.

So Iâ€™m wondering:

Is Upwork just super saturated for this kind of work right now?

Are there better platforms or communities for technical freelancing gigs (especially data-related)?

What worked for you when you were just starting out?

Is there a niche I should lean harder into?


Would love to hear from anyone whoâ€™s been through this or has some pointers. I'm open to harsh truths, hacks, or anything in between. Appreciate it!


",6,0,Majestic_Band_9071,2025-04-13 21:54:58,https://www.reddit.com/r/dataengineering/comments/1jyj4wc/freelancing_real_talk/,1,False,False,False,False,2025-04-13 21:54:58,21,Sunday,151.0,897,64.61,10,224,10.6,0,0
222,1jy0zwo,I need assistance in optimizing this ADF workflow.,"[my\_pipeline](https://preview.redd.it/6xdqaeiikjue1.png?width=1792&format=png&auto=webp&s=7ca7ab2d25d73b1f4f7869c3927fd16c0246bb04)

Hello all! I'm excited to dive into **ADF** and try out some new things.

Here, you can see we have a copy data activity that transfers files from the source ADLS to the raw ADLS location. Then, we have a Lookup named **Lkp\_archivepath** which retrieves values from the SQL server, known as the Metastore. This will get values such as **archive\_path** and **archive\_delete\_flag** (typically it will be Y or N, and sometimes the parameter will be missing as well). After that, we have a copy activity that copies files from the source ADLS to the archive location. Now, I'm encountering an issue as I'm trying to introduce this archive delete flag concept.

If the **archive\_delete\_flag** is '**Y**', it should not delete the files from the source, but it should delete the files if the **archive\_delete\_flag** is '**N**', '' or NULL, depending on the Metastore values. How can I make this work?

Looking forward to your suggestions, thanks!",5,8,wild_data_whore,2025-04-13 06:02:54,https://www.reddit.com/r/dataengineering/comments/1jy0zwo/i_need_assistance_in_optimizing_this_adf_workflow/,0,False,False,False,False,2025-04-13 06:02:54,6,Sunday,158.0,1082,64.0,10,231,10.1,1,0
223,1jy79rg,Creating AWS Glue Connection for On-prem JDBC source,"There seems to be little to no documentation(or atleast I can't find any meaningful guides), that can help me establish a successful connection with a MySQL source. 
Either getting this VPC endpoint or NAT gateway error:

InvalidInputException: VPC S3 endpoint validation failed for SubnetId: subnet-XXX. VPC: vpc-XXX. Reason: Could not find S3 endpoint or NAT gateway for subnetId: subnet-XXX in Vpc vpc-XXX

Upon creating said endpoint and NAT gateway connection halts and provides Timeout after 5 or so minutes. My JDBC connection is able to successfully establish with either something like PyMySQL package on local machine, or in Glue notebooks with Spark JDBC connection. Any help would be great. ",3,1,TableSouthern9897,2025-04-13 13:09:41,https://www.reddit.com/r/dataengineering/comments/1jy79rg/creating_aws_glue_connection_for_onprem_jdbc/,1,False,False,False,False,2025-04-13 13:09:41,13,Sunday,109.0,703,49.35,5,176,12.3,0,0
224,1jyj7gm,"I've built a ""Cursor for data"" app and looking for beta testers","Cipher42 is a ""Cursor for data"" which works by connecting to your database/data warehouse, indexing things like schema, metadata, recent used queries and then using it to provide better answers and making data analysts more productive. It took a lot of inspiration from cursor but for data related app cursor doesn't work as well as data analysis workloads are different by nature.",2,0,jekapats,2025-04-13 21:58:22,https://www.cipher42.ai/,0,False,False,False,False,2025-04-13 21:58:22,21,Sunday,62.0,381,31.55,2,104,0.0,0,0
225,1jy2obx,Data interpretation,"any book recommendations for data interpretation for ipucet bcom h paper
",2,0,Round_Eye4720,2025-04-13 08:02:09,https://www.reddit.com/r/dataengineering/comments/1jy2obx/data_interpretation/,0,False,False,False,False,2025-04-13 08:02:09,8,Sunday,11.0,73,26.47,1,22,0.0,0,0
226,1jxy77l,Want opinion about Lambdas,"Hi all.
I'd love your opinion and experience about the data pipeline I'm working on.

The pipeline is for the RAG inference system.
The user would interact with the system through an API which triggers a Lambda.

The inference consists of  4 main functions-
1. Apply query guardrails
2. Fetch relevant chunks
3. Pass query and chunks to LLM and get response 
4. Apply source attribution (additional metadata related to the data) to the response 

I've assigned 1 AWS Lambda function to each component/function totalling to 4 lambdas in the pipeline.

Can the above mentioned functions be achieved under 30 secs if they're clubbed into 1 Lambda function?

Pls clarify in comments if this information is not sufficient to answer the question.

Also, please share any documentation that suggests which approach is better ( multiple lambdas or 1 lambda)

Thank you in advance!",2,7,VeganChicken18,2025-04-13 03:03:52,https://www.reddit.com/r/dataengineering/comments/1jxy77l/want_opinion_about_lambdas/,0,False,False,False,False,2025-04-13 03:03:52,3,Sunday,143.0,872,58.38,11,230,10.4,0,1
227,1jyh77c,"Developing, testing and deploying production grade data pipelines with AWS Glue","Serious question for data engineers working with AWS Glue: How do you actually structure and test production-grade pipelines.

For simple pipelines it's straight forward: just write everything in a single job using glue's editor, run and you're good to go, but for production data pipelines, how is the gap between the local code base that is modularized ( utils, libs, etc ) bridged with glue, that apparently needs everything to be bundled into jobs?

This is the first thing I am struggling to understand, my second dilemma is about testing jobs locally.  
How does local testing happen?

**->** if we will use glue's compute engine we run into the first question of: gap between code base and single jobs.

**->** if we use open source spark locally:

1. data can be too big to be processed locally, even if we are just testing, and this might be the reason we opted for serverless spark on the first place.

  
2. Glueâ€™s customized Spark runtime behaves differently than open-source Spark, so local tests wonâ€™t fully match production behavior. This makes it hard to validate logic before deploying to Glue",1,4,Icy-Professor-1091,2025-04-13 20:28:12,https://www.reddit.com/r/dataengineering/comments/1jyh77c/developing_testing_and_deploying_production_grade/,0,False,False,False,False,2025-04-13 20:28:12,20,Sunday,187.0,1110,59.33,9,277,12.3,0,0
228,1jxtvbu,Thoughts on Acryl vs other metadata platforms,"Hi all, I'm evaluating metadata management solutions for our data platform and would appreciate any thoughts from folks who've actually implemented these tools in production.

We're currently running into scaling issues with our in-house data catalog and I think we need something more robust for governance and lineage tracking.

I've narrowed it down to Acryl (DataHub) and Collate (openmetadata) as the main contenders. I know I should look at Collibra and Alation and maybe Unity Catalog?

For context, we're a mid-sized fintech (\~500 employees) with about 30 data engineers and scientists. We're AWS with Snowflake, Airflow for orchestration, and a growing number of ML models in production.

My question list is: 

1. How these tools handle machine-scale operations 
2. How painful was it to get set up?
3. For DataHub and openmetadata specifically - is the open source version viable or is the cloud version necessary?
4. Any unexpected limitations you've hit with any of these platforms?
5. Do you feel like these grow with you as we increasingly head into AI governance? 
6. How well they integrate with existing tools (Snowflake, dbt, Looker, etc.)

If anyone has switched from one solution to another, I'd love to hear why you made the change and whether it was worth it.

Sorry for the pick list of questions - the last post on this was years ago and I was hoping for some more insights. Thanks in advance for anyone's thoughts. ",1,1,arronsky,2025-04-12 23:05:00,https://www.reddit.com/r/dataengineering/comments/1jxtvbu/thoughts_on_acryl_vs_other_metadata_platforms/,0,False,False,False,False,2025-04-12 23:05:00,23,Saturday,239.0,1442,56.45,16,377,11.9,0,0
229,1jxv28g,Help,"I'm using Airbyte Cloud because my PC doesn't have enough resources to install it. I have a Docker container running PostgreSQL on Airbyte Cloud. I want to set the PostgreSQL destination. Can anyone give me some guidance on how to do this? Should I create an SSH tunnel?

",0,1,Imaginary_Pirate_267,2025-04-13 00:06:47,https://www.reddit.com/r/dataengineering/comments/1jxv28g/help/,0,False,False,False,False,2025-04-13 00:06:47,0,Sunday,48.0,272,70.19,5,72,8.2,0,0
230,1jyegqk,What to do and how to do???,"This is a photo of my notes (not OG rewrote later) about a meet at work about this said project. The project is about migration of ms sql server to snowflake. 

The code conversion will be done using Snowconvert. 

For historic data
1. The data extraction is done using a python script using bcp command and pyodbc library 
2. The converted code from snowconvert will be used in a python script again to create all the database objects. 
3. data extracted will be loaded into internal stage and then to table 

2 and 3 will use snowflakeâ€™s python connector 

For transitional data: 
1. Use ADF to store pipeline output into an Azure blob container 
2. Use external stage to utilise this blob and load data into table 


1. My question is if you have ADF for transitional data then why not use the same thing for historic data as well (I was given the task of historic data)
2. Is there a free way to handle this transitional data as well. It needs to be enterprise level (Also what is wrong with using VS Code extension) 
3. After I showed initial approach following things were asked by mentor/friend to incorporate in this to really sell my approach (He went home after giving me no clarification about how to do this and what even are they)
- validation of data on both sides 
- partition aware extraction 
- parallely extracting data (Idts it is even possible)

I request help on where to even start looking and rate my approach I am a fresh graduate and been on job for a month. ðŸ™‚â€â†•ï¸ðŸ™‚â€â†•ï¸
",0,4,Optimal_Carrot4453,2025-04-13 18:30:34,https://i.redd.it/ttbzo15fbnue1.jpeg,0,False,False,False,False,2025-04-13 18:30:34,18,Sunday,271.0,1493,59.13,13,408,12.5,0,0
231,1jyrrh6,Data Quality Struggles!,,434,8,growth_man,2025-04-14 05:52:12,https://i.redd.it/w8r7q3zyoque1.png,0,False,False,False,False,2025-04-14 05:52:12,5,Monday,,0,206.84,1,0,0.0,0,0
232,1jz0yr9,"[video] What is Iceberg, and why is everyone talking about it?",,71,5,rmoff,2025-04-14 14:59:30,https://www.youtube.com/watch?v=TsmhRZElPvM,0,False,False,False,False,2025-04-14 14:59:30,14,Monday,,0,206.84,1,0,0.0,0,0
233,1jysxtk,Why Data Warehouses Were Created?,"The original data chaos actually started *before* spreadsheets were common. In the pre-ERP days, most business systems were siloedâ€”HR, finance, sales, you name itâ€”all running on their own. To report on anything meaningful, you had to extract data from each system, often manually. These extracts were pulled at different times, using different rules, and then stitched togethe. The result? Data quality issues. And to make matters worse, people were running these reports directly against transactional databasesâ€”systems that were supposed to be optimized for speed and reliability, not analytics. The reporting load bogged them down.

The problem was so painful for the businesses, so around the late 1980s, a few forward-thinking folksâ€”most famously Bill Inmonâ€”proposed a better way: a data warehouse.

To make matter even worse, in the late â€™00s every department had its own spreadsheet empire. Finance had one version of â€œthe truth,â€ Sales had another, and Marketing were inventing their own metrics. People would walk into meetings with totally different numbers for the same KPI.

The spreadsheet party had turned into a data chaos rave. There was no lineage, no source of truthâ€”just lots of tab-switching and passive-aggressive email threads. It wasnâ€™t just annoyingâ€”it was a risk. Businesses were making big calls on bad data. So data warehousing became common practice!

More about it: [https://www.corgineering.com/blog/How-Data-Warehouses-Were-Created](https://www.corgineering.com/blog/How-Data-Warehouses-Were-Created)

  
P.S. Thanks to u/rotr0102 I made the post at least 2x times better",37,14,LinasData,2025-04-14 07:15:27,https://www.reddit.com/r/dataengineering/comments/1jysxtk/why_data_warehouses_were_created/,0,False,2025-04-14 19:01:58,False,False,2025-04-14 07:15:27,7,Monday,229.0,1602,50.12,18,382,10.9,1,0
234,1jyu3r8,Overclocking dbt: Discord's Custom Solution in Processing Petabytes of Data,,34,8,rmoff,2025-04-14 08:43:47,https://discord.com/blog/overclocking-dbt-discords-custom-solution-in-processing-petabytes-of-data,0,False,False,False,False,2025-04-14 08:43:47,8,Monday,,0,206.84,1,0,0.0,0,0
235,1jyqtrc,Roles when career shifting out of data engineering?,"To be specific, non-code heavy work. I think Iâ€™m one of the few data engineers who hates coding and developing. All our projects and clients so far have always asked us to use ADB in developing notebooks for ETL use, and I have never touched ADF -_-

Now Iâ€™m sick of it, developing ETL stuff using pyspark or sparksql is too stressful for me and I have 0 interest in data engineering right now. 

Anyone who has successfully left the DE field? What non-code role did you choose? Iâ€™d appreciate any suggestions especially for jobs that make use of some of the less-coding side of Data Engineering.

I see lots of people going for software eng because they love coding and some go ML or Data Scientist. Maybe i just want less tech-y work right now but yeah open to any suggestions. Iâ€™m also fine with sql, as long as itâ€™s not to be used for developing sht lol",15,21,Specific_Onion2659,2025-04-14 04:50:00,https://www.reddit.com/r/dataengineering/comments/1jyqtrc/roles_when_career_shifting_out_of_data_engineering/,0,False,False,False,False,2025-04-14 04:50:00,4,Monday,159.0,857,61.97,9,231,11.0,0,0
236,1jzb3at,What database did they use?,"ChatGPT can now remember all conversations you've had across all chat sessions. Google Gemini, I think, also implemented a similar feature about two months ago with *Personalization*â€”which provides help based on your search history.  

Iâ€™d like to hear from database engineers, database administrators, and other CS/IT professionals (as well as actual humans): What kind of database do you think they use? Relational, non-relational, vector, graph, data warehouse, data lake?  

*P.S. I know I could just do deep research on ChatGPT, Gemini, and Grokâ€”but I want to hear from Redditors.",13,3,Fast_Hovercraft_7380,2025-04-14 21:51:54,https://www.reddit.com/r/dataengineering/comments/1jzb3at/what_database_did_they_use/,0,False,False,False,False,2025-04-14 21:51:54,21,Monday,89.0,585,36.49,5,156,14.6,0,0
237,1jyve41,Event Sourcing as a creative tool for developers,"Hey, I think there are better use cases for event sourcing.  
  
Event sourcing is an architecture where you capture every change in your system as an immutable event, rather than just storing the latest state. Instead of only knowing what your data looks like now, you keep a full history of how it got there. In a simple crud app that would mean that every deleted, updated, and created entry is stored in your event source, that way when you replay your events you can recreate the state that the application was in at any given time.

Most developers see event sourcing as a kind of technical safety net: - Recovering from failures - Rebuilding corrupted read models - Auditability

Surviving schema changes without too much pain

And fair enough, replaying your event stream often feels like a stressful situation. Something broke, you need to fix it, and youâ€™re crossing your fingers hoping everything rebuilds cleanly.

What if replaying your event history wasnâ€™t just for emergencies? What if it was a normal, everyday part of building your system?

Instead of treating replay as a recovery mechanism, you treat it as a development tool â€” something you use to evolve your data models, improve your logic, and shape new views of your data over time. More excitingly, it means you can derive entirely new schemas from your event history whenever your needs change.

Your database stops being the single source of truth and instead becomes what it was always meant to be: a fast, convenient cache for your data, not the place where all your logic and assumptions are locked in.

With a full event history, youâ€™re free to experiment with new read models, adapt your data structures without fear, and shape your data exactly to fit new purposes â€” like enriching fields, backfilling values, or building dedicated models for AI consumption. Replay becomes not about fixing what broke, but about continuously improving what youâ€™ve built.

And this has big implications â€” especially when it comes to AI and MCP Servers.

Most application databases arenâ€™t built for natural language querying or AI-powered insights. Their schemas are designed for transactions, not for understanding. Data is spread across normalized tables, with relationships and assumptions baked deeply into the structure.

But when you treat your event history as the source of truth, you can replay your events into purpose-built read models, specifically structured for AI consumption.

Need flat, denormalized tables for efficient semantic search? Done. Want to create a user-centric view with pre-joined context for better prompts? Easy. Youâ€™re no longer limited by your applicationâ€™s schema â€” you shape your data to fit exactly how your AI needs to consume it.

And hereâ€™s where it gets really interesting: AI itself can help you explore your data history and discover whatâ€™s valuable.

Instead of guessing which fields to include, you can use AI to interrogate your raw events, spot gaps, surface patterns, and guide you in designing smarter read models. Itâ€™s a feedback loop: your AI doesnâ€™t just query your data â€” it helps you shape it.

So instead of forcing your AI to wrestle with your transactional tables, you give it clean, dedicated models optimized for discovery, reasoning, and insight.

And the best part? You can keep iterating. As your AI use cases evolve, you simply adjust your flows and replay your events to reshape your models â€” no migrations, no backfills, no re-engineering.",14,11,No-Exam2934,2025-04-14 10:15:58,https://www.reddit.com/r/dataengineering/comments/1jyve41/event_sourcing_as_a_creative_tool_for_developers/,0,False,False,False,False,2025-04-14 10:15:58,10,Monday,570.0,3470,51.18,28,893,12.7,0,0
238,1jyxdlb,ETL for Ingesting S3 files and converting to Iceberg,"So, I'm currently working on a project (my first) to create a scalable data platform for a company. The whole thing structured around AWS, initially using DMS to migrate PostgreSQL data to S3 in parquet format (this is our raw datalake). Then using Glue jobs to read this data and create Iceberg tables which would be used in Athena queries and Quicksight. I've got a working Glue script for reading this data and perform upsert operations. Okay so now that I've given a bit of context of what I'm trying to do, let me tell you my problem.  
The client wants me to schedule this job to run every 15min or so for staging and most probably every hour for production. The data in the raw datalake is partitioned by date (for example: s3bucket/table\_name/2025/04/10/file.parquet). Now that I have to run this job every 15 min or so I'm not sure how to keep track of the files that have been processed and which haven't. Currently my script finds the current time and modifies the read command to use just the folder for the current date. But still, this means that I'll be reading all the files in the folder (processed already or not) every time the job runs during the day.   
I've looked around and found that using DynamoDB for keeping track of the files would be my best option but also found something related to Iceberg metadata files that could help me with this. I'm leaning towards the Iceberg option as I wanna make use of all its features but have too little information regarding this to implement. would absolutely appreciate it if someone could help me out with this.  
Has anyone worked with Iceberg in this matter? and if the iceberg solution isn't usable, could someone help me out with how to implement the DynamoDB way.",10,7,morpheas788,2025-04-14 12:15:20,https://www.reddit.com/r/dataengineering/comments/1jyxdlb/etl_for_ingesting_s3_files_and_converting_to/,0,False,False,False,False,2025-04-14 12:15:20,12,Monday,308.0,1736,67.59,15,443,11.1,0,1
239,1jytkai,Need Advice on solution - Mapping Inconsistent Country Names to Standardized Values,"Hi Folks,

In my current project, we are ingesting a wide variety of external public datasets. One common issue weâ€™re facing is that the **country names in these datasets are not standardized**. For example, we may encounter entries like **""Burma"" instead of ""Myanmar""**, or **""Islamic Republic of Iran"" instead of ""Iran""**.

My initial approach was to extract all unique country name variations and map them to a list of standard country names using logic such as CASE WHEN conditions or basic string-matching techniques.

However, my manager has suggested we leverage **AI/LLM-based models** to automate the mapping of these country names to a standardized list to handle new query points as well. 

I have a couple of concerns and would appreciate your thoughts:

1. **Is using AI/LLMs a suitable approach for this problem?**
2. **Can LLMs be fully reliable in these mappings, or is there a risk of incorrect matches?**
3. I was considering implementing a **feedback pipeline** that highlights any newly encountered or unmapped country names during data ingestion so we can review and incorporate logic to handle them in the code over time. Would this be a better or complementary solution?
4. Please suggest if there is some better approach.

Looking forward to your insights!",8,9,RC-05,2025-04-14 08:02:29,https://www.reddit.com/r/dataengineering/comments/1jytkai/need_advice_on_solution_mapping_inconsistent/,0,False,False,False,False,2025-04-14 08:02:29,8,Monday,206.0,1280,54.02,12,334,12.3,0,0
240,1jz0yme,Has anyone used Cube.js for operational (non-BI) use cases?,"The semantic layer in Cube looks super useful â€” defining metrics, dimensions, and joins in one place is a dream. But most use cases Iâ€™ve seen are focused on BI dashboards and analytics.

Iâ€™m wondering if anyone here has used Cube for more *operational* or *app-level* read scenarios â€” like powering parts of an internal tool, or building a unified read API across microservices (via Cube's GraphQL support). All read-only, but not just charts â€” more like structured data fetching.

Any war stories, performance considerations, or architectural tips? Curious if it holds up well when the use case isn't classic OLAP.

Thanks!",7,0,uri3001,2025-04-14 14:59:21,https://www.reddit.com/r/dataengineering/comments/1jz0yme/has_anyone_used_cubejs_for_operational_nonbi_use/,0,False,False,False,False,2025-04-14 14:59:21,14,Monday,102.0,624,54.73,6,163,13.3,0,0
241,1jz0zet,How do I document existing Pipelines?,There is lot of pipelines working in our Azure Data Factory. There is json files available for those. I am new in the team and there not very well details about those pipelines. And my boss wants me to create something which will describe how pipelines working. And looking for how do i Document those so for future anyone new in our team can understand what have done. ,3,7,UnluckyToday4275,2025-04-14 15:00:15,https://www.reddit.com/r/dataengineering/comments/1jz0zet/how_do_i_document_existing_pipelines/,0,False,False,False,False,2025-04-14 15:00:15,15,Monday,68.0,370,74.59,5,96,8.8,0,0
242,1jytgrh,Advice on data warehouse design for ERP Integration with Power BI,"Hi everyone!

Iâ€™d like to ask for your advice on designing a relational data warehouse fed from our ERP system. We plan to use Power BI as our reporting tool, and all departments in the company will rely on it for analytics.

The challenge is that teams from different departments expect **the data to be fully related and ready** to use when building dashboards, minimizing the need for additional modeling. Weâ€™re struggling to determine the best approach to meet these expectations.

What would you recommend?

Should all dimensions and facts be pre-related in the data warehouse, even if it adds complexity?

Creating separate data models in Power BI for different departments/use cases?

Handling all relationships in the data warehouse and exposing them via curated datasets?

Should we empower Power BI users to create their own data models, or enforce strict governance with documented relationships?

Thanks in advance for your insights! ",5,2,Able-Tomatillo-5122,2025-04-14 07:55:22,https://www.reddit.com/r/dataengineering/comments/1jytgrh/advice_on_data_warehouse_design_for_erp/,1,False,False,False,False,2025-04-14 07:55:22,7,Monday,150.0,946,47.79,10,259,13.3,0,0
243,1jz6bu9,Databricks Pain Points?,"Hi everyone,

My team is working on some tooling to build some user friendly ways to do things in Databricks. Our initial focus is around entity resolution, creating a simple tool that can evaluate the data in unity catalog and deduplicate tables, create identity graphs, etc.

I'm trying to get some insights from people who use Databricks day-to-day to figure out what other kinds of capabilities we'd want this thing to have if we want users to try it out. 

Some examples I have gotten from other venues so far:

* Cost optimization
* Annotating or using advanced features of Unity Catalog can't be done from the UI and users would like being able to do it without having to write a bunch of SQL
* Figuring out which libraries to use in notebooks for a specific use case

This is just an open call for input here. If you use Databricks all the time, what kind of stuff annoys you about it or is confusing?

For the record, this tool are building will be open source and this isn't an ad. The eventual tool will be free to use, I am just looking for broader input into how to make it as useful as possible.

Thanks!",2,3,caleb-amperity,2025-04-14 18:37:22,https://www.reddit.com/r/dataengineering/comments/1jz6bu9/databricks_pain_points/,0,False,False,False,False,2025-04-14 18:37:22,18,Monday,207.0,1118,50.4,7,307,13.9,0,0
244,1jz9af1,How do managed services work with vendors like ClickHouse?,"  
**Context:**  
New to data engineering. New to the cloud too. I am in charge of doing trade studies on various storage solutions for my new company. I'm gathering requirements for the system, then pricing out options that meet those requirements. At the end of all my research, I have to present my trade studies so leadership can decide how to spend their cash.  

**Question:**  
I am seeing a lot of companies that do ""managed services"" that are not native to a cloud provider like AWS. For example, I see that ClickHouse offers managed services that piggy back off of AWS or other cloud providers. 

Do they have an AWS account that they provision with their software on ec2 instances (or something), and then they give you access to it? Or do they act as consultants who come in and install ClickHouse on your own AWS account? 







",2,13,wcneill,2025-04-14 20:34:56,https://www.reddit.com/r/dataengineering/comments/1jz9af1/how_do_managed_services_work_with_vendors_like/,0,False,False,False,False,2025-04-14 20:34:56,20,Monday,146.0,843,71.95,9,209,11.0,0,0
245,1jz5r9v,"If you've been curious about what a feature store is and if you actually need one, this post might help","I've worked as both a data and ML engineer and feature stores tend to be an interesting subject. I think they're often misunderstood and quite frankly, not needed for many companies. I wanted to write the blog post to solidify my thoughts and thought it might be helpful for others here.",2,0,fithrowaway379,2025-04-14 18:14:31,https://www.daimlengineering.com/p/feature-stores-demistified,0,False,False,False,False,2025-04-14 18:14:31,18,Monday,51.0,287,71.14,3,73,10.5,0,0
246,1jz0clz,Databricks geographic coding on the cheap?,"We're migrating a bunch of geography data from local SQL Server to Azure Databricks.  Locally, we use ArcGIS to match latitude/longitude to city,state locations, and pay a fixed cost for the subscription.  We're looking for a way to do the same work on Databricks, but are having a tough time finding a cost effective ""all-you-can-eat"" way to do it.  We can't just install ArcGIS there to use or current sub.

Any ideas how to best do this geocoding work on Databricks, without breaking the bank?",2,1,stonetelescope,2025-04-14 14:33:42,https://www.reddit.com/r/dataengineering/comments/1jz0clz/databricks_geographic_coding_on_the_cheap/,0,False,False,False,False,2025-04-14 14:33:42,14,Monday,85.0,496,62.68,5,126,11.2,0,0
247,1jyu16h,dbt sqlmesh migration,Has anyone migrated their dbt cloud to sqlmesh? If so what tools did you use? How many models? How much time did take? Biggest pain points?,2,3,do-a-cte-roll,2025-04-14 08:38:03,https://www.reddit.com/r/dataengineering/comments/1jyu16h/dbt_sqlmesh_migration/,0,False,False,False,False,2025-04-14 08:38:03,8,Monday,26.0,139,91.58,5,33,6.7,0,0
248,1jyrzz3,Help with possible skill expansion or clarification on current role,"So after about 25 years of experience in what was considered DBA, I am now unemployed due to the federal job cuts and it seems DBA just isn't a role anymore. I am currently working on getting a cloud certification but the rest of my skills seem to be mixed and I am hoping someone has a more specific role I would fit into. I am also hoping to expand my skills into some newer technology but I have no clue where to even start. 

Current skills are:

Expert level SQL

Some knowledge of Azure and AWS

Python, PowerShell, GIT, .NET, C#, Idera, Vcentre, Oracle, BI, and ETL with some other minor things mixed in. 

Where should I go from here? What role could this be considered? What other skills could I gain some knowledge on?",2,1,RevolutionaryMonk190,2025-04-14 06:08:14,https://www.reddit.com/r/dataengineering/comments/1jyrzz3/help_with_possible_skill_expansion_or/,0,False,False,False,False,2025-04-14 06:08:14,6,Monday,136.0,728,71.14,8,191,10.4,0,0
249,1jzalqz,MySQL CDC for ClickHouse,,1,0,saipeerdb,2025-04-14 21:30:07,https://clickhouse.com/blog/mysql-cdc-connector-clickpipes-private-preview,1,False,False,False,False,2025-04-14 21:30:07,21,Monday,,0,206.84,1,0,0.0,0,0
250,1jz3m2a,Files to be processed in sequence on S3 bucket.,"What is the best possible solution to process the files in an S3 bucket in a sequential order. 

Use case is that an external systems generates CSV files and dump them on to S3 buckets. These CSV files consists of data from few  Oracle tables. These files needs to be processed in a sequential order in order to maintain the referential integrity of the data while loading into the Postgres RDS. If the files are not processed in an order, the load errors out with the reference data doesn't exist. What is a best solution to process the files on a S3 bucket in an order? ",1,3,Awsmason,2025-04-14 16:47:37,https://www.reddit.com/r/dataengineering/comments/1jz3m2a/files_to_be_processed_in_sequence_on_s3_bucket/,0,False,False,False,False,2025-04-14 16:47:37,16,Monday,106.0,572,61.97,6,159,11.5,0,0
251,1jz4fsw,NoSQL Database for Ticketing System,"We're working on a uni project where we need to design the database for an Ticketing system that will support around 7,000 users. Under normal circumstances, I'd definitely go with a relational database. But we're *required* to use multiple **NoSQL** databases instead. Any suggestions for NoSQL Databases?",0,4,StrongFault814,2025-04-14 17:21:23,https://www.reddit.com/r/dataengineering/comments/1jz4fsw/nosql_database_for_ticketing_system/,0,False,False,False,False,2025-04-14 17:21:23,17,Monday,47.0,306,42.58,4,86,12.6,0,0
252,1jz3810,Any success story from Microsoft Feature Stores?,"The idea is great: build once and use everywhere. But for MS Feature Store, it requires a single flat file as source for any given feature set. 

That means if I need multiple data sources, I need write code to connect to the various data sources, merge them, flatten them into a single file -- all of them done outside of Feature Stores.

For me, it creates inefficiency as the raw flattened file is created solely for the purpose of transformation within feature store. 

Plus when there is a mismatch in granularity or non-overlapping domain, I have to create different flattened files for different feature sets. That seems to be more hassles than whatever merit it may bring.

  
I would love to hear from your success stories before I put in more effort. 

",0,1,Kindly-Principle3706,2025-04-14 16:31:25,https://www.reddit.com/r/dataengineering/comments/1jz3810/any_success_story_from_microsoft_feature_stores/,0,False,False,False,False,2025-04-14 16:31:25,16,Monday,133.0,763,60.75,7,196,11.2,0,0
253,1jz026m,Recommendations for a new grad,"Hello all, I am looking for some advice on the reason of data engineering/data science (yes I know they are different). I will be graduating in May with a degree in Physics. During my time in school, I have spent considerable time doing independent study for Python, MATLAB, Java, and SQL. Due to financial constraints I am not able to pay for a certification course for these languages but I have taken free exams to get some sort of certificate that says I know what I'm talking about. I have grown to not really want to work in a lab setting, but rather a role working with numbers and data points in the abstract. So I'm looking for a role in analyzing data or creating infrastructure for data management. Do you all have any advice for a new head trying to break into the industry? Anything would be greatly appreciated.",0,6,CCrite,2025-04-14 14:21:26,https://www.reddit.com/r/dataengineering/comments/1jz026m/recommendations_for_a_new_grad/,0,False,2025-04-14 14:31:31,False,False,2025-04-14 14:21:26,14,Monday,150.0,825,52.39,8,233,11.5,0,0
254,1jyx7at,How do you improve Data Quality?,I always get different answer from different people on this.,0,17,Foreigner_Zulmi,2025-04-14 12:05:56,https://www.reddit.com/r/dataengineering/comments/1jyx7at/how_do_you_improve_data_quality/,0,False,False,False,False,2025-04-14 12:05:56,12,Monday,10.0,60,52.87,1,17,0.0,0,0
255,1jyvyxh,Fact Tables: The Backbone of Your Data Warehouse,"Check out the new blog about Fact Tables   
[https://medium.com/@adityasharmah27/fact-tables-the-backbone-of-your-data-warehouse-9a3014cc20c3](https://medium.com/@adityasharmah27/fact-tables-the-backbone-of-your-data-warehouse-9a3014cc20c3)",0,0,adityasharmah,2025-04-14 10:54:13,https://medium.com/@adityasharmah27/fact-tables-the-backbone-of-your-data-warehouse-9a3014cc20c3,0,False,False,False,False,2025-04-14 10:54:13,10,Monday,9.0,240,-242.23,1,47,0.0,1,0
256,1jyv7r9,Khatabook (YC S18) replaced Mixpanel and cut its analytics cost by 90%,"Khatabook, a leading Indian fintech company (YC 18), replaced Mixpanel with Mitzu and Segment with RudderStack to manage its massive scale of over 4 billion monthly events, achieving a 90% reduction in both data ingestion and analytics costs. By adopting a warehouse-native architecture centered on Snowflake, Khatabook enabled real-time, self-service analytics across teams while maintaining 100% data accuracy.",0,1,Still-Butterfly-3669,2025-04-14 10:03:50,https://i.redd.it/v8x1gtjvxrue1.jpeg,0,False,False,False,False,2025-04-14 10:03:50,10,Monday,58.0,412,8.2,2,115,0.0,0,0
257,1jyv9wq,One of the best Fivetran alternative,"If you're urgently looking for a Fivetran alternative, this might help

Been seeing a lot of people here caught off guard by the new Fivetran pricing. If you're in eCommerce and relying on platforms like Shopify, Amazon, TikTok, or Walmart, the shift to MAR-based billing makes things really hard to predict and for a lot of teams, hard to justify.

If youâ€™re in that boat and actively looking for alternatives, this might be helpful.

**Daton**, built by Saras Analytics, is an ETL tool specifically created for eCommerce. That focus has made a big difference for a lot of teams weâ€™ve worked with recently who needed something that aligns better with how eComm brands operate and grow.

Here are a few reasons teams are choosing it when moving off Fivetran:

**Flat, predictable pricing**  
Thereâ€™s no MAR billing. Youâ€™re not getting charged more just because your campaigns performed well or your syncs ran more often. Pricing is clear and stable, which helps a lot for brands trying to manage budgets while scaling.

**Retail-first coverage**  
Daton supports all the platforms most eComm teams rely on. Amazon, Walmart, Shopify, TikTok, Klaviyo and more are covered with production-grade connectors and logic that understands how retail data actually works.

**Built-in reporting**  
Along with pipelines, Daton includes Pulse, a reporting layer with dashboards and pre-modeled metrics like CAC, LTV, ROAS, and SKU performance. This means you can skip the BI setup phase and get straight to insights.

**Custom connectors without custom pricing**  
If you use a platform thatâ€™s not already integrated, the team will build it for you. No surprise fees. They also take care of API updates so your pipelines keep running without extra effort.

**Support thatâ€™s actually helpful**  
Youâ€™re not stuck waiting in a ticket queue. Teams get hands-on onboarding and responsive support, which is a big deal when youâ€™re trying to migrate pipelines quickly and with minimal friction.

Most eComm brands start with a stack of tools. Shopify for the storefront, a few ad platforms, email, CRM, and so on. Over time, that stack evolves. You might switch CRMs, change ad platforms, or add new tools. But Shopify stays. It grows with you. Daton is designed with the same mindset. You shouldn't have to rethink your data infrastructure every time your business changes. Itâ€™s built to scale with your brand.

If you're currently evaluating options or trying to avoid a painful renewal, Daton might be worth looking into. I work with the Saras team and happy to help , here's the link if you want to checkout [https://www.sarasanalytics.com/saras-daton](https://www.sarasanalytics.com/saras-daton)

Hope this helps !",0,7,Temporary_You5983,2025-04-14 10:07:58,https://www.reddit.com/r/dataengineering/comments/1jyv9wq/one_of_the_best_fivetran_alternative/,0,False,False,False,False,2025-04-14 10:07:58,10,Monday,428.0,2700,65.01,29,646,9.8,1,0
258,1jztqf6,US job search 2025 results,"Currently Senior DE at medium size global e-commerce tech company, looking for new job. Prepped for like 2 months Jan and Feb, and then started applying and interviewing. Here are the numbers:

Total apps: 107. 6 companies reached out for at least a phone screen. 5.6% conversion ratio.

The 6 companies where the following:

|Company|Role|Interviews|
|:-|:-|:-|
|Meta|Data Engineer|HR and then LC tech screening. Rejected after screening|
|Amazon|Data Engineer 1|Take home tech screening then LC type tech screening. Rejected after second screening|
|Root|Senior Data Engineer|HR then HM. Got rejected after HM|
|Kin|Senior Data Engineer|Only HR, got rejected after.|
|Clipboard Health|Data Engineer|Online take home screening, fairly easy but got rejected after.|
|Disney Streaming|Senior Data Engineer|Passed HR and HM interviews. Declined technical screening loop.|

At the end of the day, my current company offered me a good package to stay as well as a team change to a more architecture type role. Considering my current role salary is decent and fully remote, declined Disneys loop since I was going to be making the same while having to move to work on site in a HCOL city.

PS. Im a US Citizen.",68,19,rudboi12,2025-04-15 14:47:13,https://www.reddit.com/r/dataengineering/comments/1jztqf6/us_job_search_2025_results/,0,False,False,False,False,2025-04-15 14:47:13,14,Tuesday,188.0,1205,50.33,15,310,12.0,0,0
259,1jzpar1,Shoutout to everyone building complete lineage on unstructured data!,,41,5,Embarrassed_Spend976,2025-04-15 11:18:43,https://i.redd.it/rz42pqk2gzue1.png,0,False,False,False,False,2025-04-15 11:18:43,11,Tuesday,,0,206.84,1,0,0.0,0,0
260,1jzx8lx,Greenfield: Do you go DWH or DL/DLH?,"If you're building a data platform from scratch today, do you start with a DWH on RDBMS? Or Data Lake[House] on object storage with something like Iceberg?

I'm assuming the near dominance of Oracle/DB2/SQL Server of > ~10 years ago has shifted? And Postgres has entered the mix as a serious option? But are people building data lakes/lakehouses from the outset, or only once they breach the size of what a DWH can reliably/cost-effectively do?",28,69,rmoff,2025-04-15 17:07:52,https://www.reddit.com/r/dataengineering/comments/1jzx8lx/greenfield_do_you_go_dwh_or_dldlh/,0,False,False,False,False,2025-04-15 17:07:52,17,Tuesday,75.0,444,64.91,5,113,9.4,0,0
261,1jzq748,"Faster Data Pipelines with MCP, Cursor and DuckDB",,21,3,TransportationOk2403,2025-04-15 12:06:59,https://motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai/,0,False,False,False,False,2025-04-15 12:06:59,12,Tuesday,,0,206.84,1,0,0.0,0,0
262,1jzptxc,The Universal Data Orchestrator: The Heartbeat of Data Engineering,,6,0,sspaeti,2025-04-15 11:48:10,https://www.ssp.sh/blog/universal-data-orchestrator/,0,False,False,False,False,2025-04-15 11:48:10,11,Tuesday,,0,206.84,1,0,0.0,0,0
263,1jzo8q2,Address & Name matching technique,"Context: 
I have a dataset of company owned products like: Name: Company A, Address: 5th avenue, Product: A. 
Company A inc, Address: New york, Product B. 
Company A inc. , Address, 5th avenue New York, product C. 

I have 400 million entries like these. As you can see, addresses and names are in inconsistent formats. 
I have another dataset that will be me ground truth for companies. It has a clean name for the company along with itâ€™s parsed address. 

The objective is to match the records from the table with inconsistent formats to the ground truth, so that each product is linked to a clean company. 



Questions and help: 
- i was thinking to use google geocoding api to parse the addresses and get geocoding. Then use the geocoding to perform distance search between my my addresses and ground truth BUT i donâ€™t have the geocoding in the ground truth dataset. So, i would like to find another method to match parsed addresses without using geocoding. 

- Ideally, i would like to be able to input my parsed address and the name (maybe along with some other features like industry of activity) and get returned the top matching candidates from the ground truth dataset with a score between 0 and 1. Which approach would you suggest that fits big size datasets? 

- The method should be able to handle cases were one of my addresses could be: company A, address: Washington (meaning an approximate address that is just a city for example, sometimes the country is not even specified). I will receive several parsed addresses from this candidate as Washington is vague. What is the best practice in such cases? As the google api wonâ€™t return a single result, what can i do?

- My addresses are from all around the world, do you know if google api can handle the whole world? Would a language model be better at parsing for some regions? 

Help would be very much appreciated, thank you guys. 
",7,11,Bojack-Cowboy,2025-04-15 10:13:54,https://www.reddit.com/r/dataengineering/comments/1jzo8q2/address_name_matching_technique/,0,False,False,False,False,2025-04-15 10:13:54,10,Tuesday,334.0,1902,64.0,21,488,11.1,0,0
264,1jzi6u4,Dataverse vs. Azure SQL DB,"Thank you everyone with all of your helpful insights from my initial post! Just as the title states, I'm an intern looking to weigh the pros and cons of using Dataverse vs an Azure SQL Database (After many back and forths with IT, we've landed at these two options that were approved by our company). 

Our team plans to use Microsoft Power Apps to collect data and are now trying to figure out where to store the data. Upon talking with my supervisor, they plan to have data exported from this database to use for data analysis in SAS or RStudio, in addition to the Microsoft Power App.

What would be the better or ideal solution for this? Thank you!
Edit: Also, they want to store images as well. Any ideas on how and where to store them?",5,4,JPBOB1431,2025-04-15 03:35:41,https://www.reddit.com/r/dataengineering/comments/1jzi6u4/dataverse_vs_azure_sql_db/,0,False,2025-04-15 04:44:35,False,False,2025-04-15 03:35:41,3,Tuesday,138.0,741,59.94,7,203,11.5,0,0
265,1jzgdob,How has Business Intelligence Analytics changed the way you make decisions at work?,"Iâ€™ve been diving deep into how companies use Business Intelligence Analytics to not just track KPIs but actually transform how they operate day to day. Itâ€™s crazy how powerful real-time dashboards and predictive models have become. imagine optimizing customer experiences before they even ask for it or spotting a supply chain delay before it even happens. Curious to hear how others are using BI analytics in your field Have tools like tableau, Power BI, or even simple CRM dashboards helped your team make better decisions or is it all still gut feeling and spreadsheets?  P.S. I found an article that simplified this topic pretty well. If anyones curious Iâ€™ll drop the link below. Not a promotion just thought it broke things down nicely https://instalogic.in/blog/the-role-of-business-intelligence-analytics-what-is-it-and-why-does-it-matter/",4,6,Sadikshk2511,2025-04-15 02:01:57,https://www.reddit.com/r/dataengineering/comments/1jzgdob/how_has_business_intelligence_analytics_changed/,0,False,False,False,False,2025-04-15 02:01:57,2,Tuesday,124.0,846,36.59,7,224,13.0,1,0
266,1jzyxdw,How would you handle the ingestion of thousands of files ?,"Hello,
Iâ€™m facing a philosophical question at work and I canâ€™t find an answer that would put my brain at ease. 

Basically we work with Databricks and Pyspark for ingestion and transformation.

We have a new data provider that sends crypted and zipped files to an s3 bucket. There are a couple of thousands of files (2 years of historic).

We wanted to use dataloader from databricks. Itâ€™s basically a spark stream that scans folders, finds the files that you never ingested (it keeps track in a table) and reads the new files only and write them.
The problem is that dataloader doesnâ€™t handle encrypted and zipped files (json files inside).

We canâ€™t unzip files permanently. 

My coworker proposed that we use the autoloader to find the files (that it can do) and in that spark stream use the for each batch method to apply a lambda that does:
- get the file name (current row)
-decrypt and unzip
-hash the files (to avoid duplicates in case of failure)
-open the unzipped file using spark
-save in the final table using spark 

I argued that itâ€™s not the right place to do all that and since itâ€™s not the use case of autoloader itâ€™s not a good practice, he argues that spark is distributed and thatâ€™s the only thing we care since it allows us to do what we need quickly even though itâ€™s hard to debug (and we need to pass the s3 credentials to each executor using the lambdaâ€¦)

I proposed a homemade solution which isnâ€™t the most optimal, but it seems better and easier to maintain which is:
- use boto paginator to find files
- decrypt and unzip each file 
- write then json in the team bucket/folder
-create a monitoring table in which we save the file name, hash, status (ok/ko) and exceptions if there are any

He argues that this is not efficient since itâ€™ll only use one single node cluster and not parallelised. 

I never encountered such use case before and Iâ€™m kind of stuck, I read a lot of literature but everything seems very generic. 

Edit: we only receive 2 to 3 files daily per data feed (150mo per file on average) but we have 2 years of historical data which amounts to around 1000 files. So we need 1 run for all the historic then a daily run. 
Every feed ingested is a class instantiation (a job on a cluster with a config) so it doesnâ€™t matter if we have 10 feeds. 

Edit2: 1000 files roughly summed to 130go after unzipping. Not sure of average zip/json file though. 

What do you people think of this? Any advices ?
Thank you ",5,28,Heiwashika,2025-04-15 18:15:12,https://www.reddit.com/r/dataengineering/comments/1jzyxdw/how_would_you_handle_the_ingestion_of_thousands/,0,False,2025-04-15 19:09:42,False,False,2025-04-15 18:15:12,18,Tuesday,449.0,2452,60.18,16,622,12.2,0,1
267,1jznyyv,How much does your org spend on ETL tools monthly?,"Looking for a general estimate on how much companies spend on tools like Airbyte, Fivetran, Stitch, etc, per month?



[View Poll](https://www.reddit.com/poll/1jznyyv)",3,17,jah_reddit,2025-04-15 09:55:46,https://www.reddit.com/r/dataengineering/comments/1jznyyv/how_much_does_your_org_spend_on_etl_tools_monthly/,0,False,False,False,False,2025-04-15 09:55:46,9,Tuesday,21.0,167,24.78,1,39,0.0,1,0
268,1jzfaso,Spark sql vs Redshift tiebreaker rules during sorting,"Iâ€™m looking to move some of my teams etl away from redshift and on to AWS glue. 

Iâ€™m noticing that the spark sql data frames donâ€™t sort back in the same order in the case of having nulls vs redshift. 

My hope was to port over the Postgres sql to spark sql and end up with very similar output. 

Unfortunately itâ€™s looking like itâ€™s off. 
For instance if I have a window function for row count, the same query assigns the numbers to different rows in spark. 

What is the best path forward to get the sorting the same?",5,3,sghokie,2025-04-15 01:06:55,https://www.reddit.com/r/dataengineering/comments/1jzfaso/spark_sql_vs_redshift_tiebreaker_rules_during/,0,False,False,False,False,2025-04-15 01:06:55,1,Tuesday,99.0,519,71.65,6,136,9.3,0,1
269,1jzm2zf,PowerAutomate as an ETL Tool,"Hi!

This is a problem I am facing in my current job right now. We have a lot of RPA requirements and 300's of CSV's and Excel files are manually obtained from some interfaces and mail and customer only works with excels including reporting and operational changes are being done manually by hand.

The thing is we don't have any data. We plan to implement Power Automate to grab these files from the said interfaces. But as some of you know, PowerAutomate has SQL Connectors. 

Do you think it is ok to write files directly to a database with PowerAutomate? Have any of you experience in this? Thanks.",3,12,Brilliant_Breath9703,2025-04-15 07:42:17,https://www.reddit.com/r/dataengineering/comments/1jzm2zf/powerautomate_as_an_etl_tool/,0,False,False,False,False,2025-04-15 07:42:17,7,Tuesday,108.0,602,64.3,7,167,12.3,0,0
270,1jzgltu,Suggest best sources to master DBMS,"I recently joined as an intern in an organisation. They assigned me database technology, and they wanted me to learn everything about database and database management systems in the span of 5 months.
They suggested to me a book to learn from but it's difficult to learn from that book. 
I have an intermediate knowledge on Oracle SQL and Oracle PL/SQL. 
I want to gain much knowledge on Database and DBMS.

So i request people out there who have knowledge on databases to suggest the best sources(preffered free) to learn from scratch to advanced as soon as possible.",3,3,TrulyIntrovert45,2025-04-15 02:13:21,https://www.reddit.com/r/dataengineering/comments/1jzgltu/suggest_best_sources_to_master_dbms/,0,False,False,False,False,2025-04-15 02:13:21,2,Tuesday,98.0,567,54.93,6,152,13.0,0,0
271,1k063vg,SAP Databricks,"Curious if anyone is brave enough to leave Azure/AWS Databricks for SAP Databricks? Or if you are an SAP shop would you choose that over pure Databricks. From past experiences with SAP Iâ€™ve never been a fan of anything they do outside ERP. Personally, I believe you should separate yourself as much as possible for future contract negotiations. Also the risk of limited people singing up and you have a bunch of half baked integrations.",2,2,TowerOutrageous5939,2025-04-15 23:21:01,https://www.reddit.com/r/dataengineering/comments/1k063vg/sap_databricks/,0,False,False,False,False,2025-04-15 23:21:01,23,Tuesday,75.0,436,64.71,5,112,11.2,0,0
272,1k05igl,"Are complex data types (JSON, BSON, MAP, LIST, etc.) commonly used in Parquet?","Hey folks,

I'm building a tool to convert between Parquet and other formats (CSV, JSON, etc.).Â  You can see it here: [https://dataconverter.io/tools/parquet](https://dataconverter.io/tools/parquet)

Progress has been very good so far.Â  The question now is how far into complex Parquet types to go â€“ given than many of the target formats don't have an equivalent type.

How often do you come across Parquet files with complex or nested structures?Â  And what are you mostly seeing?

I'd appreciate any insight you can share.",3,1,delete99,2025-04-15 22:53:16,https://www.reddit.com/r/dataengineering/comments/1k05igl/are_complex_data_types_json_bson_map_list_etc/,0,False,False,False,False,2025-04-15 22:53:16,22,Tuesday,80.0,523,51.55,7,134,8.0,1,0
273,1jzvxei,Looking for advice or resources on folder structure for a Data Engineering project,"Hey everyone,  
Iâ€™m working on a Data Engineering project and I want to make sure Iâ€™m organizing everything properly from the start. I'm looking for **best practices, lessons learned, or even examples of folder structures** used in real-world data engineering projects.

Would really appreciate:

* Any **advice or personal experience** on what worked well (or didnâ€™t) for you
* **Blog posts, GitHub repos, YouTube videos**, or other resources that walk through good project structure
* Recommendations for organizing things like ETL pipelines, raw vs processed data, scripts, configs, notebooks, etc.

Thanks in advance â€” trying to avoid a mess later by doing things right early on!",2,4,Responsible_Yak_1162,2025-04-15 16:15:01,https://www.reddit.com/r/dataengineering/comments/1jzvxei/looking_for_advice_or_resources_on_folder/,0,False,False,False,False,2025-04-15 16:15:01,16,Tuesday,106.0,683,37.13,4,177,14.9,0,0
274,1jznj1b,Use the output of a cell in a Databricks notebook in another cell,"Hi,
I have a Notebook A containing multiple SQL scripts in multiple cells.  I am trying to use the output of specific cells of Notebook_A in another notebook. Eg: count of records returned in cell2 of notebook_a in the python Notebook_B.

Kindly suggest on the feasible ways to implement the above.",2,5,Interesting-Today302,2025-04-15 09:24:21,https://www.reddit.com/r/dataengineering/comments/1jznj1b/use_the_output_of_a_cell_in_a_databricks_notebook/,0,False,False,False,False,2025-04-15 09:24:21,9,Tuesday,51.0,298,58.48,4,81,11.2,0,0
275,1jzhx1e,Tracking Ongoing tasks for the team,"My team is involved in Project development work that fits perfectly in the agile framework, but we also have some ongoing tasks related to platform administration, monitoring support, continuous enhancement of security, etc. These tasks do not fit well in the agile process. How do others track such tasks and measure progress on them? Do you use specific tools for this?",2,1,PreparationScared835,2025-04-15 03:21:03,https://www.reddit.com/r/dataengineering/comments/1jzhx1e/tracking_ongoing_tasks_for_the_team/,0,False,False,False,False,2025-04-15 03:21:03,3,Tuesday,61.0,371,47.49,4,102,12.6,0,0
276,1jzh1v2,Parquet Nested Type to JSON in C++/Rust,"Hi Reddit community! This is my first Reddit post and Iâ€™m hoping I could get some help with this task Iâ€™m stuck with please!

I read a parquet file and store it in an arrow table. I want to read a parquet complex/nested column and convert it into a JSON object. I use C++ so Iâ€™m searching for libraries/tools preferably in C++ but if not, then I can try to integrate it with rust. 
What I want to do:
Say there is a parquet column in my file of type (arbitrary, just to showcase complexity): List(Struct(List(Struct(int,string,List(Struct(int, bool)))), bool))
I want to process this into a JSON object (or a json formatted string, then I can convert that into a json object). I do not want to flatten it out for my current use case. 

What I have found so far:
1. Parquet's inbuilt toString functions donâ€™t really work with structs (theyâ€™re just good for debugging)
2. havenâ€™t found anything in C++ that would do this without me having to writing a custom recursive logic, even with rapidjson
3. tried Polars with Rust but didnâ€™t get a Json yet. 

I know I can get write my custom logic to create a json formatted string, but there must be some existing libraries that do this? I've been asked to not write a custom code because they're difficult to maintain and easy to break :)

Appreciate any help!",2,14,Upper-Replacement142,2025-04-15 02:36:14,https://www.reddit.com/r/dataengineering/comments/1jzh1v2/parquet_nested_type_to_json_in_crust/,0,False,False,False,False,2025-04-15 02:36:14,2,Tuesday,235.0,1302,70.13,13,319,9.7,0,1
277,1k07pua,AI for data and analytics,"We just launched **Seda.** You can connect your data and ask questions in plain English, write and fix SQL with AI, build dashboards instantly, ask about data lineage, and auto-document your tables and metrics. Weâ€™re opening up early access now at [seda.ai](https://www.seda.ai/). It works with Postgres, Snowflake, Redshift, BigQuery, dbt, and more.",0,1,secodaHQ,2025-04-16 00:40:23,https://www.reddit.com/r/dataengineering/comments/1k07pua/ai_for_data_and_analytics/,0,False,False,False,False,2025-04-16 00:40:23,0,Wednesday,52.0,350,66.74,4,80,10.1,1,0
278,1k07g9i,"Data Governance, a safe role in the near future?","Whatâ€™s your take on the Data Governance role when it comes to job security and future opportunities, especially with how fast technology is changing, tasks getting automated, new roles popping up, and some jobs becoming obsolete?",0,3,AdministrativeBuy885,2025-04-16 00:26:34,https://www.reddit.com/r/dataengineering/comments/1k07g9i/data_governance_a_safe_role_in_the_near_future/,0,False,False,False,False,2025-04-16 00:26:34,0,Wednesday,36.0,229,26.48,1,62,0.0,0,0
279,1k05w90,Does Microsoft Purview has MDM feature?,"I know Purview is a data governance tool but does it has any MDM functionality.  From the article it seems it has integration with third party MDM solution partners such as CluedIn, profisee but I am not very clear whether or not it can do MDM by itself. 

One of my client's budget is very slim and they wanted to implement MDM. Do you think Microsoft Data Services (MDS) is an option but it looks very old to me and it seems to require a dedicated SQL server license. ",1,0,boogie_woogie_100,2025-04-15 23:10:56,https://www.reddit.com/r/dataengineering/comments/1k05w90/does_microsoft_purview_has_mdm_feature/,1,False,False,False,False,2025-04-15 23:10:56,23,Tuesday,89.0,470,57.3,4,132,12.6,0,1
280,1k05837,How do you handle datetime dimentions ?,"I had a small â€œargumentâ€ at the office today. I am building a fact table to aggregate session metrics from our Google Analytics environment. One of the columns is the of course the sessionâ€™s datetime. There are multiple reports and dashboards that do analysis at hour granularity. Ex : â€œWhat hour are visitors from this source more likely to buy hour product?â€

To address this, I creates a date and time dimention. Today, the Data Specialist had an argument with me and said this is suboptimal and a single timestamp dimention should have been created. I though this makes no sense since it would result in extreme redudancy : you would have multiple minute rows for a single day for example. 

Now I am questioning my skills as he is a specialist and teorically knows better. I am failing to understand how a single timestamp table is better than seperates time and date dimentions",1,7,mrkatatau,2025-04-15 22:39:58,https://www.reddit.com/r/dataengineering/comments/1k05837/how_do_you_handle_datetime_dimentions/,0,False,False,False,False,2025-04-15 22:39:58,22,Tuesday,154.0,883,56.05,10,239,11.6,0,0
281,1k00qv1,Best setup report builder within SaaS?,"Hi everyone,

We've built a CRM and are looking to implement a report builder in our app.

We are exploring the best solutions for our needs and it seems like we have two paths we could take:

* Option A: Build the front-end/query builder ourselves and hit read-only replica
* Option B: Build the front-end/query builder ourselves and hit a data warehouse we've built using a key-base replication mechanism on BigQuery/Snowflake, etc..
* Option C: Use third party tools like Explo etc...

About the app:

* Our stack is React, Rails, Postgres.
* Our most used table (contacts) have 20,000,000 rows
* Some of our users have custom fields

We're trying to build something scalable but most importantly not spend months in this project.  
As a result, I'm wondering about the viability of Option A vs. Option B.  
  
One important point is how to manage custom fields that our users created on some objects.

We were thinking about, for contacts for example, we were thinking about simply running with joins across the following tables

* contacts
* contacts\_custom\_fields
* companies (and any other related 1:1 table so we can query fields from related 1:1 objects)
* contacts\_calculated\_fields (materialized view to compute values from 1:many relationship like # of deals the contacts is on)

So the two questions are:

* Would managing all this on the read-only be viable for our volume and a good starting point or will we hit the performance limits soon given our volume?
* Is managing custom fields this way the right way?",1,2,birdshine7,2025-04-15 19:28:35,https://www.reddit.com/r/dataengineering/comments/1k00qv1/best_setup_report_builder_within_saas/,0,False,False,False,False,2025-04-15 19:28:35,19,Tuesday,257.0,1529,43.97,9,401,14.7,0,0
282,1k003wp,Doing a Hard Delete in Fivetran,"Wondering if doing a hard delete in fivetran is possible without a dbt connector. I did my initial sync, go to transformations and can't figure out how to just add a sql statement to run after each sync.",1,0,TheBrady4,2025-04-15 19:02:28,https://www.reddit.com/r/dataengineering/comments/1k003wp/doing_a_hard_delete_in_fivetran/,0,False,False,False,False,2025-04-15 19:02:28,19,Tuesday,38.0,203,60.65,2,58,0.0,0,0
283,1jzznos,Airflow or Prefect,"I've just started a data engineering project where Iâ€™m building a data pipeline using DuckDB and DBT, but Iâ€™m a bit unsure whether to go with Airflow or Prefect for orchestration. Any suggestions?",1,3,SomewhereStandard888,2025-04-15 18:44:36,https://www.reddit.com/r/dataengineering/comments/1jzznos/airflow_or_prefect/,1,False,False,False,False,2025-04-15 18:44:36,18,Tuesday,33.0,196,37.98,1,54,0.0,0,0
284,1jzzf00,My Experience in preparing Azure Data Engineer Associate DP-203.,"So I recently appeared for the DP-203 certification by Microsoft and want to share my learnings and strategy that I followed to crack the exam.

As you all must already be knowing that this exam is labelled as â€œ**Intermediate**â€ byÂ Microsoft themselves which is perfect in my opinion. This exam does test you in the various concepts that are required for a data engineer toÂ  master in his/her career. 

Having said that, it is not too hard to crack the exam but at the same time also not as easy as appearing for AZ-900. 

DP-203 is aimed at testing the understanding of data related concepts and various tools Microsoft has offered in its suite to make your life easier. Some topics include SQL, Modern Data Warehousing, Python, PySpark, Azure Data Factory, Azure Synapse Analytics, Azure Stream Analytics, Azure EventHubs, Azure Data Lake Storage and last but not the least Azure Databricks. You can go through the complete set of topics this exam focuses on here - [https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/?practice-assessment-type=certification#certification-take-the-exam](https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/?practice-assessment-type=certification#certification-take-the-exam)



**Courses:**

I had just taken this one course for DP-203 by Alan Rodrigues *(This is not a paid promotion. I just thought that these resources were good to refer to)* and this is a 24 hour long course which has covered all the important and core concepts clearly and precisely. What I loved the most about this course is that it is a complete hands-on course. One more thing is that the instructor very rarely mentions anything as â€œthis has already been covered in the previous sectionsâ€. If there is anything that we are using in the current section he makes sure to give a quick background on what has been covered in the earlier sections. Why this is so important is because we tend to forget some things and by just getting a refresher in a couple of sentences we are up to speed. 

For those of you who donâ€™t know, Microsoft offers access to majority resources if not all for FREE credit worth $200 for 30 days. So you simply have to sign up on their portal (insert link) and get access to all of them for 30 days. If you are residing in another country then convert dollars to your local currency. That is how much worth of free credit you will get for 30 days. 

**For example -** 

I live in India. 

1 $ = 87.789 INR 

So I got FREE credits worth 87.789 X 200 = Rs 17,557

Even when I appeared for the exam (Feb 8th, 2025) I hardly got 3-4 questions from the mock tests. But donâ€™t get disheartened. Be sure you are consistent with your learning path and take notes whenever required. As I mentioned earlier, the exam is not very hard.

**Link -** [https://www.udemy.com/course/data-engineering-on-microsoft-azure/learn/lecture/44817315?start=40#overview](https://www.udemy.com/course/data-engineering-on-microsoft-azure/learn/lecture/44817315?start=40#overview)



**Mock Tests Resources:**

So I had referred a couple of resources for taking the mocks which I have mentioned below. *(This is not a paid promotion. I just thought that these resources were good to refer to.)*



1. **Udemy Practice Tests -** [https://www.udemy.com/course/practice-exams-microsoft-azure-dp-203-data-engineering/?couponCode=KEEPLEARNING](https://www.udemy.com/course/practice-exams-microsoft-azure-dp-203-data-engineering/?couponCode=KEEPLEARNING)
2. **Microsoft Practice Assessments -** [https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/practice/assessment?assessment-type=practice&assessmentId=49&practice-assessment-type=certification](https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/practice/assessment?assessment-type=practice&assessmentId=49&practice-assessment-type=certification)
3. [https://www.examtopics.com/exams/microsoft/dp-203/](https://www.examtopics.com/exams/microsoft/dp-203/)





**DOâ€™s:**



1. Make sure that if and whenever possible you do hands-on for all the sections and videos that have been covered in the Udemy course as I am 100% sure that you will encounter certain errors and would have to explore and solve the errors by yourself. This will build a sense of confidence and achievement after being able to run the pipelines or code all by yourself. (Also donâ€™t forget to delete or pause resources whenever needed so that you get a hang of it and donâ€™t lose out on money. The instructor does tell you when to do so.)
2. Letâ€™s be very practical, nobody remembers all the resolutions or solutions to every single issue or problem faced in the past. We tend to forget things over time and hence it is very important to document everything that you think is useful and would be important in the future. Maintain an excel sheet and create two columns â€œ**Errorsâ€ and â€œLearnings/Resolution**â€ so that next time you encounter the same issue you already have a solution and donâ€™t waste time. 
3. Watch and practice at least 5-10 videos daily. This way you can complete all the videos in a month and then go back and rewatch lessons you thought were hard. Then you can start giving practice tests. 





**DON'Ts:**



1. By heart all the MCQs or answers to the questions. 
2. Refer to many resources so much so that you will get overwhelmed and not be able to focus on preparation.
3. Even refer to multiple courses from different websites.





**Conclusion:**

All in all, just make sure you do your hands on, practice regularly, give a timeline for yourself, donâ€™t mug up things, donâ€™t by heart things, make sure you use limited but quality resources for learning and practice. I am sure that by following these things you will be able to crack the exam in the first attempt itself. ",1,1,saahilrs14,2025-04-15 18:34:58,https://www.reddit.com/r/dataengineering/comments/1jzzf00/my_experience_in_preparing_azure_data_engineer/,0,False,False,False,False,2025-04-15 18:34:58,18,Tuesday,832.0,5838,44.03,44,1405,11.7,1,0
285,1jzymk5,Spark UI DAG,Just wanted ro understand if after doing an union I want to write to S3 as parquet.  Why do I see 76 task ? Is it because union actually partitioned the data ? I tried doing salting after union still I see 76 tasks for a given stage. Perhaps I see it is read parquet I am guessing something to do with committed whixh creates a temporary folder before writing to s3. Any help is appreciated. Please note I don't have access to the spark UI to debug the DAG. I have manged to give print statements and that I where I am trying to  corelate. ,1,0,cida1205,2025-04-15 18:03:08,https://www.reddit.com/r/dataengineering/comments/1jzymk5/spark_ui_dag/,0,False,False,False,False,2025-04-15 18:03:08,18,Tuesday,106.0,540,75.2,8,148,8.1,0,0
286,1jzunnw,"bigquery/sheet/tableau, need for advice","Hello everyone,

I recently joined a project that uses BigQuery for data storage, dbt for transformations, and Tableau for dashboarding. I'd like some advice on improving our current setup.

# Current Architecture

* Data pipelines run transformations using dbt
* Data from BigQuery is synchronized to Google Sheets
* Tableau reports connect to these Google Sheets (not directly to BigQuery)
* Users can modify tracking values directly in Google Sheets

# The Problems

1. **Manual Process**: Currently, the Google Sheets and Tableau connections are created manually during development
2. **Authentication Issues**: In development, Tableau connects using the individual developer's account credentials
3. **Orchestration Concerns**: We have Google Cloud Composer for orchestration, but the Google Sheets synchronization happens separately

# Questions

1. What's the best way to automate the creation and configuration of Google Sheets in this workflow? Is there a Terraform approach or another IaC solution?
2. How should we properly manage connection strings in tableau between environments, especially when moving from development (using personal accounts) to production?

Any insights from those who have worked with similar setups would be greatly appreciated!",1,0,Acceptable-Sail-4575,2025-04-15 15:24:07,https://www.reddit.com/r/dataengineering/comments/1jzunnw/bigquerysheettableau_need_for_advice/,0,False,False,False,False,2025-04-15 15:24:07,15,Tuesday,182.0,1265,28.33,10,335,14.7,0,0
287,1jztjln,Is it possible to generate an open-table/metadata store that combines multiple data sources?,"I've recently learned about open-table paradigm, which if I am interpreting correctly, is essentially a mechanism for storing metadata so that the data associated with it can be efficiently looked up and retrieved. (Please correct this understanding if it is wrong). 

My question is whether or not you could have a single metadata store or open-table that combines metadata from two different storage solutions, so that you could query both from a single CLI tool using SQL like syntax? 

And as a follow on question... I've learned about and played with AWS Athena in an online course. It uses Glue Crawler to somehow discover metadata. Is this based on an open-table paradigm? Or a different technology? ",1,3,wcneill,2025-04-15 14:39:22,https://www.reddit.com/r/dataengineering/comments/1jztjln/is_it_possible_to_generate_an_opentablemetadata/,0,False,False,False,False,2025-04-15 14:39:22,14,Tuesday,116.0,707,56.76,8,187,12.4,0,0
288,1jztbwd,API Help,"Hello, I am working on a personal ETL project with a beginning goal of trying to ingest data from Google Books API and batch insert into pg.  


Currently I have a script that cleans the API result into a list which is then inserted into pg. But, I have many repeat values each time I run this query, resulting in no data being inserted into pg.

I also notice that I get very random books that are not at all on topic for what I specific with my query parameters. e.g. title='data' and author=' '. 

  
I am wondering if anybody knows how to get only relevant data with API calls, as well as non duplicate value with each run of the script (eg persistent pagination).

  
Example of a \~320 book query.

In the first result I get somewhat data-related books. However, in the second result i get results such as: ""Homoeopathic Journal of Obstetrics, Gynaecology and Paedology"".

I understand that this is a broad query, but when I specify I end up getting very few book results(\~40-80), which is surprising because I figured a Google API would have more data. 

I may be doing this wrong, but any advice is very much appreciated.

    â¯ python3 apiClean.py
    The selfLink we get data from: https://www.googleapis.com/books/v1/volumes?q=data+inauthor:&startIndex=0&maxResults=40&printType=books&fields=items(selfLink)&key=AIzaSyDirSZjmIfQTvYgCnUZ0BhbIlrKRF8qxHw
    
    ...
    
    The selfLink we get data from: https://www.googleapis.com/books/v1/volumes?q=data+inauthor:&startIndex=240&maxResults=40&printType=books&fields=items(selfLink)&key=AIzaSyDirSZjmIfQTvYgCnUZ0BhbIlrKRF8qxHw
    
    size of result rv:320",1,1,GwHeezE,2025-04-15 14:30:37,https://www.reddit.com/r/dataengineering/comments/1jztbwd/api_help/,0,False,False,False,False,2025-04-15 14:30:37,14,Tuesday,225.0,1620,38.42,14,397,12.0,1,1
289,1jzt2ho,Need advice - Informatica production support,"Hi , i have working as a informatica production support where i need to monitor ETL jobs on daily basis and report the bottlenecks to the developer to fix the issue and im getting $9.5k/year with 5 YOE. rightnow its kind of boring and planning to move to informatica powercenter admin position since its not opensource its hard for me to self learn myself. just want to know any opensource tools related to data integration that has high in demand for administrator role would be great. ",1,4,Odd_Insect_9759,2025-04-15 14:20:01,https://www.reddit.com/r/dataengineering/comments/1jzt2ho/need_advice_informatica_production_support/,0,False,False,False,False,2025-04-15 14:20:01,14,Tuesday,86.0,487,49.86,4,132,12.6,0,0
290,1jznrvv,Database design problem for many to many data relationship...need suggestions,"I have to come up with a database design working on postgres. I have to migrate at the end almost trillions volumes of data into a postgres DB wherein CRUD operations can be run most efficiently. The data present is in the form of a many to many relationship. How the data looks is:  
  
In my old data base i have a value T1 which is connected to on average 700 values (like x1,x2,x3...x700). Here in the old DB we are saving 700 records of this connection. Similarly other values like T2,T3,T100 all have multiple connections each having a separate row

Use case:  
We need to make updates,deletions and inserts to both values of T and values of X  
for example,  
I am given That value T1 instead of 700 connections of X has now 800 connections...so i must update or insert all the new connections corresponding to T1  
And like wise if I am given , we need to update all T values X1 (say X1 has 200 connection of T) i need to insert/update or delete T values associated with X1.

  
for now, I was thinking of aggregating my data in the form of a jsonb column  
where  
Column T             Column X (jsonb)  
T1                           {""value"":\[X1,X2,X3.....X700\]}

But i will have to create another similar table where i keep column T as jsonb. Since any updates in one table needs to be synced to the other any errors may cause it to be out of sync.

Also the time taken to read and update a jsonb row will be high

Any other suggestions on how i should think about creating schema for my problem?",1,2,CollectionPerfect248,2025-04-15 09:41:51,https://www.reddit.com/r/dataengineering/comments/1jznrvv/database_design_problem_for_many_to_many_data/,0,False,False,False,False,2025-04-15 09:41:51,9,Tuesday,274.0,1509,63.22,11,394,11.7,0,0
291,1jzvfys,"Event sourcing isnâ€™t about storing history. itâ€™s about replaying it.
Discussion","Replay isnâ€™t just about fixing broken systems. Itâ€™s about rethinking how we build them in the first place. If your data architecture is driven by immutable events instead of current state, then replay stops being a recovery mechanism and starts becoming a way to continuously reshape, refine, and evolve your system with zero fear of breaking things.

Letâ€™s talk about replay :)

**Event sourcing is misunderstood**  
For most developers, event sourcing shows up as a safety mechanism. Itâ€™s there to recover from a failure, rebuild a read model, trace an audit trail, or get through a schema change without too much pain. Replay is something you reach for in the rare cases when things go sideways.

Thatâ€™s how itâ€™s typically treated. A fallback. Something reactive.

But that lens is narrow. It frames replay as an emergency tool instead of something more fundamental. When events are treated as the source of truth, replay can become a normal, repeatable part of development. Not just a way to recover, but a way to refine.

**What if replay wasnâ€™t just for emergencies?**  
What if it was a routine, even joyful, part of building your system?

Instead of treating replay as a recovery mechanism, you treat it as a development tool. Something you use to evolve your data models, improve your business logic, and shape entirely new views of your data over time. And more excitingly, it means you can derive entirely new schemas from your event history whenever your needs change.

**Why replay is so hard in most setups**  
Hereâ€™s the catch. In most event-sourced systems, **events are emitted after your app logic runs**. Your API gets the request, updates the database, and only then emits a change event. That event is a side effect, not the source of truth.

So when you want to replay, it gets tricky. You need replay-safe logic. You need to carefully version events. You need infrastructure to reprocess historical data. And you have to make absolutely sure youâ€™re not double-applying anything.

Thatâ€™s why replay often feels fragile. Itâ€™s not that the idea is bad. Itâ€™s just hard to pull off.

**But what if you flip the model?**  
What if events come first, not last?

Thatâ€™s the approach we took.

A user action, like creating a user, updating an address, or assigning a tag, sends an event. That event is immediately appended to an immutable event store, **and only then** is it passed along to the application API to validate and store in the database.

Suddenly your database isnâ€™t your source of truth. Itâ€™s just a read model. A fast, disposable output of your event stream.

So when you want to evolve your logic or reshape your data structure, all you have to do is update your flow, delete the old database, and press replay.

Thatâ€™s it.

No migrations.  
No fragile ETL jobs.  
No one-off backfills.  
Just replay your history into the new shape.

**Your data becomes fluid**  
Say youâ€™re running an e-commerce platform, and six months in, you realize you never tracked the discount code a customer used at checkout. It wasnâ€™t part of the original schema. Normally, this would mean a migration, a painful manual backfill (if the data even still exists), or writing a fragile script to stitch it in later, assuming youâ€™re lucky enough to recover it.

But with a full event history, you donâ€™t need to hack anything.

You just update your flow logic to extract the discount code from the original checkout events. Then replay them.

Within minutes, your entire dataset is updated. The new field is populated everywhere it should have been, as if it had been there from day one.

**Your database becomes what it was always meant to be**  
A cache.  
Not a source of truth.  
Something you can throw away and rebuild without fear.  
You stop treating your schema like a delicate glass sculpture and start treating it like software.

**Replay unlocks AI-native data (with MCP Servers)**  
Most application databases are optimized for transactions, not understanding. Theyâ€™re normalized, rigid, and shaped around application logic, not meaning. Thatâ€™s fine for serving an app. But for AI? Nope.

Language models thrive on context. They need denormalized, readable structures. They need relationships spelled out. They need the why, not just the what.

When you have an event history, not just state but actions and intent. You can replay those events into entirely new shapes. You can build read models that are tailored specifically for AI: flattened tables for semantic search, user-centric structures for chat interfaces, agent-friendly layouts for reasoning.

And itâ€™s not just one-and-done. You can reshape your models over and over as your use cases evolve. No migrations. No backfills. Just a new flow and a replay.

What is even more interesting is that with the help of MCP Servers AI can help you do this. By interrogating the event history with natural language prompts, it can suggest new model structures, flag gaps, and uncover meaning you didnâ€™t plan for. Itâ€™s a feedback loop: replay helps AI make sense of your data, and AI helps you decide how to replay.

And none of this works without events that store intent. Current state is just a snapshot. Events tell the story.

**So, why doesnâ€™t everyone build this way?**  
Because itâ€™s hard. You need immutable storage. Replay-safe logic. Tools to build and maintain read models. Schema evolution support. Observability. Infrastructure to safely reprocess everything.

The architecture has been around for a while â€” Martin Fowler helped popularize event sourcing nearly two decades ago. But most teams ran into the same issue: implementing it well was too complex for everyday use.

Thatâ€™s the reason behind the Flowcore Platform To make this kind of architecture not just possible, but effortless. Flowcore handles the messy parts. The ingestion, the immutability, the reprocessing, the flow management, the replay. So you can just build. You send an event, define what you want done with it, and replay it whenever you need to improve.",0,3,OAEareMyInitials,2025-04-15 15:55:44,https://www.reddit.com/r/dataengineering/comments/1jzvfys/event_sourcing_isnt_about_storing_history_its/,0,False,2025-04-15 16:09:47,False,False,2025-04-15 15:55:44,15,Tuesday,995.0,6011,68.06,85,1530,9.8,0,0
292,1jzo47v,Kimball's Approach In Harry Potter Style,"Checkout this amazing post about Kimball's Approach 

[https://medium.com/@adityasharmah27/kimballs-approach-the-sorcerer-s-stone-of-data-warehousing-9658f292eeb4](https://medium.com/@adityasharmah27/kimballs-approach-the-sorcerer-s-stone-of-data-warehousing-9658f292eeb4)",0,2,adityasharmah,2025-04-15 10:05:17,https://www.reddit.com/r/dataengineering/comments/1jzo47v/kimballs_approach_in_harry_potter_style/,0,False,False,False,False,2025-04-15 10:05:17,10,Tuesday,8.0,272,-308.89,1,48,0.0,1,0

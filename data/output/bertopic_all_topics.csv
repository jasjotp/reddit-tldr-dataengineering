post,topic,created_utc
"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline‚Äîingestion, transformation, and enrichment‚Äîas a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",-1,2025-03-29 18:35:07
"Hi üëãüèª I've been reading some responses over the last week regarding the DuckLake release, but felt like most of the pieces were missing a core advantage. Thus, I've tried my luck in writing and coding something myself, although not being in the writer business myself. 

Would be happy about your opinions. I'm still worried to miss a point here. I think, there's something lurking in the lake üê°",-1,2025-06-07 12:09:21
"Just a brief rant. I'm importing a pipe-delimited data file where one of the fields is this company name:

PC'S? NOE PROBLEM||| INCORPORATED

And no, they didn't escape the pipes in any way. Maybe exclamation points were forbidden and they got creative? Plus, this is giving my English degree a headache.

What's the worst flat file problem you've come across?",-1,2025-06-07 20:09:07
Green Data centres powered by stable geothermal energy guaranteeing Tier IV ratings and improved ESG rankings. Perfect for AI farms and high power consumption DCs,-1,2025-06-07 17:18:05
"We‚Äôre working with a system where core transactional data lives in MySQL, and related reference data is now stored in a normalized form in Postgres.

A key limitation: the apps and services consuming data from MySQL cannot directly access Postgres tables. Any access to Postgres data needs to happen through an intermediate mechanism that doesn‚Äôt expose raw tables.

We‚Äôre trying to figure out the best way to enrich MySQL-based records with data from Postgres ‚Äî especially for dashboards and read-heavy workloads ‚Äî without duplicating or syncing large amounts of data unnecessarily.

We use AWS in many parts of our stack, but not exclusively. Cost-effectiveness matters, so open-source solutions are a plus if they can meet our needs.

Curious how others have solved this in production ‚Äî particularly where data lives across systems, but clean, efficient enrichment is still needed without direct table access.",-1,2025-06-07 01:43:31
"Hey folks ‚Äî I‚Äôm working on a tool that lets you define your own XML validation rules through a UI. Things like:

* Custom tags
* Attribute requirements
* Regex patterns
* Nested tag rules

It‚Äôs for devs or teams that deal with XML in banking, healthcare, enterprise apps, etc. I‚Äôm trying to solve some of the pain points of using rigid schema files or complex editors like Oxygen or XMLSpy.

If this sounds interesting, I‚Äôd love your feedback through this quick 3‚Äì5 min survey:  
üëâ [https://docs.google.com/forms/d/e/1FAIpQLSeAgNlyezOMTyyBFmboWoG5Rnt75JD08tX8Jbz9-0weg4vjlQ/viewform?usp=dialog](https://docs.google.com/forms/d/e/1FAIpQLSeAgNlyezOMTyyBFmboWoG5Rnt75JD08tX8Jbz9-0weg4vjlQ/viewform?usp=dialog)

No email required. Just trying to build something useful, and your input would help me a lot. Thanks!",-1,2025-06-07 15:57:51
"A new event has popped up in Manchester looks significant! Some of the ex team from the wonderful bigdataldn are involved too

https://datadecoded.com/
",-1,2025-06-07 07:47:35
"Hi everyone,



I‚Äôm planning to build a directory-listing website with the following requirements:



\- Content Backend (RAG pipeline):

I have a large library of PDF files (user guides, datasheets, etc.).

I‚Äôll run them through an ML pipeline to extract structured data (tables, key facts, metadata).

Users need to be able to search and filter that extracted data very quickly and accurately.



\- User Management & Transactions:

The site will have free and paid membership tiers.

I need to store user profiles, subscription statuses, payment history, and access controls alongside the RAG content.

I want an architecture that can scale as my content library and user base grow.



My current thoughts

Documents search engine: Elasticsearch vs. Azure AI Search 

Database for user/transactional data: PostgreSQL, MySQL, or a managed cloud offering.

Any advices? about the optimal combination? is it bad having two DBs? main and secondary? if i want to sync those two will i have issues?",-1,2025-06-07 18:38:40
"Hello, hoping to display the art of the possible with this workflow.

I think it's a cool way to connect data lakes in AWS to gen AI, enabling more business users to ask technical questions without needing technical know-how.



# üó∫Ô∏è Atlas ‚Äì Map Research Agent

Atlas is an intelligent map data agent that translates natural-language prompts into SQL queries using LLMs, runs them against AWS Athena, and stores the results in Google Sheets ‚Äî no manual querying or scraping required.

With access to over 66 million schools, businesses, hospitals, religious organizations, landmarks, mountain peaks, and much more, you will be able to perform a number of analyses with ease. Whether it's for competitive analysis, outbound marketing, route optimization, and more.

This is also cheaper than Google Maps API or webscraping at scale.

The map dataset: [https://overturemaps.org/](https://overturemaps.org/)



# üí° Example Prompts

\* ‚ÄúGet every McDonald's in Ohio‚Äù

\* ‚ÄúGet every dentist office in the United States""

\* ‚ÄúGet the number of golf courses in California‚Äù



# üí° Use-cases

\* Real estate investing analysis - assess the region for businesses near a given location

\* Competitor Analysis - pull all business types, then enrich with menu data / hours of operations / etc.

\* Lead generation - find all dentist offices in the US, starting place for building your outbound strategy

  
You can see a step-by-step walkthrough here - [https://youtu.be/oTBOB4ABkoI?feature=shared](https://youtu.be/oTBOB4ABkoI?feature=shared)",-1,2025-06-07 18:14:34
I‚Äôm working on a tool that can parse this kind of PDF for shopping list ingredients (to add functionality). I‚Äôm using Python with pdfplumber but keep having issues where ingredients are joined together in one record or missing pieces entirely (especially ones that are multi-line). The varying types of numerical and fraction measurements have been an issue too. Any ideas on approach?,-1,2025-06-09 14:18:45
"[https://github.com/getml/reflect-cpp](https://github.com/getml/reflect-cpp)

I am a data engineer, ML engineer and software developer with strong background in functional programming. As such, I am a strong proponent of the ""Parse, Don't Validate"" principle (https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/).  
  
Unfortunately, C++ does not yet support reflection, which is necessary to do something apply these principles. However, after some discussions on the topic over on r/cpp, we figured out a way to do this anyway. This library emerged out of these discussions.

I have personally used this library in real-world projects and it has been very useful. I hope other people in data engineering can benefit from it as well.

And before you ask: Yes, I use C++ for data engineering. It is quite common in finance and energy or other fields where you really care about speed. ",-1,2025-04-08 06:26:36
"There have already been a few blog posts about this topic, but here‚Äôs a video that tries to do the best job of recapping how we first arrived at the table format wars with Iceberg and Delta Lake, how DuckLake‚Äôs architecture differs, and a pragmatic hands-on guide to creating your first DuckLake table.",-1,2025-06-09 12:40:56
"Is it building data pipelines to connect to a DB?
Is it automatically downloading data from a DB and creating reports or is it something else? 
I am a data scientist who would like to polish his Data Engineering skills with Python because my company is beginning to incorporate more and more Python and I think I can be helpful. ",-1,2025-04-08 15:25:54
"We recently started using Cursor, and it has been a hit internally. Engineers are happy, and some are able to take on projects in the programming language that they did not feel comfortable previously.

Of course, we are also seeing a lot of analysts who want to be a DE, building UI on top of internal services that don't need a UI, and creating unnecessary technical debt.  But so far, I feel it has pushed us to build things faster. 

What has been everyone's experience with it?",-1,2025-06-09 23:10:45
"My organization has settled on Databricks to host our data warehouse. I‚Äôm considering implementing SQLMesh for transformations.

1. Is it possible to develop the ETL pipeline without constantly running a Databricks cluster? My workflow is usually develop the SQL, run it, check resulting data and iterate, which on DBX would require me to constantly have the cluster running. 

2. Can SQLMesh transformations be run using Databricks jobs/workflows in batch?

3. Can SQLMesh be used for streaming?

I‚Äôm currently a team of 1 and mainly have experience in data science rather than engineering so any tips are welcome. I‚Äôm looking to have the least amount of maintenance points possible.",-1,2025-06-09 16:54:56
"Title.  I've only tested it. It seems like not a good solution for us (at least currently) for various reasons, but beyond that...

It seems people generally don't feel it's production ready - how specifically?  What issues have you found?",-1,2025-04-08 18:33:12
"We have no data engineers to setup a data warehouse. I was exploring etl tools like hevo and fivetran, but would like recommendations on which option has their own data warehousing provided.

My main objective is to have salesforce and quickbooks data ingested into a cloud warehouse, and i can manipulate the data myself with python/sql. Then push the manipulated data to power bi for visualization",-1,2025-06-09 06:55:36
"As the title suggests. Even if stats are not used on the job, will having stats qualifications give me an edge in the hiring process?",-1,2025-04-04 08:03:59
Can someone plz tell me some resources for this. I need in way that i can learn it and apply it cross platform if need be. Thank you.,-1,2025-06-09 09:10:43
"At my work we have a warehouse with a table for each major component, each of which has a one-to-many relationship with another table that lists its attributes. Is this common practice? It works fine for the business it seems, but it's very different from the star schema modeling I've learned.",-1,2025-06-12 01:17:24
"Let's say there's a data consulting company working within a certain industry (e.g., utilities or energy). How do they gain access to their clients' databases if they want to perform ETL or other services? How about working with their data in a cloud setting (e.g., AWS)? What is the usual process for that? Is the consulting company responsible for setting and managing AWS costs, etc.?",-1,2025-04-04 01:23:44
"Hi Data Champs,

I have been recently given chance to explore PII obfuscation technique in databricks.

I proposed using sql aes_encryption or python fernet for PII column level encryption before landing to bronze.

And use column masking on delta tables which has built in logic for group membership check and decryption so to avoid the overhead of a new view per table.

My HDE was more interested in sql approach than the fernet but fernet offers built in key rotation out of the box.

Has anyone used aes_encryption 
Is it secure, easy to work with and relatively more robust.

From my experience for data type other than binary like long, int, double it needs to be first converted to binary (don‚Äôt like it)

Apart from that usual error here and there for padding and generic error when decrypting sometimes.

So given the choice what will be your architecture 

What you will prefer, what you don‚Äôt and why

I am open to DM if you wanna üí¨ ",-1,2025-04-04 14:28:31
"Let me clarify:

We deal with food article data where the data is being manually managed by users and enriched with additional information for exmaple information about the products content size etc. 

We developed ETL pipelines to do some other business logic on that however there seem to be many cases where the data that gets to us is has some fields for example that are off by a factor of 1000 which is probably due to wrong user input. 

The consequences of that arent that dramatic but in many cases led to strange spikes in some metrics that are dependant of these values. When viewed via some dashboards in tableau for example, the customer questions whether our data is right and why the amount of expenses in this or that month are so high etc. 

How do you deal with cases like that? I mean if there are obvious value differences with a factor of 1000 I could come up with some solutions to just correct that but how do I keep the data clean of other errors?",-1,2025-06-12 10:51:51
"Hi all I think this is the place to ask this. So the background is our roofing company has switched from one CRM to another. They are still paying the old CRM because of all of the historical data that is still stored there. This data includes photos documents message history all associated with different roofing jobs. My hangup is that the old CRM is claiming that they have no way of doing any sort of massive data dump for us. They say in order to export all of that data, you have to do it using the export tool within the UI, which requires going to each individual job and exporting what you need. In other words, for every one of the 5000 jobs I would have to click into each of these Items and individually and download them.

They don‚Äôt have an API I can access, so I‚Äôm trying to figure out a way to go about this programmatically and quickly before we get charged yet another month. 

I appreciate any information in the right direction. ",-1,2025-06-12 14:12:25
"Your must have solved / practiced many SQL problems over the years, what's your most fav of them all?

",-1,2025-06-07 17:44:06
"I‚Äôm currently about the finish an early career rotational program with a top 10 bank. The rotation I am currently on and where the company is placing me post program (I tried to get placed somewhere else) is as a data engineer on a data delivery team. When I was advertised this rotation and the team I was told pretty specifically we would be using all the relevant technologies and I would be very hands on keyboard building pipelines with python , configuring cloud services and snowflake, being a part of data modeling. Mind you I‚Äôm not completely new I have experience with all this in personal projects and previous work experience as a SWE and researcher in college. 

Turns out all of that was a lie. I later learned there is an army of contractors that do the actual work. I was stuck with analyzing .egp files and other SAS files documenting it and handing off to consultants to rebuild in Talend to ingest into snowflake. The only tech that I use is Visio and Word.

I coped with that by saying after I‚Äôm out of the program I‚Äôll get to do the actual work. But I had a conversation with my manager today about what my role will be post program. He basically said there are a lot more of these SAS procedures they are porting over to talend and snowflake and I‚Äôll be documenting them and handing over to contractors so they can implement the new process. Honestly that is all really quick and easy to do because there isn‚Äôt that much complicated business logic for the LOBs we support just joins and the occasional aggregation so most days I‚Äôm not doing anything.

When I told him I would really like to be involved in the technical work or the data modeling , he said that is not my job anymore and that is what we pay the contractors to do so I can‚Äôt do it. Almost made it seem like I should be grateful and he is doing me a favor somehow.

It just feels like I was misled or even outright lied to about the position. We don‚Äôt use any of the technologies that were advertised (Drag and drop/low code tools seem like fake engineering), I don‚Äôt get to be hands on keyboard at all. Just seems like there really I no growth or opportunity in this role. I would leave but I took relocation and a signing bonus for this and if I leave too early I owe it back. I also can‚Äôt internally transfer anywhere for a year after starting my new role.

I guess my rant is just to ask what should I be doing in this situation? I work on personal projects and open source and I have gotten a few certs in the downtime at work but I don‚Äôt know if it‚Äôs enough to make sure my skills don‚Äôt atrophy while I wait out my repayment period. I consider myself a somewhat technical guy but I have been boxed into a non technical role.

",-1,2025-04-04 03:13:53
"I‚Äôm a Thoroughbred trainer with 20+ years of experience, and I‚Äôm working on a project to capture a rare kind of dataset: video footage of horses jogging for the state vet before races, **paired with the official veterinary soundness diagnosis**.

Every horse jogs before racing ‚Äî but that movement and judgment is never recorded or preserved. My plan is to:

* üìπ Record pre-race jogs using consistent camera angles
* ü©∫ Pair each video with the licensed vet‚Äôs official diagnosis
* üìÅ Store everything in a clean, machine-readable format

This would result in one of the **first real-world labeled datasets** of equine gait under live, regulatory conditions ‚Äî not lab setups.

I‚Äôm planning to submit this as a proposal to the HBPA (horsemen‚Äôs association) and eventually get recording approval at the track. I‚Äôm not building AI myself ‚Äî just aiming to structure, collect, and store the data for future use.

üí¨ **Question for the community:**  
Aside from AI lameness detection and veterinary research, where else do you see a market or need for this kind of dataset?  
Education? Insurance? Athletic modeling? Open-source biomechanical libraries?

Appreciate any feedback, market ideas, or contacts you think might find this useful.",-1,2025-06-05 00:25:48
"Hi. I am hoping I am in the right place. I am a cyber security analyst but have been charged with the set up of MS Purview data governance solution. This is because I already had the Purview permissions and knowledge due to the DLP work we were doing.

My question is has anyone been able to register and scan an Oracle ADW in Purview data maps. The Oracle ADW uses a wallet for authentication. Purview only has an option for basic authentication. I am wondering how to make it work. TIA.",-1,2025-06-05 17:58:13
"Hi everyone,

We've built a CRM and are looking to implement a report builder in our app.

We are exploring the best solutions for our needs and it seems like we have two paths we could take:

* Option A: Build the front-end/query builder ourselves and hit read-only replica
* Option B: Build the front-end/query builder ourselves and hit a data warehouse we've built using a key-base replication mechanism on BigQuery/Snowflake, etc..
* Option C: Use third party tools like Explo etc...

About the app:

* Our stack is React, Rails, Postgres.
* Our most used table (contacts) have 20,000,000 rows
* Some of our users have custom fields

We're trying to build something scalable but most importantly not spend months in this project.  
As a result, I'm wondering about the viability of Option A vs. Option B.  
  
One important point is how to manage custom fields that our users created on some objects.

We were thinking about, for contacts for example, we were thinking about simply running with joins across the following tables

* contacts
* contacts\_custom\_fields
* companies (and any other related 1:1 table so we can query fields from related 1:1 objects)
* contacts\_calculated\_fields (materialized view to compute values from 1:many relationship like # of deals the contacts is on)

So the two questions are:

* Would managing all this on the read-only be viable for our volume and a good starting point or will we hit the performance limits soon given our volume?
* Is managing custom fields this way the right way?",-1,2025-04-15 19:28:35
"Has anyone in here had cause to interact with the Geotab API? I've had solid success ingesting most of what it offers, but I'm running into a bear of a time dealing with the Rule and Zone objects. They're reasonably large (126K), but the API limits are 50K and 10K respectively. The obvious responses swing up, using last id or offsets, but somehow neither work and my pagination just stalls after the first iteration. If anyone has dealt with this, please let me know how you worked through it. If not, happy trails and thanks for reading!",-1,2025-06-04 00:52:02
"Hello. I'm slowly learning to code. I need help understanding the best way to structure and develop this project.

I would like to use exclusively python because its the only language I'm confident in. Is that okay?

My goal:

* I want to maintain a cloud-hosted database that updates automatically on a set schedule (hourly or semi hourly). I‚Äôm able to pull the data manually, but I‚Äôm struggling with setting up the automation and notification system.
* I want to run scripts when the database updates that monitor the database for certain conditions and send Telegram notifications when those conditions are met. So I can see it on my phone.
* This project is not data heavy and not resource intensive. It's not a bunch of data and its not complex triggers.

I've been using chatgpt as a resource to learn. Not code for me but I don't have enough knowledge to properly guide it on this and It's been guiding me in circles.

It has recommended me Railway as a cheap way to build this, but I'm having trouble implementing it. Is Railway even the best thing to use for my project or should I start over with something else?

In Railway I have my database setup and I don't have any problem writing the scripts. But I'm having trouble implementing an existing script to run every hour, I don't understand what service I need to create.

Any guidance is appreciated.",-1,2025-06-04 02:23:27
"[https://www.businessinsider.com/ai-hiring-white-collar-recession-jobs-tech-new-data-2025-6](https://www.businessinsider.com/ai-hiring-white-collar-recession-jobs-tech-new-data-2025-6)

Maybe I've been out of the loop to be surprised by AI making inroads on DE jobs.  
  
I can see more DBA / DE jobs being offshored over time.",-1,2025-06-04 05:56:32
"Context: 
I have a dataset of company owned products like: Name: Company A, Address: 5th avenue, Product: A. 
Company A inc, Address: New york, Product B. 
Company A inc. , Address, 5th avenue New York, product C. 

I have 400 million entries like these. As you can see, addresses and names are in inconsistent formats. 
I have another dataset that will be me ground truth for companies. It has a clean name for the company along with it‚Äôs parsed address. 

The objective is to match the records from the table with inconsistent formats to the ground truth, so that each product is linked to a clean company. 



Questions and help: 
- i was thinking to use google geocoding api to parse the addresses and get geocoding. Then use the geocoding to perform distance search between my my addresses and ground truth BUT i don‚Äôt have the geocoding in the ground truth dataset. So, i would like to find another method to match parsed addresses without using geocoding. 

- Ideally, i would like to be able to input my parsed address and the name (maybe along with some other features like industry of activity) and get returned the top matching candidates from the ground truth dataset with a score between 0 and 1. Which approach would you suggest that fits big size datasets? 

- The method should be able to handle cases were one of my addresses could be: company A, address: Washington (meaning an approximate address that is just a city for example, sometimes the country is not even specified). I will receive several parsed addresses from this candidate as Washington is vague. What is the best practice in such cases? As the google api won‚Äôt return a single result, what can i do?

- My addresses are from all around the world, do you know if google api can handle the whole world? Would a language model be better at parsing for some regions? 

Help would be very much appreciated, thank you guys. 
",-1,2025-04-15 10:13:54
"If you're building a data platform from scratch today, do you start with a DWH on RDBMS? Or Data Lake[House] on object storage with something like Iceberg?

I'm assuming the near dominance of Oracle/DB2/SQL Server of > ~10 years ago has shifted? And Postgres has entered the mix as a serious option? But are people building data lakes/lakehouses from the outset, or only once they breach the size of what a DWH can reliably/cost-effectively do?",-1,2025-04-15 17:07:52
"I'm contracting for a small company as a data analyst, I've written python scripts that run inside docker container on an AZ VM daily to get and transform the data for PBI reporting, current setup:

* API 1:
   * Call 8 different endpoints.
   * some are incremental, some are overwritten daily
   * Have 40 different API keys (think of it like a different logic unit), all calling the same things.
   * they're storing the keys in their MySQL table (I think this is bad, but I have no power over this).
* API 2 and 3:
   * four different endpoints.
   * some are incremental, some are overwritten daily
* DuckDB to transform and throw files to blob storage for reporting.

the problem lies with API 1, it takes long since I'm calling one after another.

I could rewrite the scripts to be async, but might as well make it more scalable and clean, things I'm thinking about, all of them have their own learning curve:

* using docker swarm.
* setting up Airbyte on the VM, since the annoying api is there.
* Setting up Airflow on the VM.
* moving it to Azure container App jobs and removing the VM all together.
   * this saves a bit of money, but not a big deal at this scale.
   * this is way more scalable and cleanest.
   * googling around about container apps, I can't figure out if I can orchestrate it using Azure Data Factory.
   * can't figure out how to dynamically create the replicas for the 40 Keys
      * I can either just export template and have one job for each one and add new ones as needed (not often).
      * write orchestration myself.
* write them as AZ Flex functions (in case it goes over 10 minutes), still would need to figure out orchestration.
* Move it to fabric and run them inside notebooks.

Looking for your input, thanks.",-1,2025-06-04 12:18:25
"We currently have 20-25 MSQL databases, 1 Oracle and some random files. The quantity of data is about 100-200GB per year. Data will be used for Python data science tasks, reporting in Power BI and .NET applications.

Currently there's a data-pipeline to Snowflake or RDS AWS. This has been a rough road of Indian developers with near zero experience, horrible communication with IT due to lack of capacity,... Currently there has been an outage for 3 months for one of our systems. This cost solution costs upwards of 100k for the past 1,5 year with numerous days of time waste.

We have a VMWare environment with plenty of capacity left and are looking to do a PoC with an on-premise datawarehouse. Our needs aren't that elaborate. I'm located in operations as data person but out of touch with the latest solutions.

* Cost is irrelevant if it's not >15k a year.
* About 2-3 developers working on seperate topics

",-1,2025-06-04 08:50:06
"The idea is great: build once and use everywhere. But for MS Feature Store, it requires a single flat file as source for any given feature set. 

That means if I need multiple data sources, I need write code to connect to the various data sources, merge them, flatten them into a single file -- all of them done outside of Feature Stores.

For me, it creates inefficiency as the raw flattened file is created solely for the purpose of transformation within feature store. 

Plus when there is a mismatch in granularity or non-overlapping domain, I have to create different flattened files for different feature sets. That seems to be more hassles than whatever merit it may bring.

  
I would love to hear from your success stories before I put in more effort. 

",-1,2025-04-14 16:31:25
"Has anyone in here had cause to interact with the Geotab API? I've had solid success ingesting most of what it offers, but I'm running into a bear of a time dealing with the Rule and Zone objects. They're reasonably large (126K), but the API limits are 50K and 10K respectively. The obvious responses swing up, using last id or offsets, but somehow neither work and my pagination just stalls after the first iteration. If anyone has dealt with this, please let me know how you worked through it. If not, happy trails and thanks for reading!",-1,2025-06-04 00:52:02
"  
**Context:**  
New to data engineering. New to the cloud too. I am in charge of doing trade studies on various storage solutions for my new company. I'm gathering requirements for the system, then pricing out options that meet those requirements. At the end of all my research, I have to present my trade studies so leadership can decide how to spend their cash.  

**Question:**  
I am seeing a lot of companies that do ""managed services"" that are not native to a cloud provider like AWS. For example, I see that ClickHouse offers managed services that piggy back off of AWS or other cloud providers. 

Do they have an AWS account that they provision with their software on ec2 instances (or something), and then they give you access to it? Or do they act as consultants who come in and install ClickHouse on your own AWS account? 







",-1,2025-04-14 20:34:56
"So I've been trying to set up a CI/CD pipeline for MSSQL for a bit now. I've never set one up from scratch before and I don't really have anyone in my company/department knowledgeable enough to lean on. We use GitHub for source controlling, so Github Actions is my CI/CD method  
  
Currently, I've explored the following avenues: 

* Redgate Flyway  
   * It sounds nice for migration, but the concept of having to restructure our repo layout and having to have multiple versions of the same file just with the intended changes (assuming I'm understanding how its supposed to work) seems kind of cumbersome and we're kind of trying to get away from Redgate.
* DACPAC Deployment
   * I like the idea, I like the auto diffing and how it automatically knows to alter or create or drop or whatever but this seems to have a whole partial deployment thing in the event of it failing part way through that's hard to get around for me. Not only that, but it seems to diff what's in the DB compared to source control (which, ideally is what we want) but prod has a history of hotfixes (not a deal breaker) and also, the DB settings are default ANSI NULLS Enabled: False + Quoted Identifiers Enabled: False. Modifying this setting on the DB is apparently not an option which means Devs will have to enable it at the file level in the sqlproj.
* Bash
   * Writing a custom bash script that takes only the changes meant to be applied per PR and deploys them. This however, will require plenty of testing and maintenance and I'm terrified of allowing table renames and alterations because of dataloss. Procs and Views can probably be just dropped and re-created as a means of deployment, but not really a great option for Functions and UDTs because of possible dependencies and certainly not for tables. This also has partial deployment issues that I can't skirt with transaction wrapping the entire deploy...

For reference, I work for a company where NOLOCK is commonplace in queries so locking tables for pretty much any amount of time is a non-negotiable no. I'd want the ability to rollback deployments in the event of failure, but if I'm not able to use transactions, I'm not sure what options I have since I'm inexperienced in this avenue. I'd really like some help. :(",-1,2025-06-04 18:16:42
"Hello. I'm slowly learning to code. I need help understanding the best way to structure and develop this project.

I would like to use exclusively python because its the only language I'm confident in. Is that okay?

My goal:

* I want to maintain a cloud-hosted database that updates automatically on a set schedule (hourly or semi hourly). I‚Äôm able to pull the data manually, but I‚Äôm struggling with setting up the automation and notification system.
* I want to run scripts when the database updates that monitor the database for certain conditions and send Telegram notifications when those conditions are met. So I can see it on my phone.
* This project is not data heavy and not resource intensive. It's not a bunch of data and its not complex triggers.

I've been using chatgpt as a resource to learn. Not code for me but I don't have enough knowledge to properly guide it on this and It's been guiding me in circles.

It has recommended me Railway as a cheap way to build this, but I'm having trouble implementing it. Is Railway even the best thing to use for my project or should I start over with something else?

In Railway I have my database setup and I don't have any problem writing the scripts. But I'm having trouble implementing an existing script to run every hour, I don't understand what service I need to create.

Any guidance is appreciated.",-1,2025-06-04 02:23:27
There is lot of pipelines working in our Azure Data Factory. There is json files available for those. I am new in the team and there not very well details about those pipelines. And my boss wants me to create something which will describe how pipelines working. And looking for how do i Document those so for future anyone new in our team can understand what have done. ,-1,2025-04-14 15:00:15
"Hi Folks,

In my current project, we are ingesting a wide variety of external public datasets. One common issue we‚Äôre facing is that the **country names in these datasets are not standardized**. For example, we may encounter entries like **""Burma"" instead of ""Myanmar""**, or **""Islamic Republic of Iran"" instead of ""Iran""**.

My initial approach was to extract all unique country name variations and map them to a list of standard country names using logic such as CASE WHEN conditions or basic string-matching techniques.

However, my manager has suggested we leverage **AI/LLM-based models** to automate the mapping of these country names to a standardized list to handle new query points as well. 

I have a couple of concerns and would appreciate your thoughts:

1. **Is using AI/LLMs a suitable approach for this problem?**
2. **Can LLMs be fully reliable in these mappings, or is there a risk of incorrect matches?**
3. I was considering implementing a **feedback pipeline** that highlights any newly encountered or unmapped country names during data ingestion so we can review and incorporate logic to handle them in the code over time. Would this be a better or complementary solution?
4. Please suggest if there is some better approach.

Looking forward to your insights!",-1,2025-04-14 08:02:29
"I have been working in the IT space for almost a decade now. Before that, I was part of the ""business"" - or what IT would call the customer. The first time I was on a project to implement a new global system, it was a fight. I was given spreadsheets to fill out. I wasn't told what the columns really meant or represented. It was a mess. And then of course came the issues after the deployment, the root causes and the realization that ""what? You needed to know *that*??""

Somehow, that first project led me to a career where I am the one facilitating requirements gathering. I've been in their shoes. I didn't get it. But after the mistakes, brushing up on my technical skills and understanding how systems work, I've gotten REALLY skilled at asking the right questions to tease out the information. 

But my question is this - is there ANY training out there for the customer? Our biggest bottleneck with each new deployment is that the customer has no clue what to do or even understand the work they own. They need to provide the process. The scenarios. But what I've witnessed is we start the project. The customer sits back and says ""ask away"". How do you teach a customer the engagement needed on their side? The level of detail we will ultimately need? The importance of identifying ALL likely scenarios? How do we train them so they don't have to go through the mistakes or hypercare issues to fully grasp it? 

We waste so much time going in circles. And I even sometimes get attitude and questions like - why do you need to know that? We are always tasked with going faster, and we do not have the time for this churn.",-1,2025-06-04 14:07:11
"I have a offer with 19.3 LPA gross CTC + stocks with amazon, should I go for amazon or other service based companies they are offering 24LPA . I have over all 4.6+ years of experience as a Data Engineer ",-1,2025-06-04 13:40:30
"This is a photo of my notes (not OG rewrote later) about a meet at work about this said project. The project is about migration of ms sql server to snowflake. 

The code conversion will be done using Snowconvert. 

For historic data
1. The data extraction is done using a python script using bcp command and pyodbc library 
2. The converted code from snowconvert will be used in a python script again to create all the database objects. 
3. data extracted will be loaded into internal stage and then to table 

2 and 3 will use snowflake‚Äôs python connector 

For transitional data: 
1. Use ADF to store pipeline output into an Azure blob container 
2. Use external stage to utilise this blob and load data into table 


1. My question is if you have ADF for transitional data then why not use the same thing for historic data as well (I was given the task of historic data)
2. Is there a free way to handle this transitional data as well. It needs to be enterprise level (Also what is wrong with using VS Code extension) 
3. After I showed initial approach following things were asked by mentor/friend to incorporate in this to really sell my approach (He went home after giving me no clarification about how to do this and what even are they)
- validation of data on both sides 
- partition aware extraction 
- parallely extracting data (Idts it is even possible)

I request help on where to even start looking and rate my approach I am a fresh graduate and been on job for a month. üôÇ‚Äç‚ÜïÔ∏èüôÇ‚Äç‚ÜïÔ∏è
",-1,2025-04-13 18:30:34
"Serious question for data engineers working with AWS Glue: How do you actually structure and test production-grade pipelines.

For simple pipelines it's straight forward: just write everything in a single job using glue's editor, run and you're good to go, but for production data pipelines, how is the gap between the local code base that is modularized ( utils, libs, etc ) bridged with glue, that apparently needs everything to be bundled into jobs?

This is the first thing I am struggling to understand, my second dilemma is about testing jobs locally.  
How does local testing happen?

**->** if we will use glue's compute engine we run into the first question of: gap between code base and single jobs.

**->** if we use open source spark locally:

1. data can be too big to be processed locally, even if we are just testing, and this might be the reason we opted for serverless spark on the first place.

  
2. Glue‚Äôs customized Spark runtime behaves differently than open-source Spark, so local tests won‚Äôt fully match production behavior. This makes it hard to validate logic before deploying to Glue",-1,2025-04-13 20:28:12
"Hi all.
I'd love your opinion and experience about the data pipeline I'm working on.

The pipeline is for the RAG inference system.
The user would interact with the system through an API which triggers a Lambda.

The inference consists of  4 main functions-
1. Apply query guardrails
2. Fetch relevant chunks
3. Pass query and chunks to LLM and get response 
4. Apply source attribution (additional metadata related to the data) to the response 

I've assigned 1 AWS Lambda function to each component/function totalling to 4 lambdas in the pipeline.

Can the above mentioned functions be achieved under 30 secs if they're clubbed into 1 Lambda function?

Pls clarify in comments if this information is not sufficient to answer the question.

Also, please share any documentation that suggests which approach is better ( multiple lambdas or 1 lambda)

Thank you in advance!",-1,2025-04-13 03:03:52
"Hello everyone!

So I started a techinical blog recently to document my learning insights. I asked some of my senior colleagues if they had same, but all of them do not have an online accessible portfolio aside from Github to showcase their work.  
  
Still, I believe that github is a bit difficult to navigate for non-tech people (as HR) an dthe only insight they can easily get is how active you are on it, which I personally do not believe is equal to your expertise. For instance when I was still a newbie, I would just Update [README.md](http://google.com) to reflect I was active for the day, daily.

I want to ask how fellow data engineers showcase their expertise visually. I believe that we work on sesitive company data which we cannot share openly, so I wanna know how you were able to navigate on that, too, without legal implications...

My blog is still in development (so I can't share it) and I wanna showcase my certificates there as well. I am planning to showcase my data models also, altering column names, usie publicly available datasets which'll match what I worked in my job, define requirements and use case for the general audience, then elaborate what made me choose this modelling approach over the other, stating references iwhen they come handly. Maybe I'll use PowerBI too for some basic visualization.

Please feel free to share your websites/blogs/github/vercel/portfolio you're okay with it. Thanks a lot!",-1,2025-06-05 01:44:28
"There seems to be little to no documentation(or atleast I can't find any meaningful guides), that can help me establish a successful connection with a MySQL source. 
Either getting this VPC endpoint or NAT gateway error:

InvalidInputException: VPC S3 endpoint validation failed for SubnetId: subnet-XXX. VPC: vpc-XXX. Reason: Could not find S3 endpoint or NAT gateway for subnetId: subnet-XXX in Vpc vpc-XXX

Upon creating said endpoint and NAT gateway connection halts and provides Timeout after 5 or so minutes. My JDBC connection is able to successfully establish with either something like PyMySQL package on local machine, or in Glue notebooks with Spark JDBC connection. Any help would be great. ",-1,2025-04-13 13:09:41
"Most guides on data modeling and data pipelines seem to focus on greenfield projects.

But how do you deal with a legacy data lake where there's been years of data written into tables with no changes to original source-defined schemas?

I have hundreds of table schemas which analysts want to use but can't because they have to manually go through the data catalogue and find every column containing 'x' data or simply not bothering with some tables.

How do you tackle such a legacy mess of data? Say I want to create a Kimball model that models a persons fact table as the grain, and dimensions tables for biographical and employment data. Is my only choice to just manually inspect all the different tables to find which have the kind of column I need? Note here that there wasn't even a basic normalisation of column names enforced (""phone_num"", ""phone"", ""tel"", ""phone_number"" etc) and some of this data is already in OBT form with some containing up to a hundred sparsely populated columns.

Do I apply fuzzy matching to identify source columns? Use an LLM to build massive mapping dictionaries? What are some approaches or methods I should consider when tackling this so I'm not stuck scrolling through infinite print outs? There is a metadata catalogue with some columns having been given tags to identify its content, but these aren't consistent and also have high cardinality.

From the business perspective, they want completeness, so I can't strategically pick which tables to use and ignore the rest. Is there a way I should prioritize based on integrating the largest datasets first?

The tables are a mix of both static imports and a few daily pipelines. I'm primarily working in pyspark and spark SQL ",-1,2025-04-13 14:51:05
"The course I'm taking is 10 years old so some information I'm finding is irrelevant, which prompted the following questions from me:

  
I'm learning about replication factors/rack awareness in HDFS and I'm curious about the current state of the world. How big are replication factors for massive companies today like, let's say, Uber? What about Amazon?

  
Moreover, do these tech giants even use Hadoop anymore or are they using a modernized version of it in 2025? Thank you for any insights.",-1,2025-04-12 19:20:05
"Hi r/dataengineering!

  
I wanted to share a project that I have been working on.¬†It's an intuitive data editor where you can interact with local and remote data (e.g. Athena & BigQuery). For several important tasks, it can speed you up by 10x or more. (see website for more)

  
For data engineering specifically, this would be really useful in debugging pipelines, cleaning local or remote data, and being able to easy create new tables within data warehouses etc.

I know this could be a lot faster than having to type everything out, especially if you're just poking around. I personally find myself using this before trying any manual work.

Also, for those doing complex queries, you can split them up and work with the frame visually and add queries when needed. Super useful for when you want to iteratively build an analysis or new frame¬†***without writing a super long query.***

  
As for data size, it can handle local data up to around 1B rows, and remote data is only limited by your data warehouse.

  
You don't have to migrate *anything* either.

  
If you're interested, you can check it out here: [https://www.cocoalemana.com](https://www.cocoalemana.com)

  
I'd love to hear about your workflow, and see what we can change to make it cover more data engineering use cases.

  
Cheers!

[Coco Alemana](https://preview.redd.it/02wogjj72rse1.jpg?width=3820&format=pjpg&auto=webp&s=0905bd40927b4dd7e80521568982ebe82994a5fe)

",-1,2025-04-04 05:00:10
"I have a few thousand queries that I need to execute and some groups of them have the same conditionals, that is, for a given group the same view could be used internally. My question is, can Catalyst automatically see these common expressions between the work plans? Or do I need to inform it somehow?",-1,2025-06-12 16:47:54
"What‚Äôs currently challenging for me is getting access to things.

I design a data pipeline, present it to the team that will benefit from it, and everyone gets super excited.

Then I reach out to the internal department or an external party to either grant me admin access to the platform I need, or to help me obtain an API.

A week goes by‚Äînothing. I follow up via email. Eventually, someone replies and says it's not possible to give me admin credentials. Fine. So I ask, ‚ÄúCan you help me get the API instead? It‚Äôs very straightforward.‚Äù

Another week goes by‚Äîstill nothing. I send another follow-up‚Ä¶

Now the other person is kind of frustrated (because I‚Äôm asking them to do something slightly different, even though I‚Äôm offering guidance).

What follows is just a back-and-forth with long, frustrating waiting periods in between. Meanwhile, the team I presented the pipeline or project to starts getting frustrated with me and probably thinks I‚Äôm full of crap.

Once I finally get the damn API or whatever access I needed, I complete the project in 1‚Äì2 days but delayed by weeks or even months.

Aaaaaaah!",-1,2025-04-04 02:23:50
"Hi,

With the recent tariffs in mind, are cloud providers like AWS, Azure, and Google Cloud becoming more expensive for European companies? And what about other techs like Snowflake or Databricks ‚Äì are they affected too?

Would it be wise for European businesses to consider open-source alternatives, both for cost and strategic independence?

And from a personal perspective: should we, as employees, expand our skill sets toward open-source tech stacks to stay future-proof?",-1,2025-04-04 07:31:32
"Hello,

I'm writing you because I have a problem with a side project and maybe here somebody can help me. I have to run a complex query with a potentially high number of results and it takes a lot of time. However, for my project I don't need all the results to be showed together, perhaps after some hours/days. It would be much more useful to get a stream of the partial results in real time. How can I achieve this? I would prefer to use free software, however please suggest me any solution you have in mind.

Thank you in advance!",-1,2025-04-04 07:35:07
I having worked in BI and transitioned to DE have followed best practices reading books by authors like Ralph Kimball in BI. Is there someone in DE with a similar level of reputation. I am not looking for specific technologies but rather want to pick up DE fundamentals especially in the performance and optimization space.,-1,2025-04-04 13:08:48
"I‚Äôve been part of (and led) a teams over the last decade ‚Äî in enterprises

And one tool keeps showing up everywhere: **Jira**.

It‚Äôs the ""default"" for a lot of engineering orgs. Everyone knows it. Everyone uses it.  
But **I don‚Äôt seen anyone who actually likes it.**

Not in the *""ugh it's corporate but fine""* way ‚Äî I mean people who are actively frustrated by it but still use it daily.

Here are some of the most common friction points I‚Äôve either experienced or heard from other devs/product folks:

1. **Custom workflows spiral out of control** ‚Äî What starts as ""just a few tweaks"" becomes an unmanageable mess.
2. **Slow performance** ‚Äî Large projects? Boards crawling? Yup.
3. **Search that requires sorcery** ‚Äî Good luck finding an old ticket without a detailed Jira PhD.
4. **New team members struggle to onboard** ‚Äî It‚Äôs not exactly intuitive.
5. **The ‚Äútool tax‚Äù** ‚Äî Teams spend hours updating Jira instead of moving work forward.

And yet... most teams stick with it. Because switching is painful. Because ‚Äúat least everyone knows Jira.‚Äù Because the alternative is more uncertainty.  
What's your take on this?",-1,2025-04-08 07:40:50
"I just got to know that even in today's OLAP era, but while communicating b/w the systems internally they convert it to row based storage even if the warehouses are columnar type...
This made me sickkk I never knew this at all!

So does this mean serialisation and de-serialisation?? 
I see these terms vary across many architecture ex: In spark they mention these terminologies when the data needs to searched at different instances.. they say data needs to be de-serialised which takes time...

But I am not clear how do I need to think when I hear these terminologies!!!

Source:
https://www.linkedin.com/posts/dipankar-mazumdar_dataengineering-softwareengineering-activity-7307566420828065793-LuVZ?utm_source=share&utm_medium=member_android&rcm=ACoAADeacu0BUNpPkSGeT5J-UjR35-nvjHNjhTM",-1,2025-03-30 18:29:00
"My goal is to re create something like Oracle's Net-suite, are there any help full resources on how i can go about it. i have previously worked on simple Finance management systems but this one is more complicated. i need sample ERD's books or anything helpfull atp

",-1,2025-04-08 20:09:25
"You're better able to understand the needs and goals of what you're actually working towards when you being as an analyst. Not to mention the other skills that you develop whist being an analyst. Understanding downstream requirements helps build DE pipelines carefully keeping in mind the end goals.

What are you thoughts on this?",-1,2025-04-08 18:11:04
"Hi everyone,

I am building a data warehouse for my company and because we have to process mostly spatial data I went with a postgres materialization. My stack is currently:

- dlt
- dbt
- dagster
- postgres

Now I have the use case that our developers at our company need some of the data for our software solutions to be integrated. And I would like to provide an API for easy access to the data. 

So I am wondering which solution is best for me. I have some experience in a private project with postgREST and found it pretty cool to directly use DB views and functions as endpoints for the API. But tools like FastAPI might be more mature for a production system. What would you recommend?



[View Poll](https://www.reddit.com/poll/1jxdch4)",-1,2025-04-12 09:20:05
"I‚Äôm trying to get the Debezium SQL Server connector working with a SQL Server 2016 instance, but not having much luck. The official docs mention compatibility with 2017, 2019, and 2022‚Äîbut nothing about 2016.

Is 2016 just not supported, or has anyone managed to get it working regardless?
Would love to hear if there are known limitations, workarounds, or specific gotchas for this version.",-1,2025-04-12 14:47:33
"Hey everyone! My friend and I built [Crystal](https://askcrystal.info/search), a tool to help you search through 300,000+ datasets from [data.gov](http://data.gov) using plain English.

Example queries:

* *""Air quality in NYC after 2015""*
* *""Unemployment trends in Texas""*
* *""Obesity rates in Alabama""*

It finds and ranks the most relevant datasets, with clean summaries and download links.

We made it because searching [data.gov](http://data.gov) can be frustrating ‚Äî we wanted something that feels more like asking a smart assistant than guessing keywords.

It‚Äôs in early alpha, but very usable. We‚Äôd love feedback on how useful it is for everyone's data analysis, and what features might make your work easier.

Try it out: [askcrystal.info/search](https://askcrystal.info/search)",-1,2025-04-13 17:55:33
"I often end up doing the same where clause in most of my downstream models. Like ‚Äòwhere is_active‚Äô or for a specific type like ‚Äòwhere country = xyz‚Äô.

I‚Äôm wondering when it‚Äôs a good idea to create a new model/table/views for this and when it‚Äôs not?

I found that having it makes it way simpler at first because downstream models only have to select from the filtered table to have what they need without issues. But as time flys you end up with 50 subset tables of the same thing which is not that good.

And if you don‚Äôt then you see that the same filters are reused over and over again but also that this generates issues if for example downstream models should look for 2 field for validity like ‚Äòwhere country = xyz AND is_active‚Äô.

So do you usually filter by types or not ? Or do you filter by active and non active records? Note that I could remove the non active records, but they are often needed in some downstream table since they were old customer that we might still want to see in our data.",-1,2025-04-26 09:33:19
"Hi everyone,  
I‚Äôm a 2025 new grad starting this May, and I‚Äôll be working at a small start-up as an Analytics Engineer. I‚Äôve gotten pretty solid at SQL as I‚Äôve been grinding Leetcode questions for fun, and it really helped me land the job. During my internships, I also worked a lot with dbt, Snowflake, and Airflow, so I‚Äôm fairly comfortable with the tooling side of things. 

Where I‚Äôm struggling is data modeling‚Äîspecifically the Kimball methodology, Star Schemas, and different types of dimensions and fact tables. I tried reading the Kimball book, but honestly, it felt super abstract without any hands-on practice. I get that real data modeling often involves trade-offs, business context, and actual stakeholder input, which isn‚Äôt easy to simulate on your own.

So my question is‚Äîhow can a college student or new grad start building intuition and skills in data modeling? Are there any practical resources or projects I can work through to better understand this area? And if you have any general advice for someone entering the industry in this kind of role, I‚Äôd love to hear it.

Thanks a lot!",-1,2025-04-13 21:12:25
"I just want to know why isnt databricks going public ?   
They had so many chances so good market conditions what the hell is stopping them ? ",-1,2025-04-30 05:55:33
"I've worked as both a data and ML engineer and feature stores tend to be an interesting subject. I think they're often misunderstood and quite frankly, not needed for many companies. I wanted to write the blog post to solidify my thoughts and thought it might be helpful for others here.",-1,2025-04-14 18:14:31
"If you're urgently looking for a Fivetran alternative, this might help

Been seeing a lot of people here caught off guard by the new Fivetran pricing. If you're in eCommerce and relying on platforms like Shopify, Amazon, TikTok, or Walmart, the shift to MAR-based billing makes things really hard to predict and for a lot of teams, hard to justify.

If you‚Äôre in that boat and actively looking for alternatives, this might be helpful.

**Daton**, built by Saras Analytics, is an ETL tool specifically created for eCommerce. That focus has made a big difference for a lot of teams we‚Äôve worked with recently who needed something that aligns better with how eComm brands operate and grow.

Here are a few reasons teams are choosing it when moving off Fivetran:

**Flat, predictable pricing**  
There‚Äôs no MAR billing. You‚Äôre not getting charged more just because your campaigns performed well or your syncs ran more often. Pricing is clear and stable, which helps a lot for brands trying to manage budgets while scaling.

**Retail-first coverage**  
Daton supports all the platforms most eComm teams rely on. Amazon, Walmart, Shopify, TikTok, Klaviyo and more are covered with production-grade connectors and logic that understands how retail data actually works.

**Built-in reporting**  
Along with pipelines, Daton includes Pulse, a reporting layer with dashboards and pre-modeled metrics like CAC, LTV, ROAS, and SKU performance. This means you can skip the BI setup phase and get straight to insights.

**Custom connectors without custom pricing**  
If you use a platform that‚Äôs not already integrated, the team will build it for you. No surprise fees. They also take care of API updates so your pipelines keep running without extra effort.

**Support that‚Äôs actually helpful**  
You‚Äôre not stuck waiting in a ticket queue. Teams get hands-on onboarding and responsive support, which is a big deal when you‚Äôre trying to migrate pipelines quickly and with minimal friction.

Most eComm brands start with a stack of tools. Shopify for the storefront, a few ad platforms, email, CRM, and so on. Over time, that stack evolves. You might switch CRMs, change ad platforms, or add new tools. But Shopify stays. It grows with you. Daton is designed with the same mindset. You shouldn't have to rethink your data infrastructure every time your business changes. It‚Äôs built to scale with your brand.

If you're currently evaluating options or trying to avoid a painful renewal, Daton might be worth looking into. I work with the Saras team and happy to help , here's the link if you want to checkout [https://www.sarasanalytics.com/saras-daton](https://www.sarasanalytics.com/saras-daton)

Hope this helps !",-1,2025-04-14 10:07:58
"I‚Äôve been diving deep into how companies use Business Intelligence Analytics to not just track KPIs but actually transform how they operate day to day. It‚Äôs crazy how powerful real-time dashboards and predictive models have become. imagine optimizing customer experiences before they even ask for it or spotting a supply chain delay before it even happens. Curious to hear how others are using BI analytics in your field Have tools like tableau, Power BI, or even simple CRM dashboards helped your team make better decisions or is it all still gut feeling and spreadsheets?  P.S. I found an article that simplified this topic pretty well. If anyones curious I‚Äôll drop the link below. Not a promotion just thought it broke things down nicely https://instalogic.in/blog/the-role-of-business-intelligence-analytics-what-is-it-and-why-does-it-matter/",-1,2025-04-15 02:01:57
"Looking for a general estimate on how much companies spend on tools like Airbyte, Fivetran, Stitch, etc, per month?



[View Poll](https://www.reddit.com/poll/1jznyyv)",-1,2025-04-15 09:55:46
"What‚Äôs your take on the Data Governance role when it comes to job security and future opportunities, especially with how fast technology is changing, tasks getting automated, new roles popping up, and some jobs becoming obsolete?",-1,2025-04-16 00:26:34
"I know Purview is a data governance tool but does it has any MDM functionality.  From the article it seems it has integration with third party MDM solution partners such as CluedIn, profisee but I am not very clear whether or not it can do MDM by itself. 

One of my client's budget is very slim and they wanted to implement MDM. Do you think Microsoft Data Services (MDS) is an option but it looks very old to me and it seems to require a dedicated SQL server license. ",-1,2025-04-15 23:10:56
"Hi , i have working as a informatica production support where i need to monitor ETL jobs on daily basis and report the bottlenecks to the developer to fix the issue and im getting $9.5k/year with 5 YOE. rightnow its kind of boring and planning to move to informatica powercenter admin position since its not opensource its hard for me to self learn myself. just want to know any opensource tools related to data integration that has high in demand for administrator role would be great. ",-1,2025-04-15 14:20:01
Hi! I have a final round technical screen next week for a Data Engineering Manager role. I have a strong data analytics/data science leadership background and have dipped my toes into DE from time to time over more than a decade long career. I'm looking for good prep tools for this (hands on) Manager level role.,-1,2025-04-24 19:32:50
"Hello,

For those expert in the field or has been in the field for 5 years and more, what you would say are top issues you face when it comes to data quality and observability in snowflake?

",-1,2025-04-21 03:56:30
"Hi all! I've been getting a lot of great feedback and usage from data service teams for my tool [mightymerge.io](http://mightymerge.io) (you may have come across it before).

Sharing here with you who might find it useful or know of others who might.

The basics of the tool are...

Quickly merging and splitting of very large csv type files from the web. Great at managing files with unorganized headers and of varying file types. Can merge and split all in one process. Creates header templates with transforming columns. 

Let me know what you think or have any cool ideas. Thanks all!",-1,2025-04-16 19:00:40
"After some struggle with a pipeline today, Gemini 2.5 one-shotted the solution. It's superior in most software problems compared to humans (check coders eval) and we're just two and a half years in.

The capabilities are mind-bending. Data engineering as we know it will change drastically with new AI tooling and self-adjusting infrastructure.

We know this profession will evolve drastically. What do you think where things are heading and how to hedge against AI? Become more social / human I guess üòÇ

A few hypotheses:
- pipelines and infra manages itself with much higher accuracy and less misconfigurations
- the data engineer profile will shift, they become subject matter experts, they must understand the business and do product management
- technical skills do not matter since the gap from idiot to genius is much smaller than from genius to agi/asi",-1,2025-04-02 20:34:45
"Hello guys, me and teammates want to do a project from a-z to practice what we learned in an internship we are in and we wanted to the project to be about a telecom company‚Äôs data and we have searched a lot for a dataset that mimics the datasets of real telecom companies but we never found what we are looking for so we thought about creating the data we want using AI but for some reason it‚Äôs also not working out for us so i would love to hear some suggestions about what we should do and about telecom data warehouses and databases because i feel maybe we just don‚Äôt still quite understand how telecom companies generally operate and perhaps that‚Äôs why we are not successful in generating the data.

I hope this post makes sense because i‚Äôm just very confused and don‚Äôt know what to do for this project. 

Thank you for anyone who will respond in advance!",-1,2025-04-02 11:13:35
"I tried to search the entire internet to find AbInito related tutorials/tranings. Hard luck finding anything. I came to know it's a closed source tool and everything is behind a login wall only for partner companies. 

Can anyone share me stuff they found useful?

Thanks in advance.",-1,2025-04-02 02:17:07
"I have a question regarding the preprocessing step in a project I'm working on. I have two different measurement devices that both collect time-series data. My goal is to analyze the similarity between these two signals.

Although both devices measure the same phenomenon and I've converted the units to be consistent, I'm unsure whether this is sufficient for meaningful comparison, given that the devices themselves are different and may have distinct ranges or variances.

From the literature, I‚Äôve found that z-score normalization is commonly used to address such issues. However, I‚Äôm concerned that applying z-score normalization to each dataset individually might make it impossible to compare across datasets, especially when I want to analyze multiple sessions or subjects later.

Is z-score normalization the right approach in this case? Or would it be better to normalize using a common reference (e.g., using statistics from a larger dataset)? Any guidance or references would be greatly appreciated.Thank you :)",-1,2025-03-31 02:33:18
"I came to know that most of the skillset are matching 
in this 2 fields, apart from learning SQL, pyspark.

so would this be a better switching career ?",-1,2025-03-30 07:42:11
I am trying to load/copy data from a local mysql database in my mac into azure using Data factory. Most of the material i found online suggest to created an integration runtime which requires an installation of an app aimed at windows Os. Is there a way where i could load/copy data from my mysql on mac into azure ?,-1,2025-04-02 07:13:52
"I came to know that most of the skillset are matching 
in this 2 fields, apart from learning SQL, pyspark.

so would this be a better switching career ?",-1,2025-03-30 07:42:11
Hi I‚Äôve been working through the data engineer in SQL track on DataCamp and decided to try the associate certification exam. There was quite a bit that didn‚Äôt seem to have been covered in the courses. Can anyone recommend any other resources to help me plug the gap please? Thanks,-1,2025-03-31 00:51:17
"There is a lot of hype around multimodal models, such as Qwen 2.5 VL or Omni, GOT, SmolDocling, etc. I would like to know if others made a similar experience in practice: While they can do impressive things, they still struggle with table extraction, in cases which are straight-forward for humans.

Attached is a simple example, all I need is a reconstruction of the table as a flat CSV, preserving empty all empty cells correctly. Which open source model is able to do that?

https://preview.redd.it/xg8f0624jvre1.png?width=1650&format=png&auto=webp&s=4c0a22d833cb308534abf4dc38b1b12581a6e227

",-1,2025-03-30 18:55:50
"Right now I work as a data scientist, but I find it very, very repetitive.

That's why I'm studying Data Engineering concepts.  Right now, I'm able to create pipelines to automate ETL loads into Amazon Redshift databases (sort of) using Airflow with Dicker and Kubernetes.

I'm specialized in Python, so I'm also looking at Kafka and Apache PySpark.

Anyway, I'm just starting out in this field, so I feel overwhelmed and not sure what a company expects of me.

Help me understand your role better, thank you!",-1,2025-04-02 22:20:21
"Hello guys. I work in a consultancy company and we recently got a job to set-up SQL Server as DWH and SSIS. Whole system is going to be build up from the scratch. The whole operation of the company was running on Excel spreadsheets with 20+ Excel Slave that copies and pastes some data from a source, CSV or email then presses the fancy refresh button. Company newly bought and they want to get rid of this stupid shit so SQL Server and SSIS combo is a huge improvement for them (lol).

But I want to integrate as much as fancy stuff in this project. Both of these tool will work on a Remote Desktop with no internet connection.  I want to integrate some DevOps tools into this project. I will be one of the 3 data engineers that is going to work on this project. So Git will be definitely on my list, as well as GitTea or a repo that works offline since there won't be a lot of people. But do you have any more free tools that I can use? Planning to integrate Jenkins in offline mode somehow, tsqlt for unit testing seems like a decent choice as well. dbt-core and airflow was on my list as well but my colleagues don't know any python so they are not gonna be on this list.

Do you have any other suggestions? Have you ever used a set-up like mine? I would love to hear your previous experiences as well. Thanks",-1,2025-03-31 17:29:30
"Hey guys,

I'm working as a DE in a German IT company that has about 500 employees. The company's policy regarding operating systems the employees are allowed to use is strange and unfair (IMO). All software engineers get access to Macbooks and thus, to MacOS while all other employees that have a differnt job title ""only"" get HP elite books (that are not elite at all) that run on Windows. WSL is allowed but a native Linux is not accepted because of security reasons (I don't know which security reasons).

As far as I know the company does not want other job positions to get Macbooks because the whole updating stuff for those Macbooks  is done by an external company which is quite expensive. The Windows laptops instead are maintained by an internal team.

A lot of people are very unhappy with this situation because many of them (including me) would prefer to use Linux or MacOS. Especially all DevOps are pissed because half a year ago they also got access to MacBooks but a change in the policy means that they will have to change back to Windows laptops once their MacBooks break or become too old.

My question(s): Can you choose the OS and/or hardware in your company? Do you have a clue why Linux may not be accepted? Is it really not that safe (which I cannot think of because the company has it's own data center where a lot of Linux servers run that are actually updated by an internal team)?",-1,2025-03-31 17:48:33
"TL;DR: a guy feeling stuck in the job and cannot figure out what skills are needed to move to a better position 

I am data engineer at a big 4 firm (may be just a etl developer) in india.

I work with Informatica Power Center, Oracle, Unix on the daily basis. Now, when I tried to switch companies for career boost, I realised nobody uses these tech anymore. 

Everyone uses pyspark for etl.
I though fair enough and started leaning pyspark dataframe api. I am so good with sql, pl/sql and python, so it was easy for me.

Then I came to know learning pyspark is not enough, you need to know databricks, snowflake, dbt kind of tools.

Even before making my mind to decide what to learn, things changed and now airflow/dagster, redshift, delta lake, duckdb. I don't what else is in trend now.

Honestly, It feels a lot, like the world is moving in the fastest pace possible and I cannot even decide what to do.

Every job has different tools, and to do the ""fake it till you make it"", I am afraid they would ask any niche question about the tool to which you can only answer if you have the experience.

My profile  is not even getting picked and I feel stuck in the job I am doing.

I am great at what I do, that is one reason the project is not letting me leave even after all the senior folks has left for better projects. The guy with 3 years of experience is the senior most developer and lead now.

But honestly, I dont think I can make it anymore.

If I was just stuck with something like SAP ABAP, frontend or core python, things might have been good. Recruiters will at least look at your profile even though you are not a perfect match as you can learn the rest to do the job. (I might be wrong in this thought)

But for DE roles, the job descriptions are becoming too specific to a tool and people are expecting complete data architect level of skills at 3 years.

I was so ambitious to get a job in a different country with big 4 experience, but now I can't even get a job in india.",-1,2025-04-02 09:26:11
"Hey everyone,

I‚Äôm a Data Engineer with 5 years of experience, mostly working with traditional data pipelines, cloud data warehouses(AWS and Azure) and tools like Airflow, Kafka, and Spark. However, I‚Äôve never used Databricks in a professional setting.

Lately, I see Databricks appearing more and more in job postings, and it seems like it's becoming a key player in the data world. For those of you working with Databricks, do you think it's a necessity for Data Engineers now? I see that it is mandatory requirement in job offerings but I don't have opportunity to get first experience in it.

What is your opinion, what should I do?",-1,2025-04-02 09:41:41
"How do you test a data pipeline which parses data having a lot of variation

I'm working on a project to parse pdfs (earnings calls), they have a common general structure, but sometimes I'll get variations in the data (very common, half of docs have some kind of variation). It's a pain to debug when things go wrong, I have to run tests on a lot of files which takes up time.

I want to build good tests, and learn to do this better in the future, then refactor the code (it's garbage right now)",-1,2025-04-01 09:44:04
"I just went through 4 rounds of technical interviews which were far more complex, and bombed the final round. They were the most simple SQL questions, which I tried to solve by utilizing the most complex solution. Maybe I got nervous, maybe it was a brain fart moment. 
And these are the kinds of queries I write every day in my job. 

My questions is how do I solve this problem of overestimating the problem I‚Äôve been given? 
Has anyone else faced this issue?
I am at my wits end cause I really needed this job.
",-1,2025-06-03 17:12:50
"So I was hired as a data analyst a few months ago and I have a background in software development. A few months ago I was moved to a smallish project with the objective of streamlining some administrative tasks that were all calculated ""manually"" with Excel.  By the time, all I had worked with were very basic, low code tools from the Microsoft enviroment: PBI for dashboards, Power Automate, Power Apps for data entry, Sharepoint lists, etc, so that's what I used to set it up. 

The cost for the client is basically nonexistent right now, apart from a couple of PBI licenses. The closest I've done to ETL work has been with power query, if you can even call it that. 

  
Now I'm at a point where it feels like that's not gonna cut it anymore. I'm going to be working with larger volumes of data, with more complex relationships between tables and transformations that need to be done earlier in the process. I could technically keep going with what I have but I want to actually build something durable and move towards actual data engineering, but I don't know where to start with a solution that's cost efficient and well structured. For example, I wanted to move the data from Sharepoint lists to a proper database but then we'd have to pay for multiple premium licenses to be able to connect to them in powerapps. Where do I even start?

I know the very basics of data engineering and I've done a couple of tutorial projects with Snowflake and Databricks as my team seems to want to focus on cloud based solutions. So I'm not starting from absolute scratch, but I feel pretty lost as I'm sure you can tell. I'd appreciate any kind of advice or input as to where to head from here, as I'm on my own right now.",-1,2025-04-01 11:00:15
"I'm an engineer in an unrelated field and want to understand how data migrations work for work (I might be put in charge of it at my job even though we're not data engineers).  Any good sources, preferably a video that would a mock walkthrough of one (maybe using an ETL too)?",-1,2025-04-01 17:07:19
"most of our data is on prem sql server. we also have some data sources in snowflake as well (10-15% of the data). we also connect to some api's as well using the python tool. our reporting db is sql server on prem. currently we are using alteryx, and we are researching what our options are before we have to renew our contract. any suggestions that we can explore or if someone has been through a similar scenario, what did you end up with and why? please let me know if I can add more information to the context.

also,I forgot to mention that not all of my team members are familiar with python. Looking for GUI options.

Edit: thank you all. I‚Äôll look into the mentioned options.",-1,2025-04-01 15:08:23
"I tried to search the entire internet to find AbInito related tutorials/tranings. Hard luck finding anything. I came to know it's a closed source tool and everything is behind a login wall only for partner companies. 

Can anyone share me stuff they found useful?

Thanks in advance.",-1,2025-04-02 02:17:07
"I started 6 years ago and my career has been on a growing trajectory since.

While this is very nice for me, I can‚Äôt say the same about the projects I encounter. What I mean is that I was expecting the engineering soundness of the projects I encounter to grow alongside my seniority in this field.

Instead, I‚Äôve found that regardless of where I end up (the last two companies were data consulting shops), the projects I am assigned to tend to have questionable engineering decisions (often involving an unnecessary use of Spark to move 7 rows of data).

The latest one involves ETL out of MSSQL and into object storage, using a combination of Azure synapse spark notebooks, drag and drop GUI pipelines, absolutely no tests or CICD whatsoever, and debatable modeling once data lands in the lake.

This whole thing scares me quite a lot due to the lack of guardrails, while testing and deployments are done manually. While I'd love to rewrite everything from scratch, my eng lead said since that part it's complete and there isn't a plan to change it in the future, that it's not a priority at all, and I agree with this.

What's your experience in situations like this? How do you juggle the competing priorities (client wanting new things vs. optimizing old stuff etc...)?
",-1,2025-04-01 10:51:01
"My coworker Javi Santana wrote a lengthy post about what it takes to operate large ClickHouse clusters based on his experience starting Tinybird. If you're managing any kind of OSS CH cluster, you might find this interesting.

[https://www.tinybird.co/blog-posts/what-i-learned-operating-clickhouse](https://www.tinybird.co/blog-posts/what-i-learned-operating-clickhouse)",-1,2025-04-01 14:02:37
"This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.

Examples:

* What are you working on this month?
* What was something you accomplished?
* What was something you learned recently?
* What is something frustrating you currently?

As always, sub rules apply. Please be respectful and stay curious.

**Community Links:**

* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)
* [Data Engineering Events](https://dataengineering.wiki/Community/Events)
* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)
* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)",-1,2025-04-01 16:00:57
"About 6 months ago, I led a Databricks cost optimization project where we cut down costs, improved workload speed, and made life easier for engineers. I finally had time to write it all up a few days ago‚Äîcluster family selection, autoscaling, serverless, EBS tweaks, and more. I also included a real example with numbers. If you‚Äôre using Databricks, this might help: [https://medium.com/datadarvish/databricks-cost-optimization-practical-tips-for-performance-and-savings-7665be665f52](https://medium.com/datadarvish/databricks-cost-optimization-practical-tips-for-performance-and-savings-7665be665f52)",-1,2025-04-01 16:45:48
"Hey r/dataengineering,

I've been in data engineering for about **3 years now**, and while I love what I do, I can't help but wonder: **what‚Äôs next?** With tech evolving so fast, I'm a bit concerned about what could make our current skills obsolete.

That said, Spark didn‚Äôt exactly kill the demand for Hadoop, Impala, etc.‚Äîso maybe the fear is overblown. But still, I want to make sure I'm **learning the right things** to stay ahead and not be caught off guard by layoffs or major shifts in the industry.

My current stack: **Python, SQL, Spark, AWS (Glue, Redshift, EMR), Airflow.**

What skills/tech would you bet on for the next **5-10 years**? Is it **real-time data processing? DataOps? AI/ML integration?** Would love to hear from those who‚Äôve been in the game longer!",-1,2025-04-02 20:07:35
"I am currently learning and applying data engineering into my job. I am a data analyst with three years of experience. I am trying to learn ETL to construct automated data pipelines for my reports.

Using Python programming language, I am trying to extract data from Excel file and API data sources. I am then trying to manipulate that data. In essence, I am basically trying to use a more efficient and powerful form of Microsoft's Power Query.

What are the most common Python libraries, functions, methods, etc. that data engineers frequently use during the extraction and transformation steps of their ETL work?

P.S.

Please let me know if you recommend any books or YouTube channels so that I can further improve my skillset within the ETL portion of data engineering.

Thank you all for your help. I sincerely appreciate all your expertise. I am new to data engineering, so apologies if some of my terminology is wrong.



Edit:

Thank you all for the detailed responses. I highly appreciate all of this information.",-1,2025-04-01 14:12:13
"Hey everyone, I could really use some advice from fellow engineers. I'm pretty new to the data world ‚Äî I messed up uni, then did an online analytics course, and after about a year and a half of grinding, I finally landed my first role. Along the way, I found a real passion for Python and SQL.

My first job involved a ton of patchy reporting because of messy infra and data. I started automating painful tasks using basic ETL pipelines I built myself. I showed an interest in APIs and, out of nowhere, 6 months in, I was offered a data engineering role.

Fast forward to now ‚Äî I‚Äôve been in the new role for a month, and I‚Äôm the company‚Äôs only data engineer. I‚Äôm doing a data engineering apprenticeship at the same time, which helps, but the imposter syndrome is real. The company‚Äôs been limping along with a 25-year-old piece of software that populates our SQL Server DB, and we‚Äôre now migrating to something new. I‚Äôve been asked to learn MuleSoft for ETL and replace some existing pipelines that were built in Python.

I love the subject ‚Äî I‚Äôm genuinely passionate about programming and networking ‚Äî and I‚Äôm keen to take on new tech, improve the infra, and build up strong skills. But I‚Äôm not sure if I‚Äôm going too deep too fast. For example, today I was learning Docker to deploy Python scripts, just to avoid issues with hundreds of brittle batch files that break if we update Python.

My boss seems to think MuleSoft will fully replace Python, but I see it more as a tool that complements certain workflows rather than a full replacement. What worries me more is that I don‚Äôt really have any technical peers. Most people in my team only know basic SQL, and it‚Äôs hard to communicate strategy or get proper feedback.

My current priorities are getting comfortable with MuleSoft, Git, and Docker. I‚Äôm constantly learning, but sometimes I leave work feeling overwhelmed. There‚Äôs so much broken or duct-taped together, I don‚Äôt even know where to start. I keep telling myself I don‚Äôt need to ‚Äúsave the world,‚Äù but I really want to do a good job and come away with solid experience.

Long term, they want to deploy this new software, rebuild the database, and eventually use AI to help employees query the business. There‚Äôs a shit ton to do, and I‚Äôm still figuring out basics ‚Äî like setting up a VM just so I can run Docker.

Am I jumping the gun with how I‚Äôm feeling, or is this as wild a situation as it seems? Any advice for a new engineer navigating bad infra, limited support, and a mountain of work would be seriously appreciated.",-1,2025-04-02 19:27:27
"Recruiter reached out about a role on a data governance team but the job itself is data engineering. Recruiter was sharing what was in the job post but it didn't clarify much

I'm not formally experienced with data governance but have implemented data quality tests, written documentation, etc. Is that all considered data governance? What would be data engineering responsibilities and day to day work be like on a governance team? 

Would be interested to hear especially if anyone worked in and implemented data governance from scratch, and not used 3rd party software, as this team seems to be trying to do that.",-1,2025-04-02 14:25:44
"I recently started learning dbt and was using Snowflake as my database. However, my 30-day trial has ended. Are there any free cloud databases I can use to continue learning dbt and later work on projects that I can showcase on GitHub?

Which cloud database would you recommend? Most options seem quite expensive for a learning setup.

Additionally, do you have any recommendations for dbt projects that would be valuable for hands-on practice and portfolio building?

Looking forward to your suggestions!",-1,2025-04-01 16:02:14
"As a Data Professional, do you have the skill to right the perfect regex without gpt / google? How often do interviewers test this in a DE.",-1,2025-06-03 20:03:12
Just wanted ro understand if after doing an union I want to write to S3 as parquet.  Why do I see 76 task ? Is it because union actually partitioned the data ? I tried doing salting after union still I see 76 tasks for a given stage. Perhaps I see it is read parquet I am guessing something to do with committed whixh creates a temporary folder before writing to s3. Any help is appreciated. Please note I don't have access to the spark UI to debug the DAG. I have manged to give print statements and that I where I am trying to  corelate. ,-1,2025-04-15 18:03:08
"Hello, I am working on a personal ETL project with a beginning goal of trying to ingest data from Google Books API and batch insert into pg.  


Currently I have a script that cleans the API result into a list which is then inserted into pg. But, I have many repeat values each time I run this query, resulting in no data being inserted into pg.

I also notice that I get very random books that are not at all on topic for what I specific with my query parameters. e.g. title='data' and author=' '. 

  
I am wondering if anybody knows how to get only relevant data with API calls, as well as non duplicate value with each run of the script (eg persistent pagination).

  
Example of a \~320 book query.

In the first result I get somewhat data-related books. However, in the second result i get results such as: ""Homoeopathic Journal of Obstetrics, Gynaecology and Paedology"".

I understand that this is a broad query, but when I specify I end up getting very few book results(\~40-80), which is surprising because I figured a Google API would have more data. 

I may be doing this wrong, but any advice is very much appreciated.

    ‚ùØ python3 apiClean.py
    The selfLink we get data from: https://www.googleapis.com/books/v1/volumes?q=data+inauthor:&startIndex=0&maxResults=40&printType=books&fields=items(selfLink)&key=AIzaSyDirSZjmIfQTvYgCnUZ0BhbIlrKRF8qxHw
    
    ...
    
    The selfLink we get data from: https://www.googleapis.com/books/v1/volumes?q=data+inauthor:&startIndex=240&maxResults=40&printType=books&fields=items(selfLink)&key=AIzaSyDirSZjmIfQTvYgCnUZ0BhbIlrKRF8qxHw
    
    size of result rv:320",-1,2025-04-15 14:30:37
"Hey All,

I've had an incredible year and I feel extremely lucky to be in the position I'm in. I'm a relatively new DE, but I've covered so much ground even in one year.

I'm not perfect, but I can feel my growth. Every day I am learning something new and I'm having such joy improving on my craft, my passion, and just loving my experience each day building pipelines, debugging errors, and improving upon existing infrastructure.

As I look back I wanted to share some gems or bits of valuable knowledge I've picked up along the way:

* Showing up in person to the office matters. Your communication, attitude, humbleness, kindness, and selflessness goes a long way and gets noticed. Your relationship with your client matters a lot and being able to be in person means you are the go-to engineer when people need help, education, and fixing things when they break. Working from home is great, but there are more opportunities when you show up for your client in person.
* pre-commit hooks are valuable in creating quality commits. Automatically check yourself even before creating a PR. Use hooks to format your code, scan for errors with linters, etc.
* Build pipelines with failure in mind. Always factor in exception handling, error logging, and other tools to gracefully handle when things go wrong.
* DRY - such as a basic principle but easy to forget. Any time you are repeating yourself or writing code that is duplicated, it's time to turn that into a function. And if you need to keep track of state, use OOP.
* Learn as much as you can about CI/CD. The bugs/issues in CI/CD are a different beast, but peeling back the layers it's not so bad. Practice your understanding of how it all works, it's crucial in DE.
* OOP is a valuable tool. But you need to know when to use it, it's not a hammer you use at every problem. I've seen examples of unnecessary OOP where a FP paradigm was better suited. Practice, practice, practice.
* Build pipelines that heal themselves and parametrize them so users can easily re-run them for data recovery. Use watermarks to know when the last time a table was last updated in the data lake and create logic so that the pipeline will know to recover data from a certain point in time.
* Be the documentation king/queen. Use docstrings, type hints, comments, markdown files, CHANGELOG files, README, etc. throughout your code, modules, packages, repo, etc. to make your work as clear, intentional, and easy to read as possible. Make it easy to spread this information using an appropriate knowledge management solution like Confluence.
* Volunteer to make things better without being asked. Update legacy projects/repos with the latest code or package. Build and create the features you need to make DE work easier. For example, auto-tagging commits with the version number to easily go back to the snapshot of a repo with a long history.
* Unit testing is important. Learn pytest framework, its tools, and practice making your code modular to make unit tests easier to create.
* Create and use a DE repo template using cookiecutter to create consistency in repo structures in all DE projects and include common files (yaml, .gitignore, etc.).
* Knowledge of fundamental SQL if valuable in understanding how to manipulate data. I found it made it easier understanding pandas and pyspark frameworks. ",-1,2025-04-30 10:07:50
"I have been tasked with replicating some GA4 dashboards in PowerBI. As some of the measures are non-additive, I would need the raw GA4 event data as a basis for this, otherwise reports on User metrics will not be the same as the GA4 portal.

Has anyone successfully exported GA4 raw data from Bigquery into ANOTHER dwh of a different type? Is it even possible?",-1,2025-04-24 19:09:17
"I've been messing around with CKAN and the whole Data Package spec lately, and honestly, I'm kind of surprised they barely get mentioned on this sub.

For those who haven't come across them:

CKAN is this open-source platform for publishing and managing datasets‚Äîused a lot in gov/open data circles.

Data Packages are basically a way to bundle your data (like CSVs) with a datapackage.json file that describes the schema, metadata, etc.

They're not flashy, no Spark, no dbt, no ‚ÄúAI-ready‚Äù marketing buzz - but they're super practical for sharing structured data and automating ingestion. Especially if you're dealing with datasets or anything that needs to be portable and well-documented.

So my question is: why don't we talk about them more here? Is it just too ""dataset"" focused? Too old-school? Or am I missing something about why they aren't more widely used in modern data workflows?

Curious if anyone here has actually used them in production or has thoughts on where they do/don't fit in today's stack.",-1,2025-04-30 08:40:04
"Ran into a mess debugging a late-arriving dataset. The raw and enriched data were out of sync, and tracing back the changes was a nightmare.

How do you keep versions aligned across stages? Snapshots? Lineage? Something else?",-1,2025-04-24 13:54:59
"I've got a billion small images stored in S3. I'm looking for a tool to help manage collections of these objects, as an item may be part of one, none, or multiple datasets. An image may have any number of associated annotations from human and models. 

I've been reading up on a few different OSS feature store and data management solutions, like Feast, Hopsworks, FeatureForm, DVC, LakeFS, but it's not clear whether these tools do what I'm asking, which is to make and manage collections from the individual datum (without duplicating the underlying data), as well as multiple instances of associated labels. 

Currently I'm tempted to roll out a relational DB to keep track of the image S3 keys, image metadata, collections/datasets, and labels... but surely there's a solution for this kind of thing out there already. Is it so basic it's not advertised and I missed it somehow, or is this not a typical use-case for other projects? How do you manage your datasets where the data could be included into different possibly overlapping datasets, without data duplication?",-1,2025-04-30 16:21:50
"Hey folks,

So I have this query that joins two table, selects a few columns, runs a dense rank and then filters to keep only the rank 1s. Pretty simple right ?

Here‚Äôs the kicker. The overpaid, under evolved nit wit who designed the databases didn‚Äôt add a single index on either of these tables. Both of which have upwards of 10M records. So, this simple query takes upwards of 90 mins to run and return a result set of 90K records. Unacceptable. 

So, I set out to right this cosmic wrong. My genius idea was to simplify the query to only perform the join and select the required columns. Eliminate the dense rank calculation and filtering. I would then read the data into Polars and then perform the same operations.

Yes, seems weird but here‚Äôs the reasoning. I‚Äôm accessing the data from a Tibco Data Virtualization layer. And the TDV docs themselves admit that running analytical functions on TDV causes a major performance hit. So it kinda makes sense to eliminate the analytical function. 

And it worked. Kind of. The time to read in the data from the DB was around 50 minutes. And Polars ran the dense rank and filtering in a matter of seconds. So, the total run time dropped to around half, even though I‚Äôm transferring a lot more data. Decent trade off in my book. 

But the problem is, I‚Äôm still not satisfied. I feel like there should be more I can do. I‚Äôd appreciate any suggestions and I‚Äôd be happy to provide any additional details. Thanks. ",-1,2025-04-24 17:02:30
"I am a SWE with no DE experience. I have been tasked with architecting our storage and ETL pipelines. I took a month long online course leading up to my start date, and have done a ton of research and asked you guys a lot of questions (**thank you!!**). 

All of this study/research has led me to two rough draft architectures to present to my company. I was hoping to get some constructive feedback on them, if you all would do me the honor. 

Here's some context for the images below:

1. Scale of data is many terabytes to a few petabytes uncompressed. Largely sensor data. 
2. Data is initially generated and stored on an air-gapped network. 
3. Data will be moved into a lab by detaching hard-drives. There, we will need to retain some raw data for regulatory purposes, and we will also want to perform ETL into an analytical database/warehouse. 

I have a lot of time to refine these before implementation time, and specific technologies are flexible. but next week I wan to present a *reasonable* view of the types of solutions we might use.  What do you think of this as a first draft? Any obvious show stoppers or bad ideas here?

[On Premise Rough Draft](https://preview.redd.it/9ktilxpp8uwe1.png?width=1413&format=png&auto=webp&s=bfe0ab54c719bc69a89480a42dad08e6b0f8c6a6)

[Cloud Rough Draft. ](https://preview.redd.it/8y82yjrv8uwe1.png?width=1548&format=png&auto=webp&s=1f2a64daad73fa82679d2e82c1b2383adae6910a)

",-1,2025-04-24 20:08:02
"Giving credit where it is due, read the blog post ‚Üí [https://luminousmen.com/post/change-data-capture](https://luminousmen.com/post/change-data-capture)  
  
If you want CDC that meets the all the specs in the post, we open sourced a tool üëÄ [https://github.com/sequinstream/sequin](https://github.com/sequinstream/sequin)  ",-1,2025-04-24 23:43:29
"That comes from ""Designing Data-Intensive Applications"" by Martin Kleppmann if you're wondering ",-1,2025-04-24 15:49:02
"I have seen this asked a few times, but i couldn‚Äôt see a concrete example. 

I want to move data from an on premise mysql to S3. I come from Hadoop background, and I mainly use sqoop to load from RDBMS to S3.

What is the best way to do it? So far i have tried

Data Load Tool - did not work. Somehow im having permission issues. Its using s3fs under the hood. That don‚Äôt work but boto3 does

Pyairbyte - no documentation",-1,2025-04-30 14:27:16
"I've been using S3 for years now. It's awesome. It's by far the best service from a programatic use case. However, the console interface... not so much.

  
Since AWS is axing S3 Select: 

>*After careful consideration, we have made the decision to close new customer access to Amazon S3 Select and Amazon S3 Glacier Select, effective July 25, 2024. Amazon S3 Select and Amazon S3 Glacier Select existing customers can continue to use the service as usual. AWS continues to invest in security and availability improvements for Amazon S3 Select and Amazon S3 Glacier Select, but we do not plan to introduce new capabilities.*



I'm curious as to how you all access S3 data files (e.g. Parquet, CSV, TSV, Avro, Iceberg, etc.) for debugging purposes or ad-hoc analytics?



I've done this a couple of ways over the years:

\- Download directly (slow if it's really big)

\- Access via some Python interface (slow and annoying)

\- S3 Select (RIP)

\- Creating an Athena table around the data (worst experience ever).

Neither of which is particularly nice, or efficient.

Thinking of creating a way to make this easier, but curious what everyone does, and why?",-1,2025-04-30 21:29:23
"Hi. This will be the first post of a few as I am remidiating an analytics platform. The org has opted for B/S/G in their past interation but fumbled and are now doing everything on bronze, snapshots come into the datalake and records are overwritten/deleted/inserted. There's a lot more required but I want to start with storage and regulations around data retention.

Data is coming from D365FO, currently via Synapse link.

How are you guys maintaining your INSERTS,UPDATES,DELETES to comply with SOX/J-SOX? From what I understand the organisation needs to keep any and all changes to financial records for 7 years.

My idea was Iceberg tables with daily snapshots and keeping all delta updates with the last year in hot and the older records in cold storage.

Any advice appreciated. ",-1,2025-04-23 06:15:53
"Hi everyone,  
I'm design a dimensional Sales Order schema data using the `sale_order` and `sale_order_line` tables. My fact table `sale_order_transaction` has a granularity of one row per one product ordered. I noticed that when a coupon or promotion discount is applied to a sale order, it appears as a separate line in `sale_order_line`, just like a product.

In my fact table, I'm taking only actual product lines (excluding discount lines). But this causes a mismatch:  
**The sum of** `price_total` **from sale order lines doesn't match the** `amount_total` **from the sale order.**

How do you handle this kind of situation?

* Do you include discount lines in your fact table and flag them?
* Or do you model order-level data separately from product lines?
* Any best practices or examples would be appreciated!

Thanks in advance!",-1,2025-04-23 07:03:21
"There may be some nuance in ADF that I'm missing, but I can't solve this issue. I have an ADF pipeline that has an If Condition. If the If Condition fails I want to get the error details from the Error Details box, you can get those details from the JSON. After getting the details I have a Databricks notebook that should take those details and add them to an error logging table. The Databricks notebook connects to function that acts as a stored proc, unfortunately Databricks doesn't support stored procs. I know they have videos on it, but their own software says it doesn't support stored procs. 

The issue I'm having is the Databricks notebooks fails to execute if the If Condition fails. From what I can tell the parameters aren't being passed through and the expressions used in the Base parameters aren't being executed.

 I figured it should still run on Completion, but the parameters from the If Condition are only being passed when the If Condition succeeds. Originally the If Condition was the last step of the nested pipeline, I'm adding the Databricks notebook to track when the pipeline fails on that step. The If Condition is nested within a ForEach loop. I tried to set the Databricks to run after the ForEach loop but I keep getting a BadRequest error. 

Any tips or advice is welcome, I can also add any details. ",-1,2025-04-30 15:33:31
"I have been trying to build a tool which can map the data from an unknown input file to a standardised output file where each column has a meaning to it. So many times you receive files from various clients and you need to standardise them for internal use. The objective is to be able to take any excel file as an input and be able to convert it to a standardized output file.
Using regex does not make sense due to limitations such as the names of column may differ from input file to input file (eg rate of interest or ROI or growth rate ).

Anyone with knowledge in the domain please help.",-1,2025-04-23 11:57:54
"I built StatQL after spending too many hours waiting for scripts to crawl hundreds of tenant databases in my last job (we had a db-per-tenant setup).

With StatQL you write one SQL query, hit Enter, and see a first estimate in seconds‚Äîeven if the data lives in dozens of Postgres DBs, a giant Redis keyspace, or a filesystem full of logs.

What makes it tick:

* A sampling loop keeps a fixed-size reservoir (say 1 M rows/keys/files) that‚Äôs refreshed continuously and evenly.
* An aggregation loop reruns your SQL on that reservoir, streaming back value ¬± 95 % error bars.
* As more data gets scanned by the first loop, the reservoir becomes more representative of entire population.
* Wildcards like pg.?.?.?.orders or fs.?.entries let you fan a single query across clusters, schemas, or directory trees.

Everything runs locally:¬†`pip install statql`¬†and¬†`python -m statql`¬†turns your laptop into the engine. Current connectors: PostgreSQL, Redis, filesystem‚Äîmore coming soon.

Solo side project, feedback welcome.

[https://gitlab.com/liellahat/statql](https://gitlab.com/liellahat/statql)",-1,2025-05-01 16:36:13
"does anyone know why so many pyarrow functions/classes/methods lack docstrings (or they don't show up in vs code)? is there an extension to resolve this problem? (trying to avoid pyarrow website in a separate window.)

thanks all!

https://preview.redd.it/z1u99mfhhowe1.png?width=454&format=png&auto=webp&s=87e69afa606ac15cab1f5f2a19bacff7da22f72c",-1,2025-04-24 00:35:32
"This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.

Examples:

* What are you working on this month?
* What was something you accomplished?
* What was something you learned recently?
* What is something frustrating you currently?

As always, sub rules apply. Please be respectful and stay curious.

**Community Links:**

* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)
* [Data Engineering Events](https://dataengineering.wiki/Community/Events)
* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)
* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)",-1,2025-05-01 16:00:53
"Do you usually use your Marts table which are considered finals as inputs for some intermediate ?

I‚Äôm wondering if this is bad practice or something ?

So let‚Äôs says you need the list of customers to build something that might require multiple steps (I want to avoid people saying, let‚Äôs build your model in Marts that select from Marts. Like yes I could but if there 30 transformation I‚Äôll split that in multiple chunks and I don‚Äôt want those chunks to live in Marts also). Your customer table lives in Marts, but you need it in a lot of intermediate models because you need to do some joins on it with other things. Is that ok? Is there a better way ?

Currently a lot of DS models are bind to STG directly and rebuild the same things as DE those and this makes me crazy so I want to buoy some final tables which can be used in any flows but wonder if that‚Äôs good practices because of where the ‚Äúfinal‚Äù table would live ",-1,2025-04-23 10:44:09
"I'm trying to set up snapshots on some tables with DBT and I'm having difficulty with the dbt\_valid\_to in my snapshots. It's always null. I assumed this is something to do with the syntax of the YML but no combination seems to produce the desired results of a set date like 9999-12-31.

This is the YML in the snapshots folder. The project YML has no settings for the valid to. It's aways null.

    version: 2
    
    snapshots:
    ¬†¬†- name: users_snapshot
    ¬†¬†¬†¬†config:
    ¬†¬†¬†¬†¬†¬†unique_key: user_id
    ¬†¬†¬†¬†¬†¬†strategy: check
    ¬†¬†¬†¬†¬†¬†check_cols: all
    ¬†¬†¬†¬†¬†¬†# dbt_valid_to_current: ""CAST('9999-12-31 23:59:59' AS datetime)""
    ¬†¬†¬†¬†¬†¬†# dbt_valid_to_current: ""CAST('9999-12-31' AS DATE)""
    ¬†¬†¬†¬†¬†¬†# dbt_valid_to_current: ""CAST('9999-12-31 23:59:59' AS datetime)""
    ¬†¬†¬†¬†¬†¬†dbt_valid_to_current: '2025-06-01'",-1,2025-05-01 19:15:31
"Got something exciting to share?  
The¬†[Open Source Analytics Conference - OSACon 2025](https://osacon.io/)¬†CFP¬†is now¬†officially open!  
We're going online Nov 4‚Äì5, and we want YOU to be a part of it!  
Submit your proposal and be a speaker at the leading event for open-source analytics:  
[https://sessionize.com/osacon-2025/](https://sessionize.com/osacon-2025/)",-1,2025-05-01 14:10:16
"My team is standardizing our raw data loading process, and we‚Äôre split on best practices.

I believe raw data should be stored using the correct data types (e.g., INT, DATE, BOOLEAN) to enforce consistency early and avoid silent data quality issues.
My teammate prefers storing everything as strings (VARCHAR) and validating types downstream ‚Äî rejecting or logging bad records instead of letting the load fail.

We‚Äôre curious how other teams handle this:
	‚Ä¢	Do you enforce types during ingestion?
	‚Ä¢	Do you prefer flexibility over early validation?
	‚Ä¢	What‚Äôs worked best in production?

We‚Äôre mostly working with structured data in Oracle at the moment and exploring cloud options. ",-1,2025-05-02 04:32:16
"I work for a small company so we decided to use Postgres as our DWH. It's easy, cheap and works well for our needs. 

Where it falls short is if we need to do any sort of analytical work. As soon as the queries get complex, the time to complete skyrockets. 

I started using duckDB and that helped tremendously. The only issue was the scaffolding every time just so I could do some querying was tedious and the overall experience is pretty terrible when you compare writing SQL in a notebook or script vs an editor.

I liked the duckDB UI but the non-persistent nature causes a lot of headache. This led me to build [soarSQL](https://soarsql.com/) which is a duckDB powered SQL editor. 

soarSQL has quickly become my default SQL editor at work because it makes working with OLTP databases a breeze. On top of this, I get save a some money each month because I the bulk of the processing happens on my machine locally!

It's free, so feel free to give it a shot and let me know what you think!",-1,2025-05-01 23:27:53
"Hi there, 

I have PII data in the Source db that I need to transform before sync to Destination warehouse in AirByte. 
Has anybody done this before?

In docs they suggest transforming AT Destination. But this isn‚Äôt what I‚Äôm trying to achieve. I need to transform before sync. 

Disclaimer: I already tried Google and forums, but can‚Äôt find anything

Any help appreciated ",-1,2025-04-24 15:49:56
"There may be some nuance in ADF that I'm missing, but I can't solve this issue. I have an ADF pipeline that has an If Condition. If the If Condition fails I want to get the error details from the Error Details box, you can get those details from the JSON. After getting the details I have a Databricks notebook that should take those details and add them to an error logging table. The Databricks notebook connects to function that acts as a stored proc, unfortunately Databricks doesn't support stored procs. I know they have videos on it, but their own software says it doesn't support stored procs. 

The issue I'm having is the Databricks notebooks fails to execute if the If Condition fails. From what I can tell the parameters aren't being passed through and the expressions used in the Base parameters aren't being executed.

 I figured it should still run on Completion, but the parameters from the If Condition are only being passed when the If Condition succeeds. Originally the If Condition was the last step of the nested pipeline, I'm adding the Databricks notebook to track when the pipeline fails on that step. The If Condition is nested within a ForEach loop. I tried to set the Databricks to run after the ForEach loop but I keep getting a BadRequest error. 

Any tips or advice is welcome, I can also add any details. ",-1,2025-04-30 15:33:31
"A few years ago I worked on a project that involved running distributed computations on a spark cluster (AWS ec2 machines). The data was pulled from data sources (CSV files in S3) and transformed and stored in parquet files, which were then fed in the computation engine running on spark, the output of which was mostly stored in a transactional database. The transactional db in turn powered a user interface.  

The computation engine ran as a job in the pipeline (processing high volume data) as well as upon user actions on the UI (low volume calculations). This computation engine was pretty complex component, doing a bunch of different things. Given the complexity, there was a strong need to have a properly structured code that stays maintainable, as a large team worked just on this. Also as this was the slowest component of the pipeline, there was also a need to be well versed in how spark works internally, so that well optimized code is written. The codebase was in scala.

My question is - does this component come under the purview of a data engineer or a software engineer. As I mentioned this was several years ago, and ""data engineer"" title was only gradually picking up at that time. All of us were SWE then (most transitioned into a DE role subsequently). I ask this question because I've come across several data engineers who have pretty strong demarcations around what a data engineer shouldn't be doing. And mostly I find the software engineering principles (that get used  to create a maintainable, 'enterprisey' codebase) are often ignored or underdeveloped.",-1,2025-04-24 04:04:10
"Hi All,

We are trying to build our data platform in open-source by leveraging spark. Having experienced the performance improvement in MS Fabric Spark using Native Engine (Gluten + Velox), we are trying to build spark with Gluten + Velox combo. 

I have been trying for last 3 days, but I am having problems in getting the source code to build correctly (even if I follow the exact steps in doc). I tried using the binaries (jar files) but those also crash when just starting spark. 

I want to know if you have experience in Gluten + Velox (outside MS Fabric). I see companies like Palantir, PInterest use them and they even have videos showcasing their solution, but build failures make me think the project is not yet stable. Also, MS most likely made the code more stable, but I guess they did not directly contribute to open-source. ",-1,2025-04-29 19:18:47
"So generally when we design a data warehouse we try to follow schema designs like star schema or snowflake schema, etc.

But suppose you have multiple tables which needs to be brought together and then calculate KPIs aggregated at different levels and connect it to Tableau for reporting.

In this case how to design the backend? like should I create a denormalised table with views on top of it to feed in the KPIs? What is the industry best practices or solutions for this kind of use cases?

",-1,2025-04-27 15:57:09
"My employer has acquired several smaller businesses. We now have overlapping customer bases and need to map, then migrate, the customer data.

We already have many of their customers in our system, while some are new (new customers are not an issue). For the common ones, I need to map their customer IDs from their database to ours.  
We have around 200K records; they have about 70K. The mapping needs to be based on account and address.

I‚Äôm currently using Excel, but it‚Äôs slow and inefficient.  
Could you please share best practices, methodologies, or tools that could help speed up this process? Any tips or advice would be highly appreciated!

Edit: In many cases there is no unique identifier, names and addresses are written similarly but not exactly. This causes a pain!",-1,2025-04-26 13:23:43
"Hey all,  
  
Quick question ‚Äî I'm experimenting with S3 tables, and I'm running into an issue when trying to apply LF-tags to resources in the `s3tablescatalog` (databases, tables, or views).  
Lake Formation keeps showing a message that there are no LF-tags associated with these resources.  
Meanwhile, the same tags are available and working fine for resources in the default catalog.

I haven‚Äôt found any documentation explaining this behavior ‚Äî has anyone run into this before or know why this happens?  
  
Thanks!",-1,2025-04-27 18:13:14
"Hi all, 

Was just wondering if someone could help explain how things work in the real world, let‚Äôs say you have Kafka, airflow and use python as the main language. How do companies host all of this? I realise for some services there are hosted versions offered by cloud providers but if you are running airflow in azure or AWS for example is the recommended way to use a VM? Or is there another way that this should be done? 

Thanks very much!

",-1,2025-04-28 10:18:16
"Hey everyone,

I've been using GPT-4o for a lot of my Python tasks and it's been a game-changer. However, as I'm getting deeper into Azure, AWS, and general DevOps work with Terraform, I'm finding that for longer, more complex projects, GPT-4o starts to hallucinate and lose context, even with a premium subscription.

I'm wondering if switching to a model like GPT-4o Mini or something that ""thinks longer"" would be more accurate. What's the general consensus on the best model for this kind of long-term, context-heavy infrastructure work? I'm open to trying other models like Gemini Pro or Claude's Sonnet if they're better suited for this.

",-1,2025-06-12 08:53:42
"Hello,

We have a source system that is only able to export data using a ""start"" and ""end"" date range. So for example, each day, we get a ""current month"" export for the data falling between the start of the month and the current day. We also get a ""prior month"" report each day of the data from the full prior month. Finally, we also may get a ""year to date"" file with all of the data from the start of the year to current date.

Nothing in the data export itself gives us an ""as of date"" for the record (the source system uses proprietary information to give us the data that ""falls"" within that range). All we have is the date range for the individual export to go off of.

I'm struggling to figure out how to model this data. Do I simply use three different ""fact"" models? One each for ""daily"" (sourced from the current month file), ""monthly"" (sourced from the prior month file), and ""yearly"" (sourced from the year to date file)? If I do that, how do I handle the different grains for the SCD Type 2 DIM table of the data? What should the VALID\_FROM/VALID\_TO columns be sourced from in this case? The daily makes sense (I would source VALID\_FROM/VALID\_TO from the ""end"" date of the data extract that keeps bumping out each day), but I don't know how that fits into the monthly or yearly data.

  
Any insight or help on this would be really appreciated.

Thank you!!",-1,2025-04-28 20:42:35
"Recently i've been reading ""Designing Data Intensive Applications"" and I came across a concept that made me a little confuse.

In the section that discusses the diferent partition methods (Key Range, hash, etc) we are introduced to the concept of Secondary Indexes, in which a new mapping is created to help in the search for occurences of a particular value. The book gives two examples of data partitioning methods in this scenario:

1. Partitioning Secondary Indexes By Document - The data in the distributed system is allocated to specific partition based on the key range defined to that partition (e.g.: partition 0 goes from 1-5000). 
2. Paritioning Secodary Indexes By Term - The data in the distributed system is allocated to a specific partition base on the value of a term (e.g: all documents with **term:valueX** go to partition N).

In both of the above methods a secondary index for a specific term is configured and for each value of this term a mapping like **term:value -> \[documentX1\_position, documentX2\_position\]** is created. 

My question is how does the primary index and secondary index coexist? The book states that Key Range and Hash partition in the primary index can be employed alongside with the methods mentioned above for the secondary index, but it's not making sense in my head.

For instance, if a Hash partition is employed for the data system documents that have a hash that belongs in partition N hash range will be stored there, but what if partition N has a partitioning term (e.g: **color = red**) based method for a secondary index and the document doesn't belong there (e.g.: document has **color = blue**)? Wouldn't the hash based partition mess up the idead behind partitioning based on term value?

I also thought about the possibility of the document hash being assigned based on the partition term value (e.g.: document\_hash = hash(document\[""color""\])), but then (if I'm not mistaken) we wouldn't have the advantages of uniform distribution of data between partitions that hash based partitioning brings to the table, because all of the hashes in the term partition would be the same (same values).

Maybe I didn't understood it properly, but it's not making sense in my head.",-1,2025-04-28 18:48:38
"I started a new project in which I get data about organizations from multiple sources and one of the things I need to do is match entities across the data sources, to avoid duplicates and create a single source of truth. The problem is that there is no shared attribute across the data sources. So I started doing some research and apparently this is called record linkage (or entity matching/resolution). I saw there are many techniques, from measuring text similarity to using ML. So my question is, if you faced this problem at your job, what techniques did you use? What were you biggest learnings? Do you have any advice?",-1,2025-04-26 14:00:50
"We are looking at creating a new internal database using mongodb, we have spent a lot of time with a postgres db but have faced constant schema changes as we are developing our data model and understanding of client requirements. 

It seems that the flexibility of the document structure is desirable for us as we develop but I would be curious if anyone here has similar experience and could give some insight.",-1,2025-04-26 06:35:42
"Hi everyone!

Covering another article in my Data Tech Stack Series. If interested in reading all the data tech stack previously covered (Netflix, Uber, Airbnb, etc), checkout [here](https://www.junaideffendi.com/t/tech-stack).

This time I share Data Tech Stack used by DoorDash to process hundreds of Terabytes of data every day.

DoorDash has handled over 5 billion orders, $100 billion in merchant sales, and $35 billion in Dasher earnings. Their success is fueled by a data-driven strategy, processing massive volumes of event-driven data daily.

  
The article contains the references, architectures and links, please give it a read: [https://www.junaideffendi.com/p/doordash-data-tech-stack?r=cqjft&utm\_campaign=post&utm\_medium=web&showWelcomeOnShare=false](https://www.junaideffendi.com/p/doordash-data-tech-stack?r=cqjft&utm_campaign=post&utm_medium=web&showWelcomeOnShare=false)



What company would you like see next, comment below.



Thanks",-1,2025-04-26 16:42:26
"For those with extensive experience in data engineering experience, what is the usual process for developing a pipeline for production?

I am a data analyst who is interested in learning about data engineering, and I acknowledge that I am lacking a lot of knowledge in software development, and hence the question. 

I have been picking up different tools individually (docker, terraform, GCP, Dagster etc) but I am quite puzzled at how do I piece all these tools together.

For instance, I am able to develop python script that calls an API for data, put into dataframe and ingest into postgresql, orchestras the entire process using dagster. But anything above that is beyond me. I don‚Äôt quite know how the wrap the entire process in docker, run it on GCP server etc. I am not even sure if the process is correct in the first place

For experienced data engineers, what is the usual development process? Do you guys work backwards from docker first? What are some best practices that I need to be aware of.",-1,2025-04-28 04:56:00
"I am a newbie and trying to learn Data Engineering using Azure. I am currently using the trial version with 200$ credit. While trying to create a cluster, I am getting errors. So far, I have tried changing locations, but it is not working. I tried Central Canada, East US, West US 2, Central India. Also, I tried changing size of compute, but it is getting failed as it takes too long to create a cluster. I used Personal compute. Please help a newbie out:  
This is the error:  
The requested VM size for resource 'Following SKUs have failed for Capacity Restrictions: Standard\_DS3\_v2' is currently not available in location 'eastus'. Please try another size or deploy to a different location or different zone.",-1,2025-05-02 10:21:20
"**Calling IT pros who manage workflows and scheduling**

I‚Äôm a UX researcher working on better solutions for IT teams.

If you manage complex workflows at a mid-sized company ‚Äî or are part of a smaller IT team inside a big company ‚Äî we‚Äôd love your input!

It‚Äôs just a **10-minute survey** that will be sent out

‚û°Ô∏è DM me your email if you‚Äôre in  
  
**Thank you!**

*(We will use your email to send you the survey link and to send our privacy notice. Your email will not be used in marketing efforts in any way and you may wish to remove your email and information from our database at any time.)*",-1,2025-04-25 15:07:25
"I'm currently trying to set up Schema level security inside fabric tied to a users Entra ID.

I'm using the following SQL code to create a role. Grant this role view and select permissions to a schema in the warehouse. I then add a user to this role by adding their company email to the role. 

  
CREATE ROLE schema\_limited\_reader;

GO

  
GRANT CONNECT TO schema\_limited\_reader

GO

  
GRANT SELECT

ON SCHEMA::Schema01

TO schema\_limited\_reader

GRANT VIEW

ON SCHEMA::Schema01

TO schema\_limited\_reader

  
ALTER ROLE schema\_limited\_reader ADD MEMBER \[test\_user@company.com\]



However, when the test user connects to the workspace through powerBI, they can still view and select from all the schemas in the warehouse. I know im missing something. First time working with Fabric. The test user has admin privilages at the top Fabric level, could this be overriding the security role function?

  
Would appreciate any advice. Thank you.",-1,2025-04-25 12:55:19
"Hey All,

I've had an incredible year and I feel extremely lucky to be in the position I'm in. I'm a relatively new DE, but I've covered so much ground even in one year.

I'm not perfect, but I can feel my growth. Every day I am learning something new and I'm having such joy improving on my craft, my passion, and just loving my experience each day building pipelines, debugging errors, and improving upon existing infrastructure.

As I look back I wanted to share some gems or bits of valuable knowledge I've picked up along the way:

* Showing up in person to the office matters. Your communication, attitude, humbleness, kindness, and selflessness goes a long way and gets noticed. Your relationship with your client matters a lot and being able to be in person means you are the go-to engineer when people need help, education, and fixing things when they break. Working from home is great, but there are more opportunities when you show up for your client in person.
* pre-commit hooks are valuable in creating quality commits. Automatically check yourself even before creating a PR. Use hooks to format your code, scan for errors with linters, etc.
* Build pipelines with failure in mind. Always factor in exception handling, error logging, and other tools to gracefully handle when things go wrong.
* DRY - such as a basic principle but easy to forget. Any time you are repeating yourself or writing code that is duplicated, it's time to turn that into a function. And if you need to keep track of state, use OOP.
* Learn as much as you can about CI/CD. The bugs/issues in CI/CD are a different beast, but peeling back the layers it's not so bad. Practice your understanding of how it all works, it's crucial in DE.
* OOP is a valuable tool. But you need to know when to use it, it's not a hammer you use at every problem. I've seen examples of unnecessary OOP where a FP paradigm was better suited. Practice, practice, practice.
* Build pipelines that heal themselves and parametrize them so users can easily re-run them for data recovery. Use watermarks to know when the last time a table was last updated in the data lake and create logic so that the pipeline will know to recover data from a certain point in time.
* Be the documentation king/queen. Use docstrings, type hints, comments, markdown files, CHANGELOG files, README, etc. throughout your code, modules, packages, repo, etc. to make your work as clear, intentional, and easy to read as possible. Make it easy to spread this information using an appropriate knowledge management solution like Confluence.
* Volunteer to make things better without being asked. Update legacy projects/repos with the latest code or package. Build and create the features you need to make DE work easier. For example, auto-tagging commits with the version number to easily go back to the snapshot of a repo with a long history.
* Unit testing is important. Learn pytest framework, its tools, and practice making your code modular to make unit tests easier to create.
* Create and use a DE repo template using cookiecutter to create consistency in repo structures in all DE projects and include common files (yaml, .gitignore, etc.).
* Knowledge of fundamental SQL if valuable in understanding how to manipulate data. I found it made it easier understanding pandas and pyspark frameworks. ",-1,2025-04-30 10:07:50
Have anybody try to use DuckDB as Superset cache in place of Redis? It's persistent mode looks like it can be small analytics database. But know sure if it's possible at all.,-1,2025-04-25 19:38:53
"I tend to code my own ETL processes in Python, but it's a pretty frustrating process because, when you make an API call, literally anything can come through.

What do you guys do to make foolproof ETL scripts?

My edge case:

Today, an ETL process that has successfully imported thousands or rows of data without issue got tripped up on this line:

    new_entry['utm_medium'] = tracking_code.get('c_src', '').lower() or ''

I guess, this time, ""c\_src"" was present in the data, but it was explicitly set to ""None"" so, instead of returning '', it just crashed the whole function.

Which is fine, and I can update my logic to deal with that, so I'm not looking for help with this specific issue. I'm just curious what approaches other people take to avoid this when literally anything imaginable could come in with an ETL process and, if it's not what you're expecting, it could just stop the whole process.",-1,2025-04-25 15:02:20
"I've been messing around with CKAN and the whole Data Package spec lately, and honestly, I'm kind of surprised they barely get mentioned on this sub.

For those who haven't come across them:

CKAN is this open-source platform for publishing and managing datasets‚Äîused a lot in gov/open data circles.

Data Packages are basically a way to bundle your data (like CSVs) with a datapackage.json file that describes the schema, metadata, etc.

They're not flashy, no Spark, no dbt, no ‚ÄúAI-ready‚Äù marketing buzz - but they're super practical for sharing structured data and automating ingestion. Especially if you're dealing with datasets or anything that needs to be portable and well-documented.

So my question is: why don't we talk about them more here? Is it just too ""dataset"" focused? Too old-school? Or am I missing something about why they aren't more widely used in modern data workflows?

Curious if anyone here has actually used them in production or has thoughts on where they do/don't fit in today's stack.",-1,2025-04-30 08:40:04
"I've got a billion small images stored in S3. I'm looking for a tool to help manage collections of these objects, as an item may be part of one, none, or multiple datasets. An image may have any number of associated annotations from human and models. 

I've been reading up on a few different OSS feature store and data management solutions, like Feast, Hopsworks, FeatureForm, DVC, LakeFS, but it's not clear whether these tools do what I'm asking, which is to make and manage collections from the individual datum (without duplicating the underlying data), as well as multiple instances of associated labels. 

Currently I'm tempted to roll out a relational DB to keep track of the image S3 keys, image metadata, collections/datasets, and labels... but surely there's a solution for this kind of thing out there already. Is it so basic it's not advertised and I missed it somehow, or is this not a typical use-case for other projects? How do you manage your datasets where the data could be included into different possibly overlapping datasets, without data duplication?",-1,2025-04-30 16:21:50
"Hi everyone üëã

I‚Äôm trying to work on a new project to improve my data engineering skills and would love to get some advice from people more experienced in real-world systems.

# üîÅ What I‚Äôm Trying to Do:

I previously built a Medallion Architecture project using MongoDB, Pandas, and PostgreSQL (Bronze ‚Üí Silver ‚Üí Gold). It helped me understand the basics of ELT pipelines.

Now I want to do something different, so I‚Äôm trying to build a¬†**real-time pipeline**¬†that also uses¬†**graph modeling**. Here‚Äôs my rough idea:

* Use¬†**MongoDB Atlas**¬†to store real-time event data (e.g., product views, purchases)
* Use¬†**AWS Lambda**¬†to process/clean those events.
* Push the cleaned events into¬†**Neo4j**¬†to create user-product relationships (for example:¬†`(:User)-[:VIEWED]->(:Product)`)

I‚Äôd also like to simulate the stream using Python + Faker, just to have some data coming in regularly.

# üôã‚Äç‚ôÇÔ∏è Where I‚Äôm Stuck / Need Help:

1. **Is it even a good idea to combine MongoDB and Neo4j like this?**¬†Or should I focus on just one?
2. Are there any common mistakes or traps I should watch out for with this kind of setup?
3. Any suggestions on making it more realistic or structured like a production system?

I‚Äôm still learning and trying to figure out how to make this useful, so any feedback or tips would mean a lot.

Thanks in advance üôè",-1,2025-04-24 06:11:39
"I have seen this asked a few times, but i couldn‚Äôt see a concrete example. 

I want to move data from an on premise mysql to S3. I come from Hadoop background, and I mainly use sqoop to load from RDBMS to S3.

What is the best way to do it? So far i have tried

Data Load Tool - did not work. Somehow im having permission issues. Its using s3fs under the hood. That don‚Äôt work but boto3 does

Pyairbyte - no documentation",-1,2025-04-30 14:27:16
"Here for some advice...

  
I'm hoping to build a PowerBI dashboard to display whether our team has received a file in our S3 bucket each morning. We have circa 200+ files received every morning, and we need to be aware if one of our providers hasn't delivered.

  
My hope is to set up event notifications from S3, that can be used to drive the dashboard. We know the filenames we're expecting, and the time each should arrive, but have got a little lost on the path between S3 & PowerBI.

  
We are an AWS house (mostly), so was considering using SQS, SNS, Lambda... But, still figuring out the flow. Any suggestions would be greatly appreciated! TIA",-1,2025-04-24 07:56:42
"I'm designing an ETL pipeline, and I want to automate it. My use case is not real-time, but the data is very big so I want to not waste resources. I've read about various solutions like Apache Airflow, but I've also read that simple cron jobs can do the trick. 

For context, I'm looking using Iceberg to populate a MinIO datalake with raw data coming in from Flink topics. Then, I want to schedule cron jobs to query CDC tables like the ones described here: [CDC on Iceberg](https://www.dremio.com/blog/cdc-with-apache-iceberg/). If the queries return changes, then I perform ETL on the changes and they go into a data-warehouse.   
  
Is this approach feasible? Is there a simpler way? A better way even if it isn't quite as simple?

",-1,2025-04-24 19:40:30
"Our organization is not very data savvy. 

For years, we have just handled data requests on an ad-hoc basis when business users email the IS team and ask them to query the OLTP database, which is highly normalized. 

In my view this is simply unsustainable. I am hit with so many of these ad-hoc requests that I hardly have time to develop a data warehouse. Frustratingly, the business is really bad at defining requirements, and it is not uncommon for me to produce a report via a 400-line query only for the business to say, ‚Äúoh, we actually need this, sorry.‚Äù 

In my view, we should have robust reports built in something like PowerBi that gives business users the ability to slice and dice data so we don‚Äôt have to write a new query every 20 minutes. However, developing such a report would require the business to get on the same page and adequately capture requirements in plain English. 

Is there any good software that your team is using to capture business logic in plain English? This is a nightmare. ",-1,2025-04-29 20:38:54
"**Hi everyone ‚Äì I‚Äôve checked the wiki/archives but didn‚Äôt see a recent thread on this, so I‚Äôm hoping it‚Äôs on-topic. Mods, feel free to remove if I‚Äôve missed something.**

I‚Äôm the founder of [**Notellect.ai**](http://Notellect.ai) (yes, this is self-promotion, posted under the ‚Äúonce-a-month‚Äù rule and with the Brand Affiliate tag). After \~2 months of hacking I‚Äôve opened a very small beta and would love blunt, no-fluff feedback from practitioners here.

**What it is:** An ‚Äúagentic‚Äù vibe coding platform that sits between your data and Python:

1. **Data source ‚Üí LLM ‚Üí Python ‚Üí Result**
2. Current sources: CSV/XLSX (adding DBs & warehouses next).
3. You ask a question; the LLM reasons over the files, writes Python, and drops it into an integrated cloud IDE.  (Currently it uses Pyodide with numpy and pandas and more lib supports on the way)
4. You can inspect / tweak the code, run it instantly, and the output is stored in a note for later reuse.

**Why I think it matters**

* Cursor/Windsurf-style ‚Äúvibe coding‚Äù is amazing, but data work needs transparency and repeatability.
* Most tools either hide the code or make you copy-paste between notebooks; I‚Äôm trying to keep everything in one place and 100 % visible.

**Looking for feedback on**

* Biggest missing features?
* Deal-breakers for trust/production use?
* Must-have data sources you‚Äôd want first?

**Try it / screenshots**:[ ](https://notellect.ai)[https://app.notellect.ai/login?invitation\_code=notellectbeta](https://app.notellect.ai/login?invitation_code=notellectbeta)

(use this invite link for 150 beta credits for first 100 testers)

home: [www.notellect.ai](http://www.notellect.ai)

Note for testing: Make sure to @ the files first (after uploading) before asking LLM questions to give it the context

https://preview.redd.it/sqj5njkzjjxe1.png?width=3808&format=png&auto=webp&s=ed06ac5e8bd19714c248fb5bafd260f8d9d71722

Thanks in advance for any critiques‚Äîtechnical, UX, or ‚Äúthis is pointless‚Äù are all welcome. I‚Äôll answer every comment and won‚Äôt repost for at least a month per rule #4.",-1,2025-04-28 09:09:53
"Hey guys,

I would like to hear your thoughts or suggestions on something I‚Äôm struggling with. I‚Äôm currently preparing for the Google Cloud Data Engineer certification, and I‚Äôve been going through the official study materials on Google Cloud SkillBoost. Unfortunately, I‚Äôve found the experience really disappointing.

The ""Data Engineer Learning Path"" feels overly basic and repetitive, especially if you already have some experience in the field. Up to Unit 6, they at least provide PDFs, which I could skim through. But starting from Unit 7, the content switches almost entirely to videos ‚Äî and they‚Äôre long, slow-paced, and not very engaging. Worse still, they don‚Äôt go deep enough into the topics to give me confidence for the exam.

When I compare this to other prep resources ‚Äî like books that include sample exams ‚Äî the SkillBoost material falls short in covering the level of detail and complexity needed.  


How did you prepare effectively? Did you use other resources you‚Äôd recommend?



",-1,2025-04-16 13:33:12
"Hey everyone! Wanted to share a little adventure into data engineering and AI.

We wanted to find the best developers on Github based on their code, so we cloned over 15,000 GitHub repos and analyzed their commits using LLMs to evaluate actual commit quality and technical ability.

In two days we were able to curate a dataset of 250k contributors, and hosted it on [https://www.sashimi4talent.com/](https://www.sashimi4talent.com/) . Lots of learnings into unstructured data engineering and batch inference that I'd love to share!",-1,2025-04-22 17:27:57
"  
We‚Äôre conducting a quick survey to gather insights from professionals who work with system integrations or iPaaS tools.  
‚úÖ Step 1: Take our 1-minute pre-survey  
‚úÖ Step 2: If you qualify, complete a 3-minute follow-up survey  
üéÅ Reward: Submit within 24 hours and receive a $15 Amazon gift card as a thank you!  
Help shape the future of integration tools with just 4 minutes of your time.  
üëâ [**Pre-survey Link**](https://docs.google.com/forms/d/e/1FAIpQLSfia5NJz3vtyu_wcyvLk3LDycs92ZSSM5zwB7j2vFQFM8bSzw/viewform)  
Let your experience make a difference!  






",-1,2025-05-02 11:00:39
"While I find these Buzzfeed-style quizzes somewhat‚Ä¶ *gimmicky*, they do make it easy to reflect on how your team handles core parts of your analytics stack. How does your team stack up in these areas?

**Semantic Layer Documentation:**

**Data Testing:**

* ‚úÖ Automated tests run prior to merging anything into main. Failed tests block the commit.
* üü° We do some manual testing.
* üö© We rely on users to tell us when something is wrong.

**Data Lineage:**

* ‚úÖ We know where our data comes from.
* üü° We can trace data back a few steps, but then it gets fuzzy.
* üö© Data lineage? What's that?

**Handling Data Errors:**

* ‚úÖ We feel confident our errors are reasonably limited by our tests. When errors come up, we are able to correct them and implement new tests as we see fit.
* üü° We fix errors as they come up, but don't track them.
* üö© We hope the errors go away on their own.

**Warehouse / RB Access Control:**

* ‚úÖ Our roles are defined in code (Terraform, Pulumi, etc...) and are git controlled, allowing us to reconstruct who had access to what and when.
* üü° We have basic access controls, but could be better.
* üö© Everyone has access to everything.

**Communication with Data Consumers:**

* ‚úÖ We communicate changes, but sometimes users are surprised.
* üü° We communicate major changes only.
* üö© We let users figure it out themselves.

Scoring: 

Each ‚úÖ - 0 points, Each üü° - 1 point, Each üö© - 2 points.

**0-4:** Your data practices are in good shape. 

**5-7:** Some areas could use improvement. 

**8+:** You might want to prioritize a data quality initiative.",-1,2025-04-17 18:02:57
"Hello to all,

Thank you for reading the following and talking the time to answer.

I'm a consultant and I work as...non idea what I am, maybe you'll tell me what I am.

In my current project (1+ years) I normally do stored procedures in tsql, I create reports towards Excel, sometimes powerbi, and...AND...AAAANNDDD * drums * Ms access (yeah, same as title says).

So many things happens inside ms access, mainly views from tsql and some...how can I call them? Like certain ""structures"" inside, made by a dude that was 7 years (yes, seven, S-E-V-E-N) on the project. These structures have a nice design with filters, with inputs, outputs. During this 1+ year I somehow made some modifications which worked (I was the first one surprised, most of the times I had no idea what I was doing, but it was working and nobody complained so, *shoulder pat* to me).

The thing is that I enjoy all the (buzz word incoming) * ‚ú®Ô∏è‚ú®Ô∏è‚ú®Ô∏èautomation‚ú®Ô∏è‚ú®Ô∏è‚ú®Ô∏è"" like the jobs, the procedures that do stuff etc. I enjoy tsql, is very nice. It can do a lot of shit (still trying to figure out how to send automatic mails, some procedures done by the previous dude already send emails with csv inside, for now it's black magic for me). The jobs and their schedule is pure magic. It's nice. 

Here comes the actual dilemma:

I want to do stuff. I'm taking some courses on SSIS (for now it seems it does the same as a stored procedures with extra steps+no code, but I trust the process).

How can I replace the entire ms access tool? How can I create a menu with stuff, like ""Sales, Materials, Aquisitions"" etc, where I have to put filters (as end user) to find shit. 

For every data eng. positions i see instruments required such as sql, no sql, postgresql, mongodb, airflow, snowflake, apake, hadoop, databricks, python, pyspark, Tableau, powerbi, click, aws, azure, gcp, my mother's virginity. I've taken courses (coursera / udemy) on almost all and they don't do magic. It seems they do pretty much what tsql can do (except ‚ú®Ô∏è‚ú®Ô∏è‚ú®Ô∏è cloud ‚ú®Ô∏è‚ú®Ô∏è‚ú®Ô∏è).

In python I did some things, mainly stuff about very old excel format files, since they come from a sap Oracle cloud, they come sometimes with rows/columns positioned where they shouldn't have been, so, I stead of the 99999+ rows of VBA script my predecessor did, I use 10 rows of python to do the same.

So, coming back to my question, is there something to replace Ms access? Keeping the simplicity and also the utility it has, but also ‚ú®Ô∏è‚ú®Ô∏è‚ú®Ô∏èfuture proof‚ú®Ô∏è‚ú®Ô∏è‚ú®Ô∏è, like, in 5 years when fresh people will come in my place (hopefully faster than 5y) they will have some contemporary technology to work with instead of stone age tools.

Thank you again for your time and for answering :D",-1,2025-04-17 19:19:44
"Hi everyone,

Our SaaS app that does task management allows users to add custom fields.  


I want to eventually allow filtering, grouping and ordering by these custom fields like any other task app.  
  
However, I'm stuck on the best data structure to allow this:

* jsonb column within the tasks table
* EAV column

Does anyone have any guidance on how other platform with custom fields allow/built this?  
",-1,2025-04-17 20:07:32
"Hot off the press:

* [https://ducklake.select/](https://ducklake.select/) 
* [https://duckdb.org/2025/05/27/ducklake](https://duckdb.org/2025/05/27/ducklake) 
* Associated podcasts: [https://www.youtube.com/watch?v=zeonmOO9jm4](https://www.youtube.com/watch?v=zeonmOO9jm4) 

Any thoughts from fellow DEs?",-1,2025-05-27 13:56:42
"Hey r/dataengineering, I need your help to find a solution to my dumpster fire and potentially save a soul (or two)).

I'm working together with an older dev who has been put on a project and it's a mess left behind by contractors. I noticed he's on some kind of PIP thing, and the project has a set deadline which is not realistic. It could be both of us are set up to fail. The code is the worst I have seen in my ten years in the field. No tests, no docs, a mix of prod and test, infra mixed with application code, a misunderstanding of how classes and scope work, etc.

The project itself is a ""library"" that syncing databricks with data from an external source. We query the external source and insert data into databricks, and every once in a while query the source again for changes (for sake of discussion, lets assume these are page reads per user) which need to be done incrementally. We also frequently submit new jobs to the external source with the same project. what we ingest from the source is not a lot of data, usually under 1 million rows and rarely over 100k a day. 

Roughly 75% of the code is doing computation in python for databricks, where they first pull out the dataframe and then filter it down with python and spark. The remaining 25% is code to wrap the API on the external source. All code lives in databricks and is mostly vanilla python. It is called from a notebook. (...)

My only idea is that the ""library"" should be split instead of having to do everything. The ingestion part of the source can be handled by dbt and we can make that work first. The part that holds the logic to manipulate the dataframes and submit new jobs to the external api is buggy and I feel it needs to be gradually rewritten, but we need to double the features to this part of the code base if we are to make the deadline. 

I'm already pushing back on the deadline and I'm pulling in another DE to work on this, but I am wondering what my technical approach should be.",-1,2025-04-17 22:28:41
"There is an anonymous survey about the Fivetran Pricing changes: https://forms.gle/UR7Lx3T33ffTR5du5

I guess it would be good to have a good sample size in there, so feel free to take part (2 minutes) if you're a fivetran customer.

Regardless of that, what has been the effect since the price model changes for you?",-1,2025-04-18 01:35:07
"Hey

We want to give our business users a way to query data on their own. Business users = our operations team + exec team for now

We have already documentation in place for some business definitions and for tables. And most of the business users already have a very bit of sql knowledge. 

From your experience: how hard is it to achieve this? Should we go for a tool like Wobby or Wren AI or build something ourselves?

Would love to hear your insights on this. Thx!",-1,2025-05-27 06:11:49
"Recently, I came across ""Vibe Coding"". The idea is cool, you need to use only LLM integrated with IDE like Cursor for software development. I decided to do the same but in the data engineering area. In the link you can find a description of my tests in MS Fabric. 

I'm wondering about your experiences and advices how to use LLM to support our work.

  
My Medium post: [https://medium.com/@mariusz\_kujawski/vibe-coding-in-data-engineering-microsoft-fabric-test-76e8d32db74f](https://medium.com/@mariusz_kujawski/vibe-coding-in-data-engineering-microsoft-fabric-test-76e8d32db74f)",-1,2025-04-16 06:22:06
"I‚Äôve been getting a 404 Client Error on Airbyte saying ‚Äú404 Client Error: Not Found for url: https://mixpanel.com/api/2.0/engage/revenue?project_id={}&from_date={}&to_date={}‚Äù

I‚Äôve been getting this error for the last 4-5 days  even though there‚Äôs been no issue while retrieving the information previously.

The only thing I noted was the data size quadrupled ie Airbyte started sending multiple duplicate values for the prior 4-5 days before the sync job started failing.

Has anybody else been facing a similar issue and were you able to resolve it?",-1,2025-05-27 11:21:35
"What are best practices to promote data pipelines over dev/test/prod environments? How to get data from prod to be able to either debug or create a new feature?

Any recommendations or best practices?

  
thank you",-1,2025-05-27 06:55:59
"Currently I am wondering how other teams do their development and especially testing their pipelines.

I am the sole data engineer at a medical research institute. We do everything on premise, mostly in windows world. Due to me being self taught and having no other engineers to learn from I keep implementing things the same way:

Step 1: Get some source data and do some exploration

Step 2: Design a pipeline and a model that is the foundation for the README file

Step 3: Write the main ETL script and apply some defensive programming principles

Step 4: Run the script on my sample data which would have two outcomes:

1. Everything went well? Okay, add more data and try again!

2. Something breaks? See if it is a data quality or logic error, add some nice error handling and run again!

At some point the script will run on all the currently known source data and can be released. Over the course of the process I will add logging, some DQ checks on the DB and add alerting for breaking errors. I try to keep my README up to date with my thought process and how the pipeline works and push it to our self hosted Gitea.

I tried tinkering around with pytest and added some unit tests for complicated deserialization or source data that requires external knowledge. But when I tried setting up integration testing and end to end testing it always felt like so much work. Trying to keep my test environments up to date while also delivering new solutions seems to always end up with me cutting corners on testing. 

At this point I suspect that there might be some way to make this whole testing setup more reproducable and less manual. I really want to be able to onboard new people, if we ever hire, and not let them face an untestable mess of legacy code.

Any input is highly appreciated!

 ",-1,2025-04-16 20:28:36
"Hello! I have a very good understanding of Google Sheets and Excel but for the workflow I want to create, I think I need to consider learning Big Query or something else similar. 

  
The main challenge I foresee is due to the columnar design (5k-7k columns) and I would really really like to be able to keep this. I have made versions of this using the traditional row design but I very quickly got to 10,000+ rows and the filter functions were too time consuming to apply consistently. 

  
What do you think is the best way for me to make progress? Should I basically go back to school and learn Big Query, SQL and data engineering? Or, is there another way you might recommend? 

  
Thanks so much!",-1,2025-04-16 20:31:06
"Hi data specialist,

with colleagues we are debating what would be the best solution to create list of users-id giving simple criterions.

let's take an example of line we have

    ID,GROUP,NUM
    01,group1,0.2
    02,group1,0.4
    03,group2,0.5
    04,group1,0.6
    

let say we only want the subset of user id that are part of the group1 and that have NUM > 0.3 ; it will give us 02 and 04.

We have currently theses list in S3 parquet (partionned by GROUP, NUM or other dimensionq). We want results in plain CSV files in S3. We have really a lot of it (multi billions of rows). Other constraints are we want to create theses sublist every hours (giving the fact that source are constantly changing) so relatively fast, also we have multiple ""select"" criterions and finally want to keep cost under control.

Currently we fill a big AWS Redshift cluster where we load our inputs from the datalake and make big select to output lists. It worked but clearly show its limits. Adding more dimension will definitely kill it. 

I was thinking this not a good fit as Redshift is a column oriented analytic DB. Personally I would advocate for using spark (with EMR) to directly <filter and produce S3 files. Some are arguing that we could use another Database. Ok but which? (I don't really get the why)

your take?",-1,2025-04-16 21:12:36
"Hi all,

I‚Äôm a Data Engineer working **on-site** (not remote), and I‚Äôm about to request a new workstation. I‚Äôd appreciate your input on:

* **Desktop vs laptop** for heavy data and ML workloads in an office setting
* Recommended **GPU** for data processing and occasional ML
* Your preferred **monitor setup** for productivity (size, resolution, dual screens, etc.)

Would love to hear what‚Äôs worked best for you. Thanks!",-1,2025-05-28 08:57:22
"Hi All, 

So for our ML models we are designing secure data engineering. For our ML use cases we would require data with and without customer PII. 

For now we are maintaining isolated environments for each alongside tokenisation for data that involved PII. 

Now I want to make sure that we scan the data store at each phase of ingestion and transformation. Bronze - Dumb of all data in a blob, Silver - Level 1 transformation, Gold - Level 2 transformation. 

I am trying to introduce data sanitization right when the data is pulled from the database so when it lands in bronze I dont see much PII and keeps reducing down the road. 

I also want to be reviewing the data quality at each stage alongside a lineage map while also identifying any potential bias in the dataset. 

Is there any solution that can help with this ? I know purview can do security scan, quality and lineage but its just too complicated. Any other solutions ? ",-1,2025-05-28 13:18:24
"How conceivable is it‚Äîthat ex software engineers,  maligned by A. I. will flood the DE job markets making it hard to secure employment due to high competition? 

In a way where an aspiring DE looking to break it will now find it near impossible? ",-1,2025-05-28 09:00:12
"Hey everyone, 

we are a company that relies heavy on a so called no-code middleware that combines many different aspects of typical data engineering stuff into one big platform. However we have found ourselves (finally) in the situation that we need to migrate to a lets say more fundamental tech stack that relies more on knowledge about programming, databases and sql. I wanted to ask if someone has been in the same situation and what their experiences have been. Our only option right now is to migrate for business reasons and it will happen, the only question is what we are going to use and how we will use it. 

**Background:**  
We use this platform as our main ""engine"" or tool to map various business proccess. The platform includes creation and management of various kinds of ""connectors"" including Http, as2, mail, x400 and whatnot. You can then create profiles that can get fetch and transform data based on what comes in by one of the connectors and load the data directly into your database, create files or do whatever the business logic requires. The platform provides a comprehensive amount of logging and administration. In my honest opinion, that is quite a lot that this tool can offer. Does anyone know any kind of other tool that can do the same? I heard about Apache Airflow or Apache Nifi but only on the surface. 

The same platform we are using right now has another software solution for building database entities on top of its own database structure to create ""input masks"" for users to create, change or read data and also apply business logic. We use this tool to provide whole platforms and even ""build"" basic websites. 

What would be the best tech stack to migrate to if your goal was to cover all of the above? I mean there probably is not an all in one solution but that is not what we are looking for right now. If you said to me that for example apache nifi in combination with python would be enough to cover everything our middleware provided would be more than enough for me.  
  
What is essential for us is also a good logging capability. We need to make sure that whatever data flows are happening or have happended is comprehensible in case of errors or questions. 

For input masks and simple web platforms we are currently using C# Blazor and have multiple projects that are working very well, which we could also migrate to. ",-1,2025-04-16 15:20:42
"Hi,

I'm looking for sincere advice.

I'm basically a data/analytics engineer. My tasks generally are like this

1. put configurations so that the source dataset can ingest and preprocess into aws s3 in correct file format. I've noticed sometimes filepath names randomly change without warning which would cause configs to change so I would have to be cognizant of that. 

2. the s3 output is then put into a mapping tool (which in my experience is super slow and frequently annoying to use) we have to map source -> our schema

3. once you update things in the mapping tool, it SHOULD export automatically to S3 and show in production environment after refresh, which is usually. However, keyword should. There are times where my data didn't show up and it turned out I have to 'manually export' a file to S3 without being made aware beforehand which files require manual export and which ones occur automatically through our pipeline

4. I then usually have to develop a SQL view that combines data from various sources for different purposes

The issues I'm facing lately....

A colleague left end of last year and I've noticed that my workload has dramatically changed. I've been given tasks that I can only assume were once hers from another colleague. The thing is the tasks I'm given:

1. Have zero documentation. I have no clue what the task is meant to accomplish

2. I have very vague understanding of the source data 

3. Just go off of an either previously completed script, which sometimes suffers from major issues (too many subqueries, thousands of lines of code). Try to realistically manage how/if to refactor vs. using same code and 'coming back to it later' if I have time constraints. After using similar code, randomly realize the requirements of old script changed b/c my data doesn't populate in which I have to ask my boss what the issue 

4. Me and my boss have to navigate various excel sheets and communication to play 'guess work' as to what the requirements are so we can get something out 

5. Review them with the colleague who assigned it to me who points out things are wrong OR randomly changes the requirements that causes me to make more changes and then expresses frustration 'this is unacceptable', 'this is getting delayed', 'I am getting frustrated' continuously that is making me uncomfortable in asking questions.

I do not directly interact with the stakeholders. The colleague I just mentioned is the person who does and translates requirements back. I really, honestly have no clue what is going through the stakeholders mind or how they intend to use the product. All I frequently hear is that 'they are not happy', 'I am frustrated', 'this is too slow'. I am expected to get things out within few hours to 1-2 business days. This doesn't give me enough time to ensure if I made many mistakes in the process. I will take accountability that I have made some mistakes in this process by fixing things then not checking and ensuring things are as expected that caused further delays. Overall, I am under constant pressure to churn things out ASAP and I'm struggling to keep up and feel like many mistakes are a result of the pressure to do things fast.

I have told my boss and colleague in detail (even wrote it up) that it would be helpful for me to: 1. just have 1-2 sentences as to what this project is trying to accomplish 2. better documentation.  People have agreed with me but they have not really done much b/c everybody is too busy to document since once one project is done, I'm pulled into the next. I personally am observing a technical debt problem here, but I am new to my job and new to data engineering (was previously in a different analytics role) so I am trying to figure out if this is a me issue and where I can take accountability or this speaks to broader issues with my team and I should consider another job. I am honestly thinking about starting the job search again in a few months, but I am quite discouraged with my current experience and starting to notice signs of burnout.",-1,2025-04-16 18:40:02
"Part I was super popular, so I figured I'd share Part II: [https://www.tinybird.co/blog-posts/what-i-learned-operating-clickhouse-part-ii](https://www.tinybird.co/blog-posts/what-i-learned-operating-clickhouse-part-ii)",-1,2025-04-16 14:45:54
"https://reddit.com/link/1kxf2ip/video/b77h5x55fi3f1/player

We‚Äôre seeing more and more scenarios where structured/semi-structured search (SQL, Mongo, etc.) must be combined with semantic search (vector, sentiment) to unlock real value.  
  
Take one of our recent projects:

The client wanted to analyze marketing campaign performance by asking flexible, natural questions ‚Äî from: ""What‚Äôs the sentiment around campaign X?"" to ""Pull all clicks by ID and visualize engagement over time on the fly.  
  
""Can't we just plug in an LLM and call it a day?  
  
Well ‚Äî simple integration with OpenAI (or any LLM) won't suffice.  
ChatGPT out of the box might seem to offer both fuzzy and structured queries.   
  
But without seamless integration with:

\- Vector search (to find contextually appropriate semantic data)

\- SQL/NoSQL databases (to access exact, structured/semi-structured data)‚Ä¶you'll soon find yourself limited.

Here‚Äôs why:  
  
1. Size limits ‚Äì LLMs cannot natively consume or reason on enormous datasets. You need to get the proper slice of data ahead of time.  
2. Determinism ‚Äì There is a chance that ""calculate total value since June"" will give you different answers, even if temperature = 0. SQL will not.  
3. Speed limits ‚Äì LLMs are not built for rapid high-scale data queries or real-time dashboards.  
  
In this demo, I‚Äôm showing you exactly how we solve this with a dedicated AI analytics agent for B2B review intelligence:  
  
Agent Setup  
Role: You are a B2B review analytics assistant ‚Äî your mission is to answer any user query using one of two expert tools:  
  
Vector Search Tool ‚Äî Powered by Azure AI Search  
\- Handles semantic/sentiment understanding- Ideal for open-ended questions like ""what do users think of XYZ tool?""  
\- Interprets the user‚Äôs intent and generates relevant vector search queries  
\- Used when the input is subjective, descriptive, or fuzzy  
  
Semi-Structured Search Tool ‚Äî Powered by MongoDB  
\- Handles precise lookups, aggregations, and stats  
\- Ideal for prompts like ""show reviews where RAG tools are mentioned"" or ""average rating by technology""  
\- Dynamically builds Mongo queries based on schema and request context  
\- Falls back to vector search if the structure doesn‚Äôt match but context is still relevant (e.g., tool names or technologies mentioned)  
  
As a result with have hybrid AI agent that reasons like an analyst but behaves like an engineer ‚Äî fast, reliable, and context-aware.",-1,2025-05-28 11:50:58
"Hey guys, I am a recent graduate working in data engineering. The company has poor processes and also poor documentation, the main task that I will be working on is refactoring and optimizing a script that basically re conciliates assets and customers (logic a bit complex as their supply chain can be made off tens of steps).

The current data is stored in Redshift and it's a mix of transactional and master data. I spent a lot of times going through the script (python script using psycopg2 to orchestrate execute the queries) and one of the things that struck me is that there is no incremental processing, each time the whole tracking of the supply chain gets recomputed.

I have poor guidance from my manager as he never worked on it so I am a bit lost on the methodology side. The tool is huge (hundreds of queries with more than 4000 lines, queries with over 10 joins and all the bad practices that you can think of).

TBH I am starting to get very frustrated, all the suggestions are more than welcomed.",-1,2025-04-16 14:14:14
"*Assessment/Iter... is a different term, in this context :-)*

I mean seriously. There's a vast number of data engineers out there in the world, and not that many have even given so much as an inkling to the idea of being the original author ( or a co-author ) of an ""Ace the Data Engineering Assessment"" book yet?

What gives? Alex Xu wrote his book on System Design - Volume 1 and Volume 2 - and so many folks in the world still leverage that. Martin Fowler managed to author Designing Data-Intensive Applications. Gayle authored ""*Cracking the Code Inter...*"".

What's the challenge? Is it the open-ended nature of data engineering that makes writing the books challenging? I've given some thoughts into writing one up myself :-P - it's a gap in the world that someone hasn't addressed yet, and I think someone should.  ",-1,2025-05-28 20:54:20
"Replay isn‚Äôt just about fixing broken systems. It‚Äôs about rethinking how we build them in the first place. If your data architecture is driven by immutable events instead of current state, then replay stops being a recovery mechanism and starts becoming a way to continuously reshape, refine, and evolve your system with zero fear of breaking things.

Let‚Äôs talk about replay :)

**Event sourcing is misunderstood**  
For most developers, event sourcing shows up as a safety mechanism. It‚Äôs there to recover from a failure, rebuild a read model, trace an audit trail, or get through a schema change without too much pain. Replay is something you reach for in the rare cases when things go sideways.

That‚Äôs how it‚Äôs typically treated. A fallback. Something reactive.

But that lens is narrow. It frames replay as an emergency tool instead of something more fundamental. When events are treated as the source of truth, replay can become a normal, repeatable part of development. Not just a way to recover, but a way to refine.

**What if replay wasn‚Äôt just for emergencies?**  
What if it was a routine, even joyful, part of building your system?

Instead of treating replay as a recovery mechanism, you treat it as a development tool. Something you use to evolve your data models, improve your business logic, and shape entirely new views of your data over time. And more excitingly, it means you can derive entirely new schemas from your event history whenever your needs change.

**Why replay is so hard in most setups**  
Here‚Äôs the catch. In most event-sourced systems, **events are emitted after your app logic runs**. Your API gets the request, updates the database, and only then emits a change event. That event is a side effect, not the source of truth.

So when you want to replay, it gets tricky. You need replay-safe logic. You need to carefully version events. You need infrastructure to reprocess historical data. And you have to make absolutely sure you‚Äôre not double-applying anything.

That‚Äôs why replay often feels fragile. It‚Äôs not that the idea is bad. It‚Äôs just hard to pull off.

**But what if you flip the model?**  
What if events come first, not last?

That‚Äôs the approach we took.

A user action, like creating a user, updating an address, or assigning a tag, sends an event. That event is immediately appended to an immutable event store, **and only then** is it passed along to the application API to validate and store in the database.

Suddenly your database isn‚Äôt your source of truth. It‚Äôs just a read model. A fast, disposable output of your event stream.

So when you want to evolve your logic or reshape your data structure, all you have to do is update your flow, delete the old database, and press replay.

That‚Äôs it.

No migrations.  
No fragile ETL jobs.  
No one-off backfills.  
Just replay your history into the new shape.

**Your data becomes fluid**  
Say you‚Äôre running an e-commerce platform, and six months in, you realize you never tracked the discount code a customer used at checkout. It wasn‚Äôt part of the original schema. Normally, this would mean a migration, a painful manual backfill (if the data even still exists), or writing a fragile script to stitch it in later, assuming you‚Äôre lucky enough to recover it.

But with a full event history, you don‚Äôt need to hack anything.

You just update your flow logic to extract the discount code from the original checkout events. Then replay them.

Within minutes, your entire dataset is updated. The new field is populated everywhere it should have been, as if it had been there from day one.

**Your database becomes what it was always meant to be**  
A cache.  
Not a source of truth.  
Something you can throw away and rebuild without fear.  
You stop treating your schema like a delicate glass sculpture and start treating it like software.

**Replay unlocks AI-native data (with MCP Servers)**  
Most application databases are optimized for transactions, not understanding. They‚Äôre normalized, rigid, and shaped around application logic, not meaning. That‚Äôs fine for serving an app. But for AI? Nope.

Language models thrive on context. They need denormalized, readable structures. They need relationships spelled out. They need the why, not just the what.

When you have an event history, not just state but actions and intent. You can replay those events into entirely new shapes. You can build read models that are tailored specifically for AI: flattened tables for semantic search, user-centric structures for chat interfaces, agent-friendly layouts for reasoning.

And it‚Äôs not just one-and-done. You can reshape your models over and over as your use cases evolve. No migrations. No backfills. Just a new flow and a replay.

What is even more interesting is that with the help of MCP Servers AI can help you do this. By interrogating the event history with natural language prompts, it can suggest new model structures, flag gaps, and uncover meaning you didn‚Äôt plan for. It‚Äôs a feedback loop: replay helps AI make sense of your data, and AI helps you decide how to replay.

And none of this works without events that store intent. Current state is just a snapshot. Events tell the story.

**So, why doesn‚Äôt everyone build this way?**  
Because it‚Äôs hard. You need immutable storage. Replay-safe logic. Tools to build and maintain read models. Schema evolution support. Observability. Infrastructure to safely reprocess everything.

The architecture has been around for a while ‚Äî Martin Fowler helped popularize event sourcing nearly two decades ago. But most teams ran into the same issue: implementing it well was too complex for everyday use.

That‚Äôs the reason behind the Flowcore Platform To make this kind of architecture not just possible, but effortless. Flowcore handles the messy parts. The ingestion, the immutability, the reprocessing, the flow management, the replay. So you can just build. You send an event, define what you want done with it, and replay it whenever you need to improve.",-1,2025-04-15 15:55:44
"All of us have been using Dropbox or Google Drive for storing our stuff online, right? They allow us to share files with others via URLs or email address based permissions, and in case of Google Drive, the entire workspace can be dedicated to an organization.

How to create one such system from scratch? The simplest way I can think of - is implement a raw object storage first (like S3 or Backblaze) that takes care of file replication (either directly or via Reed Solon Erasure Codes) - and once done, use that everywhere along with file metadata (like folder structure, permissions, etc.) stored in a DB to give the user an illusion of their own personal har disk for storing files.

Is this a good way? Is that how, for example, Google Drive works? What other ways are there to make a distributed file storage system like Dropbox or Google Drive?",-1,2025-05-25 06:16:40
"I am looking at some options for my company for data observability, I want to see if anyone has experience with tools like Bigeye and Soda, Monte Carlo..? What has your experience been like with them? are there good? What is lacking with those tools? what can you recommend... Basically trying to find the best tool there is, for pipelines, so our engineers do not have to keep checking multiple pipelines and control points daily (weekends included), lmk if yall do this as well lol. But I really care a lot about knowing what the tool has in terms of weaknesses, so I won't assume it does that later to only find out after integrating it lacks a pretty logical feature...",-1,2025-04-20 02:47:54
"Hi,

We are a traditional software engineering team having sole experience in developing web services so far using Java with Spring Boot. We now have a new requirement in our team to engineer data pipelines that comply with standard ETL batch protocol. 

Since our team is well equipped in working with Java and Spring Boot, we want to continue using this tech stack to establish our ETL batches. We do not want to pivot away from our regular tech stack for ETL requirements. We found Spring Batch  helps us to establish ETL compliant batches without introducing new learning friction or $ costs.

Now comes the main pain point that is dividing our team politically. 

Some team members are advocating towards decentralised scripts that are knowledgeable enough to execute independently as a standard web service in tandem with a local cron template to perform their concerned function and operated manually by hand on each of our horizontally scaled infrastructure. Their only argument is that it prevents a single point of failure without caring for the overheads of a batch manager. 

While the other part of the team wants to use the remote partitioning job feature from a mature batch processing framework (Spring Batch for example) to achieve the same functionality as of the decentralized cron driven script but in a distributed fashion over our already horizontally scaled infrastructure to have more control on the operational concerns of the execution. Their argument is deep observability, easier run and restarts, efficient cron synchronisation over different timezones and servers while risking a single point of failure.

We have a single source of truth that contains the infrastructure metadata of all servers where the batch jobs would execute so leveraging it within a batch framework makes more sense IMO to dynamically create remote partitions to execute our ETL process.

I would like to get your views on what would be the best approach to handle the implementation and architectural nature of our ETL use case?

We have a downstream data warehouse already in place for our ETL use case to write data but its managed by a different department so we can't directly integrate into it but have to do it with a non industry standard company wide red tape bureaucratic process but this is a story for another day.",-1,2025-05-25 15:20:26
"Hi guys.

I just came out from an int. with a software development company for a Data Engineering position.

I received feedback (which surprised me tbh) that said that ""I must have experience with Airflow, Spark, Kafka"" and so on ""because it's what the market is expecting you to know"".

My question is, how do you handle getting experience with these tool when Business doesn't need to? More often than not, companies don't need to deploy an Airflow server for Orchestration or a Kafka one for Streaming because they don't need to do Streaming, or even the Orchestration could be done by using Glue or ADF (for example). I see many post regarding poorly architectured solutions that rely on pyspark when the processing could've been done using pandas, and so on.

So, how do maintain relevant in a Business that apparently needs those tools, when in reality, a large part of companies doesn't need them at all, or even the tech stack is not in favor of using those tools?

Thanks.",-1,2025-05-25 16:00:52
"My background is more on the data science/stats side (with some exposure to foundational SWE concepts like data structures & algorithms) but my day-to-day in my current role involves a lot of writing data pipelines to handle large datasets. 

I mostly use SQL/Pandas/PySpark. I‚Äôm at the point where I can write correct code that gets to the right result with a passable runtime, but I want to ‚Äúlevel up‚Äù and gain a better understanding of what‚Äôs happening under the hood so I know how to optimize. 

Are there any good resources for practicing handling cases where your dataset is extremely large, or reducing inefficiencies in your code (e.g. inefficient joins, suboptimal queries, suboptimal Spark execution plans, etc)?

Or books and online resources for learning how these tools work under the hood (in terms of how they access/cache data, why certain things take longer, etc)?",-1,2025-04-23 00:41:21
"Hi,

I am new to data engineering and am trying to learn it by myself. 

So far, I have learnt that we generally process data in three stages:
- bronze/ raw/ a snapshot of original data with very little modification. 

- Silver/ performing transformations for our business purpose

- Gold / dimensionally modelling our data to be consumed by reporting tools. 
----------
I used :
- Azure Data Factory to ingest data into bronze,
 then 

- Azure DataBricks to store the raw data as delta tables and them perfomed transformations on that data in Silver layer

- Modelled Data for Gold Layer
-----------
I want to understand, how an actual real world project is executed. I see companies processing petabytes of data. How do you do that at your job?

Would really be helpful to get an overview of your execution of a project. 

Thanks.  ",-1,2025-05-23 11:39:14
"Hi, my company is using Hightouch for reverse ETL of tables from Redshift to Hubspot. Hightouch is great in its simplicity and non technical approach to integration so even business users can do the job. You  just have to provide them the table in Redshift and they can setup the sync logic and field mapping by a point and click interface. I as a data engineer can instead focus my time and effort on ingestion and data prep.

But we are using the Hightouch to such an extent that we are being force over to a more expensive price plan, 24 000$ annually.

What tools are there that have similar simplicity but have cheaper costs?",-1,2025-04-22 09:27:35
"As people complain about visual tools removing Data Engineering jobs, I'm wondering if you agree that there are cycles to the industry:

1 - we need someone to organize and control access (e.g. DBAs)
2 - we're losing velocity, fire the data people and let people make their own integrations/tables/schemas
3 - we just had another outage, we're not scaling well, we need expert advice
4 - lets create a team to enable everyone to use the data stores efficiently and correctly
5 - but now there are great tools to handle that automatically, do we really need this whole team?

I've seen at least two phases of this, first with tools like SSIS, and now with all these generative tools for writing integrations and queries.",-1,2025-05-23 21:23:26
"I have the data in id(SN), date, open, high.... format. Got this data by scraping a stock website. But for my machine learning model, i need the data in the format of 30 day frame. 30 columns with closing price of each day. how do i do that?  
chatGPT and claude just gave me codes that repeated the first column by left shifting it. if anyone knows a way to do it, please helpü•≤",-1,2025-04-22 10:22:39
"My company needs to normalize and structure blood test data that comes in PDF format, containing patient data, results, reference values, and status (when outside reference ranges). The PDFs don't have a common template because tests vary from lab to lab, and even within the same lab there are tests written with and without accents or different units of measurement... What strategy would you use for this challenge?",-1,2025-05-23 21:29:06
"Hey guys!

I just wrapped up a data analysis project looking at publicly available development permit data from the city of Fort Worth.

I did a manual export, cleaned in Postgres, then visualized the data in a Power Bi dashboard and described my findings and observations.

This project had a bit of scope creep and took about a year. I was between jobs and so I was able to devote a ton of time to it.

The data analysis here is part 3 of a series. The other two are more focused on history and context which I also found super interesting.

I would love to hear your thoughts if you read it. 

Thanks ! 

https://medium.com/sergio-ramos-data-portfolio/city-of-fort-worth-development-permits-data-analysis-99edb98de4a6  ",-1,2025-05-23 17:24:13
Bit of a background: I am currently working in Amazon as a business intelligence engineer. I plan on eventually switching to DE in 2-3 years time but first would like to gain some experience in my current role first. The main reason for doing these certifications is not only to help bolster my internal move to a DE role in amazon but outside as well when I move out of Amazon in the future. I have minimal interaction with AWS data tools except for quicksights (visualization tool). AWS DE certification is the ultimate prize but should I do the AWS SAA first? I'm still relatively new in the BIE role and have lots to learn about DE practices and core technical skills around that role. I also already have an AWS CCP certification but we all know how basic that is compared to SAA.,-1,2025-05-23 13:27:48
"I am working on an application that primarily pulls data from some local sensors (Temperature, Pressure, Humidity, etc). The application will get this data once every 15 minutes for now,  then we will aim to increase the frequency later in development.  I need to be able to store this data. I have only worked with Relational databases (Transact SQL, or Azure SQL) in the past, and this is the current choice, however, it feels overkill and rather heavy for the application. There would only really be one table of data, which would grow in size really fast.

I was wondering if there was a better way to store this sort of data that means that I can better manage this sort of data. In the future, there is a plan to build a front end to this data or introduce an API for Power BI or other reporting front ends.  ",-1,2025-04-22 19:45:25
"I realized today that I've barely touched SQL in the last 2 years. I've done some basic queries in BigQuery on a few occasions. I recently wanted to do some JOINs on a personal project and realised I kinda suck at them and I actually had to refresh my knowledge on some basics related to HAVING, GROUP BY etc. It just wasn't a significant part of my work over the last 2 years. In fact I use some python scripts I made a long time ago for executing a series of statements so I almost completely erradicated using SQL from my day-to-day.


Sometimes I feel like I'd join a call with my colleagues or people more junior than me and they could pull up anything and start blasting away any type of code or chain of terminal commands from memory  - sometimes I feel like I'm a retired software engineer and a lot of these things are a distant memory to me that I have to refresh every time I need something.


Part of the ""problem"" is that I got abstracted from a lot of things with UI tools. I barely use the terminal for managing or navigating our cloud platform because the UI fits most of my needs, so I couldn't really help you check something in the cluster using the terminal without reading the docs. I also made some scripts for interacting with our cloud so I don't have to execute long commands in the terminal. I also use a GUI tool for  git so I couldn't help you rebase in the terminal without revising how the process goes in the terminal.


TL;DR I'm approaching 7 years in this career and I use various abstractions like GUI tools and custom scripts to make my life easier and I dont keep my knowledge fresh on basics. Considering the expectations from someone with my seniorty - am I sabotaging myself in some way or am I just overthinking this?",-1,2025-04-22 09:59:39
"Copy pasting text from LinkedIn post guys‚Ä¶

Long story short:
Over the course of my career, every time I had a query to test, I found myself spamming the ‚ÄúRun‚Äù button in DataGrip or re‚Äëwriting the same boilerplate code over and over again. After some Googling, I couldn‚Äôt find an easy‚Äëto‚Äëuse PostgreSQL benchmarking library‚Äîso I wrote my own.
(Plus, `pgbenchmark` was such a good name that I couldn't resist writing a library for it)

It still has plenty of rough edges, but it‚Äôs extremely easy to use and packed with powerful features by design. Plus, it comes with a simple (but ugly) UI for ad‚Äëhoc playground experiments.

Long way to go, but stay tuned and I'm ofc open for suggestions and feature requests :)

Why should you try `pgbenchmark`?

‚Ä¢  README is very user-friendly and easy to follow <3
‚Ä¢ ‚öôÔ∏è Zero configuration: Install, point at your database, and you‚Äôre ready to go
‚Ä¢ üóø Template engine: Jinja2-like template engine to generate random queries on the fly
‚Ä¢ üìä Detailed results: Execution times, min-max-average-median, and percentile summaries  
‚Ä¢ üìà Built‚Äëin UI: Spin up a simple, no‚ÄëBS playground to explore results interactively. [WIP]

PyPI: https://pypi.org/project/pgbenchmark/
GitHub: https://github.com/GujaLomsadze/pgbenchmark

",-1,2025-04-21 17:02:02
"Hey all, so I setup an export from Salesforce to Bigquery, but I want to clean data from product and other sources and RELOAD it back into salesforce. For example, saying this customer opened X emails and so forth. 

I've done this with reverse ETL tools like Skyvia in the past, BUT after setting up the transfer from SFDC to bigquery, it really seems like it shouldn't be hard to go in the opposite direction. Am I crazy? [This is the tutorial](https://cloud.google.com/bigquery/docs/salesforce-transfer) I used for SFDC data export, but couldn't find anything for data import. ",-1,2025-05-02 04:21:44
"Based on my research:

1. TOGAF seems to be the go-to for enterprise architecture and might give me a broader IT architecture framework. [TOGAF](https://www.opengroup.org/certifications/togaf-certification-portfolio?utm_source=chatgpt.com)
2. CDMP is more focused on data governance, metadata, and overall data management best practices. [CDMP](https://www.dama.org/cpages/cdmp-information?utm_source=chatgpt.com)

I‚Äôm a data engineer with a few certs already (Databricks, dbt) and looking to expand into more strategic roles‚Äîconsulting, data architecture, etc. My company is paying for the certification, so price is not a factor.

Has anyone taken either of these certs?

* Which one did you find more practical or respected?
* Was one of them outdated material? Did you gain any value from it?
* Which one did clients or employers actually care about?
* How long did it take you and were there available study materials?

Would love to hear honest thoughts before spending the next couple of months on it haha! Or maybe there is another cert that is more valueable for learning architecture/data management? Thanks!",-1,2025-04-21 16:25:19
"Hey all, 

I am working on a project right now, it was supossed to be culmination of everything I learnt so far. Applying stuff I learnt in courses 

But as I've gone through the project I've gone through writing the code but I keep on bumping into things that'll improve my project e.g. Threading, Spark, Great Expectations, maybe FastAPI for a front end? 

Not to mention that in order to use a tool you intend to you have to learn something else, which means learning another thing, which means watching a video and down the rabbit hole you go. An example for me was having to learn Docker in order get Airflow working properly. 

I plan on finishing the project but adding on bits and pieces as I go on. However this will mean I won't be applying my skills on a diverse range of use cases. 

My goal is to kick-start a DE career in the distant future. 

So I was wondering what is the best approach? Iteration or finalisation?",-1,2025-05-24 20:47:44
"Hi Everyone,

I have overall 2 years of experience as a Data engineer.
I have been given one task to extract the data from SAP S4 to data lake gen2.
Current architecture is like below-
SAP S4 (using SLT)- BW HANA DB - ADLS Gen2(via ADF).
Can you guys help me to understand how can I extract the data. 
I have no idea about SAP source. How to handle data and CDC/SCD for incremental load.",-1,2025-04-21 14:49:32
"Hello all, I have created a backfill for a table which is about 1gb and tho the backfill finished very quickly, I am still having problems querying the database as the data is in buffering (Stream Buffer). How can I speed up the buffering and make sure the data is ready to query? 

Also, when I query the data sometimes I get the query results and sometimes I don't (same query), this is happening randomly, why is this happening?

P.S., We usually change the staleness limit to 5 mins, now sure what effect this has on the buffering tho, my rationale is, since the data is considered to be so outdated, it will get a priority in system resources when it comes to buffering. But, is there anything else we can do?

",-1,2025-04-21 06:41:31
"We are working on a chat application for enterprise (imagine Google Workspace chat or Slack kinda application - for desktop and mobile). Of course we are just getting started, so one might suggest choosing a barebone DB and some basic tools to launch the app, but anticipating traffic, we want to distill the best knowledge available out there and choose the best stack to build our product from the beginning.

For our chat application, where all typical user behaviors are there - messages, spaces, ""last seen"" or ""active"" statuses, message notifications, read receipts, etc. we need to choose a database to store all our chats. We also want to enable chat searches, and since search will inevitably lead to random chats, we want that perf to be consistently excellent.

We are planning to use Django (with channels) as our backend. What database is recommended to use with Django to persist the messages? I read that Discord used to use Cassandra, but then it started acting up due to garbage collection, so they switched rto Scylla, and they are very happy with trillions of messages on it. Is ScyllDB a good candidate for our purpose to use with Django? Do these two work together well? Can MongoDB do it (my preferred choice, but I read that it starts acting up with high number of reads or writes at the same time - which would be a basic use case for enterprise chat scenario)?",-1,2025-05-24 20:23:28
"Hi, I am about to finish my masters in data science from a tier 2 university in UK.

Ideas for Projects (Final Sem):

‚¶Å	Forecasting Hospital Bed Demand Using Public Health and Seasonal Illness Data

‚¶Å	NHS Chatbot: AI-Powered Symptom Triage and Health Information System

‚¶Å	Early Detection of Respiratory Illness Patterns Using Urban Air Quality and Emergency Hospital Visit Data

‚¶Å	Predictive Maintenance for Wind Turbines Using IoT Sensor Data  

‚¶Å	Predicting Road Surface Deterioration Using Weather and Traffic Data

‚¶Å	Traffic Sign Recognition: Real-Time Detection and Classification for Autonomous Vehicles 

‚¶Å	Optimizing Urban Heat Island (UHI) Mitigation Using Remote Sensing, Land Use, and Energy Consumption Data 

‚¶Å	British Sign Language (BSL) Recognition: Real-Time Gesture-to-Text Translation

‚¶Å	Predictive Structural Health Monitoring of Bridges Using IoT Sensor Data

These are the ideas I came up with to do my final project on, can anyone suggest if they are actually doable or not, and will they hold relevance when it comes to making your CV good for the job?? Yeah, which one should I choose??",-1,2025-05-24 06:48:41
"I'm currently learning data analyzing and I'm playing around with a covid-19 vaccination dataset that has been purposefully modified to have errors I'm to find and take care of.

The dataset has these type of coulmns: Country, FirstDose, SecondDose, DoseAdditional1-5(Seperate for each), TargetGroup and the type of vaccine. Each row is a report from a country for a specific week.  there are multiple entries from the same country on the same week since Targetgroup and vaccine change. My biggest problem when trying to clean the data is the TargetGroup column as it has quite a lot of different values such as ALL(18+), Age<18, HCW, LTCF, Age0\_4, Age5\_9, Age10\_14, Age15\_17 and some others. The thing is different countries use different groups when reporting their values so one country might use the ""ALL"" value for their adults, others use the seperate age groups AND the  ALL, others don't use all at all and when trying to get the total doses administired from a country I get double reported ones for some and when try to take care of it by making logic for what targetgroups to add I instead get underreported values.",-1,2025-05-24 14:08:35
"I don‚Äôt understand when anyone would use a non acid compliant DB. Like I understand that they are very fast can deliver a lot of data and xyz but why is it worth it and how do you make it work ?

Like is it by a second validation steps ? Instead of just writing the data all of your process write, then wait to validate if the data is store somewhere ? 

Like is it because the data itself isn‚Äôt valuable enough that even if you lost the data from one transaction it doesn‚Äôt matter ?

Like I know most social platforms use non acid compliant DB like Cassandra for example. But what happen under the hood ? Let‚Äôs say a user post something on the platform, it doesn‚Äôt just crash or say ‚Äúsent‚Äù and then it‚Äôs maybe not. Are there process to ensure that if something goes wrong the app handles it or this because this doesn‚Äôt happen very often nobody care ? Like the use will repost it‚Äôs thing if it didn‚Äôt work
Is the user or process alerted in such case and how ?

For example if this happen every 500 millions inserts and I have 500 billions records how could I even trust my data ? 

So yeah a lot of scattered question but I think the general idea is shared.",-1,2025-04-21 01:20:15
"Got a meeting coming up with high profile data analysts at my org that primarily use SAS which doesn‚Äôt like large CSV or parquet (with their current version) drawing from MSSQL/otherMScrap. I can give them all their data, daily, (5gb parquet or whatever that is ‚Äîmore‚Äî as csv) right to their doorstep in secured Shaerpoint/OnDrive folders they can sync in their OS.

Their primary complaint is slowness of SAS drawing data.  They also seem misguided with their own MSSQL DBs. Instead of using schemas, they just spin up a new DB. All tables have owner DBO. Is this normal? They don‚Äôt use Git. My heart wants to show them so many things:

DataWrangler in VS Code
DuckDB in DBeaver (or Harelquin, Vim-dadbod, the new local Motherduck UI)
Streamlit
pygwalker

Our org is pressing hard for them to adapt to using PBI/Fabric, and I feel they should go a different direction given their needs (speed), ability to upskill (they use SAS, Excel, SSMS, Cognos‚Ä¶ they do not use VS Code/any-IDE, Git, Python), and constraints (high workload, limited and fixed staff & $. Public Sector, HighEd.

My boss recommended I show them VS Code Data Wrangler. Which is fine with me‚Ä¶but they are on managed machines, have never installed/used VS Code, but let me know they ‚Äúthink its in their software center‚Äù, god knows what that means.

I‚Äôm a little worried if I screw this meeting up,  I‚Äôll kill any hope these folks would adapt/evolve, get with the times. There‚Äôs queries that take 45 min on their current setup that are sub-second on parquet/DuckDB. And as retarded as Fabric is, it‚Äôs also complicated. IMO, more complicated than the awesome FOSS stuff heavily trained by LLMs. I really think DBT would be a game changer too, but nobody at my org uses anything like it. And notebook/one-off development vs. DRY is causing real obstacles.

You guys have any advice? Where are the women DE‚Äôs? This is an area I‚Äôve failed far more, and more recent, than I‚Äôve won. 

If this comes off smug, then I tempt the Reddit gods to roast me.",-1,2025-04-20 06:24:39
"I'm curious as to how many data models you build in a day or week and why

Do you think the number of data models per month can be counted as your KPI?",-1,2025-05-25 03:23:18
"https://preview.redd.it/pefijjqa403f1.png?width=1138&format=png&auto=webp&s=d26e72f944bde942a6246067471b46554d27ad74

Granted this was data from 2023-2024, but its still strange. Why did data engineers get hit the hardest?

Source: [https://bloomberry.com/how-ai-is-disrupting-the-tech-job-market-data-from-20m-job-postings/](https://bloomberry.com/how-ai-is-disrupting-the-tech-job-market-data-from-20m-job-postings/)",-1,2025-05-25 22:08:53
"Hello everyone! I am an early career SWE (2.5 YoE) trying to land an early or mid-level data engineering role in a tech hub. I have a Python project that pulls dog listings from one of my local animal shelters daily, cleans the data, and then writes to an Azure PostgreSQL database. I also wrote some APIs for the db to pull schema data, active/recently retired listings, etc. I'm at an impasse with what to do next. I am considering three paths:

1. Build a frontend and containerize. Frontend would consist of a Django/Flask interface that shows active dog listings and/or links to a Tableau dashboard that displays data on old listings of dogs who have since left the shelter.

2. Refactor my code with PySpark. Right now I'm storing data in basic Pandas dataframes so that I can clean them and push them to a single Azure PostgreSQL node. It's a fairly small animal shelter, so I'm only handling up to 80-100 records a day, but refactoring would at least prove Spark skills.

3. Scale up and include more shelters (would probably follow #2). Right now, I'm only pulling from a single shelter that only has up to ~100 dogs at a time. I could try to scale up and include listings from all animal shelters within a certain distance from me. Only potential downside is increase in cloud budget if I have to set up multiple servers for cloud computing/db storage.

Which of these paths should I prioritize for? Open to suggestions, critiques of existing infrastructure, etc.",-1,2025-05-25 00:25:44
"Hello fellow data engineers,

Hoping for some guidance on how to evaluate an offer I just got from Amazon.

Currently working hybrid (1-2 days), \~120k in VHCOL city, offer is for \~160 in HCOL city.

My current job has been alright, but I am a team of one, and there is very little ""data engineering"" to do around here. Feel a little bit stagnant in that regard. Often just uploading Excel files and running some stored procedure/ETL. I'm looking at around 35 hours a week, pretty lax.

Not sure what to expect at Amazon, 50 hours a week, 60? I know the experience would probably be huge for my career, but not sure if I'm willing to pay with my life. I am also aware that I would go from hardly going into the office to going in every day.

Any current or prior Amazon DE's that could weigh in here? Am I walking into a death trap?",-1,2025-05-23 20:52:57
"I‚Äôm exploring open-source replacements for the following tools:
	‚Ä¢	SQL Server as data warehouse 
	‚Ä¢	SSAS (Tabular/OLAP)
	‚Ä¢	SSIS
	‚Ä¢	Power BI
	‚Ä¢	Informatica

What would you recommend as better open-source tools for each of these?

Also, if a company continues to rely on these proprietary tools long-term, what kind of problems might they face ‚Äî in terms of scalability, cost, vendor lock-in, or anything else?

Looking to understand pros, cons, and real-world experiences from others who‚Äôve explored or implemented open-source stacks. Appreciate any insights!
",-1,2025-04-17 12:12:16
"If your ELT contract is gonna end in the next 3-6months, I would love to connect. Dm me or comment and i will reach out to you.",-1,2025-05-02 04:49:54
"Hello

I need a sanity check.

I am educated and work in an unrelated field to DE. My IT experience comes from a pure layman interest in the subject where I have spent some time dabbing in python building scrapers, setting up RDBs, building scripts to connect everything and then building extraction scripts to do analysis. Ive done some scripting at work to automate annoying tasks. That said, I still consider myself a beginner.

At my workplace we are a bunch of consultants doing work mostly in excel, where we get lab data from external vendors. This lab data is then to be used in spatial analysis and comparison against regulatory limits. 

I have now identified 3-5 different ways this data is delivered to us, i.e. ways it could be ingested to a central DB. Its a combination of APIs, emails attachments, instrument readings, GPS outputs and more. Thus, Im going to try to get a very basic ETL pipeline going for at least one of these delivery points which is the easiest, an API.

Because of the way our company has chosen to operate, because we dont really have a fuckton of data and the data we have can be managed in separate folders based on project/work, we have servers on premise. We also have some beefy computers used for computations in a server room. So i could easily set up more computers to have scripts running. 

My plan is to get a old computer up and running 24/7 in one of the racks. This computer will host docker+dagster connected to a postgres db. When this is set up il spend time building automated extraction scripts based on workplace needs. I chose dagster here because it seems to be free in our usecase, modular enought that i can work on one job at a time and its python friendly. Dagster also makes it possible for me to write loads to endpoint users who are not interested in writing sql against the db.  Another important thing with the db on premise is that its going to be connected to GIS software, and i dont want to build a bunch of scripts to extract from it.

Some of the questions i have:

* If i run docker and dagster (dagster web service?) setup locally, could that cause any security issues? Its my understanding that if these are run locally they are contained within the network
* For a small ETL pipeline like this, is the setup worth it? 
* Am i missing anything?

",-1,2025-04-27 13:12:48
"What are the current trends now? I hadn't heard a lot of data governance lately, is this business still growing and in demand? Someone please share news :)",-1,2025-05-01 12:35:49
"I am working with a customer for a use case , wherein they are would like to keep on prem for sensitive loads and cloud for non sensitive workloads . Basically they want compute and storage to be divided accordingly but ultimately the end users should one unified way of accessing data based on RBAC.

I am thinking I will suggest to go for spark on kubernetes for sensitive workloads that sits on prem and the non-sensitive goes through spark on databricks. For storage , the non sensitive data will be handled in databricks lakehouse (delta tables) but for sensitive workloads there is a preference secnumcloud storages. I don‚Äôt have any idea on such storage as they are not very mainstream. Any other suggestions here for storage ? 

Also for the final serving layer should I go for a semantic layer and then abstract the data in both the cloud and on prem storage ? Or are there any other ways to abstract this ?",-1,2025-04-17 07:36:53
"Hi all, I‚Äôm building a data pipeline where sensor data is published via PubSub and processed with Apache Beam. Each producer sends 100 sensor values every 10 ms (100 Hz). I expect up to 10 producers, so ~30 GB/day total. Each producer should write to a separate table (no cross-correlation).

Requirements:

‚Ä¢	Scalable (horizontally, more producers possible) 

‚Ä¢	Low-maintenance / serverless preferred

‚Ä¢	At least 1 year of retention

‚Ä¢	Ability to download a full day‚Äôs worth of data per producer with a button click

‚Ä¢	No need for deep analytics, just daily visualization in a web UI

BigQuery seems like a good fit due to its scalability and ease of use, but I‚Äôm wondering if there are better alternatives for long-term high-frequency time-series data. Would love your thoughts!",-1,2025-04-17 14:56:02
"Anyone else find that building reliable LLM applications involves managing significant complexity and unpredictable behavior?

It seems the era where basic uptime and latency checks sufficed is largely behind us for these systems.

Tracking response quality, detecting hallucinations¬†*before*¬†they impact users, and managing token costs effectively ‚Äì key operational concerns for production LLMs. All needs to be monitored...

There are *so many* tools, every day a new shiny object comes up - **how do you go about choosing your tracing/ observability stack?**

Honestly, I wasn't sure how to go about building evals and tracing in a good way.  
I reached out to a friend who runs one of those observability startups.

That's what he had to say -

The core message was that robust observability requires multiple layers.  
**1. Tracing**¬†(to understand the full request lifecycle),  
**2. Metrics**¬†(to quantify performance, cost, and errors),  
**3 .Quality/Eval**¬†evaluation (critically assessing response validity and relevance),  
4. and¬†**Insights**¬†(to drive iterative improvements - ie what would you do with the data you observe?).

All in all - *how do you go about setting up your approach for LLMObservability?*

Oh, and the full conversation with Traceloop's CTO about obs tools and approach [is here](https://youtu.be/-Bbvd8cbjac) :)

[thanks luminousmen for the inspo!](https://preview.redd.it/e7iroq4kseve1.jpg?width=1536&format=pjpg&auto=webp&s=dd8525d5c3d85ea7fdaf447ec836966f8aca946a)",-1,2025-04-17 15:40:20
"I want a DE position where I can actually grow my technical chops instead of working on dashboards all day.  

  
Do positions like these exists?

|Role #|High‚Äësignal job‚Äëtitle keywords|Must‚Äëhave skill keywords|
|:-|:-|:-|
|**1¬†‚Äî¬†Real‚ÄëTime¬†Streaming Platform Engineer**|`Streaming¬†Data¬†EngineerReal‚ÄëTime¬†Data¬†EngineerKafka/Flink¬†EngineerSenior¬†Data¬†Engineer¬†‚Äì¬†StreamingEvent¬†Streaming¬†Platform¬†Engineer`,  ,  ,  ,  |Kafka, Flink, ksqlDB, Exactly‚Äëonce, JVM tuning, Schema¬†Registry, Prometheus/OpenTelemetry, Kubernetes/EKS, Terraform, CEP, Low‚Äëlatency|
|**2¬†‚Äî¬†Lakehouse Performance & Cost‚ÄëOptimization Engineer**|`Lakehouse¬†Data¬†EngineerBig¬†Data¬†Performance¬†EngineerData¬†Engineer¬†‚Äì¬†Iceberg/DeltaSenior¬†Data¬†Engineer¬†‚Äì¬†Lakehouse¬†OptimizationCloud¬†Analytics¬†Engineer`,  ,  ,  ,  |Apache¬†Iceberg, Delta¬†Lake, Spark Structured¬†Streaming, Parquet, AWS¬†S3/EMR, Glue¬†Catalog, Trino/Presto, Data‚Äëskipping, Cost¬†Explorer/FinOps, Airflow, dbt|
|**3¬†‚Äî¬†Distributed¬†NoSQL & OLTP‚ÄëOptimization Engineer**|`NoSQL¬†Data¬†EngineerScyllaDB/Cassandra¬†EngineerOLTP¬†Performance¬†EngineerSenior¬†Data¬†Engineer¬†‚Äì¬†NoSQLDistributed¬†Systems¬†Data¬†Engineer`,  ,  ,  ,  |ScyllaDB/Cassandra, Hotspot¬†tuning, NoSQLBench, Go¬†or¬†Java, gRPC, Debezium¬†CDC, Kafka, P99¬†latency, Prometheus/Grafana, Kubernetes, Multi‚Äëregion replication|",-1,2025-04-16 19:50:32
"Hey guys,

I‚Äôve just published my new Udemy course:  
**‚ÄúBuilding a Simple Data Analyst AI Agent with Llama and Flask‚Äù**

It‚Äôs a hands-on beginner-friendly course where you learn:

* Prompt engineering (ICL, CoT, ToT)
* Running an open-source LLM locally (Llama)
* Building a basic Flask app that uses AI to answer questions from a Postgres database (like a mini RAG system)

It might be for you if you‚Äôre curious about LLMs, RAG and want to build something simple and real.

Here‚Äôs a free coupon (limited seats):  
üëâ [https://www.udemy.com/course/building-a-simple-data-analyst-ai-agent-with-llama-and-flask/?couponCode=LAUNCH](https://www.udemy.com/course/building-a-simple-data-analyst-ai-agent-with-llama-and-flask/?couponCode=LAUNCH)

Would love to hear your feedback. If you enjoy it, a 5-star review would help a lot üôè  
Thanks and happy building!",-1,2025-05-01 14:04:16
"I'm working on building a data pipeline where I need to implement Change Data Capture (CDC), but I don't have permission to modify the source system at all ‚Äî no schema changes (like adding `is_deleted` flags), no triggers, and no access to transaction logs.

I still need to detect **deletes** from the source system. Inserts and updates are already handled through timestamp-based extracts.

Are there best practices or workarounds others use in this situation?

So far, I found that comparing primary keys between the source extract and the warehouse table can help detect missing (i.e., deleted) rows, and then I can mark those in the warehouse. Are there other patterns, tools, or strategies that have worked well for you in similar setups?

For context:

* Source system = \[insert your DB or system here, e.g., PostgreSQL used by Odoo\]
* I'm doing periodic batch loads (daily).
* I use \[tool or language you're using, e.g., Python/SQL/Apache NiFi/etc.\] for ETL.

Any help or advice would be much appreciated!",-1,2025-04-21 06:31:42
"I wrote a new article about Storage-Compute Separation: a deep dive into the concept of storage-compute separation and what it means for your business.

  
If you're into this too or have any thoughts, feel free to jump in ‚Äî I'd love to chat and exchange ideas!",-1,2025-06-04 14:40:49
"I am building a streaming pipeline in GCP for work that works like this:

Cloud Run Service --> PubSub --> Dataflow --> BigQuery


My Cloud Run Service when it starts, it watches a collections with changeStreams and then published all changes into a PubSub topic. Dataflow then streams that messages into BQ.

The service runs in VPC connector where the linked IP is whitelisted in mongodb.

My issue is with my service! It keeps failing die to timeouts when trying to publish to pubsub after a few hours running.

Ive tried batching the publishing, extending the timeout, retries.


Any suggestion? Have you done something similar?",-1,2025-04-12 20:51:58
"Hi all

I‚Äôve got two pipelines built using dbt where I have bunch of sql and python models. I‚Äôm looking to migrate both pipelines to PySpark based pipeline using EMR cluster in AWS.

I‚Äôm not worried about managing cluster but I‚Äôm here to ask your opinion about what you think would be a good migration plan? 
I‚Äôve got around 6 engineers who are relatively comfortable with PySpark.


If I were to ask you what would be your strategy to do the migration what would it be? 

These pipelines also contains bunch of stored procedures that also have a bunch of ML models.

Both are complex pipelines.

Any help or ideas would be greatly appreciated!
",-1,2025-05-02 19:58:00
Please suggest Courses/YT Channels on building ETL Pipelines in Databricks using Python. I have good knowledge on Pandas and NumPy and also used Databricks for my personal projects but never build ETL Piplines.,-1,2025-04-12 06:15:15
I am pretty confident with Dagster-dbt-sling/dlt-Aws . I would like to upskill in big data topics. Where should I start? I have seen spark is pretty the go to.  Do you have any suggestions to start with? is it better to use it in native java/scala JVM or go for for pyspark? Is it ok to train in local? Any suggestion would me much appreciated,-1,2025-04-17 16:21:56
"We‚Äôve been using Looker Studio (formerly Data Studio) to build reporting dashboards for digital marketing and SEO data. At first, things worked fine‚Äîbut as datasets grew, dashboard performance dropped significantly.



The biggest bottlenecks were:

‚Ä¢ Overuse of blended data sources

‚Ä¢ Direct querying of large GA4 datasets

‚Ä¢ Too many calculated fields applied in the visualization layer



To fix this, we adjusted our approach on the data engineering side:

‚Ä¢ Moved most calculations (e.g., conversion rates, ROAS) to the query layer in BigQuery

‚Ä¢ Created materialized views for campaign-level summaries

‚Ä¢ Used scheduled queries to pre-aggregate weekly and monthly data

‚Ä¢ Limited Looker Studio to one direct connector per dashboard and cached data where possible



Result: dashboards now load in \~3 seconds instead of 15‚Äì20, and we can scale them across accounts with minimal changes.



Just sharing this in case others are using BI tools on top of large datasets‚Äîinterested to hear how others here are managing dashboard performance from a data pipeline perspective.",-1,2025-04-08 17:43:22
"Hello, Marcos from the Airbyte Team.

For those who may not be familiar, Airbyte is an open-source data integration (EL) platform with over 500 connectors for APIs, databases, and file storage.

In our last release we added several new features to our no-code Connector Builder:

* [GraphQL Support](https://docs.airbyte.com/connector-development/config-based/understanding-the-yaml-file/request-options#graphql-request-injection): In addition to REST, you can now make requests to GraphQL APIs (and properly handle pagination!)
* [Async Data Requests](https://docs.airbyte.com/connector-development/connector-builder-ui/async-streams): There are some reporting APIs that do not return responses immediately. For instance, with Google Ads.¬† You can now request a custom report from these sources and wait for the report to be processed and downloaded.
* [Custom Python Code Components](https://docs.airbyte.com/connector-development/connector-builder-ui/custom-components): We recognize that some APIs behave uniquely‚Äîfor example, by returning records as key-value pairs instead of arrays or by not ordering data correctly. To address these cases, our open-source platform now supports custom Python components that extend the capabilities of the no-code framework without blocking you from building your connector.

We believe these updates will make connector development faster and more accessible, helping you get the most out of your data integration projects.

We understand there are discussions about the trade-offs between no-code and low-code solutions. At Airbyte, transitioning from fully coded connectors to a low-code approach allowed us to maintain a large connector catalog using standard components.¬† We were also able to create a better build and test process directly in the UI. Users frequently give us the feedback that the no-code connector Builder enables less technical users to create and ship connectors. This reduces the workload on senior data engineers allowing them to focus on critical data pipelines.

Something else that has been top of mind is speed and performance. With a robust and stable connector framework, the engineering team has been dedicating significant resources to introduce concurrency to enhance sync speed. You can read this[ blog post](https://airbyte.com/blog/improving-connector-sync-speed-up-to-10x-faster) about how the team implemented concurrency in the Klaviyo connector, resulting in a speed increase of about 10x for syncs.

I hope you like the news! Let me know if you want to discuss any missing features or provide feedback about Airbyte.",-1,2025-04-04 12:26:58
"I am working in an IT services company with Analytics projects delivered for clients. Is there scope in data governance certifications or programs I can take up to stay relevant? Is the area of data governance going to get much more prominent?

Thanks in advance",-1,2025-06-04 02:47:10
"So I moved internally from a system analyst to a data engineer. I feel the hard part is done for me already. We are replicating hundreds of views from a SQL server to AWS redshift. We use glue, airflow, s3, redshift, data zone. We have a custom developed tool to do the glue jobs of extracting from source to s3. I just got to feed it parameters, run the air flow jobs, create the table scripts, transform the datatypes to redshift compatible ones. I do check in some code but most of the terraform ground work is laid out by the devops team, I'm just adding in my json file, SQL scripts, etc. I'm not doing any python, not much terraform, basic SQL. I'm new but I feel like I'm in a cushy cheating position.",-1,2025-04-23 03:18:05
"Hey r/dataengineering,

I'm currently transitioning from a software engineering role to data engineering, and I've identified a potential project at my company that I think would be a great learning experience and a chance to introduce some data engineering best practices.

Project Overview:

We have a dashboard that displays employee utilization data, sourced from two main systems: Harvest (time tracking) and Forecast (projected utilization).

Current Process:

* Harvest Data: Currently, we're using cron jobs running on an EC2 instance to periodically pull data from Harvest.
* Forecast Data: Due to the lack of an API, we're relying on Playwright (web scraping) to extract data from their web reports, which are then saved to S3.
* Data Processing: Another cron job on EC2 processes the S3 reports and loads the data into a PostgreSQL database.
* Dashboard: A custom frontend application (using Azure OAuth) queries the PostgreSQL database to display the utilization data.

Proposed Solution:

I'm proposing a serverless architecture on AWS, using the following components:

* API Gateway + Lambda: To create a robust API for our frontend application.
* Lambda for ETL: To automate data extraction, transformation, and loading from Harvest and Forecast.
* AWS Step Functions: To orchestrate the data pipeline and manage dependencies.
* Amazon RDS PostgreSQL: To serve as our data warehouse for analytical queries.
* API Gateway Authorizer: To integrate Azure OAuth authentication.
* CI/CD with CodePipeline and CodeBuild: To automate testing and deployment.
* Docker and SAM CLI: For local development and testing.

My Goals:

* Gain hands-on experience with AWS serverless technologies.
* Implement data engineering best practices for ETL and data warehousing.
* Improve the reliability and scalability of our data pipeline.
* Potentially expand this architecture to serve as a central data warehouse for other company analytical data.

My Questions:

1. For those with experience in similar projects, what are some key considerations or potential challenges I should be aware of?
2. Any advice on best practices for designing and implementing a serverless data pipeline on AWS?
3. Are there any specific AWS services or tools that you would recommend for this project?
4. How would you recommend getting started on a project like this, what would you focus on first?
5. What would be some good ways to test this type of system?

I'm eager to learn and contribute, and I appreciate any insights or advice you can offer.

Thanks!",-1,2025-04-02 14:57:52
"Found this on LinkedIn posted by a recruiter. It‚Äôs pretty bad if they filter out based on these criteria. It sounds to me like ‚ÄúI‚Äôm looking for someone to drive a Toyota but you‚Äôve only driven Honda!‚Äù

In a field like DE where the tech stack keeps evolving pretty fast I find this pretty surprising that recruiters are getting such instructions from the hiring manager! 

Have you seen your company differentiate based just on stack? ",-1,2025-05-01 07:04:03
"Hi all! I‚Äôve been using Great Expectations (GX) locally for data quality checks, but I‚Äôm struggling to set it up in Azure Databricks. Any tips or working examples would be amazing!",-1,2025-04-30 22:50:12
"With tools like ChatGPT generating queries instantly and so many no-code/low-code solutions out there, is it still worth spending serious time learning SQL?

I get that companies still ask SQL questions during technical assessments, but from what I‚Äôve learned so far, it feels pretty straightforward. I understand the basics, and honestly, asking someone to write SQL from scratch as part of a screening or evaluation seems kinda pointless. It doesn‚Äôt really prove anything valuable in my opinion‚Äîespecially when most of us just look up the syntax or use tools anyway.

Would love to hear how others feel about this‚Äîespecially people working in data, engineering, or hiring roles. Am I wrong ?",-1,2025-06-04 14:31:27
"We scraped the Shopify GraphQL docs with code examples using our Postgres-compatible database. Here's the link to the repo:

[https://github.com/lsd-so/Shopify-GraphQL-Spec](https://github.com/lsd-so/Shopify-GraphQL-Spec)",-1,2025-04-16 23:00:06
"Do you use Spark to parallelize/dstribute/batch existing code and etls, or do you use it as a etl-transformation tool like could be dlt or dbt or similar?

I am trying to understand what personal projects I can do to learn it but it is not obvious to me what kind of idea would it be best. Also because I don‚Äôt believe using it on my local laptop would present the same challanges of using it on a real cluster/cloud environment. Can you prove me wrong and share some wisdom?

Also, would be ok to integrate it in Dagster or an orchestrator in general, or it can be used an orchestrator itself with a scheduler as well? ",-1,2025-05-02 11:17:53
"Hello !

I'm trying to practice building a full data pipeline from A to Z using the following architecture. I'm a beginner and tried to put together something that seems optimal using different technologies.

Here's the flow I came up with:

üìç Events ‚Üí Kafka ‚Üí  Spark Streaming ‚Üí  AWS S3 ‚Üí ‚ùÑÔ∏è Snowpipe ‚Üí  Airflow ‚Üí dbt ‚Üí üìä BI (Power BI)

I have a few questions before diving in:

* Does this architecture make sense overall?
* Is using AWS S3 as a data lake feeding into Snowflake a common and solid approach? (From what I read, Snowflake seems more scalable and easier to work with than Redshift.)
* Do you see anything that looks off or could be improved?

Thanks a lot in advance for your feedback !",-1,2025-05-01 18:12:45
"Colleges and universities today are sitting on a goldmine of data‚Äîfrom enrollment records to student performance reports‚Äîbut few have the infrastructure to use that information strategically.

A modern **data warehouse** consolidates all institutional data in one place, allowing universities to:  
üîπ Spot early signs of student disengagement  
üîπ Optimize resource allocation  
üîπ Speed up reporting processes for accreditation and funding  
üîπ Improve operational decision-making across departments

Without a strong data strategy, higher ed institutions risk falling behind in today's competitive and fast-changing landscape.

Learn how a smart data warehouse approach can drive better results for students and operations ‚ûî [Full article here](https://data-sleek.com/blog/data-warehousing-student-success-efficiency/)

\#DataDriven #HigherEdStrategy #StudentRetention #UniversityLeadership",-1,2025-04-30 19:54:27
"Hey r/dataengineering community, I‚Äôm diving into system integration and need your insights! If you‚Äôve used middleware like MuleSoft, Workato, Celigo, Zapier, or others, please share your experience:

**1. Which integration software/solutions does your organization currently use?**

**2. When does your organization typically pursue integration solutions?**  
a. During new system implementations  
b. When scaling operations  
c. When facing pain points (e.g., data silos, manual processes)

**3. What are your biggest challenges with integration solutions?**

**4. If offered as complimentary services, which would be most valuable from a third-party integration partner?**  
a. Full integration assessment or discovery workshop  
b. Proof of concept for a pressing need  
c. Hands-on support during an integration sprint  
d. Post integration health-check/assessment  
e. Technical training for the team  
f. Pre-built connectors or templates  
g. None of these. Something else.

Drop your thoughts below‚Äîlet‚Äôs share some knowledge!",-1,2025-04-28 19:01:19
"Currently, my organization uses a licensed tool from a specific vendor for ETL needs. We are paying a hefty amount for licensing fees and are not receiving support on time. As the tool is completely managed by the vendor, we are not able to make any modifications independently.

Can you suggest a few open-source options? Also, I'm looking for round-the-clock support for the same tool.",-1,2025-04-30 12:05:31
"Been thinking a lot about how broken access to computing has become in AI.

We‚Äôve reached a point where training and inference demand insane GPU power, but almost everything is gated behind AWS, GCP, and Azure. If you‚Äôre a startup, indie dev, or research lab, good luck affording it. Even if you can, there‚Äôs the compliance overhead, opaque usage policies, and the quiet reality that all your data and models sit in someone else‚Äôs walled garden.

This centralization creates 3 big issues:

* Cost barriers lock out innovation
* Surveillance and compliance risks go up
* Local/grassroots AI development gets stifled

I came across a project recently, Ocean Nodes, that proposes a decentralized alternative. The idea is to create a permissionless compute layer where anyone can contribute idle GPUs or CPUs. Developers can run containerized workloads (training, inference, validation), and everything is cryptographically verified. It‚Äôs essentially DePIN combined with AI workloads.

Not saying it solves everything overnight, but it flips the model: instead of a few hyperscalers owning all the compute, we can build a network where anyone contributes and anyone can access. Trust is built in by design, not by paperwork.

Has anyone here tried running AI jobs on decentralized infrastructure or looked into Ocean Nodes? Does this kind of model actually have legs for serious ML workloads?  Would love to hear thoughts.",-1,2025-05-28 17:00:11
"Currently, my organization uses a licensed tool from a specific vendor for ETL needs. We are paying a hefty amount for licensing fees and are not receiving support on time. As the tool is completely managed by the vendor, we are not able to make any modifications independently.

Can you suggest a few open-source options? Also, I'm looking for round-the-clock support for the same tool.",-1,2025-04-30 12:05:31
"Hi all,  
I am currently working as a data engineer \~3 YOE currently on notice period of 90 days and Iam looking for guidance on how to upskill and prepare myself to land a job at a top tier company (like FAANG, product-based, or top tech startups).

**My current tech stack:**

* **Languages**: Python, SQL, PLSQL
* **Cloud/Tools**: Snowflake, AWS (Glue, Lambda, S3, EC2, SNS, SQS, Step Functions), Airflow
* **Frameworks**: PySpark (beginner to intermediate), Spark SQL, Snowpark, DBT, Flask, Streamlit
* **Others**: Git, CI/CD, DevOps basics, Schema Change, basic ML knowledge

**What I‚Äôve worked on:**

* designed and scaled etl pipelines with AWS Glue and S3 supporting 10M+ daily records
* developed PySpark jobs for large-scale data transformations 
* built near real time and batch pipelines using Glue, Lambda, Snowpipe, Step Functions, etc.
* Created a Streamlit based analytics dashboard on Snowflake
* worked with RBAC, data masking, CDC, performance tuning in Snowflake
* Built a reusable ETL and Audit Balance Control
* experience with CICD pipelines for code promotion and automation

I feel I have a good base but want to know:

* What skills or tools should I focus on next?
* Is my current stack aligned with what top companies expect?
* Should I go deeper into pyspark or explore something like kafka, kubernetes, data modeling
* How important are system design or coding DSA for data engineer interviews?

would really appreciate any feedback, suggestions, or learning paths.

thanks in advance",-1,2025-04-30 08:17:17
"Hi, everyone!

I'm a solo data consultant and over the past few years, I‚Äôve been helping companies in Europe build their data stacks.

I noticed I was repeatedly performing the same tasks across my projects: setting up dbt, configuring Snowflake, and, more recently, migrating to Iceberg data lakes.

So I've been working on a solution for the past few months called [**Boring Data**](http://boringdata.io).

It's a set of Terraform templates ready to be deployed in AWS and/or Snowflake with pre-built integrations for ELT tools and orchestrators.

I think these templates are a great fit for many projects:

* Pay once, own it forever
* Get started fast
* Full control

I'd love to get feedback on this approach, which isn't very common (from what I've seen) in the data industry.

Is Terraform commonly used on your teams, or is that a barrier to using templates like these?

Is there a starter template that you'd wished you had for an implementation in the past?",-1,2025-04-02 11:40:59
Has anyone got a good product here or was it just VC hype from two years ago?,-1,2025-04-29 19:25:32
"Hello,

I've been on the lookout for quite some time for a tool that can help validate the data flow/quality between different systems and also verify the output of files(Some systems generate multiple files bases on some rules on the database). Ideally, this tool should be open source to allow for greater flexibility and customization.

Do you have any recommendations or know of any tools that fit this description?

",-1,2025-04-30 06:26:09
"So I am working on a project where I have to analyze Financial transactions and interpret the nature of transaction (Goods/Service/Contract/etc), I'm using OCR to extract text from Image based PDFs, but the problem is, the extracted data doesn't make a lot of sense. but using non-OCR PDF to text just results in an empty string, so I have to use the OCR method using pytesseract.

Please, can someone tell me what's the correct way of doing this, how do I make the extracted data readable or usable? Any tips or suggestions would be helpful, thanks :)",-1,2025-04-16 17:22:44
"Unlike web development, where you get instant feedback through a local web server, mimicking that fast development loop is much harder when working with SQL.

Caching part of the data locally is kinda the only way to speed up feedback during development.

Instant SQL uses the power of in-process DuckDB to provide immediate feedback, offering a potential step forward in making SQL debugging and iteration faster and smoother.

What are your current strategies for easier SQL debugging and faster iteration?

",-1,2025-04-24 07:29:19
"I‚Äôve been diving deep into the costs of running browser-based scraping at scale, and I wanted to share some insights on what it takes to run 1,000 browser requests, comparing commercial solutions to self-hosting (DIY). This is based on some research I did, and I‚Äôd love to hear your thoughts, tips, or experiences scaling your own scraping setups.

# Why Use Browsers for Scraping?

Browsers are often essential for two big reasons:

* **JavaScript Rendering**: Many modern websites rely on JavaScript to load content. Without a browser, you‚Äôre stuck with raw HTML that might not show the data you need.
* **Avoiding Detection**: Raw HTTP requests can scream ‚Äúbot‚Äù to websites, increasing the chance of bans. Browsers mimic human behavior, helping you stay under the radar and reduce proxy churn.

The downside? Running browsers at scale can get expensive fast. So, what‚Äôs the actual cost of 1,000 browser requests?

# Commercial Solutions: The Easy Path

Commercial JavaScript rendering services handle the browser infrastructure for you, which is great for speed and simplicity. I looked at high-volume pricing from several providers (check the blog link below for specifics). On average, costs for 1,000 requests range from \~$0.30 to $0.80, depending on the provider and features like proxy support or premium rendering options.

These services are plug-and-play, but I wondered if rolling my own setup could be cheaper. Spoiler: it often is, if you‚Äôre willing to put in the work.

# Self-Hosting: The DIY Route

To get a sense of self-hosting costs, I focused on running browsers in the cloud, excluding proxies for now (those are a separate headache). The main cost driver is your cloud provider. For this analysis, I assumed each browser needs \~2GB RAM, 1 CPU, and takes \~10 seconds to load a page.

# Option 1: Serverless Functions

Serverless platforms (like AWS Lambda, Google Cloud Functions, etc.) are great for handling bursts of requests, but cold starts can be a pain, anywhere from 2 to 15 seconds, depending on the provider. You‚Äôre also charged for the entire time the function is active. Here‚Äôs what I found for 1,000 requests:

* Typical costs range from \~$0.24 to $0.52, with cheaper options around $0.24‚Äì$0.29 for providers with lower compute rates.

# Option 2: Virtual Servers

Virtual servers are more hands-on but can be significantly cheaper‚Äîoften by a factor of \~3. I looked at machines with 4GB RAM and 2 CPUs, capable of running 2 browsers simultaneously. Costs for 1,000 requests:

* Prices range from \~$0.08 to $0.12, with the lowest around $0.08‚Äì$0.10 for budget-friendly providers.

**Pro Tip**: Committing to long-term contracts (1‚Äì3 years) can cut these costs by 30‚Äì50%.

For a detailed breakdown of how I calculated these numbers, check out the full blog post here (replace with your actual blog link).

# When Does DIY Make Sense?

To figure out when self-hosting beats commercial providers, I came up with a rough formula:

    (commercial price - your cost) √ó monthly requests ‚â§ 2 √ó engineer salary

* **Commercial price**: Assume \~$0.36/1,000 requests (a rough average).
* **Your cost**: Depends on your setup (e.g., \~$0.24/1,000 for serverless, \~$0.08/1,000 for virtual servers).
* **Engineer salary**: I used \~$80,000/year (rough average for a senior data engineer).
* **Requests**: Your monthly request volume.

For serverless setups, the breakeven point is around \~108 million requests/month (\~3.6M/day). For virtual servers, it‚Äôs lower, around \~48 million requests/month (\~1.6M/day). So, if you‚Äôre scraping 1.6M‚Äì3.6M requests¬†*per day*, self-hosting might save you money. Below that, commercial providers are often easier, especially if you want to:

* Launch quickly.
* Focus on your core project and outsource infrastructure.

**Note**: These numbers don‚Äôt include proxy costs, which can increase expenses and shift the breakeven point.

# Key Takeaways

Scaling browser-based scraping is all about trade-offs. Commercial solutions are fantastic for getting started or keeping things simple, but if you‚Äôre hitting millions of requests daily, self-hosting can save you a lot if you‚Äôve got the engineering resources to manage it. At high volumes, it‚Äôs worth exploring both options or even negotiating with providers for better rates.

For the full analysis, including specific provider comparisons and cost calculations, check out my blog post here (replace with your actual blog link).

What‚Äôs your experience with scaling browser-based scraping? Have you gone the DIY route or stuck with commercial providers? Any tips or horror stories to share?",-1,2025-04-23 14:51:45
"Bringing SQL and AI together to query unstructured data directly in Microsoft Fabric at 60% lower cost‚Äîno pipelines, no workarounds, just fast answers.

How this works:  
\- Decentralized Architecture: No driver node means no bottlenecks‚Äîperfect for high concurrency.  
\- Kubernetes Autoscaling: Pay only for actual CPU usage, potentially cutting costs by up to 60%.  
\- Optimized Execution: Features like vectorized processing and stage fusion help reduce query latency.  
\- Security Compliance: Fully honors Fabric‚Äôs security model with row-level filtering and IAM integration.

Check out the full blog here: [https://www.e6data.com/blog/vector-search-in-fabric-e6data-semantic-sql-embedding-performance](https://www.e6data.com/blog/vector-search-in-fabric-e6data-semantic-sql-embedding-performance)

",-1,2025-04-17 03:57:21
Building a timeseries processing tool. Think Beam on steroids. Looking for input on what people **really** need from timeseries processing. All opinions welcome!,-1,2025-04-22 17:19:42
"Hello,

I've been on the lookout for quite some time for a tool that can help validate the data flow/quality between different systems and also verify the output of files(Some systems generate multiple files bases on some rules on the database). Ideally, this tool should be open source to allow for greater flexibility and customization.

Do you have any recommendations or know of any tools that fit this description?

",-1,2025-04-30 06:26:09
"i am building a pipeline that writes data to a sql table (in azure). currently, the pipeline cleans the data in python, and it uses the pandas to_sql() method to write to sql.

i wanted to enforce constraints on the sql table, but im struggling with error handling.

for example, suppose column X has a value of -1, but there is a sql table constraint requiring X > 0. when the pipelines tries to write to sql, it throws a generic error msg that doesn‚Äôt specify the problematic column(s).

is there a way to get detailed error msgs?

or, more generally, is there a better way to go about enforcing data validity?

thanks all! :)

",-1,2025-04-17 00:25:53
"I studying Software Engineering (Data specialty next year) but I want to get into DE, I am working on a project including PySpark  (As Scala is dying) , NoSQL and BI (for dashboards); but I am getting overwhelmed because I don't how/what to do;  
PySpark drove me crazy because of the sensitive exceptions of UDFs and Pickle Lock error, so each time I think to give up and change career vision.  
Anyone had the same experience?",-1,2025-04-29 20:05:17
"I built¬†**nbcat**, a lightweight CLI tool that lets you preview Jupyter notebooks right in your terminal ‚Äî no web UI, no Jupyter server, no fuss.

üîπ Minimal dependencies  
üîπ Handles¬†*all*¬†notebook versions (even ancient ones)  
üîπ Works with remote files ‚Äî no need to download first  
üîπ Super fast and clean output

Most tools I found were either outdated or bloated with half-working features. I just wanted a no-nonsense way to view notebooks over SSH or in my daily terminal workflow ‚Äî so I made one.   
  
Here is a link to repo [https://github.com/akopdev/nbcat](https://github.com/akopdev/nbcat)",-1,2025-05-02 21:56:17
"Hello all,

Wanted to share my data compression library, CXcompress, that - when used with zstd - offers performance improvements over zstd alone. Please check it out and let me know what you think!",-1,2025-06-03 20:19:39
"I am working in an IT services company with Analytics projects delivered for clients. Is there scope in data governance certifications or programs I can take up to stay relevant? Is the area of data governance going to get much more prominent?

Thanks in advance",-1,2025-06-04 02:47:10
"Hey all,

I've been learning Spark/PySpark recently and I'm curious about how production projects are typically structured and organized.

My background is in DBT, where each model (table/view) is defined in a SQL file, and DBT builds a DAG automatically using `ref()` calls. For example:

    -- modelB.sql
    SELECT colA FROM {{ ref('modelA') }}
    

This ensures `modelA` runs before `modelB`. DBT handles the dependency graph for you, parallelizes independent models for faster builds, and allows for targeted runs using tags. It also supports automated tests defined in YAML files, which run before the associated models.

I'm wondering how similar functionality is achieved in Databricks. Is lineage managed manually, or is there a framework to define dependencies and parallelism? How are tests defined and automatically executed? I'd also like to understand how this works in vanilla Spark without Databricks.

  
TLDR - How are Databricks or vanilla Spark projects organized in production. How are things like 100s of tables, lineage/DAGs, orchestration, and tests managed?

Thanks!",-1,2025-06-04 00:44:28
"Our marketing and operations teams are constantly requesting Salesforce data in BigQuery, but setting up a proper pipeline always becomes a development bottleneck. Engineering doesn't have the resources to maintain connectors or write custom scripts every quarter.

How are other teams handling this without needing a full-time data engineer?",-1,2025-06-03 13:08:56
"Does anyone know if this exists in the open source space?

- Jupyter or Jupyter like notebooks
- Can run sql directly
- Supports autocomplete of database schema
- Language server for Postgres sql / syntax highlighting / linting etc.

In other words: is there an alternative to jetbrains dataspell?


",-1,2025-05-28 18:14:20
"Anyone that's interested in going to World Summit AI USA, please DM me or message me on Instagram @sznxar. I have tickets for $200. The ticket value is $700. 

",-1,2025-05-25 21:18:18
"Hey folks,

We‚Äôre seeing a pattern across modernization efforts: **Data migration** ‚Äî especially when moving from legacy monoliths to microservices or SaaS architectures ‚Äî is still **painfully ad hoc**.

Sure, the core ELT pipeline can be wired up with AWS tools like **DMS**, **Glue**, and **Airflow**. But we keep running into these *repetitive, unsolved pain points*:

* Pre-migration risk profiling (null ratios, low-entropy fields, unexpected schema drift)
* Field-level data lineage from source ‚Üí target
* Dry run simulations for pre-launch sign-off
* Post-migration validation (hash diffs, rules, anomaly checks)
* Data owner/steward approvals (governance checkpoints)
* Observability and traceability when things go wrong

We‚Äôve had to script or manually patch this stuff over and over ‚Äî across different clients and environments. Which made us wonder:

# Are These Just Gaps in the Ecosystem?

We're trying to validate:

* Are others running into these same repeatable challenges?
* How are you handling governance, validation, and observability in migrations?
* If you‚Äôve extended the AWS-native stack, how did you approach things like steward approvals or validation logic?
* Has anyone tried solving this at the **platform level** ‚Äî e.g., a reusable layer over AWS services, or even a standalone open-source toolset?
* If AWS-native isn't enough, what **open-source options** could form the foundation of a more robust migration framework?

We‚Äôre not trying to pitch anything ‚Äî just seriously considering whether these pain points are universal enough to justify a more structured solution (possibly even SaaS/platform-level). Would love to learn how others are approaching it.

Thanks in advance.",-1,2025-05-28 10:51:27
"I built a tool that turns JSON (and YAML, XML, CSV) into interactive diagrams.

It now supports **JSON Schema validation directly on the diagrams**, invalid fields are highlighted in red, and you can click nodes to see error details. Changes revalidate automatically as you edit.

No sign-up required to try it out.

Would love your thoughts: [https://todiagram.com/editor](https://todiagram.com/editor)

https://preview.redd.it/ezylfmjh7uxe1.png?width=662&format=png&auto=webp&s=421603d58797355ed21d1c960a7c1e81bae82379

https://preview.redd.it/w5r0ai0j7uxe1.png?width=1341&format=png&auto=webp&s=4f1262d95403ff48c18a56312193f27bffa37f00

https://preview.redd.it/0jyvc84k7uxe1.png?width=619&format=png&auto=webp&s=5929344b3dc25865d36ce5889a77a27e43947662",-1,2025-04-29 21:00:55
"You can now define, run and monitor data pipelines inside Postgres ü™Ñüêò Why setup Airflow, compute, and a bunch of scripts just to move data around your DB?

[https://github.com/mattlianje/pg\_pipeline](https://github.com/mattlianje/pg_pipeline)

\- Define pipelines using JSON config  
\- Reference outputs of other stages using \~>  
\- Use parameters with $(param) in queries  
\- Get built-in stats and tracking  
  
Meant for the 80‚Äì90% case: internal ETL and analytical tasks where the data already lives in Postgres.  
  
It‚Äôs minimal, scriptable, and plays nice with pg\_cron. 

Feedback welcome! üôá‚Äç‚ôÇÔ∏è",-1,2025-05-27 16:51:26
I saw this book was recently published. Anyone look into this book and have any opinions? Already reading through DDIA and always looking for books and resources to help improve at work.,-1,2025-05-28 12:13:26
"EDIT:  
This sub is way bigger than I expected, I have received enough comments for now and may re-add this story once the shame has subsided. Thank you for all you're help ",-1,2025-05-27 08:58:31
"I was planning to move my pipeline's processing code from pandas to polars, but then I found out about duckdb and that some people are using it just as a faster data processing library. But my question is, does this make sense? Or would I be better off just switching to polars? What are the tradeoffs here?

Edit: important info I forgot to include. This is in a small org setting, where the current data pipeline is: data ingested from a pg database amd csv/parquet files, orchestration with dagster and most processing with pandas, processed data loaded to database",-1,2025-05-01 16:45:52
"I‚Äôm part of a small team that‚Äôs built an on-premise tool for Apache NiFi ‚Äî aimed at making flow deployment and environment promotion way faster and error-free, especially for teams that deal with strict data control requirements (think banking, healthcare, gov, etc.).
We‚Äôre prepping some educational content (blogs, webinars, posts), and I‚Äôd love to ask:

What kinds of NiFi-related topics would actually interest you?

More technical (e.g., automating version control, CI/CD for NiFi, handling large-scale deployments)?

Or more strategic (e.g., cost-saving strategies, managing flows across regulated environments)?
Also:

- Which industries do you think care most about on-prem NiFi?
- Who usually owns these problems in your world ‚Äî data engineers, platform teams, DevOps?
- Where do you usually go for info like this ‚Äî Reddit, Slack communities, LinkedIn groups, or something else?

Not selling anything ‚Äî just trying to build content that‚Äôs actually useful, not fluff.

Would seriously appreciate any insights or even pet peeves you‚Äôre willing to share. 

Thanks in advance!",-1,2025-05-27 18:51:52
"I am working as a data analyst and I would like to switch into data engineering field. So I would like to study and prepare for the Google Cloud Professional Data Engineer Exam  . As I am new to this , please kindly let me know the effective learning materials. 
Would appreciate a lot! 
Thanks in advance . ",-1,2025-05-27 14:09:49
"My application is distributed across several AWS accounts, and it writes logs to Amazon CloudWatch Logs in the¬†`.json.gz`¬†format. These logs are streamed using a subscription filter to a centralized Kinesis Data Stream, which is then connected to a Kinesis Data Firehose. The Firehose buffers, compresses, and delivers the logs to Amazon S3 following the flow:  
**CloudWatch Logs ‚Üí Kinesis Data Stream ‚Üí Kinesis Data Firehose ‚Üí S3**

I‚Äôm currently testing some scenarios and encountering challenges when trying to write this data directly to the AWS Glue Data Catalog. The difficulty arises because the JSON files are deeply nested (up to four levels deep) as shown in the example below.

  
I would like to hear suggestions on how to handle this. I have tested Lambda Transformations but I am getting errors since my json is 12x longer than that. I wonder if Kinesis Firehose can handle that without any coding. I researched but it appears not to handle that nested level.

    {
      ""order_id"": ""ORD-2024-001234"",
      ""order_status"": ""completed"",
      ""customer"": {
        ""customer_id"": ""CUST-789456"",
        ""personal_info"": {
          ""first_name"": ""John"",
          ""last_name"": ""Doe"",
          ""phone"": {
            ""country_code"": ""+1"",
            ""number"": ""555-0123""
          }
        }
      }
    }
    ",-1,2025-05-25 03:38:23
"Hey r/dataengineering !  
I have been working on this for the last month and i am making some progress, I would to know if it is in the right direction!  
I want to make it as easy as possible to create deploy and manage data pipelines

I would love any feedback, feel free to message me directly comment or email me at [james@octopipe.com](mailto:james@octopipe.com)

Huge thanks in advance!",-1,2025-05-27 10:41:42
"I'm building ETL pipelines using ADF for orchestration and Databricks notebooks for logic. Each notebook handles one task (e.g., dimension load, filtering, joins, aggregations), and pipelines are parameterized.

The issue: joins and aggregations need to be separated, but Databricks doesn‚Äôt allow sharing persisted data across notebooks easily. That forces me to write intermediate tables to storage.

**Is this the right approach?**

* Should I combine multiple steps (e.g., join + aggregate) into one notebook to reduce I/O?
* Or is there a better way to keep it modular without hurting performance?

Any feedback on best practices would be appreciated.",-1,2025-05-23 08:12:35
"Just curious ‚Äî if you're a data engineer using Linux as your main OS, how‚Äôs the experience been? Pros, cons, would you recommend it?",-1,2025-05-28 20:46:18
"As promised here: [https://www.reddit.com/r/dataengineering/comments/1kc9jd4/just\_launched\_a\_course\_on\_building\_a\_simple\_ai/](https://www.reddit.com/r/dataengineering/comments/1kc9jd4/just_launched_a_course_on_building_a_simple_ai/)

  
I have created another free link:  
[https://www.udemy.com/course/building-a-simple-data-analyst-ai-agent-with-llama-and-flask/?couponCode=REDDIT](https://www.udemy.com/course/building-a-simple-data-analyst-ai-agent-with-llama-and-flask/?couponCode=REDDIT)

 Thank you so much for the support!! I really appreciate the feedback!  
",-1,2025-05-27 17:16:42
"Hour 1: ""Design a system for 1 billion users

Hour 2: ""Optimize this Flink job processing 50TB daily""

Hour 3: ""Explain data lineage across global markets""

The process was brutal but fair. They really want to know if you can handle TikTok-scale data challenges.

Plot twist #1: I actually got the 2022 offer but rejected 2024 üéâ

Sharing everything I [full storye](https://medium.com/endtoenddata/tiktok-data-engineer-interview-process-e5c9ac44131e):

Anyone else have  horror stories that turned into success? Drop them below!

\#TikTok #DataEngineering # #TechCareers #BigTech",-1,2025-06-03 08:12:33
"Hey all,

I've been learning Spark/PySpark recently and I'm curious about how production projects are typically structured and organized.

My background is in DBT, where each model (table/view) is defined in a SQL file, and DBT builds a DAG automatically using `ref()` calls. For example:

    -- modelB.sql
    SELECT colA FROM {{ ref('modelA') }}
    

This ensures `modelA` runs before `modelB`. DBT handles the dependency graph for you, parallelizes independent models for faster builds, and allows for targeted runs using tags. It also supports automated tests defined in YAML files, which run before the associated models.

I'm wondering how similar functionality is achieved in Databricks. Is lineage managed manually, or is there a framework to define dependencies and parallelism? How are tests defined and automatically executed? I'd also like to understand how this works in vanilla Spark without Databricks.

  
TLDR - How are Databricks or vanilla Spark projects organized in production. How are things like 100s of tables, lineage/DAGs, orchestration, and tests managed?

Thanks!",-1,2025-06-04 00:44:28
"I was recently appointed as Head of Data Governance and have started drafting policies. Would like to ask for advise on how I can build a data governance program. Where do I start? Is adopting the DAMA Framework a good strategy? Note that we are a small, fairly startup organization.

Would appreciate your inputs.",-1,2025-05-25 10:19:15
"Lately I've been digging into Lakehouse stuff and thinking of putting together a few blog posts to share what I've learned.

 If you're into this too or have any thoughts, feel free to jump in‚Äîwould love to chat and swap ideas!",-1,2025-05-28 13:21:51
"I‚Äôm working with ~500GB of partitioned Parquet files stored in S3. The data is primarily used for ML model training and evaluation ‚Äî I rarely read the full dataset, mostly filtered subsets based on partitions.

I‚Äôm evaluating two options:
	1.	Python (Pandas/Polars) ‚Äî reading directly from S3 using tools like s3fs, pyarrow.dataset, etc., running on either local machine or SageMaker.
	2.	AWS Athena ‚Äî creating external tables over the same partitioned Parquet files and querying using SQL.

What I care about:
	‚Ä¢	Cost-effectiveness ‚Äî Athena charges per TB scanned; Python reads would run on local/SageMaker.
	‚Ä¢	Performance ‚Äî especially for slicing subsets and preparing data for ML pipelines.
	‚Ä¢	Flexibility ‚Äî need to do transformations (feature engineering, filtering, joins) before passing to ML models.

Which approach would you recommend for this kind of workflow?",-1,2025-04-25 14:02:06
"Hey folks,

I'm a data engineer at [e6data](https://www.e6data.com/), and we've been working on integrating our engine with Microsoft Fabric. We recently ran some benchmarks (TPC-DS) and observed around a **33% improvement in SQL query performance** while also significantly reducing costs compared to native Fabric compute engines.

Here's what our integration specifically enables:

* **33% faster SQL queries** directly on data stored in OneLake (TPC-DS benchmark results).
* **2-3x cost reduction** by optimizing compute efficiency.
* **Zero data movement**: direct querying of data from OneLake.
* Native vector search support for AI-driven workflows.
* Scalable to 1000+ QPS with sub-second latency and real-time autoscaling.
* Enterprise-level security measures.

We've documented our approach and benchmark results: [https://www.e6data.com/blog/e6data-fabric-increased-performance-optimized-capacity](https://www.e6data.com/blog/e6data-fabric-increased-performance-optimized-capacity)

We'd genuinely appreciate your thoughts, feedback, or questions about our approach or experiences with similar integrations.

https://preview.redd.it/vx65oc5zd2se1.jpg?width=6636&format=pjpg&auto=webp&s=57a2a75cd11a8cc616d04a042a12055bddfe5b4b

",-1,2025-03-31 17:57:18
"Hi community,

After working as a data analyst for several years, I've noticed a gap in tools for interactively exploring complex ETL pipeline dependencies. Many solutions handle smaller pipelines well, but struggle with 200+ tasks.

For larger pipelines, we need robust traversal features, like collapsing/expanding nodes to focus on specific sections during development or debugging. I've used `networkx` and `mermaid` for subgraph visualization, but an interactive UI would be more efficient.

I've developed a prototype and am seeking example cases to test it. I'm looking for pipelines with 60+ tasks and complex dependencies. I'm particularly interested in the challenges you face with these large pipelines. At my workplace, we have a 1500+ task pipeline, and I'm curious if this is a typical scale.

Specifically, I'd like to know:

* What challenges do you face when visualizing and managing large pipelines?
* Are pipelines with 1500+ tasks common?
* What features would you find most useful in a tool for this purpose?

If you can share sanitized examples or describe the complexity of your pipelines, it would be very helpful.

Thanks.",-1,2025-03-30 20:29:36
"I‚Äôm exploring LLMs to make sense of large volumes of logs‚Äîespecially from data tools like DataStage, Airflow, or Spark‚Äîand I‚Äôm curious:
	‚Ä¢	Has anyone used an LLM to analyze logs, classify errors, or summarize root causes?
	‚Ä¢	Are there any working log analysis use cases (not theoretical) that actually made life easier?
	‚Ä¢	Any open-source projects or commercial tools that impressed you?
	‚Ä¢	What didn‚Äôt work when you tried using AI/LLMs on logs?

Looking for real examples, good or bad. I‚Äôm building something similar and want to avoid wasting cycles on what‚Äôs already been tried.",-1,2025-05-23 06:02:01
"Real-world data engineering practice! üèóÔ∏è Built an end-to-end **data pipeline** using GCP, BigQuery, dbt, and Airflow to analyze Airbnb trends. Learning + hands-on = the best combo! üí°  
\#dezoomcamp #dataengineering #learningbydoing",-1,2025-03-31 00:57:43
"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",-1,2025-03-29 18:12:13
"Hi all üëã



I‚Äôm working on a take-home assignment for a full-time Data Engineer role and want to sanity-check my approach before submitting.



The task:



\-Build a data ingestion pipeline using Golang + RabbitMQ + MySQL



\-Use proper Go project structure (golang-standards/project-layout)



\-Publish 3 messages into RabbitMQ (goroutine)



\-Consume messages and write into MySQL (payment\_events)



\-On primary key conflict, insert into skipped\_messages table



\-Dockerize with docker-compose



What I‚Äôve built:



‚úÖ Modular Go project (cmd/, internal/, config/, etc.)

‚úÖ Dockerized stack: MySQL, RabbitMQ, app containers with healthchecks

‚úÖ Config via .env (godotenv)

‚úÖ Publisher: Sends 3 payloads via goroutine

‚úÖ Consumer: Reads from RabbitMQ ‚Üí inserts into MySQL

‚úÖ Duplicate handling: catches MySQL Error 1062 ‚Üí redirects to skipped\_messages

‚úÖ Safe handling of multiple duplicate retries (no crashes)

‚úÖ Connection retry logic (RabbitMQ, MySQL)

‚úÖ Graceful shutdown handling

‚úÖ /health endpoint for liveness

‚úÖ Unit tests for publisher/consumer

‚úÖ Fully documented test plan covering all scenarios



Where I need input:



While this covers everything in the task, I‚Äôm wondering:



\-Is this level enough for real-world interviews?



\-Are they implicitly expecting more? (e.g. DLQs, better observability, structured logging, metrics, operational touches)



\-Would adding more ""engineering maturity"" signals strengthen my submission?



Not looking to over-engineer it, but I want to avoid being seen as too basic.",-1,2025-06-12 15:54:38
"Hey everyone,  
I'm looking for some real-world input from folks who have enabled Change Data Capture (CDC) on SQL Server in production environments.

We're exploring CDC to stream changes from specific tables into a Kafka pipeline using Debezium. Our approach is *not* to turn it on across the entire database‚Äîonly on a small set of high-value tables.

However, I‚Äôm running into some organizational pushback. There‚Äôs a general concern about performance degradation, but so far it‚Äôs been more of a blanket objection than a discussion grounded in specific metrics or observed issues.

If you've enabled CDC on SQL Server:

* What kind of performance overhead did you notice, if any?
* Was it CPU, disk I/O, log growth, query latency‚Äîor all of the above?
* Did the overhead vary significantly based on table size, write frequency, or number of columns?
* Any best practices you followed to minimize the impact?

Would appreciate hearing from folks who've lived through this decision‚Äîespecially if you were in a situation where it wasn‚Äôt universally accepted at first.

Thanks in advance!",-1,2025-06-12 09:30:52
"If you‚Äôve ever been part of a team that had to rewrite a large, complex ETL system that‚Äôs been running for year what was your overall strategy?
	‚Ä¢	How did you approach planning and scoping the rewrite?
	‚Ä¢	What kind of questions did you ask upfront?
	‚Ä¢	How did you handle unknowns buried in legacy logic?
	‚Ä¢	What helped you ensure improvements in cost, performance, and data quality?
	‚Ä¢	Did you go for a full re-architecture or a phased refactor?

Curious to hear how others tackled this challenge, what worked, and what didn‚Äôt.",-1,2025-05-23 05:58:09
"really appreciate your support and feedback!

In my current project as a Data Engineer, I faced a very real and tricky challenge ‚Äî we had to schedule and run 50‚Äì100 Databricks jobs, but our cluster could only handle 10 jobs in parallel.

Many people (even experienced ones) confuse the max_concurrent_runs setting in Databricks. So I shared:

What it really means

Our first approach using Task dependencies (and what didn‚Äôt work well)

And finally‚Ä¶

A smarter solution using Python and concurrency to run 100 jobs, 10 at a time


The blog includes real use-case, mistakes we made, and even Python code to implement the solution!

If you're working with Databricks, or just curious about parallelism, Python concurrency, or running jar files efficiently, this one is for you.
Would love your feedback, reshares, or even a simple like to reach more learners!

Let‚Äôs grow together, one real-world solution at a time",-1,2025-05-24 07:39:48
"Hey everyone,

I wanted to get your thoughts on a potential career move. I've been working primarily with Databricks and Spark, and I really enjoy the flexibility and power of working with distributed compute and Python pipelines.

Now I‚Äôve got a job offer from a company that‚Äôs heavily invested in the Snowflake + Dbt stack. It‚Äôs a solid offer, but I‚Äôm hesitant about moving into something that‚Äôs much more SQL-centric. I worry that going ""all in"" on SQL might limit my growth or pigeonhole me into a narrower role over time.

I feel like this would push me away from core software engineering practices, given that SQL lacks features like OOP, unit testing, etc...

Is Snowflake/Dbt still seen as a strong direction for data engineering, or would it be a step sideways/backwards compared to staying in the Spark ecosystem?

Appreciate any insights!",-1,2025-05-25 00:40:42
"I have posted this in r/databricks too but thought I would post here as well to get more insight. 

I‚Äôve got a function that:

*     Creates a Delta table if one doesn‚Äôt exist
*     Upserts into it if the table is already there

Now I‚Äôm trying to wrap this in PyTest unit-tests and I‚Äôm hitting a wall: where should the test write the Delta table?

* Using tempfile / tmp_path fixtures doesn‚Äôt work, because when I run the tests from VS Code the Spark session is remote and looks for the ‚Äúlocal‚Äù temp directory on the cluster and fails. 
*  It also doesn't have permission to write to a temp dirctory on the cluster due to unity catalog permissions
*     I worked around it by pointing the test at an ABFSS path in ADLS, then deleting it afterwards. It works, but it doesn't feel ""proper"" I guess.

The problem seems to be databricks-connect using the defined spark session to run on the cluster instead of locally .

Does anyone have any insights or tips with unit testing in a Databricks environment?",-1,2025-04-27 14:50:45
"Hi all, full disclosure I‚Äôm looking for feedback on my first Medium post: https://medium.com/@shuu1203/reducing-peak-memory-usage-in-trino-a-sql-first-approach-fc687f07d617

I‚Äôm fairly new to Data Engineering (or actually, Analytics Engineering) (began in January with moving to a new project) and was wondering if I could write something up I found interesting to work on. I‚Äôm unsure if the nature of the post is even something of worthy substance to anyone else.

I appreciate any honest feedback.",-1,2025-05-25 15:33:19
"I‚Äôm currently working as an SAP developer with 13 years of experience, mostly focused on ABAP, SAP EWM, and backend logic. I‚Äôm now planning a career transition into data engineering, and my target is a Data Engineer role at Amazon.

I already have strong experience in SQL and database design, and I‚Äôve worked with complex data flows in enterprise environments. I‚Äôm planning to take a Data Engineering Bootcamp on Coursera to build a solid foundation in modern tools and frameworks.

Before I go all in, I‚Äôd love some advice:
	‚Ä¢	Which specific skills or tools should I focus on to break into a DE role at Amazon?
	‚Ä¢	Are there any must-have certifications or project ideas that can help me stand out?
	‚Ä¢	How much weight does my SAP experience carry when applying to cloud data roles?
	‚Ä¢	Any recommendations for open-source projects or hands-on practice platforms?

Would appreciate any input from folks who made similar transitions or are working in the DE space at big tech.

Thanks in advance!
",-1,2025-05-25 15:23:48
"Hey guys!

Quite new to the DE space, (am from a ML background). I would love to know what are the biggest issues you face when using existing solutions in your data engineering pipeline.

I find the best way to learn about a new field is learning about the problems with existing solutions and diving straight into solving those problems",-1,2025-05-25 21:32:14
"Hi guys, I've released the next version for the Arkalos data framework. It now has a simple and DX-friendly Python migrations, DDL and DML query builder, powered by sqlglot and ibis:

    class Migration(DatabaseMigration):
    
        def up(self):
    
            with DB().createTable('users') as table:
                table.col('id').id()
                table.col('name').string(64).notNull()
                table.col('email').string().notNull()
                table.col('is_admin').boolean().notNull().default('FALSE')
                table.col('created_at').datetime().notNull().defaultNow()
                table.col('updated_at').datetime().notNull().defaultNow()
                table.indexUnique('email')
    
    
            # you can run actual Python here in between and then alter a table
    
    
    
        def down(self):
            DB().dropTable('users')



There is also a new and partial support for the DuckDB warehouse, and 3 data warehouse layers are now available built-in:

    from arkalos import DWH()
    
    DWH().raw()... # Raw (bronze) layer
    DWH().clean()... # Clean (silver) layer
    DWH().BI()... # BI (gold) layer



Low-level query builder, if you just need that SQL:

    from arkalos.schema.ddl.table_builder import TableBuilder
    
    with TableBuilder('my_table', alter=True) as table:
        ...
    
    sql = table.sql(dialect='sqlite')



**GitHub and Docs:**

Docs: [https://arkalos.com/docs/migrations/](https://arkalos.com/docs/migrations/)

GitHub: [https://github.com/arkaloscom/arkalos/](https://github.com/arkaloscom/arkalos/)",-1,2025-06-05 16:07:13
"Sharing my latest ‚ÄòInside Data Engineering‚Äô article featuring veteran Daniel Beach, who‚Äôs been working in Data Engineering since before it was cool.

This would help if you are looking to break into Data Engineering.

**What to Expect:**

* Inside the Day-to-Day¬†‚Äì See what life as a data engineer really looks like on the ground.
* Breaking In¬†‚Äì Explore the skills, tools, and career paths that can get you started.
* Tech Pulse¬†‚Äì Keep up with the latest trends, tools, and industry shifts shaping the field.
* Real Challenges¬†‚Äì Uncover the obstacles engineers tackle beyond the textbook.
* Myth-Busting¬†‚Äì Set the record straight on common data engineering misunderstandings.
* Voices from the Field¬†‚Äì Get inspired by stories and insights from experienced pros.

**Reach out if you like:**

* To be the guest and share your experiences & journey.
* To provide feedback and suggestions on how we can improve the quality of questions.
* To suggest guests for the future articles.",-1,2025-05-25 14:22:47
"Hi everyone,

I‚Äôve made an open-source TUI application in Python called Mongo Analyser that runs right in your terminal and helps you get a clear picture of what‚Äôs inside your MongoDB databases. It connects to MongoDB instances (Atlas or local), scans collections to infer field types and nested document structures, shows collection stats (document counts, indexes, and storage size), and lets you view sample documents. Instead of running `db.collection.find()` commands, you can use a simple text UI and even chat with an AI model (currently provided by Ollama, OpenAI, or Google) for schema explanations, query suggestions, etc.

Project's GitHub repository: [https://github.com/habedi/mongo-analyser](https://github.com/habedi/mongo-analyser)

The project is in the beta stage, and suggestions and feedback are welcome.",-1,2025-06-04 19:13:36
"im writing \~5 million rows from a pandas dataframe to an azure sql database. however, it's super slow.

any ideas on how to speed things up? ive been troubleshooting for days, but to no avail.

Simplified version of code:

    import pandas as pd
    import sqlalchemy
    
    engine = sqlalchemy.create_engine(""<url>"", fast_executemany=True)
    with engine.begin() as conn:
        df.to_sql(
            name=""<table>"",
            con=conn,
            if_exists=""fail"",
            chunksize=1000,
            dtype=<dictionary of data types>,
        )

database metrics:

https://preview.redd.it/4bw00ejoa8xe1.png?width=851&format=png&auto=webp&s=73e2dc92d1ee43b3f4b1ce58f29175da2c251862",-1,2025-04-26 19:13:35
"where exactly does n8n fit into your data engineering stack, if at all?

I‚Äôm evaluating it for workflow automation and ETL coordination. Before I commit time to wiring it in, I‚Äôd like to know:
	‚Ä¢	Is n8n reliable enough for production-grade pipelines?
	‚Ä¢	Are you using it for full ETL (extract, transform, load) or just as an orchestration and alerting layer?
	‚Ä¢	Where has it actually added value vs. where has it been a bottleneck?
	‚Ä¢	Any use cases with AI/ML integration like anomaly detection, classification, or intelligent alerting?

Not looking for marketing fluff‚Äîjust practical feedback on how (or if) it works for serious data workflows.

Thanks in advance. Would appreciate any sample flows, gotchas, or success stories.",-1,2025-05-23 05:52:42
"Hi all,  
I am currently working as a data engineer \~3 YOE currently on notice period of 90 days and Iam looking for guidance on how to upskill and prepare myself to land a job at a top tier company (like FAANG, product-based, or top tech startups).

**My current tech stack:**

* **Languages**: Python, SQL, PLSQL
* **Cloud/Tools**: Snowflake, AWS (Glue, Lambda, S3, EC2, SNS, SQS, Step Functions), Airflow
* **Frameworks**: PySpark (beginner to intermediate), Spark SQL, Snowpark, DBT, Flask, Streamlit
* **Others**: Git, CI/CD, DevOps basics, Schema Change, basic ML knowledge

**What I‚Äôve worked on:**

* designed and scaled etl pipelines with AWS Glue and S3 supporting 10M+ daily records
* developed PySpark jobs for large-scale data transformations 
* built near real time and batch pipelines using Glue, Lambda, Snowpipe, Step Functions, etc.
* Created a Streamlit based analytics dashboard on Snowflake
* worked with RBAC, data masking, CDC, performance tuning in Snowflake
* Built a reusable ETL and Audit Balance Control
* experience with CICD pipelines for code promotion and automation

I feel I have a good base but want to know:

* What skills or tools should I focus on next?
* Is my current stack aligned with what top companies expect?
* Should I go deeper into pyspark or explore something like kafka, kubernetes, data modeling
* How important are system design or coding DSA for data engineer interviews?

would really appreciate any feedback, suggestions, or learning paths.

thanks in advance",-1,2025-04-30 08:17:17
"Hi guys,

I'm building a small Spark cluster on Kubernetes and wonder how I can create a metastore for it? Are there any resources or tutorials? I have read the documentation, but it is not clear enough.  I hope some experts can shed light on this. Thank you in advance!",-1,2025-04-28 12:07:53
Looking for opinions from professionals. ,-1,2025-04-22 20:24:51
"If you‚Äôre working in market research, product intelligence, or anything that involves scraping data at¬†[scale](https://www.promptcloud.com/blog/simple-web-scraping-project-solutions/?utm_source=reddit&utm_medium=social&utm_campaign=socialpost_22april2025), you know one thing: not all scraper tools are built the same.

Some break under load. Others get blocked on every other site. And a few‚Ä¶ well, let‚Äôs say they need a dev team babysitting them 24/7.

We put together a practical guide that breaks down the¬†**10 must-have features**¬†every serious online data scraper tool should have. Think:  
‚úÖ Scalability for millions of pages  
‚úÖ Scheduling & Automation  
‚úÖ Anti-blocking tech  
‚úÖ Multiple export formats  
‚úÖ Built-in data cleaning  
‚úÖ And yes, legal compliance too

It‚Äôs not just theory; we included real-world use cases, from lead generation to price tracking, sentiment analysis, and training AI models.

If your team relies on web data for growth, this post is worth the scroll.  
üëâ¬†[Read the full breakdown here](https://www.promptcloud.com/blog/top-10-features-of-a-data-scraper-tool/?utm_source=reddit&utm_medium=social&utm_campaign=socialpost_22april2025)  
üëâ¬†[Schedule a demo](https://www.promptcloud.com/schedule-a-demo/?utm_source=reddit&utm_medium=social&utm_campaign=socialpost_22april2025promptcloud.com)¬†if you're done wasting time on brittle scrapers.

I would love to hear from others who are¬†[scraping at scale](https://www.promptcloud.com/blog/managed-web-scraping-for-data-collection/). What‚Äôs the one feature you¬†*need*¬†in your tool?",-1,2025-04-22 11:29:31
"Wondering if anybody can explain the differences of filter system, block storage, file storage, object storage, other types of storage?, in easy words and in analogy any please in an order that makes sense to you the most. Please can you also add hardware and open source and close source software technologies as examples for each type of these storage and systems. The simplest example would be my SSD or HDD in laptops. ",-1,2025-04-27 21:56:15
"I work on a data platform and currently we have several new ingestions coming in Databricks, Medallion architecture. 

I asked the 2 incoming sources to fill in table schema which contains column name, description, data type, primary key and constraints. Most important are data types and constraints in terms of tracking valid and invalid records. 

We are cureently at the stage to start tracking dq across the whole platform. So i am wondering what is the best way to start with this?

I had the idea to ingest everythig as is to bronze layer. Then before going to silver, check if recoeds are following the data shema, are constraints met (f.e. values within specified ranges, formatting of timestamps etc). If there are records which do not meet these rules, i was thinking about putting them to quarantine. 

My question, how to quarantine them? And if there are faulty records found, should we immediately alert the source or only if a certain percentage of records are faulty?

Also should we add another column in silver 'valid' which would signify if the record is meeting the table schema and constraints defined? So that would be the way to use this column and report on % of faulty records which could be a part of a DQ dashboard?

",-1,2025-04-26 09:26:12
"Lately I‚Äôve been wondering: is the title ‚ÄúData Engineer‚Äù starting to lose its meaning?

This isn‚Äôt a complaint or a gatekeeping rant‚ÄîI love how accessible the tech industry has become. Bootcamps, online resources, and community content have opened doors for so many people. But at the same time, I can‚Äôt help but feel that the role is being diluted.

What once required a solid foundation in Computer Science‚Äîdata structures, algorithms, systems design, software engineering principles‚Äîhas increasingly become something you can ‚Äúlearn‚Äù in a few weeks. The job often gets reduced to moving data from point A to point B, orchestrating some tools, and calling it a day. And that‚Äôs fine on the surface‚Äîuntil you realize that many of these pipelines lack test coverage, versioning discipline, clear modularity, or even basic error handling.

Maybe I‚Äôm wrong. Maybe this is exactly what democratization looks like, and it‚Äôs a good thing. But I do wonder: are we trading depth for speed? And if so, what happens to the long-term quality of the systems we build?

Curious to hear what others think‚Äîespecially those with different backgrounds or who transitioned into DE through non-traditional paths.
",-1,2025-04-23 13:31:24
"Hi! This is Phil - Founder of¬†[GizmoData](https://gizmodata.com). We have a new commercial database engine product called:¬†[GizmoSQL](https://gizmodata.com/gizmosql)¬†\- built with Apache Arrow Flight SQL (for remote connectivity) and DuckDB (or optionally: SQLite) as a back-end execution engine.

This product allows you to run DuckDB or SQLite as a server (remotely) - harnessing the power of computers in the cloud - which typically have more CPUs, more memory, and faster storage (NVMe) than your laptop. In fact, running GizmoSQL on a modern arm64-based VM in Azure, GCP, or AWS allows you to run at terabyte scale - with equivalent (or better) performance - for a fraction of the cost of other popular platforms such as Snowflake, BigQuery, or Databricks SQL.

**GizmoSQL**¬†is self-hosted (for now) - with a possible SaaS offering in the near future. It has these features to differentiate it from ""base"" DuckDB:

* Run DuckDB or SQLite as a server (remote connectivity)
* Concurrency - allows multiple users to work simultaneously - with independent, ACID-compliant sessions
* Security
   * Authentication
   * TLS for encryption of traffic to/from the database
* Static executable with Arrow Flight SQL, DuckDB, SQLite, and JWT-CPP built-in. There are no dependencies to install - just a single executable file to run
* Free for use in development, evaluation, and testing
* Easily containerized for running in the Cloud - especially in Kubernetes
* Easy to talk to - with ADBC, JDBC, and ODBC drivers, and now a Websocket proxy server (created by GizmoData) - so it is easy to use with javascript frameworks
   * Use it with Tableau, PowerBI, Apache Superset dashboards, and more
* Easy to work with in Python - use ADBC, or the new experimental Ibis back-end - details here:¬†[https://github.com/gizmodata/ibis-gizmosql](https://github.com/gizmodata/ibis-gizmosql)

Because it is powered by DuckDB - GizmoSQL can work with the popular open-source data formats - such as Iceberg, Delta Lake, Parquet, and more.

GizmoSQL performs very well (when running DuckDB as its back-end execution engine) - check out our graph comparing popular SQL engines for TPC-H at scale-factor 1 Terabyte - on the homepage at:¬†[https://gizmodata.com/gizmosql](https://gizmodata.com/gizmosql)¬†\- there you will find it also costs far less than other options.

We would love to get your feedback on the software - it is easy to get started for free in two different ways:

* For a limited time - try GizmoSQL online on our dime - with the SQL Query Navigator - it just requires a quick registration and sign-in to get going - at:¬†[https://app.gizmodata.com](https://app.gizmodata.com)¬†\- where we have a read-only 1TB TPC-H database mounted for you to query in real-time. It is running on an Azure Cobalt 100 VM - with local NVMe SSD's - so it should be quite zippy.
* Download and self-host GizmoSQL - using our Docker image or executables for Linux and macOS for both x86-64 and arm64 architectures. See our README at:¬†[https://github.com/gizmodata/gizmosql-public](https://github.com/gizmodata/gizmosql-public)¬†for details on how to easily and quickly get started that way

Thank you for taking a look at GizmoSQL. We are excited and are glad to answer any questions you may have!

* **Public facing repo (README):**¬†[https://github.com/gizmodata/gizmosql-public?tab=readme-ov-file](https://github.com/gizmodata/gizmosql-public?tab=readme-ov-file)
* **HomePage**:¬†[https://gizmodata.com/gizmosql](https://gizmodata.com/gizmosql)
* **ProductHunt:**¬†[https://www.producthunt.com/posts/gizmosql?embed=true&utm\_source=badge-featured&utm\_medium=badge&utm\_souce=badge-gizmosql](https://www.producthunt.com/posts/gizmosql?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-gizmosql)
* **Try GizmoSQL online:**¬†[https://app.gizmodata.com](https://app.gizmodata.com)
* **GizmoSQL in action video:**¬†[https://youtu.be/QSlE6FWlAaM](https://youtu.be/QSlE6FWlAaM)",-1,2025-04-08 13:30:54
"One of the silent killers of query performance in complex analytical workloads is redundant computation, especially when the same subquery or expression gets evaluated multiple times in a single query plan.

We recently tackled this at e6data by introducing Automatic CTE Detection inside our query planner. Our core idea? Detect repeated expressions or subplans in the logical plan, factor them into common table expressions (CTEs), and reuse the computed result.

Click the link to read our full blog. ",-1,2025-04-25 11:27:53
"Hi everyone, I am working on a project currently where we have a MySQL database. We are using clickhouse as our warehouse. 

What we need to achieve is to reflect the data from MySQL to clickhouse for certain tables. For this, I found a few ways and am looking to get some insights on which method has the most potential and if there are other methods as welp:

1. Use the MySQL engine in clickhouse. 

Pros: No need to store data in clickhouse as it can just proxy it directly from MySQL.

Cons: This however puts extra reads on MySQL and doesn't help us if MySQL ever goes down. 

2. Use signals to send the data to clickhouse whenever there is a change in MySQL.

Pros: We don't have a lot of tables currently so it's the quickest to setup. 

Cons: Extremely inefficient and not scalable. 

3. Use some sort of third party sink to achieve this. I have found this https://github.com/Altinity/clickhouse-sink-connector which seems to do the job but it has way too many open issues and not sure if it is reliable enough. Plus, it complicates our tech stack which we are looking not to do. 

I'm open to any other ideas. We would ideally not want to duplicate this data in clickhouse but if that's the last resort we would go for it. 

Thanks in advance. 

P.S, I am a beginner in data engineering so feel free to correct me if I've used some wrong jargons or if I am seriously deviating from the right path. ",-1,2025-04-08 06:49:07
The most recent discussion I saw about NetCDF basically said it's outdated and to use HDF5 (15 years ago). Any thoughts on it now?,-1,2025-04-23 15:23:57
"link: www.filtrjobs.com

I was tired of finding irrelevant postings, so i built a tool for myself

Many companies (like airbnb/meta) hire data engineers, but postings are titled software engineer. So if you search for data engineer in linkedin its hard to find those SWE roles that are actually data engineering roles

**How it works**

You upload your CV and I automatically create a query:

`""Find ${title listed in CV} jobs with experience similar to ${your CV bullets}""`

and it ranks all job postings based on match

I built this for Engineering roles (SWE/ML) in the US only. It's 100% free as im running it within free tiers!

**Resource List:**

I did a ton of research to find cheap hosting ways. Here are my best finds:

* Free 5GB Postgres via¬†[aiven.io](http://aiven.io/):¬†[https://aiven.io/free-postgresql-database](https://aiven.io/free-postgresql-database)
* Free 15GB Postgres from¬†[xata.io](http://xata.io/):¬†[https://xata.io/blog/postgres-free-tier](https://xata.io/blog/postgres-free-tier)
* CockroachDB:¬†[https://www.cockroachlabs.com/docs/cockroachcloud/quickstart-trial-cluster](https://www.cockroachlabs.com/docs/cockroachcloud/quickstart-trial-cluster)

I'm hosting on¬†[modal.com](http://modal.com/)¬†which gives you 30$/mo of free GPU usage",-1,2025-04-24 00:58:49
"
Hey everyone!

I‚Äôve started a GitHub repository aimed at collecting ready-to-use data recipes and API wrappers ‚Äì so anyone can quickly access and use real-world data without the usual setup hassle. It‚Äôs designed to be super friendly for first-time contributors, students, and anyone looking to explore or share useful data sources.

üîó https://github.com/leftkats/DataPytheon

The goal is to make data more accessible and practical for learning, projects, and prototyping. I‚Äôd love your thoughts on it!

Know of any similar repositories? Please share!
Found it interesting? A star would mean a lot !

Want to contribute? PRs are very welcome!

Thank you for reading !",-1,2025-06-07 14:32:34
"I've got a text analytics project for crypto I am working on in python and R. I want to make the results public on a website.

I need a database which will be updated with new data (for example every 24 hours). Which is the better platform to start off with if I want to launch it fast and preferrably cheap?

[https://streamlit.io/](https://streamlit.io/)

[https://render.com/](https://render.com/)

[https://www.heroku.com/](https://www.heroku.com/)

[https://www.digitalocean.com/](https://www.digitalocean.com/)",-1,2025-04-24 03:04:10
"Using NiFi for years and after trying both hybrid and private cloud setups, I still find myself relying on a full on-premise environment. With cloud, I faced challenges like unpredictable performance, latency in site-to-site flows, compliance concerns, and hidden costs with high-throughput workloads. Even private cloud didn‚Äôt give me the level of control I need for debugging, tuning, and data governance. On-prem may not scale like the cloud, but for real-time, sensitive data flows‚Äîit‚Äôs just more reliable.

 Curious if others have had similar experiences and stuck with on-prem for the same reasons.",-1,2025-05-26 11:14:00
"I'm noticing a trend at work (mid-size financial tech company) where more of our data engineering work is overlapping with enterprise architecture stuff. Things like aligning data pipelines with ""long-term business capability maps"", or justifying infra decisions to solution architects in EA review boards.

It did make me think that maybe it's worth getting a [TOGAF certification](https://www.advisedskills.com/enterprise-architecture/togaf-ea-foundation-and-practitioner-level-1-and-2) like this. It's online and maybe easier to do, and could be useful if I'm always in meetings with architects who throw around terminology from ADM phases or talk about ""baseline architectures"" and ""transition states.""

But basically, I get the high-level stuff, but I haven't had any formal training in EA frameworks. So is this happening everywhere? Do I need TOGAF as a data engineer, is it really useful in your day-to-day? Or more like a checkbox for your CV?",-1,2025-04-24 16:25:24
"[my\_pipeline](https://preview.redd.it/6xdqaeiikjue1.png?width=1792&format=png&auto=webp&s=7ca7ab2d25d73b1f4f7869c3927fd16c0246bb04)

Hello all! I'm excited to dive into **ADF** and try out some new things.

Here, you can see we have a copy data activity that transfers files from the source ADLS to the raw ADLS location. Then, we have a Lookup named **Lkp\_archivepath** which retrieves values from the SQL server, known as the Metastore. This will get values such as **archive\_path** and **archive\_delete\_flag** (typically it will be Y or N, and sometimes the parameter will be missing as well). After that, we have a copy activity that copies files from the source ADLS to the archive location. Now, I'm encountering an issue as I'm trying to introduce this archive delete flag concept.

If the **archive\_delete\_flag** is '**Y**', it should not delete the files from the source, but it should delete the files if the **archive\_delete\_flag** is '**N**', '' or NULL, depending on the Metastore values. How can I make this work?

Looking forward to your suggestions, thanks!",-1,2025-04-13 06:02:54
"Hello,

We are starting a new Big Data project in my company with Cloudera, Hive, Hadoop HDFS, and a medallion architecture, but I have some questions about ""Bronze"" layer.

Our source is a FTP and in this FTP are allocated the daily/monthly files (.txt, .csv, .xlsx...).  
We bring those files to our HDFS in separated in folders by date (E.G: xxxx/2025/4)

Here start my doubts:  
\- Our bronze layer are those files in the HDFS?  
\- For build our bronze layer, we need to load those files incrementally into a ""bronze table"" partitioned by date

Reading on internet I saw that we have to do the second option, but I saw that option like a rubbish table

Which will be the best approach?

  
For the other layers, I don't have any doubts.",-1,2025-04-01 07:20:04
"Been working in industrial data for years and finally had enough of the traditional historian nonsense. You know the drill - proprietary formats, per-tag licensing, gigabyte updates that break on slow connections, and support that makes you want to pull your hair out. So, we tried something different. Replaced the whole stack with:

* Telegraf for data collection (700+ OPC UA tags)
* InfluxDB Core for edge storage
* Azure Data Explorer for long-term analytics
* Grafana for dashboards

Results after implementation:  
‚úÖ Reduced latency & complexity  
‚úÖ Cut licensing costs  
‚úÖ Simplified troubleshooting  
‚úÖ Familiar tools (Grafana, PowerBI)

The gotchas:

* Manual config files (but honestly, not worse than historian setup)
* More frequent updates to manage
* Potential breaking changes in new versions

Worth noting - this isn't just theory. We have a working implementation with real OT data flowing through it. Anyone else tired of paying through the nose for overcomplicated historian systems?

Full technical breakdown and architecture diagrams:¬†[https://h3xagn.com/designing-a-modern-industrial-data-stack-part-1/](https://h3xagn.com/designing-a-modern-industrial-data-stack-part-1/)",-1,2025-06-07 09:24:10
"Hey everyone,  
I'm trying to use Prefect for one of my projects. I really believe it's a great tool, but I've found the official docs a bit hard to follow at times. I also tried using AI to help me learn, but it seems like a lot of the advice is based on outdated methods.  
Does anyone know of any good tutorials, courses, or other resources for learning Prefect (ideally up-to-date with the latest version)? Would really appreciate any recommendations",-1,2025-04-23 00:20:30
"Hey all,

I've just created my second mini-project. Again, just to practice the skill I have learnt through DataCamp's courses.

  
I imported London's weather data via OpenWeather's API, cleaned it and created a database from it (STAR Schema)

  
If I had to do it again I will probably write functions instead of doing transformations manually. I really don't know why I didn't start of using function

  
I think my next project will include multiple different data sources and will also include some form of orchestration.

Here is the link: [https://www.datacamp.com/datalab/w/6aa0a025-9fe8-4291-bafd-67e1fc0d0005/edit](https://www.datacamp.com/datalab/w/6aa0a025-9fe8-4291-bafd-67e1fc0d0005/edit)

Any and all feedback is welcome.

Thanks!",-1,2025-04-02 17:19:33
"Which is your preferred way to host your data catalog inside of gcp? I know that inside of aws, glue is the preferred way?  
I know that it can make sense to use dataproc Metastore and/or big data lake Metastore.

I know that there are also a lot open source tools that you can use?

what do you prefer? what's your experience?",-1,2025-04-02 11:51:11
Databricks announced free editiin for learning and developing which I think is great but it may reduce databricks consultant/engineers' salaries with market being flooded by newly trained engineers...i think informatica did the same many years ago and I remember there was a large pool of informatica engineers but less jobs...what do you think guys?,-1,2025-06-12 00:54:19
"**AnuDB** \- a lightweight, embedded document database.

# Key Features

* **Embedded & Serverless**: Runs directly within your application - no separate server process required
* **JSON Document Storage**: Store and query complex JSON documents with ease
* **High Performance**: Built on RocksDB's LSM-tree architecture for optimized write performance
* **C++11 Compatible**: Works with most embedded device environments that adopt C++11
* **Cross-Platform**: Supports both Windows and Linux (including embedded Linux platforms)
* **Flexible Querying**: Rich query capabilities including equality, comparison, logical operators and sorting
* **Indexing**: Create indexes on frequently accessed fields to speed up queries
* **Compression**: Optional ZSTD compression support to reduce storage footprint
* **Transactional Properties**: Inherits atomic operations and configurable durability from RocksDB
* **Import/Export**: Easy JSON import and export for data migration or integration with other systems

Checkout README for more info: [https://github.com/hash-anu/AnuDB](https://github.com/hash-anu/AnuDB)",-1,2025-03-30 04:26:19
"Until about a month ago hiring seemed to be freezed - lot of fake job postings, people posting google form links collecting resumes, reposting old job roles on linkedin...  Then since about three weeks ago, it seemed like hring is restarted. But now I am having my doubts again - ghosted by recruiters after first screening even told me my CV fits the role well. And not getting other shortlists too. Another thing is huge range of experience 3 yrs - 7 yrs , 2 yrs to 9 yrs experience being posted for majority of the JDs. Obviously if a 7 yrs candidate and if a 3 yrs candidate applies to the same role, they would prefer the 7 yrs exp candidate. What's going on these days? Are they not hiring anyone below 6/7 yrs work exp at all?",-1,2025-04-02 08:21:32
"Hey everyone! üëã

I‚Äôm working on a¬†**Conceptual Data Model (MCD)**¬†for a training management system and I‚Äôd love to get some feedback

The main elements of the system are:

* **Formateurs**¬†(trainers) teach¬†**Modules**
* Each¬†**Module**¬†is scheduled into one or more¬†**S√©ances**¬†(sessions)
* **Stagiaires**¬†(trainees) can participate in sessions, and their participation can be marked as ""Present"" or ""Absent""
* If a trainee is absent, there can be a¬†**Justification**¬†linked to that absence

I decided to merge the ""Assistance"" (Assister) and ‚ÄúAbsence‚Äù (Absenter) relationships into a single¬†**Participation**¬†relationship with a possible attribute like¬†`Status`, and added a link from participation to a¬†**Justification**¬†(0 or 1).

Does this structure look correct to you? Any suggestions to improve the logic, simplify it further, or potential pitfalls I should watch out for?

Thanks in advance for your help

https://preview.redd.it/mpn8p43kk0we1.png?width=806&format=png&auto=webp&s=ea0c00e582b94a8168a3d991d90cef5439c3bee9

",-1,2025-04-20 16:09:19
"**AnuDB** \- a lightweight, embedded document database.

# Key Features

* **Embedded & Serverless**: Runs directly within your application - no separate server process required
* **JSON Document Storage**: Store and query complex JSON documents with ease
* **High Performance**: Built on RocksDB's LSM-tree architecture for optimized write performance
* **C++11 Compatible**: Works with most embedded device environments that adopt C++11
* **Cross-Platform**: Supports both Windows and Linux (including embedded Linux platforms)
* **Flexible Querying**: Rich query capabilities including equality, comparison, logical operators and sorting
* **Indexing**: Create indexes on frequently accessed fields to speed up queries
* **Compression**: Optional ZSTD compression support to reduce storage footprint
* **Transactional Properties**: Inherits atomic operations and configurable durability from RocksDB
* **Import/Export**: Easy JSON import and export for data migration or integration with other systems

Checkout README for more info: [https://github.com/hash-anu/AnuDB](https://github.com/hash-anu/AnuDB)",-1,2025-03-30 04:26:19
"I have been researching some easier ways to build integrations and was suggested by a founder to look up Leen. They seem like a relatively new startups, \~2y old. Their docs look pretty compelling and straightforward, but curious is anyone has heard or used them or a similar service. ",-1,2025-04-20 04:42:52
"Hello all! I'm a software engineer, and I have very limited experience with data science and related fields. However, I work for a company that develops tools for data scientists and that somewhat requires me to dive deeper into this field.

I'm slowly getting into it, but what I kinda struggle with is understanding DE tools landscape. There are so much of them and it's hard for me (without practical expreience in the field) to determine which are actually used, which are just hype and not really used in production anywhere, and which technologies might be not widely discussed anymore, but still used in a lot of (perhaps legacy) setups.

To figure this out, I decided the best solution is to ask people who actually work with data lol. So would you mind sharing in the comments what technologies you use in your job? Would be super helpful if you also include a bit of information about what you use these tools for.",-1,2025-06-12 15:46:05
Will any company hire me? What certificate could I obtain that would help me?,-1,2025-03-30 11:12:06
"Hello folks, I‚Äôm new to data engineering and currently exploring the field.
I come from a software development background with 3 years of experience, and I‚Äôm quite comfortable with Python, especially libraries like Pandas and NumPy. I'm now trying to understand the tools and technologies commonly used in the data engineering domain.

I‚Äôve seen that Scala is often mentioned in relation to big data frameworks like Apache Spark. I‚Äôm curious‚Äîis learning Scala important or beneficial for a data engineering role? Or can I stick with Python for most use cases?
",-1,2025-04-21 16:40:59
Hi r/dataengineering community. Trying to replace excel based reports that connect to databases and have in-built data transformation logic across worksheets. Is there a utility or platform you have used to help decipher and document the data dependencies / data lineage from excel?,-1,2025-04-12 08:37:34
"End-to-end pipeline **deployment steps**:  
1Ô∏è‚É£ Terraform: Set up GCS & BigQuery  
2Ô∏è‚É£ Python: Load data to GCS  
3Ô∏è‚É£ dbt: Transform data  
4Ô∏è‚É£ Airflow: Orchestrate  
5Ô∏è‚É£ Looker: Visualize üìä  
\#dezoomcamp",-1,2025-03-31 00:56:16
"Lessons learned from loading Airbnb data into GCP:  
‚úîÔ∏è Use **autodetect schema** in BigQuery for flexibility  
‚úîÔ∏è Handle CSV quirks with proper configs  
‚úîÔ∏è Optimize storage with partitioning  
\#dezoomcamp #gcp #dataengineering",-1,2025-03-31 00:54:01
Will any company hire me? What certificate could I obtain that would help me?,-1,2025-03-30 11:12:06
As title says ,-1,2025-03-30 07:53:33
"i‚Äôm designing functions to clean data for two separate pipelines: one has small string inputs, the other has medium-size pandas inputs. both pipelines require the same manipulations.

for example, which is a better design: clean\_v0 or clean\_v1?

that is, should i standardize object types inside or outside the cleaning function?

thanks all! this community has been a life saver :)

",-1,2025-03-31 16:48:55
"Here are my device specifications:
- Processor: Intel(R) Core(TM) i3-4010U @ 1.70GHz
- RAM: 8 GB
- GPU: AMD Radeon R5 M230 (VRAM: 2 GB)

I tried running Ubuntu in a virtual machine, but it was really slow. So now I'm wondering: if I use WSL instead, will the performance be better and more usable? I really don't like using dual boot setups.

I mainly want to use Linux for learning data engineering and DevOps.",-1,2025-04-21 10:30:35
"I‚Äôve never had a positive experience using low/no code tools but my company is looking to explore Prophecy to streamline our data pipeline development. 

If you‚Äôve used Prophecy in production or even during a POC, I‚Äôm curious to hear your unbiased opinions. If you don‚Äôt mind answering a few questions at the top of my head:

How much development time are you actually saving?

Any pain points, limitations, or roadblocks?

Any portability issues with the code it generates?

How well does it scale for complex workflows?

How does the Git integration feel?",-1,2025-04-21 16:02:00
"I am trying to get max value from lakehouse table using script , as we cannot use lakehouse in the lookup, trying with script.

I have script inside a for loop, and I am constructing the below query

@{concat(‚Äòselect max(‚Äòitem().inc_col, ‚Äò) from ‚Äò, item().trgt_schema, ‚Äò.‚Äô, item().trgt_table)}

It is throwing argument{0} is null or empty. Pramter name:parakey.

Just wanted to know if anyone has encountered this issue?

And in the for loop I have the expression as mentioned in the above pic.
",-1,2025-06-03 18:22:31
Enjoy ‚ù§Ô∏è,-1,2025-04-22 17:09:13
As title says ,-1,2025-03-30 07:53:33
"i have to send data from bigquery using aws glue to rds, i need to understand how to create big query source node in glue that can access a view from big query , is it by selecting table or custom query option... also what to add in materialization dataset , i dont have that ??? i have tried using table option , added view details there but then i get an error that view is not enabled in data preview section.

",-1,2025-03-29 21:14:57
"Check out the new blog about Fact Tables   
[https://medium.com/@adityasharmah27/fact-tables-the-backbone-of-your-data-warehouse-9a3014cc20c3](https://medium.com/@adityasharmah27/fact-tables-the-backbone-of-your-data-warehouse-9a3014cc20c3)",-1,2025-04-14 10:54:13
"Hello everyone,

I work as a Data Engineer in a team where we have set up a fairly standard but robust processing chain:
	‚Ä¢ We have ‚Äúraw‚Äù tables in BigQuery
	‚Ä¢ We make transformations to move from the fine mesh (transaction) to the aggregate mesh.
	‚Ä¢ Then we export a copy of this data into PostgreSQL
	‚Ä¢ The backend relies on these tables to power a web application allowing businesses to make dynamic multi-mesh aggregations

And there‚Ä¶ we are being told that we are going to replace this web application with Dataiku.
The idea is to keep the processing in BigQuery, but for business users to do their exploration directly via Dataiku instead of going through the app.

I am divided:
	‚Ä¢ I understand that Dataiku can give more autonomy to professions
	‚Ä¢ But I find that it is not designed for dynamic or multi-mesh visualization
	‚Ä¢ And that seems a little rigid to me compared to a web front which offered more control, more logic, and a real UX

Have any of you experienced a similar situation?
Do you think Dataiku can really replace a web analytics app?
Or is there a risk of ‚Äúswitching everything to no-code‚Äù for cases that are not so simple?

Thank you for your feedback!",-1,2025-05-02 15:38:50
"We're working on a uni project where we need to design the database for an Ticketing system that will support around 7,000 users. Under normal circumstances, I'd definitely go with a relational database. But we're *required* to use multiple **NoSQL** databases instead. Any suggestions for NoSQL Databases?",-1,2025-04-14 17:21:23
Is there a cloud service that guarantees data residency in Switzerland in compliance with Swiss data protection regulations?,-1,2025-06-03 16:17:36
"Hi all,

I am teaching myself Data Engineering. I am working on a project that incorporates everything I know so far and this includes getting data via Web scraping. 

I think I underestimated how hard it would be. I've taken a course on webscraping but I underestimated the depth that exists, the tools available as well as the fact that the site itself can be an antagonist and try to stop you from scraping. 

This is not to mention that you need a good understanding of HTML and website; which for me, as a person who only knows coding through the eyes of databases and pandas was quite a shock. 

Anyways, I just wanted to know how relevant webscraping is in the toolbox of a data engineers. 

Thanks ",-1,2025-04-26 20:37:14
"I'm using Airbyte Cloud because my PC doesn't have enough resources to install it. I have a Docker container running PostgreSQL on Airbyte Cloud. I want to set the PostgreSQL destination. Can anyone give me some guidance on how to do this? Should I create an SSH tunnel?

",-1,2025-04-13 00:06:47
"Just curious. I just moved from the Salesforce  to the Microsoft ecosystem. I'm currently publishing my PowerBI dashboards and posting them in a SharePoint page so everything lives organized in the same place. 

Looking for different and better ideas.

Thank you in advance ",-1,2025-04-24 10:55:34
"In IICS, will I see cloud resource contention if I have all of my development env's (Dev,QA,SIT,PRE) in the same Prod Org as Sub Orgs? Is it best practice to have development envirioments outside of the Prod Org as a seperate Org?",-1,2025-04-25 14:52:02
"We‚Äôre in the process of replacing our current ETL tool, Talend. Right now, our setup reads files from blob storage, uses a SQL database to manage metadata, and outputs transformed/structured data into another SQL database.

The proposed new stack includes that we use python with the following components:

* **Blob storage**
* **Lakehouse** (Iceberg)
* **Polars** for working with dataframes
* **DuckDB** for SQL querying
* **Pydantic** for data validation
* **Dagster** for orchestration and data lineage

This open-source approach is new to me, so I‚Äôm looking for insights from those who might have experience with any of these tools or with similar migrations. What are the pros and cons I should be aware of? Any lessons learned or potential pitfalls?

Appreciate your thoughts!",-1,2025-06-04 13:46:07
"I was given the following assignment as part of a job application. Would love to hear if people think this is reasonable or overkill for a take-home test:

**Assignment Summary:**

* Build a **Python data pipeline** and expose it via an **API**.
* The API must:
   * Accept a **venue ID**, **start date**, and **end date**.
   * Use Open-Meteo's historical weather API to fetch **hourly weather data** for the specified range and location.
   * Extract 10+ parameters (e.g., temperature, precipitation, snowfall, etc.).
   * Store the data in a **cloud-hosted database**.
   * Return success or error responses accordingly.
* Design the database schema for storing the weather data.
* Use **OpenAPI 3.0** to document the API.
* Deploy on **any cloud provider** (AWS, Azure, or GCP), including:
   * Database
   * API runtime
   * API Gateway or equivalent
* Set up **CI/CD pipeline** for the solution.
* Include a **README** with setup and testing instructions (Postman or Curl).
* Implement **QA checks in SQL** for data consistency.

Does this feel like a reasonable assignment for a take-home? How much time would you expect this to take?",-1,2025-04-13 05:13:59
"DE‚Äôs who are currently working on the tech stack such as ADLS , ADF , Synapse , Azure SQL DB and mostly importantly Databricks within Azure ecosystem. Could you please brief me a bit about your current project architecture, like from what all sources you are fetching the data, how you are staging it , where ETL pipelines are being built , what is the serving layer (Data Warehouse) for reporting teams and how Databricks is being used in this entire architecture?, Its just my curiosity to understand, how people are using Azure ecosystem to cater to their current project requirements in their organizations‚Ä¶ ",-1,2025-06-03 21:04:30
Looking to hear from user stories that actually use Apache Paimon at scale in production,-1,2025-06-04 06:04:10
"Hello, I am looking for some feedback on how other organizations handle PII and PHI access for software devs and data engineers. I feel like my company's practices are very sloppy and I am the only one that cares. We dont have good environment separation as many DE's do dev in a single snowflake account that is pointed at production AWS where there is PII and PHI. The level of access is concerning to me not only for leakage, but this goes against the best practices for development that I've always known. I've started an initiative to build separate dev,stage, prod accounts with masked data in the lower environments, but this always gets put on the back burner for urgent client asks. Looking for a sanity check as I wonder, at times, if I am overthinking it. I would love to know how others have dealt with access to production data. Do your DE's work in a separate cloud account or separate set of servers? Is PII/PHI allowed in the environments where dev work is being done?",-1,2025-04-25 12:32:04
"Testing is a well established discipline in software engineering, entire careers are built around ensuring code reliability. But in data engineering, testing often feels like an afterthought.

Despite building complex pipelines that drive business-critical decisions, many data engineers still lack consistent testing practices. Meanwhile, software engineers lean heavily on unit tests, integration tests, and continuous testing as standard procedure.

The truth is, data pipelines *are* software. And when they fail, the consequences: bad data, broken dashboards, compliance issues‚Äîcan be just as serious as buggy code.

I've written a some of articles where I build a dbt project and implement tests, explain why they matter, where to use them.  
  
If you're interested, check it out.",-1,2025-06-03 08:49:54
"I'm looking for a good quality suite and stumbled on Soda recently, but I don't see much discussion here, which I find weird. Anyone here using it, or abandoned it?",-1,2025-04-25 20:26:28
"I've done so many technical interviews, and there's one recurring pattern that I'm noticing.

The need for developers who can write code or design systems to power infrastructure for machine learning model teams?

But why is this so up-and-coming? We've tackled major infrastructure-related challenges in the past ( think Big Data, Hadoop, Spark, Flink, Map Reduce ), where we needed to deploy large clusters of distributed machines to do efficient computation?

Can't the same set of techniques or paradigms - sourced from distributed systems or performance research into Operating Systems - also be applied to the ML model space? What gives? ",-1,2025-05-28 20:43:15
"The semantic layer in Cube looks super useful ‚Äî defining metrics, dimensions, and joins in one place is a dream. But most use cases I‚Äôve seen are focused on BI dashboards and analytics.

I‚Äôm wondering if anyone here has used Cube for more *operational* or *app-level* read scenarios ‚Äî like powering parts of an internal tool, or building a unified read API across microservices (via Cube's GraphQL support). All read-only, but not just charts ‚Äî more like structured data fetching.

Any war stories, performance considerations, or architectural tips? Curious if it holds up well when the use case isn't classic OLAP.

Thanks!",-1,2025-04-14 14:59:21
I am a aspiring Data Engineer currently doing personal projects. I just wanna know how Acceptance criteria of a User story in Data Engineering look like.,-1,2025-04-25 14:39:35
"Hey folks,  
I recently wrote about an idea I've been experimenting with at work,  
**Self-Optimizing Pipelines**: ETL workflows that adjust their behavior dynamically based on real-time performance metrics (like latency, error rates, or throughput).

Instead of manually fixing pipeline failures, the system:\\n- Reduces batch sizes\\n- Adjusts retry policies\\n- Changes resource allocation\\n- Chooses better transformation paths

All happening¬†*mid-flight*, without human babysitting.

Here's the Medium article where I detail the architecture (Kafka + Airflow + Snowflake + decision engine): [https://medium.com/@indrasenamanga/pipelines-that-learn-building-self-optimizing-etl-systems-with-real-time-feedback-2ee6a6b59079](https://medium.com/@indrasenamanga/pipelines-that-learn-building-self-optimizing-etl-systems-with-real-time-feedback-2ee6a6b59079)

Has anyone here tried something similar? Would love to hear how you're pushing the limits of automated, intelligent data engineering.",-1,2025-04-27 07:32:31
"After 3 years and 580+ research papers, I finally launched synthetic datasets for 9 rheumatic diseases.

180+ features per patient, demographics, labs, diagnoses, medications, with realistic variance.
No real patient data, just research-grade samples to raise awareness, teach, and explore chronic illness patterns.

Free sample sets (1,000 patients per disease) now live. 

More coming soon. Check it out and have fun, thank you all!
",-1,2025-04-28 03:22:38
"What practice do you follow for the functional design documentation? The team uses the Agile framework to break down big projects into small, sizeable tasks, The same team also works on tickets to fix existing issues and enhancements to extend existing functionalities. We will build a functional area in a big project and continue to enhance it with smaller updates in the later sprints. 

Has anyone been in this situation? do you create a functional design document and keep updating it or build one document per story? Please share a template if something is working for you.

Thanks!",-1,2025-04-24 16:57:35
"I've recently learned about open-table paradigm, which if I am interpreting correctly, is essentially a mechanism for storing metadata so that the data associated with it can be efficiently looked up and retrieved. (Please correct this understanding if it is wrong). 

My question is whether or not you could have a single metadata store or open-table that combines metadata from two different storage solutions, so that you could query both from a single CLI tool using SQL like syntax? 

And as a follow on question... I've learned about and played with AWS Athena in an online course. It uses Glue Crawler to somehow discover metadata. Is this based on an open-table paradigm? Or a different technology? ",-1,2025-04-15 14:39:22
"I'm using Airbyte Cloud because my PC doesn't have enough resources to install it. I have a Docker container running PostgreSQL on Airbyte Cloud. I want to set the PostgreSQL destination. Can anyone give me some guidance on how to do this? Should I create an SSH tunnel?

",-1,2025-04-13 00:06:47
"Hi, i'm currently swapping my company data warehouse to a more modular solution using, among other things, a data lake.

I'm using Trino to set up a cluster and using it to connect to my AWS glue catalog and access my data on S3 buckets.

So, while setting Trino up, i was looking at their docs and some forum answers, and why does everywhere i look, people suggest ludicrous powerful machines as a baseline for trino? People recomend 64GB m5.4xlarge as a baseline for EACH worker? saying stuff like ""200GB should be enough for a starting point"".

I get it, Trino might be a really good solution for big datasets, and some bigger companies might just not care about expending 5k USD monthly only on EC2. But a smaller company with 4 employees, a startup, specially one located on other regions beyond us-east, simply saying you need 5x 4xlarge instances is, well, a lot...  
(for comparison, in my country, 5kUSD pays the salary of all members of the team and cover most of our other costs. and we have above average salaries for staff engineers...)  
  
I initially set my Trino cluster up with a 8gb ram machine and workers with 4 gb (t3.large and t3.medium on aws Ec2) and trino is actually working well, I have a 2TB dataset, which for many, is actually enough space.

Am I missing something? Is Trino bad as a simple solution for something like simply replacing athena queries costs and having more control over my data? Should i be looking somewhere else? Or is this just simply a problem of ""usually companies have a bigger budget?""

How can i get what is really a minimum baseline for using it?

",-1,2025-04-24 20:46:51
"Hey folks, I‚Äôm working on an idea for a SaaS platform and would love your honest thoughts.

The idea is simple:
You connect your existing database (MySQL, PostgreSQL, etc.), and then you can just type what you want in plain English like:

‚ÄúShow me the top 10 customers by revenue last year‚Äù

‚ÄúFind users who haven‚Äôt logged in since January‚Äù

‚ÄúJoin orders and payments and calculate the refund rate by product category‚Äù


No matter how complex the query is, the platform generates the correct SQL for you. It‚Äôs meant to save time, especially for non-SQL-savvy teams or even analysts who want to move faster.

Do you think this would be useful in your workflow? What would make this genuinely valuable to you?
",-1,2025-04-26 06:01:23
"[https://github.com/tontinton/miso](https://github.com/tontinton/miso)

Other than the obvious stuff like:

* Make it faster (benchmarking + improving implementation)
* Make it spool to disk to handle queries larger than memory
* Make it distributed to handle queries larger than memory / disk
* Implement a simple query language frontend for faster onboarding, something like KQL

Currently I only support [quickwit](https://quickwit.io/), and can pretty easily add elasticsearch support, but what other JSON databases would you think are the best fit? Datadog logs? MongoDB? Clickhouse jsons? Snowflake VARIANTs?

What features can a query engine that treats semi-structured data as a first class citizen have, that trino cannot?",-1,2025-04-12 17:02:24
"Hey all,

DE with 5.5 years of experience across a few big tech companies. I recently switched jobs and started a role at a company whose primary platform is Palantir Foundry - in all my years in data, I have yet to meet folks who are super well versed in Foundry or see companies hiring specifically for Foundry experience. Foundry seems powerful, but more of a niche walled garden that prioritizes low code/no code and where infrastructure is obfuscated. 

Admittedly, I didn‚Äôt know much about Foundry when I jumped into this opportunity, but it seemed like a good upwards move for me. The company is in hyper growth mode, and the benefits are great. 

I‚Äôm wondering from others who may have experience whether or not my general skills will stagnate and if I‚Äôll be less marketable in the future.? I plan to keep working on side projects that use more ‚Äúcommon‚Äù orchestration + compute + storage stacks, but want thoughts from others. 

",-1,2025-06-04 23:52:13
"
Been messing around with different observability platforms lately and stumbled on Rakuten SixthSense. Didn‚Äôt expect much at first, but honestly‚Ä¶ it‚Äôs pretty slick.


Full-stack observability

Works well with distributed tracing

Real-time insights on latency, failures, and anomalies

UI isn‚Äôt bloated like some of the others (looking at Dynatrace/NewRelic)

They offer a free trial and an interactive sandbox demo, no credit card required.

If you‚Äôre into tracing APIs, services, or debugging async failures, this is worth checking out.

Free Trial
Interactive Demo

Not affiliated. Just a dev who‚Äôs tired of overpriced tools with clunky UX.
This one‚Äôs lean, fast, and does the job.

Anyone else tried this?
",-1,2025-06-09 13:40:17
"Khatabook, a leading Indian fintech company (YC 18), replaced Mixpanel with Mitzu and Segment with RudderStack to manage its massive scale of over 4 billion monthly events, achieving a 90% reduction in both data ingestion and analytics costs. By adopting a warehouse-native architecture centered on Snowflake, Khatabook enabled real-time, self-service analytics across teams while maintaining 100% data accuracy.",-1,2025-04-14 10:03:50
"I'm trying to figure out what might be the best way to divide environment by dev/staging/prod in apache iceberg.

On my first thought, Using multiple catalogs corresponding to each environments(dev/staging/prod) would be fine.

    # prod catalog <> prod environment 
    
    SparkSession.builder \
        .config(""spark.sql.catalog.iceberg_prod"", ""org.apache.iceberg.spark.SparkCatalog"") \
        .config(""spark.sql.catalog.iceberg_prod.catalog-impl"", ""org.apache.iceberg.aws.glue.GlueCatalog"") \
        .config(""spark.sql.catalog.iceberg_prod.warehouse"", ""s3://prod-datalake/iceberg_prod/"")
    
    
    
    spark.sql(""SELECT * FROM client.client_log"")  # Context is iceberg_prod.client.client_log
    
    
    
    
    # dev catalog <> dev environment 
    
    SparkSession.builder \
        .config(""spark.sql.catalog.iceberg_dev"", ""org.apache.iceberg.spark.SparkCatalog"") \
        .config(""spark.sql.catalog.iceberg_dev.catalog-impl"", ""org.apache.iceberg.aws.glue.GlueCatalog"") \
        .config(""spark.sql.catalog.iceberg_dev.warehouse"", ""s3://dev-datalake/iceberg_dev/"")
    
    
    spark.sql(""SELECT * FROM client.client_log"")  # Context is iceberg_dev.client.client_log

I assume, using this way, I can keep my source code(source query) unchanged and use the code in different environment (dev, prod)

    # I don't have to specify certian environment in the code and I can keep my code unchanged regardless of environment.
    
    spark.sql(""SELECT * FROM client.client_log"")

If this isn't gonna work, what might be the reason?

I just wonder how do you guys set up and divide dev and prod environment using iceberg.",-1,2025-06-09 14:03:37
I am aws data engineer. I am struggling with system design rounds. Can you suggest me how to improve myself on this,-1,2025-05-27 07:23:55
"

The previous days we recorded a podcast episode with an ex-colleague of mine. 

We dived into the details of Data Architect role and I think this is an interesting one with value for anyone who is interested in data engineering and data architecture. We discuss about data solutions, systems integration in the payments and fintech industry and other interesting stuff! Enjoy! 

[https://open.spotify.com/episode/18NE120gcqOhaf5BdeRrfP?si=4V6o16dnSeKaUaL57sdVng](https://open.spotify.com/episode/18NE120gcqOhaf5BdeRrfP?si=4V6o16dnSeKaUaL57sdVng)",-1,2025-04-25 07:35:33
"Hey everyone,

Recently, a friend of mine mentioned an architecture that's been stuck in my head:

`Sources ‚Üí Streaming ‚Üí PostgreSQL (raw + incremental dbt modeling every few minutes) ‚Üí Streaming ‚Üí DW (BigQuery/Snowflake, read-only)`

The idea is that PostgreSQL handles all intermediate modeling incrementally (with dbt) before pushing analytics-ready data into a purely analytical DW.

Has anyone else seen or tried this approach?

It sounds appealing for cost reasons and clean separation of concerns, but I'm curious about practical trade-offs and real-world experiences.

Thoughts?

",-1,2025-06-05 00:18:41
"Good morning,
I bring questions about data engineering. I started the role a few months ago and I have programmed, but less than web development.
I am a person interested in classes, abstractions and design patterns.
I see that Python is used a lot and I have never used it for large or robust projects.
Is data engineering programming complex systems? Or is it mainly scripting?
",0,2025-06-05 19:05:02
"Hey fellow data engineers üëã

Hope you're all doing well!

I recently transitioned into data engineering from a different field, and I‚Äôm enjoying the work overall ‚Äî we use tools like Airflow, SQL, BigQuery, and Python, and spend a lot of time building pipelines, writing scripts, managing DAGs, etc.

But one thing I‚Äôve noticed is that in cross-functional meetings or planning discussions, management or leads often refer to us as ""developers"" ‚Äî like when estimating the time for a feature or pipeline delivery, they‚Äôll say ‚Äúit depends on the developers‚Äù (referring to our data team). Even other teams commonly call us ""devs.""

This has me wondering:

Is this just common industry language?

Or is it a sign that the data engineering role is being blended into general development work?

Do you also feel that your work is viewed more like backend/dev work than a specialized data role?


Just curious how others experience this. Would love to hear what your role looks like in practice and how your org views data engineering as a discipline.

Thanks!
",0,2025-06-05 09:24:39
"Which postgrad is more worth it for the data job market in 2025: Database Systems Engineering or Data Science?

The Database Systems track focuses on pipelines, data modeling, SQL, and governance. The Data Science one leans more into Python, machine learning, and analytics.

Right now, my work is basically Analytics Engineering for BI ‚Äì I build pipelines, model data, and create dashboards.

I'm trying to figure out which path gives the best balance between risk and return:

Risk: Skill gaps, high competition, or being out of sync with what companies want.

Return: Salary, job demand, and growth potential.


Which one lines up better with where the data market is going?",0,2025-04-30 09:56:09
"If I'm advanced in Python, how challenging would it be to pick up C++?  Can you recommend any reasonably priced online courses?
",0,2025-06-12 16:20:46
"My current tech stack at my work is SSMS for modifying sql queries , debug existing code, ADF, synapse data warehouse, MS DevOps, visual studio for version control. No pyspark, airflow etc all those data engineering tools that talk mention on this subreddit. 

What should I do to make myself to be more competitive and more like a data engineer ",0,2025-06-04 22:59:01
"I work as a software engineer (more of a data engineer) in non-profit cancer research under an NIH grant. It was my first job out of university, and I've been there for four years. Today, my boss informed me that our funding will almost certainly be cut drastically in a couple of months, leading to layoffs. 

Most of my current work is building ETL pipelines, primarily using GCP, Python, and BigQuery. (I also maintain a legacy Java web data platform for researchers.) My existing skills are solid, but I likely have some gaps. I believe in the work I've been doing, but... at least this is a good opportunity to grow? I could do my current job in my sleep at this point.

I only have a few months to pick up a new skill. Job listings talk about Spark, Airflow, Kafka, Snowflake... if you were in my position, what would you add to your skill set? Thank you for any advice you can offer!",0,2025-06-12 04:15:17
"Hi everyone,

I‚Äôm currently working as a Data Engineer at Infosys. I joined in September 2024 and graduated the same year. It's been about 9 months, but I feel like I‚Äôm not learning enough or growing in my current role.

I‚Äôm seriously considering a switch to a startup or product-based company where I can gain better experience and skills.

I‚Äôd appreciate your guidance on:

* How to approach the job search effectively
* Ways to stand out while applying
* What are the chances of getting shortlisted with my background
* Any tips or resources that helped you in a similar situation

Thanks a lot in advance for your support and advice!",0,2025-06-04 17:49:07
"Hey guys üëãüèΩ 
Just wondering what‚Äôs the best way you have to learn new technologies and get them to a level that is competent enough to work in a project.

On my side, to learn the theory I‚Äôve been asking ChatGPT to ask me questions about that technology and correct my answers if they‚Äôre wrong - this way I consolidate some knowledge. For the practical part I struggle a little bit more (I lose motivation pretty fast tbh) but I usually do the basics following the QuickStarts from the documentation.

Do you have any learning hack? Tip or trick?",0,2025-06-04 14:26:39
"Which postgrad is more worth it for the data job market in 2025: Database Systems Engineering or Data Science?

The Database Systems track focuses on pipelines, data modeling, SQL, and governance. The Data Science one leans more into Python, machine learning, and analytics.

Right now, my work is basically Analytics Engineering for BI ‚Äì I build pipelines, model data, and create dashboards.

I'm trying to figure out which path gives the best balance between risk and return:

Risk: Skill gaps, high competition, or being out of sync with what companies want.

Return: Salary, job demand, and growth potential.


Which one lines up better with where the data market is going?",0,2025-04-30 09:56:09
"I'm just starting out in data engineering and still consider myself a noob. I have a question: in the era of AI, what should I really focus on? Should I spend time trying to understand every little detail of syntax in Python, SQL, or other tools? Or is it enough to be just comfortable reading and understanding code, so I can focus more on concepts like data modeling, data architecture, and system design‚Äîthings that might be harder for AI to fully automate?

Am I on the right track thinking this way?",0,2025-04-29 19:57:37
I have seen most data analyst going for power bi and tableau what should data engineers should learn to upskill themselves in  between these two? ,0,2025-06-04 13:05:56
"Hi all,

I‚Äôve recently restarted my job search and wanted to combine it with helping someone else at the same time.

I‚Äôm planning to go through the Blind 75 challenge - 1 problem a day for the next 75 days. The best way for me to really learn is by teaching, so I‚Äôm looking for someone who‚Äôd like to volunteer as a study partner/student.

I‚Äôll explain one problem each day, discuss the approach, and we can solve it together or review it afterwards. I‚Äôm in the UK timezone, so we‚Äôll work out a schedule that suits both of us.

",0,2025-06-03 06:29:18
"Hey folks,

I‚Äôm a Data Engineer (DE) currently working onsite in Budapest with around 4 years of experience. My current CTC is equivalent to ~9.3 M HUF(Hungarian Forint) per annum. I‚Äôm skilled in: C++, Python, SQL

Cloud Computing (primarily Microsoft Azure, ADF, etc.)

I‚Äôm at a point where I‚Äôm wondering ‚Äî should I consider switching domains from DE to SDE, or should I look for better opportunities within the Data Engineering space?

While I enjoy data work, sometimes I feel SDE roles might offer more growth, flexibility, or compensation down the line ‚Äî especially in product-based companies. But I‚Äôm also aware DE is growing fast with big data, ML pipelines, and real-time processing.

Has anyone here made a similar switch or faced the same dilemma? Would love to hear your thoughts, experiences, or any guidance!

Thanks in advance ",0,2025-06-04 10:55:21
I've been working as a data engineer since an year and almost 2 months now. Since pyspark is used in databricks and codes are not really that difficult except a few concepts of using different functions and tough complex sql queries. But if i want to switch from this job in the later part of the year i gotta have some hands on experience with python. I have been trying out Data factory and Databricks courses separately apart from my work to hone those skills but python is what i feel will be really required upto some levels in technical interviews of Junior of Senior data engineers. Please suggest me a good coding practice website or course which can help me be up to the mark of a data engineer and how much is enough to learn for now?(Ofcourse there's no limit to it). :),0,2025-05-23 22:01:30
"Hi everyone,

I‚Äôm a junior Data Engineer with about 1 year of experience working with Snowflake in a large-scale retail project (Inditex). I studied Computer Engineering and recently completed a Master‚Äôs in Big Data. I got decent grades, but I wasn‚Äôt top of my class ‚Äî not good enough to unlock prestigious scholarships or academic opportunities.

Right now, I‚Äôm trying to figure out what really makes a difference when trying to grow professionally in this field, especially for someone without an exceptional academic track record. I‚Äôm ambitious and constantly learning, and I want to grow fast and reach high-impact roles, ideally abroad in the future.

Some questions I‚Äôm grappling with:
	‚Ä¢	Are certifications (like the Snowflake one) worth it for standing out?
	‚Ä¢	Would a private master‚Äôs or MBA from a well-known school help open doors, even if I‚Äôm not doing it for the learning itself? If so, which ones are actually respected in the data world?
	‚Ä¢	I‚Äôm also working on personal projects (investment tools, dashboards) that I use for myself and publish on GitHub. Is it worth adapting them for the public or making them more portfolio-ready?

I‚Äôd love to hear from others who were in a similar position: what helped you stand out? What do hiring managers and companies actually value when considering junior profiles?

Thanks a lot!
",0,2025-06-03 09:50:55
"Hi everyone,

I know this might be a repeat question, but I couldn't find any answers in all previous posts I read, so thank you in advance for your patience.

I'm currently studying a range of Data Engineering technologies‚ÄîAirflow, Snowflake, DBT, and PySpark‚Äîand I plan to expand into Cloud and DevOps tools as well. My German level is B2 in listening and reading, and about B1 in speaking. I‚Äôm a non-EU Master's student in Germany with about one year left until graduation.

My goal is to build solid proficiency in both the tech stack and the German language over the next year, and then begin applying for jobs. I have no professional experience yet.

But to be honest‚ÄîI've been pushing myself really hard for the past few years, and I‚Äôm now at the edge of burnout. Recently, I've seen many Reddit posts saying the junior job market is brutal, the IT sector is struggling, and there's a looming threat from AI automation.

I feel lost and mentally exhausted. I'm not sure if all this effort will pay off, and I'm starting to wonder if I should just enjoy my remaining time in the EU and then head back home.


My questions are:

1. Is there still a realistic chance for someone like me (zero experience, but good German skills and strong tech learning) to break into the German job market‚Äîespecially in Data Engineering, Cloud Engineering, or even DevOps (I know DevOps is usually a mid-senior role, but still curious)?


2. Do you think the job market for Data Engineers in Germany will improve in the next 1‚Äì2 years? Or is it becoming oversaturated?


I‚Äôd really appreciate any honest thoughts or advice. Thanks again for reading.
",0,2025-06-09 09:27:40
"I'm starting a new career in data, and what I've been noticing is that a lot of these courses and platforms only teach surface-level skills in SQL, Python, etc. Maybe because they think learners will learn the in-depth skills on the job? I just wanted to point out that this program has already helped me understand the why behind the tools and skills, and I've only just started. I'm learning that I have gaps and the program has helped me understand advanced concepts, clean code, and optimization. It's been helpful in giving me a strategic, focused, and structured plan to know how to be a better data professional. Just wanted to point this out!",0,2025-06-04 16:53:24
"So I was hoping to job hunt after finishing the [DataTalks.club](http://DataTalks.club) Zoomcamp but I ended up not fully finishing the curriculum (*Spark & Kafka*) because of a combination of RL issues. I'd say it'd take another personal project and about 4-8 weeks to learn the basics of them.

**I'm considering these options:**

* Do I apply to train-to-hire programs like Revature now and try to fill out those skills with the help of a mentor in a group setting.
* Or do I skill build and do the personal project first then try applying to DE and other roles (e.g. DA, DevOps, Backend Engineering) along side the train-to-hire programs?

I can think of a few reasons for either.

Any feedback is welcome, including things I probably hadn't considered.

P.S. [my final project](https://github.com/MichaelSalata/compare-my-biometrics) \- [qualifications](https://docs.google.com/document/d/1NlyR8epSti_MD31crqarEo3QWgZAz1en5BK-8ACnZWw/edit?usp=sharing)",0,2025-06-09 01:46:30
"I found out that a new data engineer coming onto my team is making a few thousand more than me (a senior thats been with the company several years) annually, despite this new DE having less direct/applicable experience than me. Having to be a bit vague for obvious reasons. I have been a top individual contributor on my team every year. Every review I've received from management is overwhelmingly positive. This new DE and I are in the same geographic area, so thats not the explanation. 

How should I broach this with my management without: 
 - revealing that I am 100% sure what this new DE is making, 
 - threatening to leave if they don't up my pay,
 - getting myself on the short list for layoffs

We just finished our annual reviews. This pay disparity is even after I received a meager merit raise. 

Anyone else navigated this? Am I really going to have to company hop just to get paid a fair market salary? I want to stay at this company. I like what I do, but I also need more money to make ends meet.

EDIT (copying a comment I left): I guess I should have said this in the original post, but I already tried this before our annual reviews. I provided evidence of my contribution, asked for a specific annual salary increase, and wanted it to be part of my annual increase which had a specific deadline. 

What I ended up getting was a bunch of excuses as to why it wasn't possible, empty promises of things they might be able to do for me later this year, and a meager merit raise well below inflation.

So, to take your advice and many others here, sounds like I should just start looking elsewhere.",0,2025-05-23 17:31:03
"Hey there, I'm in my last semester of 3rd year pursuing CSE-Data Science and my cllg is not doing so great like every tier 3 colleges does.. i wanted to know that focusing on these topics: Data Science, Data Engineering, AI Engineering( LLM'S, AI agents, transformers etc.) as well as some concepts of AWS and System Design. I was focused on becoming Data analyst or Data Scientist but for the analyst part there's lot of non tech folks which raised the competition and for becoming the data scientist u need lot of experience in analytics side. 

I had an 1:1 session with some employees where they stated that focusing on multiple skills will raise the chances of getting hired and lower the chances of getting laid off. I had doubt regarding this, it would be helpful for replying this question as u have tried asking gpt, perplexity they are just beating around the bush.

And im planning to make a study plan so that less than 12 months i could be ready for placement drive too",0,2025-04-28 14:07:53
"Currently on a job search and I've noticed that healthcare companies seem to be really particular about having prior experience working with healthcare data. Well over half the time there's some knockout question on the application along the lines of ""Do you have *x* years of prior experience working with healthcare data?"" 

Any ideas why this might be? At first my thought was HIPAA and other regulations but there are plenty of other heavily regulated sectors that don't do this, i.e. finance and telecom.",0,2025-06-12 02:18:13
"Hi, I've been trying for a long time to figure out which area of  IT I'm interested in, and I settled on data engineering. I would like to know how promising and in demand this field is relative to frontend/backend development?

Also I have chosen the following technology stack to start developing one by one:

SQL -> Python -> Airflow -> PostgreSQL -> Docker.

Is this stack sufficient for a beginner? Also what level of maths do you need to have for data engineering? Is it worth to go deep into maths analysis ?",0,2025-05-28 21:01:30
"Hi everyone,



I'm exploring a career path in \*\*data engineering or data science\*\*, and I‚Äôm currently looking for a solid bootcamp that fits well with my background and goals.



A bit about me:



\- I've been working in the \*\*crypto and blockchain\*\* space for over 4 years

\- I‚Äôve been writing \*\*Solidity smart contracts\*\* for 2 years

\- I completed several blockchain-focused bootcamps including:

  \- Chainlink Bootcamps (VRF, Cross-Chain, Functions, Automation)

  \- Encode Club

  \- Cyfrin Updraft

\- For the past year, I‚Äôve been diving into the \*\*security and auditing\*\* side of smart contracts

\- I‚Äôve completed a \*\*non-basic SQL course\*\* and a \*\*basic Python course\*\*



Now, I‚Äôd like to expand my skill set into \*\*data engineering\*\* or \*\*data science\*\* and am looking for a program that offers:



\- \*\*Strong curriculum\*\* in data engineering/data science (not just data analytics)

\- \*\*On-site or on-campus\*\* options (though I‚Äôm open to online if it‚Äôs truly strong)

\- \*\*Job support\*\*, career coaching, or hiring partner network

\- Regions I‚Äôm open to: \*\*Europe, UAE, Canada, Turkey, Southeast Asia\*\*

\- Instruction in \*\*English\*\*



If you‚Äôve attended a bootcamp or know someone who did, I‚Äôd really appreciate any insight on:



\- Bootcamp name

\- What you liked (or didn‚Äôt like)

\- If it helped with getting a job

\- Whether you‚Äôd recommend it now



Thanks in advance üôè I‚Äôd love any tips or personal experiences, even short ones!



Feel free to comment or DM me if you prefer chatting privately.

",0,2025-05-28 06:03:35
"I'm a self-taught programmer turned data engineer, and a data scientist on my team (who is definitely the best programmer on the team) gave me this book. I found it incredibly insightful and it will definitely influence how I approach projects going forward.

I've also read Fundamentals of Data Engineering and didn't find it very valuable. It felt like a word soup compared to The Pragmatic Programmer, and by the end, it didn‚Äôt really cover anything I hadn‚Äôt already picked up in my first 1-2 years of on-the-job DE experience. I tend to find that very in-depth books are better used as references. Sometimes I even think the internet is a more useful reference than those really dense, almost textbook-like books.

Are there any data engineering books that give a good overview of the techniques, processes, and systems involved. Something at a level that helps me retain the content, maybe take a few notes, but doesn‚Äôt immediately dive deep into every topic? Ideally, I'd prefer to only dig deeper into specific areas when they become relevant in my work.",0,2025-06-07 19:53:22
"Hey Reddit,

I currently work as a BA/project lead in the ESG space, and I‚Äôve spotted a business gap in the geospatial data industry that I‚Äôd love to explore as a potential DAAS (Data-as-a-Service) venture.

I have solid product ownership and requirements gathering skills, understand the data sources well, and have a good grasp of database structuring. 

However, I don't have coding skills‚Äîso I‚Äôm wondering how best to approach this.
Where would you start if you were in my shoes?

Additionally, any recommendations for low-code/no-code data platforms that could help me build an MVP myself would be hugely appreciated! Open to general advice too.

Thanks in advance!",0,2025-04-17 13:45:09
"Hey folks,

Just wanted to share a bit and maybe get some advice or help if anyone‚Äôs around.

I‚Äôve been working as a data engineer for about 5 years now ‚Äî real hands-on experience building pipelines, working with cloud tools, databases, all that stuff. My last job was a contract role and it ended a little over a month ago. Since then, I‚Äôve been applying non-stop ‚Äî probably hundreds of applications by now.

I‚Äôm not just spraying my CV either. I actually read each job description and tweak my CV to match what they‚Äôre asking for. I know the market is tough right now, but it‚Äôs been a bit frustrating not hearing back much, even with solid experience.

Just wondering if anyone here has any tips that helped them during their job search, or maybe knows someone hiring or could point me in the right direction. I‚Äôm open to full-time, contract, freelance ‚Äî anything to get back to work.

Appreciate any help or advice. Thanks for reading.
",0,2025-05-23 23:43:01
"I'm a final-year student who loves computer science and math, and I‚Äôm passionate about becoming an ML engineer.
However, it's very hard to land an ML engineer job as a fresh graduate, especially in my country. So, I‚Äôm considering studying data engineering to guarantee a job, since it's the first step in the data lifecycle.
My plan is to work as a data engineer for 2‚Äì3 years and then transition into an ML engineer role.

Does this sound like solid reasoning?
Or are DE (Data Engineering) and ML (Machine Learning) too different, since DE leans more toward software engineering than data science?",0,2025-04-28 11:17:00
"I am a DE with 2 years of experience, but my background is mainly in statistics. I have been offered a position as an ML Engineer (de facto Data Scientist, but also working on deployment - it is a smaller IT department, so my scope of duties will be simply quite wide). 

The position is interesting, and there are multiple pros and cons to it (that I do not want to discuss in this post). However my question is a bit more general - in 2025, with all the LLMs performing quite well with code generation and fixing, which path would you say is more stable long-term - sticking to DE and becoming better and better at it, or moving more towards ML and doing data science projects?

Furthermore, I also wonder about growth in each field - in ML/DS, my fear is that I am not PhD nor excellent mathematician. In DE, on the other hand, my fear is lack of my solid CS/SWE foundations (as my background is more in statistics). 

Ultimately, it is just an honest question, as I am very curious of your perspective on the matter - does moving towards data science projects (XGBoost and other algorithms) in 2025 from DE (PySpark and Airflow) makes sense in 2025? Which path would you say is more reasonable, and what kind of growth I can expect for each position? Personally I am a bit reluctant to switch simply since I have already dedicated 2 years to growing as an DE, but on the other hand I also see how much more and more of my tasks can be automated. Thanks for tips and honest suggestions!",0,2025-03-30 17:47:04
"I've been working in IT for the past 6.5 years. I started as a Java Developer for a year before transitioning into Data Engineering, where I worked with Airflow, GCP, Python, and SQL (BigQuery).

In June 2022, I joined my second company as a Data Engineer, but after just six months, the project was shelved, and I was moved to a Java-based project (Spring Boot, Kafka, etc.). This happened during a market downturn and layoffs, so I was grateful to still have a job.

Now, after two years in this role, I feel stuck. I struggle with coding, don‚Äôt enjoy Java, and constantly feel like an imposter. I know for sure that I don‚Äôt want to continue in Java and Spring Boot. However, I‚Äôve stayed in this role because it‚Äôs high-paying, and I have family responsibilities (supporting a family of five).

I want to transition back into Data Engineering, but now the job market expects a higher level of expertise given my experience and salary range. I‚Äôm unsure about the best way to upskill and make this switch without a major setback.

How can I strategically transition back into Data Engineering while balancing financial stability? Would love advice from those who have made similar career shifts.

Thanks in advance!",0,2025-03-30 17:08:46
"I am a DE with 2 years of experience, but my background is mainly in statistics. I have been offered a position as an ML Engineer (de facto Data Scientist, but also working on deployment - it is a smaller IT department, so my scope of duties will be simply quite wide). 

The position is interesting, and there are multiple pros and cons to it (that I do not want to discuss in this post). However my question is a bit more general - in 2025, with all the LLMs performing quite well with code generation and fixing, which path would you say is more stable long-term - sticking to DE and becoming better and better at it, or moving more towards ML and doing data science projects?

Furthermore, I also wonder about growth in each field - in ML/DS, my fear is that I am not PhD nor excellent mathematician. In DE, on the other hand, my fear is lack of my solid CS/SWE foundations (as my background is more in statistics). 

Ultimately, it is just an honest question, as I am very curious of your perspective on the matter - does moving towards data science projects (XGBoost and other algorithms) in 2025 from DE (PySpark and Airflow) makes sense in 2025? Which path would you say is more reasonable, and what kind of growth I can expect for each position? Personally I am a bit reluctant to switch simply since I have already dedicated 2 years to growing as an DE, but on the other hand I also see how much more and more of my tasks can be automated. Thanks for tips and honest suggestions!",0,2025-03-30 17:47:04
"I've been working in IT for the past 6.5 years. I started as a Java Developer for a year before transitioning into Data Engineering, where I worked with Airflow, GCP, Python, and SQL (BigQuery).

In June 2022, I joined my second company as a Data Engineer, but after just six months, the project was shelved, and I was moved to a Java-based project (Spring Boot, Kafka, etc.). This happened during a market downturn and layoffs, so I was grateful to still have a job.

Now, after two years in this role, I feel stuck. I struggle with coding, don‚Äôt enjoy Java, and constantly feel like an imposter. I know for sure that I don‚Äôt want to continue in Java and Spring Boot. However, I‚Äôve stayed in this role because it‚Äôs high-paying, and I have family responsibilities (supporting a family of five).

I want to transition back into Data Engineering, but now the job market expects a higher level of expertise given my experience and salary range. I‚Äôm unsure about the best way to upskill and make this switch without a major setback.

How can I strategically transition back into Data Engineering while balancing financial stability? Would love advice from those who have made similar career shifts.

Thanks in advance!",0,2025-03-30 17:08:46
"I‚Äôm a final-year student and I'm really confused between two fields: DevOps and Data Engineering.
I have one main question:
Is DevOps a broader career path where it's relatively very easy to shift into areas like DataOps, MLOps, or CyberOps?
And is Data Engineering a more specialized field, making it harder to transition into any other areas?
Or are both fields similar in terms of career flexibility?",0,2025-04-26 14:52:37
"I am so confused. I am looking for roles in BI/analytics/data science and it seems data engineering has just taken over the entire thing or most of it, atleast. BI and DBA is just gone and everyone now wants cloud dev ops and data engineering stack as part of a BI/analytics role? Am I now supposed to become a software engineer and learn all this stack (airflow, airtable, dbt, hadoop, pyspark, cloud, devops etc?) - this seems so overwhelming to me! How am I supposed to know all this in addition to data science, strategy, stakeholder management, program management, team leadership....so damn exhausting! Any advice on how to navigate the job market and land BI/data analytics/data science roles and how much realistic data engineering am I supposed to learn?",0,2025-03-30 20:21:44
I'm just curious about this because these 2 companies have been very popular over the last few years.,0,2025-03-31 01:12:17
"Hey DE,

I‚Äôd love to get your perspective on my situation.

# My Background

I‚Äôm a Brazilian Mechanical Engineer with 3 years of experience in the Data field‚Äîstarted as a Data Analyst for 1.5 years, then transitioned into Data Engineering. Next week, I‚Äôll be starting as a Data Architect at a multinational with 100,000+ employees, mainly working with the Azure stack.

# The Plan

My girlfriend and I are planning to move to Australia for about a year to travel and build memories together before settling down (marriage, house, etc.). This new job came unexpectedly, but it offers a good salary (\~$2,000 USD/month).

The idea is to:

* Move to Australia
* Work hard & save around $1,000 USD/month
* Travel as much as possible for \~2 years
* Return and re-enter the data field

# The Challenge

The work visa limitation allows me to stay only 6 months with the same employer, making it tough to get good Data Engineering jobs. So, I plan to work in any job that pays well (fruit picking, hospitality, etc.), and my girlfriend will do the same.

# The Concern

When I return, how hard will it be to get back into the data field after a \~2-year break?

* I‚Äôll have enough savings to stay unemployed for about a year if needed.
* This isn‚Äôt all my savings‚ÄîI have the equivalent of 6 years of salary in reserve.
* I regularly get recruiter messages on LinkedIn.
* I speak Portuguese, English, and Spanish fluently.

Given your experience, how risky is this career break? is totally crazy ? Would you recommend a different approach? Any advice would be appreciated!",0,2025-03-31 14:34:32
"

Here is some background, I'm currently in the interviewing process for a presales solution architect at Databricks in Canada. I am currently employed as a senior manager at a consulting firm where I largely work on technical project delivery. I understand the role at Databrick is more client conversation and less technical, but what I'm trying to evaluate is how did others shift from people management to a presales roles and also whether I should target for a senior or specialist solution architect role rather than a solution architect.

I am fairly technical and solution most of the work and deep dive into day-to-day technical issues.",0,2025-04-24 12:32:38
"My background is in finance and economics. I've worked with data for the past 3 years mainly using SQL, python and power bi. On the side I've developed low-code apps and VB apps for small businesses, with the ultimate goal to automate their processes and offer analytics. I have now some foundation on OOP too. I'm in a point of my life in which I could go for the DE path with some more study or learn SWE, I have the time to do it and the resources to pay for online courses if needed (no bootcamps though), let's say I can study whatever I want for the next two years. I'm 30, what would you do in my case?",0,2025-04-24 02:06:27
"Wanted to get the opinion from the community on Robotics Engineering from anyone with some experience. My experience is about 3 years in industry as a Data engineer and 1 as an ML engineer.

I'm willing to do a part time Msc (paid out my own pocket). Just not sure if it's worth it in the north of the UK.

The TDLR is: 
- I think robotics is really interesting 
- its where i think the next big innovations are gonna be (using AI) and I'd love to be a part of it.

Just weighing up the sacrifice of a currently comfy  career vs something more interesting to me. Data plumbing (and ai plumbing) isn't particularly exciting but it's definitely paying the bills.",0,2025-04-24 19:12:12
"Hello everyone,

I'm a Junior IT Consultant in Data Engineering in Germany with about two years of experience, and I hold a Master's degree in Data Science. My career has been focused on data concepts, but I'm increasingly interested in transitioning into the field of Data Security.

I've been researching this career path but haven't found much documentation or many examples of people who have successfully made a similar switch from Data Engineering to Data Security.

Could anyone offer recommendations or insights on the process for transitioning into a Data Security role from a Data Engineering background?

Thank you in advance for your help! üòä",0,2025-04-23 22:09:23
"Hi Everyone,
I have been working as an Oracle DBA for a while now, but I am not enjoying what am I doing. A year ago, I got interested in data engineering and tried to self-learn while juggling a full-time job, GRE prep(planning to go for masters as it‚Äôs always been my dream), and everything else‚Ä¶ safe to say, it wasn‚Äôt easy. Since my job didn‚Äôt really involve coding and I ended up with mostly theoretical knowledge. I do know Python, Azure(again theoretical knowledge) and SQL (thanks to work), but I still have a long way to go in data engineering. Now that I‚Äôm finally taking this step, I am thinking to quit my current job and put all my efforts solely on switching from DBA to data engineering. I‚Äôd really appreciate any advice on how to go about this what tech stacks I should focus on and whether transitioning within six months is realistic.",0,2025-04-02 18:51:20
"Hello everyone,

First of all English is not my first language so I apologize if there are mistakes or if everything is not clear.

I've been working for 6 years and my career path is not very consistent.  
I started in non-technical positions for 3 years and then moved on to a more technical one. 

For 3 years I had a very diversified job with software development (Php, Python), database management, Linux system administration, a bit of Cloud and a big part of ‚ÄúData‚Äù with ETL flows (Talend) and a lot of SQL. The project was quite large and the team very small, so I was working on several tasks at once.

I really enjoyed the Data part and I got it into my head that I wanted to be a 'real' Data Engineer and not just drag and drop on Talend.

I was just starting my research when a friend of mine contacted me because a software engineer position was opening up in his company. I went through the recruitment process and accepted their proposal.

  
As in my previous position, I'll be working on a lot of things (mobile development, backend, a bit of frontend, cloud, devops) and the salary offered was 20% higher than what I had in my previous job. (I'm now at 48k‚Ç¨ and I don't live in a big city).  
The offer was really attractive and as the market is a bit complicated at the moment, I accepted.

But I'm wondering if this choice will take me even further away from the Data Engineer job i wanted.

Do you find my career path coherent?  
Could I switch back to Data in a few years' time?

Thank you for reading me !",0,2025-04-02 07:34:18
"Hi all of you,

I was wondering this as I‚Äôm a newbie DE about to start an internship in couple days, I‚Äôm curious about this as I might wanna know what‚Äôs gonna be and how am I gonna feel I get some experience.

So it will be really helpful to do this kind of dumb questions and maybe not only me might find useful this information.

So do you really really consider your job stressful? Or now that you (could it be) are and expert in this field and product or services of your company is totally EZ

Thanks in advance",0,2025-04-24 00:44:07
"I currently work as a mid-level DE (3y) and I‚Äôve recently been offered an opportunity in Consulting. I‚Äôm clueless what rate I should ask for. Should it be 25% more than what I currently earn? 50% more? Double!? 

I know that leaping into consulting means compromising job stability and higher expectations for deliveries, so I want to ask for a much higher rate without high or low balling a ridiculous offer. Does someone have experience going from DE to consultant DE? Thanks!",0,2025-04-04 16:11:28
"Hello all. I'm currently a ML engineer looking to become a data engineer. More specifically, I'm not just looking to be a SQL monkey, but a data engineer who is designing, building, and maintaining scalable, and reliable data infrastructure and platform, building out data lakes, etc. If I'm interested in these types of roles, what should I be learning? I use AWS at work, so I know things like DynamoDB and RDS are quite important. 

I already know SQL and Python, but how do I go about learning DataOps and the infra to support data warehouse/datalakes?",0,2025-04-17 00:40:00
"Hi everyone,

I‚Äôve just received two job offers ‚Äî one from Codec for a Data Engineer role and another from Optum for a Data Analyst position. I'm feeling a bit confused about which one to go with.

Can anyone share insights on the roles or the companies that might help me decide? I'm especially curious about growth opportunities, work-life balance, and long-term career prospects in each.

Would love to hear your thoughts on:

Company culture and work-life balance

Tech stack and learning opportunities

Long-term prospects in Data Engineer vs Data Analyst roles at these companies

Thanks in advance for your help!",0,2025-04-20 21:00:34
"I'm currently exploring potential career paths in Data Science, Data Engineering, and Business Analysis. I don't have any prior coding experience, and I'm trying to figure out where to begin.

What would you say are the essential programming languages or tools a beginner should learn first to build a solid foundation in any (or all) of these fields?

Some follow-up questions I'm also curious about:

* Should I focus on one field first (like Data Science) and then branch out, or is there overlap in the skill sets?
* How important is SQL compared to Python or R at the beginning?
* Are there any specific resources (courses, books, platforms) you'd recommend for absolute beginners?
* How long did it take you to feel confident in your coding abilities when transitioning into one of these fields?

",0,2025-04-17 03:12:02
"Hello fellow DEs!

I‚Äôm hoping to get some career advice from the experienced folks in this sub.

I have 4.5 YOE and a related master‚Äôs degree. Most of my experience has been in DE consulting, but earlier this year I grew tired of the consulting grind and began looking for something new. I applied to a bunch of roles, including a few at Meta, but never made it past initial screenings.

Fast forward to now ‚Äî I landed a senior DE position at a well-known crypto exchange about 4 months ago. I‚Äôm enjoying it so far: I‚Äôve been given a lot of autonomy, there‚Äôs room for impactful infrastructure work, and I‚Äôm helping shape how data is handled org-wide. We use a fairly modern stack: Snowflake, Databricks, Airflow, AWS, etc.

A technical recruiter from Meta recently reached out to say they‚Äôre hiring DEs (L4/L5) and invited me to begin technical interviews.

I‚Äôm torn on what decision would be best for my career: Should I pursue the opportunity at Meta, or stay in my current role and keep building?

Here are some things I‚Äôm weighing:

* Prestige: Having work experience at a company like Meta could open doors for me in the future.
* Tech stack: I‚Äôve heard Meta uses mostly in-house tools (some open sourced), and I worry that might hurt future job transitions where industry-standard tools are more relevant.
* Role scope: I‚Äôve read that DEs at Meta may do work closer to analytics engineering. I enjoy analytics, but I‚Äôd miss the more technical DE aspects.
* Compensation: I‚Äôm currently making \~$160K base + pre-IPO equity + bonus potential. Meta‚Äôs base range is similar, but equity would likely be more valuable and far lower risk.
* Location: My current role is entirely remote. I would have to relocate to accommodate Meta's hybrid in person requirement.

So if you were in my shoes, what would you do? I appreciate any thoughts or advice!",0,2025-04-21 00:06:47
"I recently joined as an intern in an organisation. They assigned me database technology, and they wanted me to learn everything about database and database management systems in the span of 5 months.
They suggested to me a book to learn from but it's difficult to learn from that book. 
I have an intermediate knowledge on Oracle SQL and Oracle PL/SQL. 
I want to gain much knowledge on Database and DBMS.

So i request people out there who have knowledge on databases to suggest the best sources(preffered free) to learn from scratch to advanced as soon as possible.",0,2025-04-15 02:13:21
"Hi , Probably the first post in this subreddit but I find lot of useful tutorials and content to learn from.

May I know, if you had to start on a data space, what are the blind spots, areas you will look out for, what books / courses I should rely on.

I have seen posts on asking to stay on Software Engineer, the new role is still software engineering but in data team. 

Additionally, I see lot of tools  and especially now data coincide with machine learning. I would like to know what kind of tools really made a difference. 

Edit::
I am moving to the company where they are just starting on the data-space, so going to probably struggle through getting the data into one place, cleaning data etc",0,2025-04-21 14:31:37
"
I am looking for some honest feedback on how well positioned I am to break into data engineering and where I could still level up. I am currently based in the US. I really enjoy the technical side of analytics. I know python is my biggest area of improvement for now. Here is my background, track and plan:

Background:
	Bachelor‚Äôs degree in Data Analytics

	3 years of experience as a Data Analyst (heavy SQL, light Python)

	Daily practice improving my SQL (window functions, CTEs, optimization, etc)

	Building a portfolio on GitHub that includes real-world SQL problems and code

	Actively working on Python fundamentals and plan to move into ETL building soon

Goals before applying:
Build 3 to 5 end-to-end projects involving data extraction, cleaning, transformation, and loading

Learn basic Airflow, dbt, and cloud services (likely AWS S3 and Lambda first)

Post everything to GitHub with strong documentation and clear READMEs

Questions:
1. Based on this track, how close am I to being competitive for an entry-level or junior data engineering role? 
2. Are there any major gaps I am not seeing? 

3. Should I prioritize certain tools or skills earlier to make myself more attractive?
4. Any advice on how I should structure my portfolio to stand out? Any certs I should get to be considered?",0,2025-04-28 16:23:19
"
Hey folks,
I‚Äôm a data/software engineer trying to break into freelancing, and honestly, I could use some advice. I‚Äôve been focusing on niches like building ETL pipelines, automation tools, web scraping, data mining, and even playing around with RAG bots.

I‚Äôve been on Upwork for about 2 months now and only landed one small scraping gig so far. My portfolio is pretty solid (or at least I think it is), but I‚Äôm not getting much traction, barely any invites.

So I‚Äôm wondering:

Is Upwork just super saturated for this kind of work right now?

Are there better platforms or communities for technical freelancing gigs (especially data-related)?

What worked for you when you were just starting out?

Is there a niche I should lean harder into?


Would love to hear from anyone who‚Äôs been through this or has some pointers. I'm open to harsh truths, hacks, or anything in between. Appreciate it!


",0,2025-04-13 21:54:58
Looking to build a portfolio of DE projects. Where should I start? Or what must I include?,0,2025-04-22 22:34:41
"I‚Äôm looking for recommendations for a solid online course to learn Data Engineering. Less than a year ago, I started a new role as a BI developer. Most of my work involves creating data models and reports in Power BI using T-SQL and DAX, but lately I‚Äôve been tasked with handling tickets related to reports showing incorrect data on the ETL side.

We use Wherescape for our ETL processes, but I‚Äôve struggled to find good learning material for this tool. There's no formal training and everyone learns on the job. There‚Äôs so much to analyze during investigations, especially when reverse-engineering the problem.

I‚Äôm a visual learner, so I‚Äôd love recommendations for courses with videos and hands-on practice. Any suggestions? Thanks!

Edit: Most posts asking for course recommendations are 1 year older or more. Some links doesn't work anymore or are not found when I look it up. ",0,2025-04-12 21:39:00
"I've been searching for a new opportunity over the last few years (500+ applications) and have finally received an offer I'm strongly considering. I would really like to hear some outside opinions.

## Current position
- Analytics Lead
- $126k base, 10% bonus
- Tool stack: on-prem SQL Server, SSIS, Power BI, some Python/R
- Downsides: 
	- Incoherent/non-existent corporate data strategy
	- 3 days required in-office (~20-minute commute)
	- Lack of executive support for data and analytics
	- Data Scientist and Data Engineer roles have recently been eliminated
	- No clear path for additional growth or progression 
	- A significant part of the job involves training/mentoring several inexperienced analysts, which I don't enjoy
- Upsides: 
	- Very stable company (no risk of layoffs)
	- Very good relationship with direct manager

## New offer
- Senior Data Analyst
- $130k base, 10% bonus
- Tool stack: BigQuery, FiveTran, dbt / SQLMesh, Looker Studio, GSheets
- Downsides:
	- High-growth company, potentially volatile industry
- Upsides:
	- Fully remote
	- Working alongside experienced data engineers

Other info/significant factors:
- My current company paid for my MSDS degree, and they are within their right to claw back the entire ~$37k tuition if I leave.  I'm prepared to pay this, but it's a big factor in the decision.
- At this stage in my career, I'm putting a very high value on growth/development opportunities

Am I crazy to consider a lateral move that involves a significant amount of uncompensated risk, just for a potentially better learning and growth opportunity?",0,2025-04-12 21:49:54
"I got an offer from a company that does data consulting/contracting. It‚Äôs a medium sized company (~many dozens to hundreds of employees), but I‚Äôd be sitting in a team of 10 working on a specific contract. I‚Äôd be the only data engineer. The rest of the team has data science or software engineering titles.

I‚Äôve never been on a team with that kind of set up. I‚Äôm wondering if others have sit in an org like that. How was it? What was the line ‚Äî typically ‚Äî between you and software engineers?",0,2025-04-22 18:55:45
"Hi everyone,

I‚Äôm currently working as an IDQ and CDQ developer for a US-based project, with about 2 years of overall experience

I‚Äôm really passionate about growing in this space and want to deepen my knowledge, especially in data quality and data governance . 

I‚Äôve recently started reading the DAMA DMBOK2 to build a strong foundation.

I‚Äôm here to connect with experienced professionals and like-minded individuals to learn, share insights, and get guidance on how to navigate and grow in this domain.

Any tips, resources, or advice would be truly appreciated. Looking forward to learning from all of you!

Thank you!
",0,2025-04-12 11:45:32
"I am getting into this role, I wondered how other people became data engineers? Most didn't start as a junior data engineer; some came from an analyst(business or data), software engineers, or database administrators. 

What helped you become one or motivated you to become one?",0,2025-04-08 09:39:06
"I'm an Engineer with an MBA. I've spent 5 years at a steelplant and 5 years working in finance for the government.

In the past five years have been building data pipelines in Synapse off D365 data models that I have built with a vendor in SQL/Power BI. I have gained quite a bit of experience in this timeframe, but would actually like more data engineering experience.

Should I try to land a role in the data engineering department where I would get first hand experience in data engineering tools and frameworks or just keep doing what I am doing in Finance and learning as I go.

I make decent money for the city I live in, but I feel like the end to end would definitely help me land other roles in the future that would branch out from just financial reporting and data.

Especially in the capacity for remote work if for some reason company or job gets moved to another city.",0,2025-04-12 20:13:03
"I graduated last August with a bachelors degree in Math from a good university. The job market already sucked then and it sucked even more considering I only had one internship and it was not related to my field. I ended up getting a job as a data analyst through networking, but it was a basically an extended internship and I now work in the IT department doing basic IT things and some data engineering.

My company wants me to move to another state and I have already done some work there for the past 3 months but I do not want to continue working in IT. I can also tell that the company I work for is going to shit at least in regards to the IT department given how many experienced people we have lost in the past year.

After thinking about it, I would rather be a full time ETL developer or data engineer. I actually have a part time gig as a data engineer for a startup but it is not enough to cover the bills right now.

**My question is how dumb would it be for me to quit my current job and work on getting certifications (I found some stuff on coursera but I am open to other ideas) to learn things like databricks, T-SQL, SSIS, SSRS, etc?** I have about one year of experience under my belt as a data analyst for a small company but I only really used Cognos Analytics, Python, and Excel.

I have about 6 months of expenses saved up where I could not work at all but with my part time gig and maybe some other low wage job I could make it last like a year and a half.

EDIT: I did not make it clear but I currently have a side job as a microsoft fabric data engineer and while the program has bad reviews on reddit, I am still learning Power BI, Azure, PySpark, Databricks, and some other stuff. It actually has covered my expenses for the past three months (if I did not have my full time job) but it might not be consistent. I am mostly wondering if quitting my current job which is basically as an IT helpdesk technician and still doing this side job while also getting certifications from Microsoft, Tableau, etc would allow me to get some kind of legit data engineering job in the near future. I was also thinking of making my own website and listing some of my own side projects and things I have worked on for this data engineering job.",0,2025-06-12 02:23:24
"
Same as above.

Any list of company‚Äôs that give equal pay to Data engineers same as SDE??",0,2025-06-11 18:14:48
"
Hey everyone,
I'm currently trying to shift my focus toward freelancing, and I‚Äôd love to hear some honest thoughts and experiences.

I have a background in Python programming and a decent understanding of statistics. I‚Äôve built small automation scripts, done data analysis projects on my own, and I‚Äôm learning more every day. I‚Äôve also started exploring the idea of building a simple SaaS product, but money is tight and I need to start generating income soon.

My questions are:

Is there realistic demand for beginner-to-intermediate data scientists or Python devs in the freelance market?

What kind of projects should I be aiming for to get started?

What are businesses really looking for when they hire a freelance data scientist?
Is it dashboards, insights, predictive modeling, cleaning data, reporting? I‚Äôd love to hear how you match your skills to their expectations.


Any advice, guidance, or even real talk is super appreciated. I‚Äôm just trying to figure out the smartest path forward right now. Thanks a lot!",0,2025-04-30 12:10:12
"Background: 

I‚Äôm on a support data engineering team and have recently been working on a full stack project that helped me learn new tools and skills that I was really interested in. I really enjoyed the work and felt like I was growing in the right direction. I also expressed my interest in this work with my manager and he said he would forward any similar projects to me as they come. 

The issue: 

However, now my manager seems to have changed his mind and told me that any future full stack opportunities will go to other team members instead of me because ‚ÄúI‚Äôve had enough.‚Äù His reasoning is that projects should rotate through the team members one by one before coming back to me. While I understand wanting to give everyone opportunities, this feels like it‚Äôs limiting my ability to build expertise and grow in areas where I‚Äôm performing well.

I‚Äôm also sensing some tension from teammates who seem to think I‚Äôm being ‚Äúgreedy‚Äù for wanting to continue with this type of work, even though I‚Äôm just trying to advance my career like anyone else would.

My question: I‚Äôm considering talking to the director of data engineering about potentially switching to a different team that focuses more on core data engineering work rather than support. Is this a reasonable move, or should I try to work things out with my current manager first?

Additional context:
- I‚Äôve been doing well in the full stack/data engineering work and it aligns with my career goals
- This seems to be part of a broader pattern where I feel like growth opportunities are being limited
- The team culture feels like it discourages ambition or self-advocacy

Has anyone been in a similar situation? How did you handle it?

TL;DR: Manager is rotating opportunities away from me after I had success with a project. Considering switching teams. Good idea or should I try to resolve this first?",0,2025-05-23 16:01:41
"I am a mechanical engineer slowly converted into an analytics/data engineer. I'm only around 1.5 years into data engineering and 3 years into working closely with data.

My team primarily works almost exclusively in Databricks, ADF, and Power BI. I've taken a variety of Databricks courses and I recently finished reading Fundamental of Data Engineering but I feel like neither of those have been quite as valuable as I would have hoped. Yes I get small nuggets of info that I didn't know here and there but it feels like a large majority of the info Is not really relevant or is very surface level. Yet it takes a lot of time to go through.   
  
I feel like I have gotten significantly more value out of simply learning on the job. Doing projects and researching questions as they come up. I'm sure there are very nuanced, highly technical questions that come up when working with specific scenarios like IoT or banking information but I don't really experience that.

I've also worked on some wed development side projects in the past that require a DB on that backend and that real life experience has also taught me a lot about both programming principles and optimizing DBs/Queries.

I have three other books that I would consider reading:

* Pragmatic Programmer
* Designing Data Intensive Applications
* Kimball's Data Warehouse Guide

  
I know at least the bottom two are way more technical but is it worth fully reading through from someone who learns better so hands on? Should I just skim through them and look up some basics that I can further deep dive once I know I need it? Or is there really value in reading through it and taking notes? How do you guys approach learning at different points in your career?",0,2025-05-23 20:26:24
"I‚Äôm trying to better understand the key learnings that only come with experience.

Whether it‚Äôs a technical skill, a mindset shift, a lesson or any relatable piece of knowledge, I‚Äôd love to hear what you wish you had known early on.",0,2025-05-24 12:33:34
"Hi All!  

I was looking for some advice.  I want to make a career switch and move into a new role.  I am torn between **AI/ML Engineer** and **Data Engineer.**

I read recently that out of those two roles, DE might be the more 'future-proofed' role as it is less likely to be automated.  Whereas with the AI/ML Engineer role, with AutoML and foundation models reducing the need for building models from scratch, and many companies opting to use pretrained models rather than build custom ones, the AI/ML Engineer role might start to be at risk.

What do people think about the future of these two roles, in terms of demand and being ""future-proofed""?  Would you say one is ""safer"" than the other?",0,2025-05-24 12:33:33
"Hi All.  I am looking to make the shift towards a career as a Data Engineer.

To help me with this, I am looking to do a Masters Degree.  

Out of the following, which MSc do you think would give me the best shot at finding a Data Engineering role?

Option 1 - [https://www.napier.ac.uk/courses/msc-data-engineering-postgraduate-online-learning](https://www.napier.ac.uk/courses/msc-data-engineering-postgraduate-online-learning)  
Option 2 - [https://www.stir.ac.uk/courses/pg-taught/big-data-online/?utm\_source=chatgpt.com#panel\_1\_2](https://www.stir.ac.uk/courses/pg-taught/big-data-online/?utm_source=chatgpt.com#panel_1_2)

Thanks,  
Matt",0,2025-05-24 12:20:29
"I‚Äôm 26, based in London, have 3 years experience in data engineering, just started a new role in a fintech - base salary ¬£70k.

Trying to map out a bit of a career path that I can look to as a guide, goal is frankly just to make as much money as possible over the next 5-10 years. 

Should I be looking to move into a bank in a couple years time, and then maybe a trading firm? I‚Äôd like to stay in finance ideally. 

Wondering at what level does the London market max out, and whether should I be looking to move to the US sooner than later? 

Any thoughts you guys have would be much appreciated! ",0,2025-05-25 07:43:12
"Basically it, as a DA, I‚Äôm trying to make my move to the DE path and I have been practicing this modern stack for couple months already, think I might have a interim level hitting to a Jr. but i was wondering if someone here can tell me if this still being a decent stack and I can start applying for jobs with it.

Also a the same time what‚Äôs the minimum I should know to do to defend myself as a competitive DE.

Thanks",0,2025-06-03 23:23:51
"I am an undergraduate student in applied mathematics with some experience in data science projects, but I would like to move toward the engineering field. For this, I need ideas for a scientific initiation project in data engineering.

To avoid being too generalist, I would prefer to apply it in the field of biomedicine or biology, if possible.

I have an idea of creating a data warehouse for genome studies, but I am not sure if this would be too complex for an undergraduate research project.",0,2025-05-26 14:40:37
"Hi everyone,

I‚Äôm currently finishing my Master's in Data Science and will officially graduate in June next year. I‚Äôll have about a month of free time coming up, and I want to use it wisely to break into data engineering.

I‚Äôve narrowed it down to two options:

Study for and pass a Microsoft-certified data engineering exam (probably the DP-203 ‚Äì Azure Data Engineer Associate).

Start a small ETL/data pipeline project with a few friends, maybe deploy it on the cloud (Azure or AWS) and publish everything on GitHub.

My long-term goal is to land a data engineering or cloud engineering role. I'm already familiar with Python, SQL, and some Spark basics. Not much industry experience yet, but I want to show I'm serious about this path.

What would be more valuable at this stage ‚Äì having a certification on my cv, or showcasing a real project with code and design decisions?

Would love to hear from anyone who‚Äôs already in the field or has gone through the same decision process. Any advice is appreciated!

Thanks in advance ",0,2025-05-25 10:21:29
"I got laid off from Amazon after COVID when they outsourced our BI team to India and replaced half our workflow with automation. The ones who stayed weren‚Äôt better at SQL or Python - they just had better people skills. 

For two months, I applied to every job on LinkedIn and heard nothing. Then I stopped. I laid in bed, doomscrolled 5+ hours a day, and watched my motivation rot. I thought I was just tired. Then my girlfriend left me - and that cracked something open.

In that heartbreak haze, I realized something brutal: I hadn‚Äôt grown in years. Since college, I hadn‚Äôt finished a single book - five whole years of mental autopilot. 

Meanwhile, some of my friends - people who foresaw the layoffs, the AI boom, the chaos - were now running startups, freelancing like pros, or negotiating raises with confidence. What did they all have in common? They never stop self growth and they read. Daily.

So I ran a stupid little experiment: finish one book. Just one. I picked a memoir that mirrored my burnout. Then another. Then I tried a business book. Then a psychology one. I kept going. It‚Äôs been 7 months now, and I‚Äôm not the same person.

Reading daily didn‚Äôt just help me ‚Äúget smarter.‚Äù It reprogrammed how I think. My mindset, work ethic, even how I speak in interviews - it all changed. I want to share this in case someone else out there feels as stuck and brain-fogged as I did. You‚Äôre not lazy. You just need better inputs. Start feeding your mind again.

As someone with ADHD, reading daily wasn‚Äôt easy at first. My brain wanted dopamine, not paragraphs. I‚Äôd reread the same page five times. That‚Äôs why these tools helped - they made learning actually stick, even on days I couldn‚Äôt sit still. Here‚Äôs what worked for me:
- The Almanack of Naval Ravikant: This book completely rewired how I think about wealth, happiness, and leverage. Naval‚Äôs mindset is pure clarity.

- Principles by Ray Dalio: The founder of Bridgewater lays out the rules he used to build one of the biggest hedge funds in the world. It‚Äôs not just about work - it‚Äôs about how to think. Easily one of the most eye-opening books I‚Äôve ever read.

- Can‚Äôt Hurt Me by David Goggins: NYT Bestseller. His brutal honesty about trauma and self-discipline lit a fire in me. This book will slap your excuses in the face.

- Deep Work by Cal Newport: Productivity bible. Made me rethink how shallow my work had become. Best book on regaining focus in a distracted world.

- The Psychology of Money by Morgan Housel:  Super digestible. Helped me stop making emotional money decisions. Best finance book I‚Äôve ever read, period.


Other tools & podcasts that helped 
- Lenny‚Äôs Newsletter: the best newsletter if you're in tech or product. Lenny (ex-Airbnb PM) shares real frameworks, growth tactics, and hiring advice. It's like free mentorship from a top-tier operator.

- BeFreed: A friend who worked at Google put me on this. It‚Äôs a smart reading & book summary app that lets you customize how you read/listen: 10 min skims, 40 min deep dives, 20 min podcast-style explainers, or flashcards to help stuff actually stick. 

it also remembers your favs, highlights, goals and recommend books that best fit your goal.

 I tested it on books I‚Äôd already read and the deep dives covered ~80% of the key ideas. Now I finished 10+ books per month and I recommend it to all my friends who never had time or energy to read daily.

- Ash: A friend told me about this when I was totally burnt out. It‚Äôs like therapy-lite for work stress - quick check-ins, calming tools, and mindset prompts that actually helped me feel human again.

- The Tim Ferriss Show - podcast ‚Äì Endless value bombs. He interviews top performers and always digs deep into their habits and books.

Tbh, I used to think reading was just a checkbox for ‚Äúsmart‚Äù people. Now I see it as survival. It‚Äôs how you claw your way back when your mind is broken.

If you‚Äôre burnt out, heartbroken, or just numb - don‚Äôt wait for motivation. Pick up any book that speaks to what you‚Äôre feeling. Let it rewire you. Let it remind you that people before you have already written the answers.

You don‚Äôt need to figure everything out alone. You just need to start reading again.",0,2025-05-26 18:46:11
"Hey everyone,

I wanted to start a conversation about the growing expectation for data professionals to become more ""full-stack."" Especially in the Brazilian market, I've noticed a trend, or even a pressure, for people to take on more responsibilities across the entire data workflow, sometimes beyond their original role.

I‚Äôve been working as a Data Engineer for a little over a year now, focusing mainly on EL processes, building data pipelines and delivering datasets to the primary layer. From there, Analytics Engineers usually take over and apply transformations. I hold certifications in Airflow (Astronomer) and Databricks Data Engineer Fundamentals, and I‚Äôm currently thinking about diving into DBT, mainly through personal projects.

Recently, I received the suggestion that being full-stack in data is the ideal, or even necessary, path to follow. That got me thinking:

How far should we go in expanding our technical scope?  
Are we sacrificing depth for breadth?  
Is this expectation more common for Data Engineers than for AEs or Data Scientists?  
Is being full-stack really an advantage in the long run, or just a sign of immaturity or lack of process in some organizations?

I‚Äôd love to hear your thoughts, especially from those who have faced this kind of situation or work in more structured data teams.",0,2025-05-26 12:47:37
"I‚Äôm a data engineer (yoe 10+)with a strong background and experience in SQL, ETL development, data warehousing , analytics. Also have strong cloud experience and credentials. 
Not  strong on the programming side, but can get the work done. Done some certifications and courses in ML. Have theoretical knowledge and done some poc projects but have no production experience in it yet. 

How can I transition to ML Engineering and AI Engineering? What do I need to be up skilled in? Any bootcamps, certifications, courses etc. that I can pursue. ",0,2025-05-26 02:35:48
"My company is in the market for a training database package. Any recommendations on what to go for/avoid? We use Civica HR, so something compatible with that would be ideal.",0,2025-05-26 12:53:40
"Hi all. As the title suggests‚Ä¶ I was wondering for someone looking to move into a Data Engineering role (no previous experience outside of data analysis with SQL and Excel), how steep is the learning curve with regards to the tooling and techniques?

Thanks in advance. ",0,2025-05-27 12:22:55
"Guys, 

I am working in an MNC, Total 3.5 exp. 

Joined in as an tech enthusiast in organisation,  deployed in a support project, due to money (rotational client visits) I was in the project, now I want to focus on career and make a switch.

Technologies worked on Data platforms Bigdata, Kafka, ETL. I am not able to perform well in coding due to lack of practice and also I am biting more than I can chew. Cloud platforms, data warehousing, etl, development etc... 

Need some guidance to lead the correct path, i couldn't decide which one to prefer as I have constraints. ",0,2025-05-26 16:51:19
"Hi folks,

I'm working as a cloud engineer and just received an offer as a DE. The new company is much smaller, with fewer benefits and pay, but it's growing fast because it focuses on ML/AI. Should I take this opportunity or stay in my current position?  A little about my situation: I'm currently on the bench at a large international company; there are no projects, and it makes me anxious.  However, I'm also afraid the gloomy economy will affect the new company, which is much smaller and less international. Has anyone faced a similar situation? How did you decide? I hope to hear your advice. Thanks in advance!",0,2025-05-27 17:07:04
"If you‚Äôre now working as a data engineer but didn‚Äôt start your career in this role, what were you doing before?

Was it software dev, analytics, sysadmin, academia, something totally unrelated? What pushed you toward data engineering, and how was the transition for you?

",0,2025-05-23 21:40:46
"I recently finished my degree in Computer Science and worked part-time throughout my studies, including on many personal projects in the data domain. I‚Äôm very confident in my technical skills: I can (and have) built large systems and my own SaaS projects. I know all the ins and outs of the basic data-engineering tools, SQL, Python, Pandas, PySpark, and have experience with the entire software-engineering stack (Docker, CI/CD, Kubernetes, even front-end). I also have a solid grasp of statistics.

About a year ago, I was hired at a company that had previously outsourced all IT to external firms. I got the job through the CEO of a company where I‚Äôd interned previously. He‚Äôs now the CTO of this new company and is building the entire IT department from scratch. The reason he was hired is to transform this traditional company, whose industry is being significantly disrupted by tech, into a ‚Äútech‚Äù company. You can really tell the CEO cares about that: in a little over one year, we‚Äôve grown to 15+ developers, and the culture has changed a lot.

I now have the privilege of being trusted with the responsibility of building the entire data infrastructure from scratch. I have total authority over all tech decisions, although I don‚Äôt have much experience with how mature data teams operate. Since I‚Äôm a total open-source nerd and we‚Äôre based in Europe, we want to rely on as few American cloud providers as possible, I‚Äôve set up the current infrastructure like this:

* **Airflow** (running in our Kubernetes cluster)
* **ClickHouse DWH** (also running in our Kubernetes cluster)
* **Spark** (you guessed it, running in our cluster)
* **Goose** for SQL migrations in our warehouse

Some conceptual decisions I‚Äôve made so far:

1. Data ingestion from different sources (Salesforce, multiple products, etc.) runs through Airflow, using simple Pandas scripts to load into the DWH (about 200 k rows per day).
2. ClickHouse is our DWH, and Spark connects to ClickHouse so that all analytics runs through Spark against ClickHouse. If you have any tips on how to structure the different data layers (Ingestion/datamart etc), please!

What I want to implement next are typical software-engineering practices, dev/prod environments, testing, etc. As I mentioned, I have a lot of experience in classical SWE within corporate environments, so I want to apply as much from that as possible. In my research, I‚Äôve found that you basically just copy the entire environment for dev and prod, which makes sense, but sounds expensive computing wise. We will soon start hiring additional DE/DA/DS.

My question is: What technical or organizational decisions do you think are important and valuable? What have you seen work (or not work) in your experience as a data engineer? Are there problems you only discover once your team has grown? I want to get in front of those issues as early as possible. Like I said, I have a lot of experience in how to build SWE projects in a corporate environment. Any things I am not thinking about that will sooner or later come to haunt me in my DE team? Any tips on how to setup my DWH architecture? How does your DWH look conceptually?",0,2025-06-03 16:07:14
"I am currently working as etl and pl sql developer and BI developer on oracle systems.
Learning snowflake and GCP. I have 10 YOE.

How can I transition to architect level role or lead kind of role.",0,2025-04-30 16:04:36
"Hello guys, 
I‚Äôm a data analyst with > 1 yr exp. My work revolves mostly on building dashboards from big query schemas/tables created by other team. We use Data studio and power bi to build dashboards now. Recently they‚Äôve planned to build in native and they‚Äôre using tools like bolt where if gives code and also dashboard with what use they want and integration through highcharts . Now all my job is to write a sql query and i‚Äôm scared that it‚Äôs replacing my job. I‚Äôm planning to job shift in 2-3 months. 

i only know sql , and just some visualisation tools and i have worked on the client side for some requirements. I‚Äôm also thinking of changing to data engineer what tools should i learn ? . Is DSA important?  I‚Äôm having difficulty figuring out what is happening in the data engineer roles and how deep the ai is involved . Some suggestions please üôè ",0,2025-04-30 15:47:02
"I am currently working as etl and pl sql developer and BI developer on oracle systems.
Learning snowflake and GCP. I have 10 YOE.

How can I transition to architect level role or lead kind of role.",0,2025-04-30 16:04:36
"
Hey everyone,
I'm currently trying to shift my focus toward freelancing, and I‚Äôd love to hear some honest thoughts and experiences.

I have a background in Python programming and a decent understanding of statistics. I‚Äôve built small automation scripts, done data analysis projects on my own, and I‚Äôm learning more every day. I‚Äôve also started exploring the idea of building a simple SaaS product, but money is tight and I need to start generating income soon.

My questions are:

Is there realistic demand for beginner-to-intermediate data scientists or Python devs in the freelance market?

What kind of projects should I be aiming for to get started?

What are businesses really looking for when they hire a freelance data scientist?
Is it dashboards, insights, predictive modeling, cleaning data, reporting? I‚Äôd love to hear how you match your skills to their expectations.


Any advice, guidance, or even real talk is super appreciated. I‚Äôm just trying to figure out the smartest path forward right now. Thanks a lot!",0,2025-04-30 12:10:12
"Hello guys, 
I‚Äôm a data analyst with > 1 yr exp. My work revolves mostly on building dashboards from big query schemas/tables created by other team. We use Data studio and power bi to build dashboards now. Recently they‚Äôve planned to build in native and they‚Äôre using tools like bolt where if gives code and also dashboard with what use they want and integration through highcharts . Now all my job is to write a sql query and i‚Äôm scared that it‚Äôs replacing my job. I‚Äôm planning to job shift in 2-3 months. 

i only know sql , and just some visualisation tools and i have worked on the client side for some requirements. I‚Äôm also thinking of changing to data engineer what tools should i learn ? . Is DSA important?  I‚Äôm having difficulty figuring out what is happening in the data engineer roles and how deep the ai is involved . Some suggestions please üôè ",0,2025-04-30 15:47:02
"Hi all! I am in field of sales of Microsoft analytics products. I am a strategic sales executive and was able to do well so far by showing my expertise on the business case of embracing cloud based analytical solutions. However, my role is now being changed to be more technical and before I can learn about Microsoft products I need to learn the basis of data engineering databases and everythjng that comes along with it. Let's just say I know how do to analytics on excel.. Need to learn everything in 30 days and willing to put in as many as 6 hours everyday.. Where do I start? How do I become an intelligent analytics professional who has a working knowledge of the fundamentals and then become someone who can understand Microsoft / AWS/ GCP specific products. For context, my undergrad and post grad is in business (MBA)",0,2025-06-04 17:24:58
"Hello everyone, I'm looking for course recommendations as I transition into a Data Architect role within my company. My background includes several years as a Developer (proficient in C++, C#, and Golang) and as a DBA (Oracle and SQL Server). While I have some foundational knowledge in data analysis, I'm eager to deepen my expertise specifically for a Data Architect position. I've explored a few online learning platforms like Coursera (specifically the IBM Data Architect Professional Certificate), DataCamp, and Codecademy. From my initial research, Coursera's offerings seem more comprehensive and aligned with data architecture principles. However, I'm located in Brazil, and the cost of Coursera is significantly higher compared to DataCamp. Considering my background and the need to specialize in data architecture, and keeping in mind the cost difference in Brazil, what courses or learning paths would you recommend? Are there any other platforms or specific courses I should consider? Any insights or suggestions based on your experience would be greatly appreciated!",0,2025-05-02 21:15:42
"I am currently pursuing my master's in computer science and I have no idea how do I get in DE... I am already following a 'roadmap' (I am done with python basics, sql basics, etl/elt concepts) from one of those how to become a de videos you find in YouTube as well as taking a pyspark course in udemy.... 
I am like a new born in de and I still have no confidence if what am doing is the right thing.
Well I came across this post on reddit and now I am curious... How do you stand out? Like what do you put in your cv to stand out as an entry level data engineer. What kind of projects are people expecting?
There was this other post on reddit that said ""there's no such thing as entry level in data engineering"" if that's the case how do I navigate and be successful between people who have years and years of experience? This is so overwhelming üò≠",0,2025-05-01 14:22:20
"My team has been working to hire some folks for a Data Engineering role. We are restricted to hiring in certain regions right now. But in short, one thing that I have noticed is that it seems like HR is bringing us a lot of people who say they had a ""Data Engineer"" background, but really the type of work they describe doing is very basic and more on the DevOps level. E.G. configuring and tuning big data infrastructure.

Is this a common misconception that companies have about the Data Engineering title, where they confuse DevOps for Data Engineering? And if we need someone with a solid coding background, should we be targeting Software Engineers instead?",0,2025-05-02 17:02:52
"
Hi everyone,

I‚Äôm based in the U.S. and have around 8 years of experience as a data engineer, primarily working with legacy ETL tools like Ab Initio and Informatica. I was laid off last year, and since then, I‚Äôve been struggling to find roles that still value those tools.

Realizing the market has moved on, I took time to upskill myself ‚Äì I‚Äôve been learning Python, Apache Spark, and have also brushed up on advanced SQL. I‚Äôve completed several online courses and done some hands-on practice, but when it comes to actual job interviews (especially those first calls with hiring managers), I‚Äôm not making it through.

This has really shaken my confidence. I‚Äôm beginning to worry: did I wait too long to make the shift? Is my career in data engineering over?

If anyone has been in a similar situation or has advice on how to bridge this gap, especially when transitioning from legacy tech to modern stacks, I‚Äôd really appreciate your thoughts.

Thanks in advance!
",0,2025-05-02 05:13:20
"We‚Äôre planning to pursue the training and certification simultaneously, but the course is quite expensive (around $5,000 USD each). Is this certification currently recognized in the industry, and is it worth the investment?",0,2025-05-02 13:58:23
"background:  
had internships as a backend developer in college, no return offer for any backend roles due to head count. HR got me to try for a data role, passed the interviews

feeling a bit apprehensive as i have 0 prior experience. The role seems to expect a lot from me and the company's work culture is intense (FAANG-adjacent). I'm starting the job in about a month, what i've done so far is :

\- read DDIA  
\- look up on spark's documentation (one of their tech stack used)

Any tips on what are the key skills to obtain / how to better prepare as a fresher? Thanks in advance.",0,2025-05-02 14:41:19
"Basically it, as a DA, I‚Äôm trying to make my move to the DE path and I have been practicing this modern stack for couple months already, think I might have a interim level hitting to a Jr. but i was wondering if someone here can tell me if this still being a decent stack and I can start applying for jobs with it.

Also a the same time what‚Äôs the minimum I should know to do to defend myself as a competitive DE.

Thanks",0,2025-06-03 23:23:51
"I‚Äôm curious to hear from those who‚Äôve navigated this journey: What‚Äôs the best way to get your foot in the door as a new grad data engineer in the current market? Whether it‚Äôs networking tips, specific skills to focus on, or creative project ideas to stand out.",0,2025-06-03 13:21:58
"I‚Äôm currently a BI Engineer at a Fortune 50 subsidiary, where I‚Äôve been for 1.5 years (previously a Data Analyst for 1.5 years). I just got an offer for a fully remote Data Engineering role at a 4,000-person healthcare intelligence company, paying $120K vs my current $92K. The new role aligns with the career path I‚Äôve been aiming for since graduating, and everyone I interviewed with had been there for 5‚Äì10+ years with clear promotion paths. My current job is stable, low stress, and the team is great, but I feel like I‚Äôve learned all I can. No one on my team has been promoted in years, even those with more tenure, so growth isn‚Äôt guaranteed. I‚Äôm just nervous about making a jump in today‚Äôs market, from what I‚Äôve research the company has good reviews on Glassdoor as well as good financials from what I was able to gather but still would appreciate any advice from people who‚Äôve made a similar move.",0,2025-05-01 19:15:02
"Esperienza come SWE e buona conoscenza di Python.
Zero esperienza nel mondo dati.

Vorrei switchare a data engineer: il mondo mi affascina, √® una figura in crescita e la paga √® buona.

Qualcuno di voi √® recentemente riuscito a fare questo cambio di carriera? se si, come?",0,2025-06-03 07:46:00
"I‚Äôm 27 and have been working in customer service ever since I graduated with a degree in business administration. While the experience has taught me a lot, the job has become really stressful over time.

Recently, I‚Äôve developed a strong interest in data and started exploring different career paths in the field, specially data engineering. The problem is, my technical background is quite basic, and I sometimes worry that it might be too late to make a switch now, compared to others who got into tech earlier. 

For those who‚Äôve made a similar switch or are in the field, do you think 27 is too late to start from scratch and build a career in data engineering? Any advice?
",0,2025-06-11 17:48:49
"
I have 6 years of experience as BI Engineer consultant. I‚Äôm from north Europe but I‚Äôm looking for new opportunities to move either to Spain, Switzerland, Germany, applying almost for everything but all I get it‚Äôs that they moved forward with other candidates. I also apply for those jobs that are fully remote in US, Europe so I can move to cheaper countries in Asia or south Europe but even that‚Äôs impossible to catch something.

What did happen in this field is it really hard for everyone and not only me ? Or it‚Äôs an area that got really saturated? 
",0,2025-04-28 21:45:25
"Is studying all these Python topics important and essential for a data engineer, especially Object-Oriented Programming (OOP)? Or is it a waste of time, and should I only focus on the basics that will help me as a data engineer? I‚Äôm in my final year of college and want to make sure I‚Äôm prioritizing the right skills.

Here are the topics I‚Äôve been considering:
- Intro for Python
- Printing and Syntax Errors
- Data Types and Variables
- Operators
- Selection
- Loops
- Debugging
- Functions
- Recursive Functions
- Classes & Objects
- Memory and Mutability
- Lists, Tuples, Strings
- Set and Dictionary
- Modules and Packages
- Builtin Modules
- Files
- Exceptions
- More on Functions
- Recursive functions
- Object Oriented Programming
- OOP: UML Class Diagram
- OOP: Inheritance
- OOP: Polymorphism
- OOP: Operator Overloading",0,2025-04-22 12:44:33
"I have a bachelor‚Äôs and master‚Äôs degree in Business Analytics/Data Analytics respectively. I graduated from my master‚Äôs program in 2021, and started my first job as a data engineer upon graduation. Even though my background was analytics based, I had a connection that worked within the company and trusted I could pick up more of the backend engineering easily. I worked for that company for almost 3 years and unfortunately, got close to no applicable experience. They had previously outsourced their data engineering so we faced constant roadblocks with security in trying to build out our pipelines and data stack. In short, most of our time was spent arguing with security for reasons we needed access to data/tools/etc to do our job. They laid our entire team off last year and the job search has been brutal since. I‚Äôve only gotten 3 engineering interviews from hundreds of applications and I‚Äôve made it to the final round during each, only to be rejected because of technical engineering questions/problems I didn‚Äôt know how to figure out. I am very discouraged and wondering if data engineering is the right field for me. The data sphere is ever evolving and daunting, I already feel too far behind from my unfortunate first job experience. Some backend engineering concepts are still difficult for me to wrap my head around and I know now I much prefer the analysis side of things. I‚Äôm really hoping for some encouragement and suggestions on other routes to take as a very early career data professional. I‚Äôm feeling very burnt out and hopeless in this already difficult job market",0,2025-04-24 16:14:05
"Little background about myself; I have been working as full stack developer hybrid, decided to move to UK for MSc in Data Science. I‚Äôve worked in a startup so I know my way around learning new things quick. Pretty good at Django, SQL, Python(Please don‚Äôt say Django is Python, it‚Äôs not). The company I have joined is focused on travel, and are onboarding a data team.

They have told me they aren‚Äôt expecting me to create wonders but grow myself into it. The head of data is an awesome person, and was impressed the amount of knowledge I knew.

Now you are wondering why am I asking this question? Basically, I want to make sure I can secure a visa sponsorship and want to work hard, learn as much as possible. I have moved country to get this job and want to settle over here. 


",0,2025-04-25 00:37:39
"Hello all, I am looking for some advice on the reason of data engineering/data science (yes I know they are different). I will be graduating in May with a degree in Physics. During my time in school, I have spent considerable time doing independent study for Python, MATLAB, Java, and SQL. Due to financial constraints I am not able to pay for a certification course for these languages but I have taken free exams to get some sort of certificate that says I know what I'm talking about. I have grown to not really want to work in a lab setting, but rather a role working with numbers and data points in the abstract. So I'm looking for a role in analyzing data or creating infrastructure for data management. Do you all have any advice for a new head trying to break into the industry? Anything would be greatly appreciated.",0,2025-04-14 14:21:26
"Hey all, Im a DE with 3 YoE in the US. I switched careers a year out from university and landed a DE role at a consulting company. I had been applying to anything with Data in the title, but loved the role through and through initially. (Techstack mainly PySpark and AWS).

Now, the clients are not buying the need for new data pipelines or the need for DE work in general so the role is more so of a data analyst, writing SQL queries for dashboards/reports (Also curious if this is common in the DE field to switch to reporting work?). Looking to work with more seasoned data teams and get more practice with devops skills and writing code but worried I just dont have enough YoE to be trusted with an in house DE role. 

Ive started applying again but only heard back from consulting firms, any tips/insights for improving my chances landing a role at a non consulting firm? Is the grass greener?",0,2025-04-28 16:08:21
"A little background about myself, I am in my mid 40s, based Europe and currently looking to get a new career or simply a job.  I did a BS in information systems in 2003 and worked as a sys admin and then as a linux dev guy until 2007.  I then switched careers, got a business degree and started working in consulting (banking).  For the past few years I have been a freelancer. 

My last freelance project ended in Dec 2023 and while searching for another job I fell ill and needed surgeries and was not capable of doing much until last month. Since then I have been looking for work and the freelance project work for banks in Europe is drying up. 

Since I know how to program (I did some scripting as a consultant every now and then in VBA and Python) and since the data field is growing I was wondering if I could switch to being a Data Engineer?

\* Will recruiters and mangers consider my profile if I get some certifications?

\* Is age a barrier in finding work?  Will my 1.5 year long career break prevent me from getting a job?

\* Are there freelance projects/gigs available in this field and what skills/background are needed to break into the field.

\* Any other advice tips you have for someone in my position.  What other careers could/should I consider?",0,2025-04-21 16:24:42
I always get different answer from different people on this.,0,2025-04-14 12:05:56
"My predictive modeling folks, beginner here could use some feedback guidance. Go easy on me, this is my first machine learning/predictive model project and I had very basic python experience before this.


I‚Äôve been working on a personal project building a model that predicts NFL player performance using full career, game-by-game data for any offensive player who logged a snap between 2017‚Äì2024. 

I trained the model using data through 2023 with XGBoost Regressor, and then used actual 2024 matchups ‚Äî including player demographics (age, team, position, depth chart) and opponent defensive stats (Pass YPG, Rush YPG, Points Allowed, etc.) ‚Äî as inputs to predict game-level performance in 2024.

The model performs really well for some stats (e.g., R¬≤ > 0.875 for Completions, Pass Attempts, CMP%, Pass Yards, and Passer Rating), but others ‚Äî like Touchdowns, Fumbles, or Yards per Target ‚Äî aren‚Äôt as strong.

Here‚Äôs where I need input:

-What‚Äôs a solid baseline R¬≤, RMSE, and MAE to aim for ‚Äî and does that benchmark shift depending on the industry?

-Could trying other models/a combination of models improve the weaker stats? Should I use different models for different stat categories (e.g., XGBoost for high-R¬≤ ones, something else for low-R¬≤)?

-How do you typically decide which model is the best fit? Trial and error? Is there a structured way to choose based on the stat being predicted?

-I used XGBRegressor based on common recommendations ‚Äî are there variants of XGBoost or alternatives you'd suggest trying? Any others you like better?

-Are these considered ‚Äúgood‚Äù model results for sports data?

-Are sports models generally harder to predict than industries like retail, finance, or real estate?

-What should my next step be if I want to make this model more complete and reliable (more accurate) across all stat types?

-How do people generally feel about manually adding in more intangible stats to tweak data and model performance? Example: Adding an injury index/strength multiplier for a Defense that has a lot of injuries, or more player‚Äôs coming back from injury, etc.? Is this a generally accepted method or not really utilized?

Any advice, criticism, resources, or just general direction is welcomed.",0,2025-04-09 02:27:57
"I'm coming into college for Fall 25  originally intending to major in Global Studies and Chinese Language. My original desire was to work for The State Department in some capacity, and to be a subject matter expert in Chinese Affairs - potentially in the Diplomatic Security Service, or as an analyst for the FBI/CIA, etc - something where I can serve the US.

I didn't expect to get into Cal, and this changed everything for me, I suddenly feel emboldened to reach out of my comfort level - and I excelled in statistics and enjoyed the class in HS over 10 years ago. Data Science seems like a great fit, and the career opportunities are very appealing after talking to a few of my peers in the field. It's also something that the State Department / FBI / CIA / etc. would need. I'm going to take all the courses needed to declare the major (Data C8, Calc 1, Calc 2, etc) and do a comprehensive review.

  
I feel like there's a lot of overlap in Global Studies / Chinese Language, which is why I had it as my original double major goal.   
Just looking at the requirements is making it clear I'll easily bust the 136 or so credit course limit for transfer students. Even without the credit limit, the Semesters will be extremely rigorous. 

So I'm thinking of ""just"" choosing either Global Studies or Chinese Language. I can see pros/cons to both. In a perfect world, I would Triple Major. I would love to be a diplomat, and discuss global policy and implications of policy decisions. An advisor in told me I could still do that without taking Global Studies classes, and I would have more expertise and knowledge if I studied Chinese Language as a Major.

Any input?

Some background about me: I'm a bit older, more mature, and prepared to work hard. I took 18 credit semesters for both semesters at my community college, worked 40 hours a week, and did monthly National Guard obligations - so a high course load doesn't intimidate me.",0,2025-04-24 00:48:44
"Had a mental
Breakdown the other day from a lot of things on my plate, school, wife, turnover on rentals and midterm exams all at the same time vying for my attention. I just could not take it anymore and broke down. 

Assuming a complete vacume. Do the mental breakdowns stop once in the field or is it more of a you just get used to it type of thing?",0,2025-04-17 00:54:18
"For a Senior DE, which companies have a relevant tech stack, pay well, and have decent WLB outside of FAANG? 

EDIT: US-based, remote, $200k+ base salary ",0,2025-04-20 23:34:15
"Currently Senior DE at medium size global e-commerce tech company, looking for new job. Prepped for like 2 months Jan and Feb, and then started applying and interviewing. Here are the numbers:

Total apps: 107. 6 companies reached out for at least a phone screen. 5.6% conversion ratio.

The 6 companies where the following:

|Company|Role|Interviews|
|:-|:-|:-|
|Meta|Data Engineer|HR and then LC tech screening. Rejected after screening|
|Amazon|Data Engineer 1|Take home tech screening then LC type tech screening. Rejected after second screening|
|Root|Senior Data Engineer|HR then HM. Got rejected after HM|
|Kin|Senior Data Engineer|Only HR, got rejected after.|
|Clipboard Health|Data Engineer|Online take home screening, fairly easy but got rejected after.|
|Disney Streaming|Senior Data Engineer|Passed HR and HM interviews. Declined technical screening loop.|

At the end of the day, my current company offered me a good package to stay as well as a team change to a more architecture type role. Considering my current role salary is decent and fully remote, declined Disneys loop since I was going to be making the same while having to move to work on site in a HCOL city.

PS. Im a US Citizen.",0,2025-04-15 14:47:13
"What is the difference between a junior and senior in this role? How much can you really know in data engineering; get the data, clean it, dump it somewhere with a cloud service. 

But what would take someone from a junior role to a senior role? Is it just the number years of experience? 

",0,2025-03-30 07:52:21
"Hello all, 

I've been interviewing for a proper Junior Data Engineer position and have been doing well in the rounds so far. I've done my recruiter call, HR call and coding assessment. Waiting on the 4th.

I want to be great. I am willing to learn from those of you who are more experienced than me.  
  
Can anyone share examples from their own careers on attitude, communication, soft skills, time management, charisma, willingness to learn and other soft skills that I should keep in mind. Or maybe what I should not do instead.

How should I approach the technical side? There are 1000's of technologies to learn. So I have been learning basics with soft skills and hoping everything works out.

3 years ago I had a labour job and did well in that too. So this grind has caused me to rewire my brain to work in tech and corporate work. I am aiming for 20 years more in this field.

Any insights are appreciated.

Thanks!",0,2025-03-30 18:19:54
"So after about 25 years of experience in what was considered DBA, I am now unemployed due to the federal job cuts and it seems DBA just isn't a role anymore. I am currently working on getting a cloud certification but the rest of my skills seem to be mixed and I am hoping someone has a more specific role I would fit into. I am also hoping to expand my skills into some newer technology but I have no clue where to even start. 

Current skills are:

Expert level SQL

Some knowledge of Azure and AWS

Python, PowerShell, GIT, .NET, C#, Idera, Vcentre, Oracle, BI, and ETL with some other minor things mixed in. 

Where should I go from here? What role could this be considered? What other skills could I gain some knowledge on?",0,2025-04-14 06:08:14
"What is the difference between a junior and senior in this role? How much can you really know in data engineering; get the data, clean it, dump it somewhere with a cloud service. 

But what would take someone from a junior role to a senior role? Is it just the number years of experience? 

",0,2025-03-30 07:52:21
The field of data engineering goes as far back as the mid 2000s when it was called different things. Around that time SSIS came out and Google made their hdfs paper. What did people use for data manipulation where now Python would be used. Was it still Python2? ,0,2025-04-21 21:31:08
"I am scared my job is a lightning strike that doesnt exist elsewhere. Im classified as a ‚Äúdata engineer‚Äù but only work in snowflake building datasets for tableau. Basically im a middle man between IT who ingests the data and then analysts who visualize in tableau. I live in fear (lol) that if i were to lose this job i would qualify for nothing else because i havent touched python or any ingesting tools or tableau and any visualizing tools in years. 
Am as as out of the norm as i feel?",0,2025-04-02 13:33:31
"any book recommendations for data interpretation for ipucet bcom h paper
",0,2025-04-13 08:02:09
"I know it varies by company blah blah blah, but also aside from a Google search, what have you guys in the field noticed to be core differences between these positions? ",0,2025-04-01 23:53:39
"
About a month ago I was hired at a very small startup (3 employees including me) to be their ""data engineer and analyst"", replacing the previous data engineer who moved on to a grad scheme.

I recently graduated in a non-CS discipline, so my Python and SQL skills aren't exactly amazing but I'm a fast learner. It helps that the other employees are non-technical and the previous data engineer was extremely helpful while training me.

The job has been going well so far. I can see myself getting my skills up to a good standard, and it's a great role to learn the ropes BUT I can't see myself in this role for longer than a year or two. So what should I prepare for next? A more demanding data engineer job? Further education?

I'd like to have a technical job in the financial sector within the next 5-6 years e.g. data engineer for a quant firm.
",0,2025-04-02 09:23:29
"I currently work at a Big Tech and have 3 YoE. My role is a mix of Full-Stack + Data Engineering. 

I want to keep preparing for interviews on the side, and to do that I need to know which role to aim for. 

Pros of SWE:
- more jobs positions 
- I have already invested 300 hours into DSA Leetcode. Don‚Äôt have to start DE prep from scratch
-Maybe better quality of work/pay(?)

Pros of DE:
- targeting a niche has always given me more callbacks
- if I practice a lot of sql, the interviews at FAANG could be gamed. FAANG do ask DSA but they barely scratch the surface

My thoughts:
Ideally I want to crack the SWE role at a FAANG as I like both roles equally but SWE pays 20% more. If I don‚Äôt get callbacks for SWE, then securing a similar pay through a DE role at FAANG is lucrative too. 
I‚Äôd be completely fine with doing DE, but I feel uneasy wasting the 100s of hours I spent on DSA. 

Applying for both jobs is sub optimal as I can only sink my time into SQL or DSA | system design or data modelling. 

What do you folks suggest? 
",0,2025-04-12 22:17:20
"I know it varies by company blah blah blah, but also aside from a Google search, what have you guys in the field noticed to be core differences between these positions? ",0,2025-04-01 23:53:39
"Hey everyone,
I recently interviewed for a Data Engineer role, but when I got the offer letter, the designation was ‚ÄúSoftware Engineer‚Äù. When I asked HR, they said the company uses generic titles based on experience, not specific roles.

Is this common practice?

",0,2025-04-23 18:51:59
"Am I missing some key skills?

# Summary

Scientist and engineer with a Ph.D. in physics and extensive experience in data engineering and biomedical data science, including bioinformatics and biostatistics. Specializes in complex data curation, analysis pipeline development on high-performance computing clusters, and cloud-based computational infrastructure. Dedicated to leveraging data to address real-world challenges.

# Work Experience

# Founder / Director

# Autism All Grown Up (https://aagu.org) 10/2023 - Present

* Founded and directs a nonprofit focused on the unmet needs of Autistic adults in Oregon, Securing over $60k of funding in less than six months.
* Coordinates writing and submitting grants, 20 in five months.
* Builds partnerships with community organizations by collaborating on shared interests and goals.
* Coordinates employees and volunteers.
* Designs and manages programs.

# Biomedical Data Scientist

# Freelancer 08/2022 -12/2023

* Worked with collaborators to launch a corporate-academic collaborative research project integrating multiple large-scale public genomic data sets into a graph database suitable for machine learning, oncology, and oncological drug repurposing.
* Performed analysis to assess overexpressed proteins related to toxic response from exercise in a human study.

# Senior Research Engineer

# OHSU | Center for Health Systems Effectiveness 11/2022 -10/2023

* Reduced compute time of a data analysis pipeline for calculating quality measures by 90% by parallelizing and porting to a high-performance computing (HPC) SLURM cluster, increasing researchers' access to data.
* Increased the performance of an ETL pipeline for staging Medicare claims data by 50% by removing bottlenecks and removing unnecessary steps.
* Championed better package management by transitioning the research group to the Conda package manager, resulting in 80% fewer package-related programming bottlenecks and reduced sysadmin time.
* Wrote comprehensive user documentation and training for pipeline usage published on enterprise GitHub.
* Supported researchers and data engineers through training and mentorship in R programming, package management, and high-performance computing best practices.

# Bioinformatics Scientist

# Providence | Earl A. Chiles Research Institute 08/2020 -06/2022

* Created a reproducible ETL pipeline for generating a drug-repurposing graph database that cleans, harmonizes, and processes over four billion rows of data from 10 different cancer databases, including clinical variants, clinical tumor sequencing data, tumor cell-line drug response data, variant allele frequencies, and gene essentiality.
* Located errors in combined WES tumor variant calls and suggested methods to resolve them.
* Scaled up ETL and analysis pipelines for WES and WGS variant analysis using BigQuery and Google Cloud Platform.
* Helped automate dockerized workflows for RNA-Seq analysis on the Google Cloud Platform.

# Computational Biologist

# OHSU | Casey Eye Institute 07/2018 -04/2020

* Extracted obscured information from messy human microbiome data by fine-tuning statistical models.
* Created a reproducible notebook-based pipeline for automated statistical analysis with custom parameters on a high-performance computing cluster and produced automated reports.
* Analyzed 16-S rRNA microbiome sequencing data by performing phylogenetic associations, diversity analysis, and multiple statistical tests to identify significant associations with age-related macular degeneration, contributing to two publications.

# Computational Biologist

# Oregon Health & Science University, Bioinformatics Core 11/2015 -06/2017

* Automated image region selection for an IHC image analysis pipeline, increasing throughput 100x and allowing high-throughput analysis for cancer research.
* Created a templated and automated pipeline to perform parameterized ChIP-Seq analysis on a high-performance computing cluster and generate automated reports.
* Programmed custom LIMS dashboard elements using R and Javascript (Plotly) for real-time visualization of cancer SMMART trials.
* Installed and managed research-oriented Linux servers and performed systems administration.
* Conducted RNA-Seq analysis.
* Mentored and trained coworkers in programming and high-performance computing.

# IT Support Technician

# Volpentest HAMMER Federal Training Center 08/2014 -11/2015

* Helped develop a ColdFusion website to publish and schedule safety courses to be used on the Hanford site.
* Vetted, selected, and managed a SAAS library management system.
* Built and managed two MS Access databases with entry forms, comprehensive reports, and a macro to email library users about their accounts.

# Education

# Ph.D. in Physics 05/2005

Indiana University Bloomington

# Bachelor of Science in Physics 06/1998

The Evergreen State College

# Certifications

# Human Subjects Research (HSR) 11/2022 -11/2025

# Responsible Conduct of Research (RCR) 11/2022 -11/2025

# Award

# Outstanding Graduate Student in Research 05/2005

Indiana University

# Skills

**Data Science & Engineering:** ETL, Data harmonization, SQL, Cloud (GCP), Docker, HPC (SLURM), Jupyter Notebooks, Graphics and visualization, Documentation. Containerized workflows (Docker, Singularity), statistical analysis and modeling, and mathematical modeling.

**Bioinformatics, Computational Biology, & Genomics:** DNA/RNA sequencing (WES, WGS, DNA-Seq, RNA-Seq, ChIP-Seq, 16s rRNA), Variant calling, Microbiome analysis, Transcriptomics, DepMap, ClinVar, KEGG.

**Programming & Development:** *Expert***:** R, Bash; *Strong***:** Python, SQL, HTML/CSS/JS; *Familiar***:** Matlab, C++, Java.

**Healthcare Analytics:** ICD-10, CPT, HCPCS, CMS, SNOMED, Medicaid claims, Quality Metrics (HEDIS).

**Linux & Systems Administration:** Server configuration, Web servers, Package management, SLURM, HTCondor.",0,2025-05-26 19:06:07
"I used to work as a Tableau developer and honestly, life felt simpler. I still had deadlines, but the work was more visual, less complex, and didn‚Äôt bleed into my personal time as much.

Now that I'm in data engineering, I feel like I‚Äôm constantly thinking about pipelines, bugs, unexpected data issues, or some tool update I haven‚Äôt kept up with. Even on vacation, I catch myself checking Slack or thinking about the next sprint. I turned 30 recently and started wondering‚Ä¶ is this normal career pressure, imposter syndrome, or am I chasing too much of management approval?

Is anyone else feeling this way? Is the stress worth it long term?",0,2025-04-01 05:25:58
"Basically the title. I'm trying to get out of data engineering since it's just really boring and trivial to me for almost any task, and the ones that are hard are just really tedious. A lot of repetitive query writing and just overall not something I'm enjoying.

I've always enjoyed ML and distributed systems, so I think MLE would be a perfect fit for me. I have 2 YOE if you're only counting post graduation and 3 if you count internship. I know MLE may not be the ""perfect"" fit for researching models, but if I want to get into actual research for modern LLM models, I'd need to get a PhD, and I just don't have the drive for that.

Background: did UG at a top 200 public school. Doing MS at Georgia Tech with ML specialization. Should finish that in 2026 end of summer or end of fall depending if I want to take a 1 course semester for a break.

  
I guess my main question is whether it's easier to swap into MLE from DE directly or go SWE then MLE with the master's completion. I haven't been seriously applying since I recently (Jan 2025) started a new DE role (thinking it would be more interesting since it's FinTech instead of Healthcare, but it's still boring). I would like to hear others' experience swapping into MLE, and potential ways I could make myself more hirable. I would specifically like a remote role also if possible (not original) but I would definitely take the right role in person or hybrid if it was a good company and good comp with interesting stuff. To put in perspective I'm making about 95k + bonus right now, so I don't think my comp requirements are too high. 

I've also started applying to SWE roles just to see if something interesting comes up, but again just looking for advice / experience from others. Sorry if the post was unstructured lol I'm tired.",0,2025-04-22 04:19:55
I'm a data analyst with 3 years of experience expecting an offer for a Data Engineer role from a non-tech company in the Dallas area. I'm currently in a LCOL area and am worried the pay won't even out with my current salary after COL. I have a Master's in a technical area but not data analytics or CS. Is 95-100K reasonable?,0,2025-04-23 00:51:48
"Hi folks,  
I got two offers in hand, one is from EY GDS for 10.5LPA + 5% VBA (which I heard people actually get around 10-20% on a A or B rating) and Deloitte India 11 LPA + 10% VPB (Didn't accepted the offer yet, asked for 14 LPA ). Which one should I join, which is better in terms of projects, work culture and career growth. I have 5 days to decide.",0,2025-04-24 09:06:20
"I'm just starting out in data engineering and still consider myself a noob. I have a question: in the era of AI, what should I really focus on? Should I spend time trying to understand every little detail of syntax in Python, SQL, or other tools? Or is it enough to be just comfortable reading and understanding code, so I can focus more on concepts like data modeling, data architecture, and system design‚Äîthings that might be harder for AI to fully automate?

Am I on the right track thinking this way?",0,2025-04-23 05:26:19
"I have been working for a consulting firm for the past 5 years. The kind of work they assign me to is fairly basic - developing pipelines using Informatica and writing SQL queries for it. That's been majority of my experience. For the past # months, I've been assigned to a PowerBI developer role, but I just tweak the data/queries to do what the client asks. When I try to apply for data engineering/etl roles, I get asked what I think are pretty advanced questions - for example I got asked about what gaps I have noticed in Microsoft Fabric and what are best practices for data modeling etc. I tend to give general answera based on my research and theoretical answers, but I can never relate it to my actual experience because day to day I don't do anything high level. I get asked about how I optimzied queries or pipelines, the truth is I worked with small enough datasets that I never really had to do anything. Again, I give answers based on my research - like indexing or partitioning but I feel the people asking questions are always looking for more. 

I cannot leave or take a break, I'm on a visa, but how do I actually get further then. Is anyone else feeling the same? ",0,2025-04-02 22:56:33
"I've graduated in CS (lots of data heavy coursework) this semester at a reasonable university with 2 years of internship experience in data analysis/engineering positions. 

I've almost finished reading Fundamentals of Data Engineering, which solidified my knowledge. I could use more book suggestions as a next step.

",0,2025-04-30 17:59:00
"I am a data engineer from China with three years of post - undergraduate experience. 
I spent the first two years engaged in big data development in the financial industry, mainly working on data collection, data governance, report development, and data warehouse development in banks. 
Last year, I switched to a large internet company for data development. A significant part of my work there was the crowd portrait labeling project. I developed some labels according to the needs of operations and products. Besides, based on my understanding of the business, I created some rule - based and algorithmic predictive labels. 
The algorithmic label part was something I had no previous contact with, and I found myself quite interested in it. I would like to know how I can develop if I go down this path in the future.",0,2025-04-26 16:23:42
"After a year of self teaching I managed to secure an internal career move to data engineering from finance 

What I am wondering is long term will my non IT background matter/discount me against other candidates? I have a degree in accountancy and I am a qualified accountant but I am considering doing a masters in data or computing if it will be beneficial longer term

Thanks",0,2025-04-12 13:54:29
"I've graduated in CS (lots of data heavy coursework) this semester at a reasonable university with 2 years of internship experience in data analysis/engineering positions. 

I've almost finished reading Fundamentals of Data Engineering, which solidified my knowledge. I could use more book suggestions as a next step.

",0,2025-04-30 17:59:00
"Have been trying to land a DE role with a non DE title as the current role for almost an year with no success.My current title is Data Warehouse Engineer with most of my focused around Databricks,Pyspark/Python,SQL and AWS services.

I have a total of 8 years of experience with the following titles.

SQL DBA

BI Data Engineer

Data Warehouse Engineer

Since I have 8 years of experience, I get rejected when I apply for DE roles that require only 3 years of experience.
It‚Äôs a tough ride so far.

Wondering how to go from here.

",0,2025-05-02 12:22:17
All senior DEs with 10-15 YOE can guide how much devOps should the DEs should know and if we learn Devops what are the benefits plus career path we can have down the line . ,0,2025-05-24 03:06:16
"Fellow data engineers...esp those working in banking sector...how many of you have been told to take on ops team role under the guise of 'devsecops'?...is it now the new norm?
I feel it impacts productivity of a developer",0,2025-04-27 21:12:21
"Hello everyone,

I‚Äôm currently a 3rd-year university student at a relatively large, middle-of-the-road American university. I am switching into Data Science from engineering, and would like to become a data engineer or data scientist once I graduate. Right now I‚Äôve had a part-time student data scientist position sponsored by my university for about a year working ~15 hours a week during the school year and ~25-30 hours a week during breaks. I haven‚Äôt had any internships, since I just switched into the Data Science major. I‚Äôm also considering taking a minor in statistics, and I want to set myself up for success in Data Engineering once I graduate. Given my situation, what advice would you offer? I‚Äôm not sure if a Master‚Äôs is useful in the field, or if a PhD is important. Are there majors which would make me better equipped for the field, and how can I set myself up best to get an internship for Summer 2026? My current workplace has told me frequently that I would likely have a full-time offer waiting when I graduate if I‚Äôm interested. 



Thank you for any advice you have.",0,2025-04-25 23:03:27
"To be specific, non-code heavy work. I think I‚Äôm one of the few data engineers who hates coding and developing. All our projects and clients so far have always asked us to use ADB in developing notebooks for ETL use, and I have never touched ADF -_-

Now I‚Äôm sick of it, developing ETL stuff using pyspark or sparksql is too stressful for me and I have 0 interest in data engineering right now. 

Anyone who has successfully left the DE field? What non-code role did you choose? I‚Äôd appreciate any suggestions especially for jobs that make use of some of the less-coding side of Data Engineering.

I see lots of people going for software eng because they love coding and some go ML or Data Scientist. Maybe i just want less tech-y work right now but yeah open to any suggestions. I‚Äôm also fine with sql, as long as it‚Äôs not to be used for developing sht lol",0,2025-04-14 04:50:00
"Little background about myself; I have been working as full stack developer hybrid, decided to move to UK for MSc in Data Science. I‚Äôve worked in a startup so I know my way around learning new things quick. Pretty good at Django, SQL, Python(Please don‚Äôt say Django is Python, it‚Äôs not). The company I have joined is focused on travel, and are onboarding a data team.

They have told me they aren‚Äôt expecting me to create wonders but grow myself into it. The head of data is an awesome person, and was impressed the amount of knowledge I knew.

Now you are wondering why am I asking this question? Basically, I want to make sure I can secure a visa sponsorship and want to work hard, learn as much as possible. I have moved country to get this job and want to settle over here. 


",0,2025-04-25 00:37:39
"Hi all. I‚Äôm considering another degree to put off paying back student loans. In the US if you‚Äôre in school at least part time (6 hours every long semester) your loans will be in deferment and not impacting your credit. I‚Äôm curious what degree (preferably online) has the best ROI. I‚Äôm a Senior Azure Data Engineer and I already have a Bachelor‚Äôs and Master‚Äôs degree in Management Information Systems. I was thinking of maybe getting an associates in Computer Science from a community college then getting a Masters in Computer Science. I‚Äôm open to suggestions. Unfortunately I don‚Äôt think there‚Äôs an official master or bachelor‚Äôs of data engineering, otherwise I‚Äôd do that. I‚Äôm not interested in management yet so an MBA is a highly unlikely. Cybersecurity is cool but I like my career in data. Maybe if there‚Äôs no other options. Thanks in advance. 

PS. This isn‚Äôt a political post. I don‚Äôt care whether people pay student loans or not, I just don‚Äôt want to pay mine yet. ",0,2025-04-24 03:20:18
"Hello, a few months ago I graduated for a ""Data Science in Business"" MSc degree in France (Paris) and I started looking for a job as a Junior Data Scientist, I kept my options open by applying in different sectors, job types and regions in France, even in Europe in general as I am fluent in both French and English. Today, it's been almost 8 months since I started applying (even before I graduated), but without success. During my internship as a data scientist in the retail sector, I found myself doing some ""data engineering"" tasks like working a lot on the cloud (GCP) and doing a lot of SQL in Bigquery, I know it's not much compared to what a real data engineer does on his daily tasks, but it was a new thing for me and I enjoyed doing it. At the end of my internship, I learned that unlike internships in the US, where it's considered a trial period to get hired, here in France it's considered more like a way to get some work done for cheap... well, especially in big companies. I understand that it's not always like that, but that's what I've noticed from many students.

Anyway, during those few months after the internship, I started learning tools like Spark, AWS, and some of Airflow. I'm thinking that maybe I have a better chance to get a job in data engineering, because a lot of people say that it's getting harder and harder to find a job as a data scientist, especially for juniors. So is this a good idea for me? Because it's been like 3-4 months applying for Data Engineering jobs, still nothing. If so, is there more I need to learn? Or should I stick to Data Science profil, and look in other places, like Germany for example?

Sorry for making this post long, but I wanted to give the big picture first.",0,2025-04-22 14:10:56
"Hello everyone,

I‚Äôm currently a 3rd-year university student at a relatively large, middle-of-the-road American university. I am switching into Data Science from engineering, and would like to become a data engineer or data scientist once I graduate. Right now I‚Äôve had a part-time student data scientist position sponsored by my university for about a year working ~15 hours a week during the school year and ~25-30 hours a week during breaks. I haven‚Äôt had any internships, since I just switched into the Data Science major. I‚Äôm also considering taking a minor in statistics, and I want to set myself up for success in Data Engineering once I graduate. Given my situation, what advice would you offer? I‚Äôm not sure if a Master‚Äôs is useful in the field, or if a PhD is important. Are there majors which would make me better equipped for the field, and how can I set myself up best to get an internship for Summer 2026? My current workplace has told me frequently that I would likely have a full-time offer waiting when I graduate if I‚Äôm interested. 



Thank you for any advice you have.",0,2025-04-25 23:03:27
"I come from a non-technical degree and self-taught background and I work for a US non-profit where I wear many hats; data engineer, Microsoft Power Platform developer, Data Analyst, and User Support. I want to move to a more specialized DE role. We currently have an on-premise SQL Server stack with a pipeline managed by SSIS packages that feed into an SSAS cube as our warehouse for reporting in Power BI reports that I also develop.

Our senior DE retired last year and I have been solely managing and trying to modernize the pipeline and warehouse since as much as I can with an on-premise setup. I pushed for a promotion and raise in the wake of that but the organization is stubborn and it was denied. I have completed the Data Talks Studio DE Zoomcamp certificate in an effort to show that I am eager to move into more cloud based data engineering despite my limited professional experience.

I need to leave this job as they are unwilling to match my responsibilities with an appropriate salary. My question to the sub is what approach should I take to my job search? Where should I be looking for jobs? What kinds of jobs should I be looking for? Should I look for bridge roles like Data Analyst or Analytics Engineer? If anyone would be willing to mentor me through this a bit, that would also be greatly appreciated.",0,2025-04-24 23:57:00
"Hello! I have an education in data analytics and a few years job experience as a data engineer in the insurance industry. I‚Äôve also been a bartender for almost a decade during school and sometimes one the weekends even when I was a data engineer. I have a passion for the service/food &bev/hospitality industry, but haven‚Äôt come across many jobs or met anyone yet in the data sphere that works in these industry. Does anyone have any insight into breaking into that industry as a data scientist? Thank you!",0,2025-04-24 23:30:22
"For someone wanting to move into a Data Engineer role (no previous experience), would the following MSc be worth it? Would it set me up in the right direction?

https://www.stir.ac.uk/courses/pg-taught/big-data-online/?utm_source=chatgpt.com#accordion-panel-16",0,2025-05-27 17:33:21
"I'm in my early 30s and I currently work as a lead data engineer at a large university. I have 9 years of work experience since finishing grad school. My bachelors and masters are both in biology related fields. Leading up to this role, I've worked as a bioinformatician and as a data analyst. My goal is perhaps in the next 10-15 years, I'd like to hit the director level at my current institition.

The university has an employee degree program. I'm looking at either an executive MBA (top 15) or a masters in information science (not sure about info sci, but top 10 for computer science).

My university covers all the tuition, but I would be on the hook for taxes for tuition over the amount of $5,250 a year. The EMBA would end up costing me tens of thousands in tax liability. I think potentially up to 50k in taxes over the 2 years. On the other hand, the masters in info sci would cost me only probably around 10k in taxes.

I feel that at this point, the EMBA be more helpful for my career than my masters in info sci would be. It seems that a lot of folks at the director level at my current institution have an MBA, but not sure if they completed the program before or after reaching the director level. Also, there's always an option of me taking CS/IS classes on the side.

I'd love to hear some thoughts!",0,2025-06-03 05:55:33
"Hey, before this gets taken down \*I have read the wiki and it did not answer my question\*

I've just signed the contract for a Data Engineering role, but it lists me as a Research and BI Analyst without any mention of data engineering. I should note I'm gonna be an intern and I have zero corporate experience so job titles are new territory for me, sorry if it's really obvious and I'm being clueless.

Is this is a type of data engineer? Have they made a mistake on the contract? Does BI stand for Business Intelligence? What do I even do???

The Analyst bit makes me quite happy because that's what I ultimately want to do in the future but I'm kind of confused as to how this is data engineering as all my other research leading up to this contract tells me Data Analysts and Data Engineers are different lol any help appreciated, thank you!",0,2025-04-02 17:36:47
"Greetings, this is my first post here. I've been working in DE for the last 5 years now doing various things with Airflow and Dagster.  I have a question regarding design of data flow from APIs to our database.

I am using Dagster/Python to perform the API pulls and loads into Snowflake.

My team lead insists that we load JSON data into our Snowflake RAW\_DATA in the following way:

ID (should be a surrogate/non-native PK)  
PAYLOAD (raw JSON payload, either as a VARCHAR or VARIANT type)  
CREATED\_DATE (timestamp this row was created in Snowflake)  
UPDATE\_DATE (timestamp this row was updated in Snowflake)

Flattening of the payload then happens in SQL as a plain View, which we currently autogenerate using Python and manually edit and add to Snowflake.

He does not want us (DE team) to use DBT to do any transforming of RAW\_DATA. DBT is only for the Data Analyst team to use for creating models.

The main advantage I see to this approach is flexibility if the JSON schema changes. You can freely append/drop/insert/reorder/rename columns. whereas a normal table you can only drop, append, and rename.

On the downside, it is slow and clunky to parse with SQL and access the data as a view. It just seems inefficient to have to recompute the view and parse all those JSON payloads whenever you want to access the table.  
  
I'd much rather do the flattening in Python, either manually or using dlt. Some JSON payloads I 'pre-flatten' in Python to make them easier to parse in SQL.

Is there a better way, or is this how you all handle this as well?",1,2025-04-08 23:56:12
"**NOTE: I do not work for Cloudflare and I have no monetary interest in Cloudflare.**

Hey guys, I just came across R2 Data Catalog and it is amazing. Basically, it allows developers to use R2 object storage (which is S3 compatible) as a data lakehouse using Apache Iceberg. It already supports Spark (scala and pyspark), Snowflake and PyIceberg. For now, we have to run the query processing engines outside Cloudflare. [https://developers.cloudflare.com/r2/data-catalog/](https://developers.cloudflare.com/r2/data-catalog/)

I find this exciting because it makes easy for beginners like me to get started with data engineering. I remember how much time I have spent while configuring EMR clusters while keeping an eye on my wallet. I found myself more concerned about my wallet rather than actually getting my hands dirty with data engineering. The whole product line focuses on actually building something and not spending endless hours in configuring the services.

Currently, Cloudflare has the following products which I think are useful for any data engineering project.

1. Cloudflare Workers: Serverless functions.[Docs](https://developers.cloudflare.com/workers/)
2. Cloudflare Workflows: Multistep applications - workflows using Cloudflare Workers.[Docs](https://developers.cloudflare.com/workflows/)
3. D1: Serverless SQL database SQLite's semantics.[Docs](https://developers.cloudflare.com/d1/)
4. R2 Object Storage: S3 compatible object storage.[Docs](https://developers.cloudflare.com/r2/)
5. R2 Data Catalog: Managed Apache Iceberg data catalog which works with Spark (Scala, PySpark), Snowflake, PyIceberg [Docs](https://developers.cloudflare.com/r2/data-catalog/)

  
I'd like your thoughts on this.",1,2025-04-27 18:54:48
"Currently, we're ""streaming"" data by having an Azure Function write event grid messages to csv in blob storage, and then by having snowpipe ingest them. There's about a million csv's generated daily. The blob is not partitioned at all.

What's the best way to ingest/delete everything? Snowpipe has a configuration error, and a portion of the data hasn't been loaded, ever. ADF was pretty slow when I tested it out.

This was all done by consultants before I was in house btw.


edit: I was a bit unclear in my message. I mean, that we've had snowpipe ingesting these files. However, now we need to re-ingest the billion or so small .csv's that are in the blob, to compare the data to the already ingested data.

What further complicates this is:

- some files have two additional columns
- we also need to parse the filename to a column
- there is absolutely no partitioning at all",1,2025-04-08 06:14:25
"Hello

  
Is it possible to insert data into Apache iceberg without initially defining it's schema, so that schema is updated after examining the stored data?",1,2025-04-21 21:10:47
"Hi, sorry this is a bit of a noob question. I have a few long time series I want to use for machine learning.

So e.g. x\_1 \~ t\_1, t\_2, ..., t\_billion

and i have just like 20 or something x

  
So intuitively I feel like it should be stored in a row oriented format since i can quickly search across the time indicies I want to use. Like I'd say I want all of the time series points at t = 20,345:20,400 to plug into ml. Instead of I want all the xs then pick out a specific index from each x.

I saw on a post around 8 months ago that parquet is the way to go. So parquet being a columnar format I thought maybe if I just transpose my series and try to save it, then it's fine.

But that made the write time go from 15 seconds (when I it's t row, and x time series) to 20+ minutes (I stopped the process after a while since I didn't know when it would end). So I'm not really sure what to do at this point. Maybe keep it as column format and keep re-reading the same rows each time? Or change to a different type of data storage?",1,2025-04-21 21:26:23
"Hello!   
  
I have a bit challenging question about how to design a datapipeline. 

I use databricks to handle the movement and transformation from schema to schema (layer). I use a raw schema where table resides with standard columns such as business\_valid\_from, business\_valid\_to, and for bi-temporality these tables also have applied\_valid\_from and applied\_valid\_to.

I am about to extract data from these raw tables into my enrichment layer where I wish to join and transform 2 or more tables into 1 table.

I only wish to extract the last changed data from the raw vault (delta load) since last extract (timestamp determined either by the max date in encrichment table or the last runtime in a metadata table). 

What I find difficult is fx if I have 2 tables (table\_a and table\_b) that I need to extract new data from. Then I need to ensure that if table\_a has a changed row from 1 week ago and table\_b does does not have changed row from 1 week ago - then I will get rows from table\_a but none from table\_b and when I join these two tables then table\_a will not get any data from table\_b (either null or no rows if I use inner join).

How can I ensure that if table\_a has updated/changed rows  from some time back then I will also could find these 'joinable' rows in table\_b even if these rows has not been updated?

(extra note on this)  
Before anyone says that I need to delta load each table separately and deterimine what business dates that will be needed for all tables - then please know I have already done that. That solution is not great because there is always some row that has been updated, and that row has a business\_valid\_from long ago fx 2012. This would result in a long list of business days that will be needed for all table - and then it defeats the purpose of the delta load.

Thanks!",1,2025-04-25 10:49:09
"I currently have a Parquet file with 193 million rows and 39 columns. I‚Äôm trying to upload it into an Iceberg table stored in S3.

Right now, I‚Äôm using Python with the pyiceberg package and appending the data in batches of 100,000 rows. However, this approach doesn‚Äôt seem optimal‚Äîit‚Äôs taking quite a bit of time.

I‚Äôd love to hear how others are handling this. What‚Äôs the most efficient method you‚Äôve found for uploading large Parquet files or DataFrames into Iceberg tables in S3?",1,2025-04-21 15:13:08
"Hey, r/dataengineering 

I‚Äôm working on a project where I need to move data from our cloud-hosted databases into Snowflake, and I‚Äôm trying to figure out the best tool for the job. Ideally, I‚Äôd like something that‚Äôs cost-effective and scales well. 

If you‚Äôve done this before, what did you use?
Would love to hear about your experience‚Äîhow reliable it is, how much it roughly costs, and any pros/cons you‚Äôve noticed. Appreciate any insights!

[View Poll](https://www.reddit.com/poll/1jr70yg)",1,2025-04-04 08:30:40
"I receive various files at different intervals which are not defined. Can be every seconds, hour, daily, etc.

I don‚Äôt have any indication also of when something is finished. For example, it‚Äôs highly possible to have 100 files that would end up being 100% of my daily table, but I receive them scattered over 15min-30 when the data become available and my ingestion process ingest it. Can be 1 to 12 hours after the day is over.

Not that‚Äôs it‚Äôs also possible to have 10000 very small files per day.

I‚Äôm wondering how is this solves with Iceberg tables. Very newbie Iceberg guy here. Like I don‚Äôt see throughput write benchmark anywhere but I figure that rewriting the metadata files must be a big overhead if there‚Äôs a very large amount of files so inserting every times there‚Äôs a new one must not be the ideal solution.

I‚Äôve read some medium post saying that there was a snapshot feature which track new files so you don‚Äôt have to do some fancy things to load them incrementally. But again if every insert is a query that change the metadata files it must be bad at some point.

Do you wait and usually build a process to store a list of files before inserting them or is this a feature build somewhere already in a doc I can‚Äôt find ?

Any help would be appreciated.

",1,2025-04-12 23:44:32
"I've been playing around with duckdb + iceberg recently and I think it's got a huge amount of promise. Thought I'd do a short blog about it. 

Happy to awnser any questions on the topic! ",1,2025-05-28 10:37:15
"We are an AWS and Databricks shop. We want to explore open source engines for cost savings and reduce vendor lock. 

We want to introduce iceberg. This interoperability with Flink, Snowflake, Trino. 

We are considering Glue,  Snowflake-version-of-Polaris or another catalog.

I appreciate any recommendations and experices from this group.

  
Databricks unity-uniform enables reading the data as a iceberg table but we cannot write a table using Flink. We use Trino and Snowflake for reads.

",1,2025-04-02 19:59:54
"Exciting news, a new blog post about Snowflake architecture. Dive in and explore all the amazing features! 

https://medium.com/@adityasharmah27/understanding-snowflake-architecture-a-beginners-guide-to-cloud-data-warehousing-22a6f4e3a6be?sk=40c0128a3f07d30ba0cd92ab710112ae",1,2025-04-12 06:38:55
"I have a iceberg table which is partitioned by truncate(10, requestedtime).

requestedtime column(partition column) is basically string data type in a datetime format like this: 2025-05-30T19:33:43.193660573. and I want the dataset to be partitioned like ""2025-05-30"", ""2025-06-01"", so I created table with this query CREATE TABLE table (...) PARTITIONED BY truncate(10, requestedtime)

In S3,  the iceberg table technically is partitioned by

requestedtime\_trunc=2025-05-30/

requestedtime\_trunc=2025-05-31/

requestedtime\_trunc=2025-06-01/

Here's a problem I have.

When I try below query from spark engine,

""SELECT count(\*) FROM table WHERE substr(requestedtime,1,10) = '2025-05-30'""

The spark engine look through whole dataset, not a requested partition (requestedtime\_trunc=2025-05-30).

What SELECT query would be appropriate to only look through selected partition?

p.s) In AWS Athena,  the query ""SELECT count(\*) FROM table WHERE substr(requestedtime,1,10) = '2025-05-30'"" worked fine and used only requested partition data.",1,2025-06-09 06:23:49
"Curious if any data teams are using¬†**Snowflake as a tracking layer for Airflow DAG/task statuses**, and then visualizing that in¬†**Grafana**?

We‚Äôre exploring a setup where:

* Airflow task-level or DAG-level statuses (success/failure/timing) are written to a¬†**Snowflake table**¬†using custom callbacks or logging tasks
* Grafana dashboards are built directly over Snowflake to monitor job health, trends, and SLAs

Has anyone done something similar?

* How‚Äôs the performance and cost of Snowflake for frequent inserts?
* Any tips for schema design or batching strategies?
* Would love to hear what worked, what didn‚Äôt, and whether you moved away from this approach.

Thanks in advance!",1,2025-05-26 16:13:08
"Hello everyone,

Our infra setup for CDC looks like this:

MySQL > Debezium connectors > Kafka > Sink (built in house > BigQuery 

Recently I came across Debezium server iceberg:  [https://github.com/memiiso/debezium-server-iceberg/tree/master](https://github.com/memiiso/debezium-server-iceberg/tree/master), and it looks promising as it cuts the Kafka part and it ingests the data directly to Iceberg. 

My problem is to use Iceberg in GCS. I know that there is the BigLake metastore that can be used, which i tested with BigQuery and it works fine. The issue I'm facing is to properly configure the BigLake metastore in my application.properties. 

In Iceberg [documentation](https://iceberg.apache.org/docs/nightly/kafka-connect/#google-gcs-configuration-example) they are showing something like this:

    ""iceberg.catalog.type"": ""rest"",
    ""iceberg.catalog.uri"": ""https://catalog:8181"",
    ""iceberg.catalog.warehouse"": ""gs://bucket-name/warehouse"",
    ""iceberg.catalog.io-impl"": ""org.apache.iceberg.google.gcs.GCSFileIO""

But I'm not sure if BigLake has exposed REST APIs? I tried to use the REST point that i used for creating the catalog 

    https://biglake.googleapis.com/v1/projects/sproject/locations/mylocation/catalogs/mycatalog

But it seems not working. Has anyone succeeded in implementing a similar setup? ",1,2025-04-16 16:32:20
"Hi, I am not sure if I can ask this so please let me know if it is not right to do so.

I am currently working on setting up Trino to query data stored in Hadoop (+Hive Metastore) to eventually query data to BI tools. Lets say my current data is currently stored in as /meters name/sub-meters name/multiple time-series.parquet:

\`\`\`

/meters/

meter1/

meter1a/

part-\*.parquet

meter1b/

part-\*.parquet

meter2/

meter2a/

part-\*.parquet

...

\`\`\`

Each sub-meter has different columns (mixed data types) to each one another.  and there are around 20 sub-meters

I can think of 2 ways to set up schemas in hive metastore:

\- create multiple tables for each meter + add partitions by year-month-day (optional). Create views to combine tables to query data from and manually add meter names as a new column.

\- Use long format and create general partitions such as meter/sub-meters:

|timestamp|meter|sub\_meter|metric\_name|metric\_value (DOUBLE)|metric\_text (STRING)|
|:-|:-|:-|:-|:-|:-|
|2024-01-01 00:00:00|meter1|meter1a|voltage|220.5|NULL|
|2024-01-01 00:00:00|meter1|meter1a|status|NULL|""OK""|

The second one seems more practical but I am not sure if it is a proper way to store data. Any advice? Thank you!",1,2025-05-27 20:35:06
"I'm fairly new to the idea of ETL even though I've read about and followed it for years; however, the implementation is what I have a question about.

Our needs have migrated towards the idea of Spark so I'm thinking of building our pipeline in Scala.  I've used it on and off in the past so it's not a foreign language for me.

However, the question I have is should I build our workflow and hard code it from A-Z (data ingestion, create or replace, populate tables) outside of snowflake, or is it better practice to have it fragmented and saved as snowflake worksheets?  My aim with this change would be strongly typed services that can't be ""accidentally"" fired off.

I'm thinking the pipeline would be more of a spot instance that is fired off with certain configs with the A-Z only allowed for certain logins.  There aren't many people on the team but there are people working with tables that have drop permissions (not from me) and I just want to be prepared for disasters and recovery.

It's like a mini-dream whereas I'm in full control of the data and ingestion pipelines but everything is sql currently.  Therefore, we are building from scratch right now and the Scala system would mainly be a disaster recovery so made to repopulate tables, or to ingest a new set of raw data to be transformed and loaded (updates).

This is a non-profit so I don't want to load them up with huge bills (databricks) so I do want to do most of the stuff myself with the help of apache.  I understand there are numerous options but essentially it's going to be like this

Scala server -> Apache Spark -> ML Categorization From Spark -> Snowflake

Since we are ingesting data I figured we should mix in the machine learning while transforming and processing to save on time and headaches.

WHY I DIDN'T CHOOSE SNOWPARK:  
After looking over snowpark I see it as a great gateway for people either needing pure speed, or those who are newer to software engineering and needing a box to be in.  I'm well-versed in pandas, numpy, etc. so I wanted to be able to break the mold at any point.  I know this may not be preferable for snowflake people but I have about a decade of experience writing complex software systems, and I didn't want vendor lock-in so I hope that can be respected to some extent.  If I am blatantly wrong then please let me know how snowpark is better.  

  
Note: I do see snowpark offers Scala (or something like that); however, the point isn't solely to use Scala, I come from Golang and want a sturdy pipeline that won't run into breaking changes and make it a JVM shop.

Any other advice from engineers here on other things I should recommend would be greatly appreciated as well.  Scraping is a huge concern, which is why I chose Golang off the bat, but scraping new data can't objectively be the main priority, I feel like there are other things that I might be unaware of.  Maybe a checklist of things that I can make sure we have just so we don't run into major issues then I catch the blame shift.

Therefore, please be gentle I am not the most well-versed in data engineering but I do see it as a fascinating discipline that I'd like to find a niche in if possible.",1,2025-04-18 04:51:51
"Hey everyone. 
I am working on a project to convert a very large dumps of files (csv,dat,etc) and want to convert these files to parquet format. 

There are 45 million files.
Data size of the files range from 
1kb to 83gb. 
41 million of these files are < 3mb. 
I am exploring tools and technologies to use to do this conversion. 
I see that i would require 2 solutions. 1 for high volume low memory files. Other for bigger files",1,2025-04-30 07:05:47
"Hi
My team need to sync data on a huge tables and huge amount of tables from snowflake to pg on some trigger (we are using temporal), 
We looked on CDC stuff but we think this overkill.
Can someone advise on some tool? 
",1,2025-04-21 13:50:44
"Hey guyz , I want to sync data across dbs , I have code that can transfer about 300k rows in 18secs , so speed is not a issue . Issue is how to find out what to transfer in other terms what got changed 

For specific we are using azure sql server 19 

There are two tables 
Table A
Table B

Table B is replicate of Table A . We process data in Table A and need to send the data back to Table B 

The tables will have 1 million rows each 

And about 1000 rows will get changed per etl .

One of the approach was to generate hashes but even if u generate hashes 

You will still compare 1 million hashes to 1 million hashes making it O(N)

This there better way to do this ",1,2025-05-27 05:36:10
"What would this data pipeline look like? The total data size is 5TB on postgres and it is for a typical SaaS B2B2C product

Here is what the part of the data pipeline looks like

1. Source DB: Postgres running on RDS
2. AWS Database migration service -> Streams parquet into a s3 bucket
3. We have also exported the full db data into a different s3 bucket - this time almost matches the CDC start time

What we need on the other end is a good cost effective data lake to do analytics and reporting on - as real time as possible

I tried to set something up with pyiceberg to go iceberg -

\- Iceberg tables mirror the schema of posgtres tables

\- Each table is partitioned by account\_id and created\_date

I was able to load the full data easily but handling the CDC data is a challenge as the updates are damn slow. It feels impractical now - I am not sure if I should just append data to iceberg and get the latest row version by some other technique?

how is this typically done? Copy on write or merge on read?

What other ways of doing something like this exist that can work with 5TB data with 100GB data changes every day?",1,2025-04-20 17:31:13
"_Context: I'm a professional software engineer, but mostly self-taught in the world of data engineering. So there are probably things I don't know that I don't know! I've been doing this for about 8 years but only recently learned about DBT and SQLMesh, for example._

I'm working on an ELT pipeline that converts input files of various formats into Parquet files on Google Cloud Storage, which subsequently need to be loaded into BigQuery tables (append-only).

- The Extract processes drop files into GCS at unspecified times.

- The Transform processes convert _newly created_ files to Parquet and drops the result back into GCS.

- The Load process needs to load the _newly created_ files into BigQuery, making sure to load every file exactly once.

To process only new (or failed) files, I guess there are two main approaches:

1. Query the output, see what's missing, then process that. Seems simple, but has scalability limitations because you need to list the entire history. Would need to query both GCS and BQ to compare what files are still missing.

2. Have some external system or work queue that keeps track of incomplete work. Scales better, but has the potential to go out of sync with reality (e.g. if Extract fails to write to the work queue, the file is never transformed or loaded).

I suppose this is a common problem that everyone has solved already. What are the best practices around this? Is there any (ideally FOSS) tooling that could help me?",1,2025-05-26 09:36:25
"Hello all, I am trying to implement dbt and snowflake on a personal project, most of my experience comes from databricks so I would like to know if the best approach for this would be to:
1- a server dedicated to dbt that will connect to snowflake and execute transformations.
2- snowflake of course deployed in azure .
3- azure data factory for raw ingestion and to schedule the transformation pipeline and future dbt dataquality pipelines.

What you guys think about this? ",1,2025-04-02 15:54:12
"Hi everyone,

I work in a small ad tech company, I have events coming as impression, click, conversion. 

We have an aggregated table which is used for user-facing reporting.

Right now, the data stream is like Kafka topic -> Hive parquet table -> a SQL server 

So we have click, conversion, and the aggregated table on SQL server

The data size per day on sql server is \~ 2 GB for aggregated,  \~2 GB for clicks, and 500mb for conversion.   
  
Impression being too large is not stored in SQL Server, its stored on Hive parquet table only.

Requirements - 

1.  We frequently update conversion and click data. Hence, we keep updating aggregated data as well.

2. New column addition is frequent( once a month). Currently, this requires changes in lots of Hive QL and SQL procedures

  

My question is, I want to move all these stats tables away from SQL server.  Please suggest where can we move where updating of data is possible.



Daily row count of tables -   
aggregated table \~ 20 mil  
impression \~ 20 mil ( stored in Hive parquet only)  
click \~ 2 mil  
conversion \~ 200k",1,2025-05-02 19:30:55
I am trying to see which one is better iceberg or hudi in AWS environment. Any suggestions for handling peta byte scale data ?,1,2025-05-28 06:57:30
"I‚Äôve been experimenting with data formats like Parquet and Iceberg, and recently came across [Lance](). I wanted to try building something around it.

So I put together a simple Digital Asset Manager (DAM) where:

* Images are uploaded and vectorized using CLIP
* Vectors are stored in Lance format directly on Cloudflare R2
* Search is done via Lance, comparing natural language queries to image vectors
* The whole thing runs on [Fly.io](http://Fly.io) across three small FastAPI apps (upload, search, frontend)

No Postgres or Mongo. No AI, Just object storage and files.

You can try it here: [https://metabare.com/](https://metabare.com/)  
Code: [https://github.com/gordonmurray/metabare.com](https://github.com/gordonmurray/metabare.com)

Would love feedback or ideas on where to take it next ‚Äî I‚Äôm planning to add image tracking and store that usage data in Parquet or Iceberg on R2 as well.",1,2025-05-25 19:10:01
"So i have this project that will need to ingest data from SAP HANA into snowflake, it can be considered as any on-premise DB using JBDC, the big issue is, I cannot use any external ETL services as per project requirements. What is the best path to follow?  


I need to fetch the data in bulk for some tables with truncate / copy into, and some tables need to be incremental with little (10 min) delay. The tables do not contain any watermark, modified time or anything...  


There isnt much data, 20M rows tops.

If you guys can give me a hand, i'm new to snowflake and strugling to find any sources on this.",1,2025-04-01 11:15:13
"I've been using Snowflake for a while for just data warehousing projects (analytics) where I update the data twice per day.

  
I have now a Use Case where I need to do some reads and writes to sql tables every hour (every 10 min would be even better but not necessary). **The purpose is not only analytics but also operational.**

  
I estimate every request **costs me 0.01$,** which is quite high.

I was thinking of using **Postgresql** instead of Snowflake but I will need to invest time and resources to build it and maintain it. 

  
I was wondering if you can give me **your opinion** about building **near real time or hourly projects** in Snowflake. Does it make sense ? or is it a clear no-go ?

  
Thanks!

  
",1,2025-03-31 14:57:29
"I'm in the middle of choosing a dataframe framework to communicate with my cloud database. The setup is that we have to use python and snowflake. I'm not sure about what to use snowpark or ibis.  
  
**ibis**  
Ibis definitely has the advantage of choosing more than 20 backends. In the case of a migration that would become handy.  
The local testing capabilities are to be found out. If I would set up a local duck db I could test locally, with the same behaviour in duckdb and snowflake. The down sites are that I would have another dependency (ibis) and most probably not all features are implemented that snowflake provides. f.e UDTF.

**snowflake**  
The worst/clostest coupling to snowflake. I have no option to choose a backend but I have all the capabilites and if I dont snowflakes customer support would most likely help me.

If I dont need the capability of multiple backends, it is an unnessesary abstraction layer

What are your thoughts?",1,2025-06-03 19:34:01
"Hi, once the new Cortex Multimodal possibility came out, I realized that I can finally create the Not-A-Hot-Dog -app using purely Snowflake tools.

The code is only 30 lines and needs only SQL statements to create the STAGE to store images taken my Streamlit camera -app: ->

[https://www.recordlydata.com/blog/not-a-hot-dog-in-snowflake](https://www.recordlydata.com/blog/not-a-hot-dog-in-snowflake)",1,2025-04-25 07:39:15
"We at OLake (Fast database to Apache Iceberg replication,¬†[**open-source**](https://github.com/datazip-inc/olake)) will soon support  Iceberg‚Äôs Hidden Partitioning and wider catalog support hence we are organising our 6th community call.

**What to expect in the call:**

1. Sync Data from a Database into Apache Iceberg using one of the following catalogs (REST, Hive, Glue, JDBC)
2. Explore how Iceberg Partitioning will play out here \[new feature\]
3. Query the data using a popular lakehouse query tool.

**When**:

* **Date**: 28th April (Monday) 2025 at 16:30 IST (04:30 PM).
* **RSVP here**¬†\-¬†[https://lu.ma/s2tr10oz](https://lu.ma/s2tr10oz)¬†\[make sure to add to your calendars\]",1,2025-04-22 06:30:30
"I'm evaluating ways to load data into Iceberg tables and trying to wrap my head around the ecosystem.

Are people using Spark, Flink, Trino, or something else entirely?

Ideally looking for something that can handle CDC from databases (e.g., Postgres or SQL Server) and write into Iceberg efficiently. Bonus if it's not super complex to set up.

Curious what folks here are using and what the tradeoffs are.",1,2025-04-21 14:01:35
"So, I'm currently working on a project (my first) to create a scalable data platform for a company. The whole thing structured around AWS, initially using DMS to migrate PostgreSQL data to S3 in parquet format (this is our raw datalake). Then using Glue jobs to read this data and create Iceberg tables which would be used in Athena queries and Quicksight. I've got a working Glue script for reading this data and perform upsert operations. Okay so now that I've given a bit of context of what I'm trying to do, let me tell you my problem.  
The client wants me to schedule this job to run every 15min or so for staging and most probably every hour for production. The data in the raw datalake is partitioned by date (for example: s3bucket/table\_name/2025/04/10/file.parquet). Now that I have to run this job every 15 min or so I'm not sure how to keep track of the files that have been processed and which haven't. Currently my script finds the current time and modifies the read command to use just the folder for the current date. But still, this means that I'll be reading all the files in the folder (processed already or not) every time the job runs during the day.   
I've looked around and found that using DynamoDB for keeping track of the files would be my best option but also found something related to Iceberg metadata files that could help me with this. I'm leaning towards the Iceberg option as I wanna make use of all its features but have too little information regarding this to implement. would absolutely appreciate it if someone could help me out with this.  
Has anyone worked with Iceberg in this matter? and if the iceberg solution isn't usable, could someone help me out with how to implement the DynamoDB way.",1,2025-04-14 12:15:20
"Noob questions incoming!

**Context:**   
I'm designing my project's storage and data pipelines, but am new to data engineering. I'm trying to understand the ins and outs of various solutions for the task of reading/writing diverse types of very large data.   
  
From a theoretical standpoint, I understand that Iceberg is a standard for organizing metadata about files. Metadata organized to the Iceberg standard allows for the creation of ""Iceberg tables"" that can be queried with a familiar SQL-like syntax.

I'm trying to understand how this would fit into a real world scenario... For example, lets say I use object storage, and there are a bunch of pre-existing parquet files and maybe some images in there. Could be anything...

**Question 1:**  
How is the metadata/tables initially generated for all this existing data? I know AWS has the Glue Crawler. Is something like that used? 

Or do you have to manually create the tables, and then somehow point the tables to the correct parquet files that contain the data associated with that table?

**Question 2:**  
Okay, now assume I have object storage and metadata/tables all generated for files in storage. Someone comes along and drops a new parquet file into some bucket. I'm assuming that I would need some orchestration utility that is monitoring my storage and kicking off some script to add the new data to the appropriate tables? Or is it done some other way?

**Question 3:**   
I assume that there are query engines out there that are implemented to the Iceberg standard for creating and reading Iceberg metadata/tables, and fetching data based on those tables. For example, I've read that SparkQL and Trino have Iceberg ""connectors"". So essentially the power of Iceberg can't be leveraged if your tech stack doesn't implement compliant readers/writers? How prolific are Iceberg compatible query engines?

",1,2025-04-22 20:21:49
"I currently use teams events where I set a day on my calendar to update keys, but there has to be a better way. How do you guys do it?

Edit: The idea is to renew keys before they expire and there are no errors in the pipelines",1,2025-04-08 12:26:25
"Hi I was wondering if anyone here has used Airbyte to push CDC changes from DynamoDb to Snowflake.  If so what was your experience, what was the size of your tables and did you have any latency issues.",1,2025-06-04 02:46:08
"Hi Reddit community! This is my first Reddit post and I‚Äôm hoping I could get some help with this task I‚Äôm stuck with please!

I read a parquet file and store it in an arrow table. I want to read a parquet complex/nested column and convert it into a JSON object. I use C++ so I‚Äôm searching for libraries/tools preferably in C++ but if not, then I can try to integrate it with rust. 
What I want to do:
Say there is a parquet column in my file of type (arbitrary, just to showcase complexity): List(Struct(List(Struct(int,string,List(Struct(int, bool)))), bool))
I want to process this into a JSON object (or a json formatted string, then I can convert that into a json object). I do not want to flatten it out for my current use case. 

What I have found so far:
1. Parquet's inbuilt toString functions don‚Äôt really work with structs (they‚Äôre just good for debugging)
2. haven‚Äôt found anything in C++ that would do this without me having to writing a custom recursive logic, even with rapidjson
3. tried Polars with Rust but didn‚Äôt get a Json yet. 

I know I can get write my custom logic to create a json formatted string, but there must be some existing libraries that do this? I've been asked to not write a custom code because they're difficult to maintain and easy to break :)

Appreciate any help!",1,2025-04-15 02:36:14
"Hey, I think there are better use cases for event sourcing.  
  
Event sourcing is an architecture where you capture every change in your system as an immutable event, rather than just storing the latest state. Instead of only knowing what your data looks like now, you keep a full history of how it got there. In a simple crud app that would mean that every deleted, updated, and created entry is stored in your event source, that way when you replay your events you can recreate the state that the application was in at any given time.

Most developers see event sourcing as a kind of technical safety net: - Recovering from failures - Rebuilding corrupted read models - Auditability

Surviving schema changes without too much pain

And fair enough, replaying your event stream often feels like a stressful situation. Something broke, you need to fix it, and you‚Äôre crossing your fingers hoping everything rebuilds cleanly.

What if replaying your event history wasn‚Äôt just for emergencies? What if it was a normal, everyday part of building your system?

Instead of treating replay as a recovery mechanism, you treat it as a development tool ‚Äî something you use to evolve your data models, improve your logic, and shape new views of your data over time. More excitingly, it means you can derive entirely new schemas from your event history whenever your needs change.

Your database stops being the single source of truth and instead becomes what it was always meant to be: a fast, convenient cache for your data, not the place where all your logic and assumptions are locked in.

With a full event history, you‚Äôre free to experiment with new read models, adapt your data structures without fear, and shape your data exactly to fit new purposes ‚Äî like enriching fields, backfilling values, or building dedicated models for AI consumption. Replay becomes not about fixing what broke, but about continuously improving what you‚Äôve built.

And this has big implications ‚Äî especially when it comes to AI and MCP Servers.

Most application databases aren‚Äôt built for natural language querying or AI-powered insights. Their schemas are designed for transactions, not for understanding. Data is spread across normalized tables, with relationships and assumptions baked deeply into the structure.

But when you treat your event history as the source of truth, you can replay your events into purpose-built read models, specifically structured for AI consumption.

Need flat, denormalized tables for efficient semantic search? Done. Want to create a user-centric view with pre-joined context for better prompts? Easy. You‚Äôre no longer limited by your application‚Äôs schema ‚Äî you shape your data to fit exactly how your AI needs to consume it.

And here‚Äôs where it gets really interesting: AI itself can help you explore your data history and discover what‚Äôs valuable.

Instead of guessing which fields to include, you can use AI to interrogate your raw events, spot gaps, surface patterns, and guide you in designing smarter read models. It‚Äôs a feedback loop: your AI doesn‚Äôt just query your data ‚Äî it helps you shape it.

So instead of forcing your AI to wrestle with your transactional tables, you give it clean, dedicated models optimized for discovery, reasoning, and insight.

And the best part? You can keep iterating. As your AI use cases evolve, you simply adjust your flows and replay your events to reshape your models ‚Äî no migrations, no backfills, no re-engineering.",1,2025-04-14 10:15:58
"Hi everyone,
Once per day, my team needs to mirror a lot of tables from snowflake to postgres. 
Currently, we are copying data with script written with GO.
do you familiar with tools, or any idea what is the best way to mirror the tables?",1,2025-04-08 16:38:21
"I receive various files at different intervals which are not defined. Can be every seconds, hour, daily, etc.

I don‚Äôt have any indication also of when something is finished. For example, it‚Äôs highly possible to have 100 files that would end up being 100% of my daily table, but I receive them scattered over 15min-30 when the data become available and my ingestion process ingest it. Can be 1 to 12 hours after the day is over.

Not that‚Äôs it‚Äôs also possible to have 10000 very small files per day.

I‚Äôm wondering how is this solves with Iceberg tables. Very newbie Iceberg guy here. Like I don‚Äôt see throughput write benchmark anywhere but I figure that rewriting the metadata files must be a big overhead if there‚Äôs a very large amount of files so inserting every times there‚Äôs a new one must not be the ideal solution.

I‚Äôve read some medium post saying that there was a snapshot feature which track new files so you don‚Äôt have to do some fancy things to load them incrementally. But again if every insert is a query that change the metadata files it must be bad at some point.

Do you wait and usually build a process to store a list of files before inserting them or is this a feature build somewhere already in a doc I can‚Äôt find ?

Any help would be appreciated.

",1,2025-04-12 23:44:32
"Super basic flow description - We have Kafka writing parquet files to S3 which is our Apache Iceberg data layer supporting various tables containing the corresponding event data. We then have periodically run ETL jobs that create other Iceberg tables (based off of the ""upstream"" tables) that support analytics, visualization, etc.

These jobs run a `CREATE OR REPLACE <table_name>` sql statement, so full table refresh each time. We'd like to be able to also support some type of change data capture technique to avoid always dropping/creating tables and the cost and time associated with that.  Simply capturing new/modified records would be an acceptable start. Can anyone suggest how we can approach this. This is kinda new territory for our team. Thanks.",1,2025-06-04 23:12:38
"I've worked with Snowflake for a while and understood that storage was separated from compute. In my head that makes sense but practically speaking realized I didn't know how a query is processed and data is loaded from storage onto a DW. Is there anything special going on? 

For example, let's say I have a table employees without any partitioning and run a basic query of `select department, count(*) from employees where start_date > '2020-01-01'` and using a Large data warehouse. Can someone explain what happens after I hit run on the query until I see the results?",1,2025-04-24 15:03:28
"We‚Äôve been working on optimizing how we store distributed traces in Parseable using Apache Parquet. Columnar formats like Parquet make a huge difference for performance when you‚Äôre dealing with billions of events in large systems. Check out how we efficiently manage trace data and leverage smart caching for faster, more flexible queries.

[https://www.parseable.com/blog/opentelemetry-traces-to-parquet-the-good-and-the-good](https://www.parseable.com/blog/opentelemetry-traces-to-parquet-the-good-and-the-good)",1,2025-04-28 17:09:47
"Hi I'm the author of [Icebird](https://github.com/hyparam/icebird) and [Hyparquet](https://github.com/hyparam/hyparquet) which are new open-source implementations of Iceberg and Parquet written entirely in JavaScript.

Why re-write Parquet and Iceberg in javascript? Because it enables building data applications in the browser with a drastically simplified stack. Usually accessing iceberg requires a backend, often with full spark processing, or paying for cloud based OLAP. Icebird allows the browser to directly fetch Iceberg tables from S3 storage, without the need for backend servers.

I am excited about the new kinds of data applications than can be built with modern data formats, and bringing them to the browser with hyparquet and icebird. Building these libraries has been a labor-of-love -- I hope they can benefit the data engineering community. Let me know your thoughts!",1,2025-04-24 19:02:18
"We are getting data from different systems to lake using fabric pipelines and then we are copying the successful tables to warehouse and doing some validations.we are doing full loads from source to lake and lake to warehouse right now. Our source does not have timestamp or cdc , we cannot make any modifications on source. We want to get only upsert data to warehouse from lake, looking for some suggestions.

",1,2025-05-02 11:47:16
"Hi- our Snowflake cost is super high. Around ~600k/year. We are using DBT core for transformation and some long running queries and batch jobs. Assuming these are shooting up our cost! 

What should I do to start lowering our cost for SF? ",1,2025-06-12 14:14:16
"I am trying to capture change in data in a table, and trying to perform scd type 1  via upserts.

But it seems that vanilla parquet does not supports upserts, hence need help in how we can achieve to capture only when there‚Äôs a change in the data

Currently the source table runs daily with full load and has only one date column which has one distinct value of the last run date of the job.

Any idea what is a way around?",1,2025-04-22 18:11:23
"I recently joined a new team that maintains an existing AWS Glue to Snowflake pipeline, and building another one.

The pattern that's been chosen is to use tasks that kick off stored procedures. There are some tasks that update Snowflake tables by running a SQL statement, and there are other tasks that updates those tasks whenever the SQL statement need to change. These changes are usually adding a new column/table and reading data in from a stream.

After a few months of working with this and testing, it seems clunky to use tasks like this. More I read, tasks should be used for more static infrequent changes. The clunky part is having to suspend the root task, update the child task and make sure the updated version is used when it runs, otherwise it wouldn't insert the new schema changes, and so on etc.

Is this the normal established pattern, or are there better ones?

I thought about maybe, instead of using tasks for the SQL, use a Snowflake table to store the SQL string? That would reduce the number of tasks, and avoid having to suspend/restart.",1,2025-04-22 17:31:31
"Lakehouse Data Processing with AWS Lambda, DuckDB, and Iceberg

In this exploration, we aim to demonstrate the feasibility of creating a lightweight data processing pipeline for a Lake House using AWS Lambda, DuckDB, and Cloudflare‚Äôs R2 Iceberg. Here‚Äôs a step-by-step guide read more


Columnar storage is a data organization method that stores data by columns rather than rows, optimizing for analytical queries. This approach allows for more efficient compression and faster processing of large datasets. Two popular columnar storage formats are Apache Parquet and Apache Avro.

https://www.huddleandgo.work/de#what-is-columnar-storage

",1,2025-05-27 03:04:50
"Wrote a blog post based on my experiences working with high-cardinality telemetry data and the challenges it poses for storage and query performance.

The post dives into how using **Apache Parquet** and a **columnar-first design** helps mitigate these issues, by isolating cardinality per column, enabling better compression, selective scans, and avoiding the combinatorial blow-up seen in time-series or row-based systems.

It includes some complexity analysis and practical examples. Thought it might be helpful for anyone dealing with observability pipelines, log analytics, or large-scale event data.

üëâ [https://www.parseable.com/blog/high-cardinality-meets-columnar-time-series-system](https://www.parseable.com/blog/high-cardinality-meets-columnar-time-series-system)",1,2025-04-17 18:30:00
"Hi I was wondering if anyone here has used Airbyte to push CDC changes from DynamoDb to Snowflake.  If so what was your experience, what was the size of your tables and did you have any latency issues.",1,2025-06-04 02:46:08
"Hey everyone, I was doing a POC with Delta tables for a real-time data pipeline and started doubting if Delta even is a good fit for high-volume, real-time data ingestion. 

Here‚Äôs the scenario: 
- We're consuming data from multiple Kafka topics (about 5), each representing a different stage in an event lifecycle. 

- Data is ingested every 60 seconds with small micro-batches. (we cannot tweak the micro batch frequency much as near real-time data is a requirement)

- We‚Äôre using Delta tables to store and upsert the data based on unique keys, and we‚Äôve partitioned the table by date. 


While Delta provides great features like ACID transactions, schema enforcement, and time travel, I‚Äôm running into issues with table bloat. Despite only having a few days‚Äô worth of data, the table size is growing rapidly, and optimization commands aren‚Äôt having the expected effect. 

From what I‚Äôve read, Delta can handle real-time data well, but there are some challenges that I'm facing in particular:
- File fragmentation: Delta writes new files every time there‚Äôs a change, which is result in many  files and inefficient storage (around 100-110 files per partition - table partitioned by date). 

- Frequent Upserts: In this real-time system where data is constantly updated, Delta is ending up rewriting large portions of the table at high frequency, leading to excessive disk usage. 

- Performance: For very high-frequency writes, the merge process is becoming slow, and the table size is growing quickly without proper maintenance. 

To give some facts on the POC: The realtime data ingestion to delta ran for 24 hours full, the physical accumulated was 390 GB, the count of rows was 110 million.

The main outcome of this POC for me was that there's a ton of storage overhead as the data size stacks up extremely fast!

For reference, the overall objective for this setup is to be able to perform near real time analytics on this data and use the data for ML.

Has anyone here worked with Delta tables for high-volume, real-time data pipelines? Would love to hear your thoughts on whether they‚Äôre a good fit for such a scenario or not. ",1,2025-04-23 18:24:15
"Hey everyone. 
I am working on a project to convert a very large dumps of files (csv,dat,etc) and want to convert these files to parquet format. 

There are 45 million files.
Data size of the files range from 
1kb to 83gb. 
41 million of these files are < 3mb. 
I am exploring tools and technologies to use to do this conversion. 
I see that i would require 2 solutions. 1 for high volume low memory files. Other for bigger files",1,2025-04-30 07:05:47
"I have data that is stored bitemporally, with system start/end fields. Is there a way to migrate this to an iceberg table where the iceberg time travel functionality can be populated with the actual system times backdated? This way the time travel functionality will be useful, instead of all of the data being reflected at the migration date. ",1,2025-05-01 15:34:47
"Hey folks,

I'm building a tool to convert between Parquet and other formats (CSV, JSON, etc.).¬† You can see it here: [https://dataconverter.io/tools/parquet](https://dataconverter.io/tools/parquet)

Progress has been very good so far.¬† The question now is how far into complex Parquet types to go ‚Äì given than many of the target formats don't have an equivalent type.

How often do you come across Parquet files with complex or nested structures?¬† And what are you mostly seeing?

I'd appreciate any insight you can share.",1,2025-04-15 22:53:16
"TLDR; My company wants to replace our pipelines with some all-in-one ‚ÄúAI agent‚Äù platform

I‚Äôm a lone data engineer in a mid-size retail/logistics company that runs SAP ERP (moving to HANA soon). Historically, every department pulled SAP data into Excel, calculated things manually, and got conflicting numbers. I was hired into a small analytics unit to centralize this. I‚Äôve automated data pulls from SAP exports, APIs, scrapers, and built pipelines into SQL Server. It‚Äôs traceable, consistent, and used regularly.

Now, our new CEO wants to ‚Äúcentralize everything‚Äù and ‚Äúgo AI-driven‚Äù by bringing in a no-name platform that offers:

\- Limited source connectors for a basic data lake/warehouse setup

\- A simple SQL interface + visualization tools

\- And the worst of it all: an AI agent PER DEPARTMENT

Each department will have its own AI ‚Äúinstance‚Äù with manually provided business context. Example: ‚ÄúThis is how finance defines tenure,‚Äù or ‚ÄúSales counts revenue like this.‚Äù Then managers are supposed to just ask the AI for a metric, and it will generate SQL and return the result. Supposedly, this will replace 95‚Äì97% of reporting, instantly (and the CTO/CEO believe it).

Obviously, I‚Äôm extremely skeptical:

\- Even with perfect prompts and context, if the underlying data is inconsistent (e.g. rehire dates in free text, missing fields, label mismatches), the AI will silently get it wrong.

\- There‚Äôs no way to audit mistakes, so if a number looks off, it‚Äôs unclear who‚Äôs accountable. If a manager believes it, it may go unchallenged.

\- The answer to every flaw from them is: ‚Äúthe context was insufficient‚Äù or ‚Äúyou didn‚Äôt prompt it right.‚Äù That‚Äôs not sustainable or realistic

\- Also some people (probs including me) will have to manage and maintain all the departmental context logic, deal with messy results, and take the blame when AI gets it wrong.

\- Meanwhile, we already have a working, auditable, centralized system that could scale better with a real warehouse and a few more hires. They just don't want to hire a team or I have to convince them somehow (bc they think that this is a cheaper, more efficient alternative).

I‚Äôm still relatively new in this company and I feel like I‚Äôm not taken seriously, but I want to push back before we go too far, I'll switch jobs probably soon anyway but I'm actually concerned about my team.

How do I convince the management that this is a bad idea?",2,2025-06-05 11:52:01
"The original data chaos actually started *before* spreadsheets were common. In the pre-ERP days, most business systems were siloed‚ÄîHR, finance, sales, you name it‚Äîall running on their own. To report on anything meaningful, you had to extract data from each system, often manually. These extracts were pulled at different times, using different rules, and then stitched togethe. The result? Data quality issues. And to make matters worse, people were running these reports directly against transactional databases‚Äîsystems that were supposed to be optimized for speed and reliability, not analytics. The reporting load bogged them down.

The problem was so painful for the businesses, so around the late 1980s, a few forward-thinking folks‚Äîmost famously Bill Inmon‚Äîproposed a better way: a data warehouse.

To make matter even worse, in the late ‚Äô00s every department had its own spreadsheet empire. Finance had one version of ‚Äúthe truth,‚Äù Sales had another, and Marketing were inventing their own metrics. People would walk into meetings with totally different numbers for the same KPI.

The spreadsheet party had turned into a data chaos rave. There was no lineage, no source of truth‚Äîjust lots of tab-switching and passive-aggressive email threads. It wasn‚Äôt just annoying‚Äîit was a risk. Businesses were making big calls on bad data. So data warehousing became common practice!

More about it: [https://www.corgineering.com/blog/How-Data-Warehouses-Were-Created](https://www.corgineering.com/blog/How-Data-Warehouses-Were-Created)

  
P.S. Thanks to u/rotr0102 I made the post at least 2x times better",2,2025-04-14 07:15:27
"We worked with this e-commerce client last month (kitchen products company, can't name names) who was dealing with data chaos.

When they came to us, their situation was rough. Dashboards taking forever to load, some poor analyst manually combining data from 5 different sources, and their CEO breathing down everyone's neck for daily conversion reports. Classic spreadsheet hell that we've all seen before.

We spent about two weeks redesigning their entire data architecture. Built them a proper [**data warehouse solution** ](https://datafortune.com/services/enterprise-data-management/data-warehouse/)with automated ETL pipelines that consolidated everything into one central location. Created some logical data models and connected it all to their existing BI tools.

The transformation was honestly pretty incredible to watch. Reports that used to take hours now run in seconds. Their analyst actually took a vacation for the first time in a year. And we got this really nice email from their CTO saying we'd ""changed how they make decisions"" which gave us all the warm fuzzies.

It's projects like these that remind us why we got into this field in the first place. There's something so satisfying about taking a messy data situation and turning it into something clean and efficient that actually helps people do their jobs better.",2,2025-04-04 14:20:44
"The hardest part of working in data isn‚Äôt the technical complexity. It‚Äôs watching poor decisions get embedded into the foundation of a system, knowing exactly how and when they will cause failure.

A proper cleanse layer was defined but never used. The logic meant to transform data was never written. The production script still contains the original consultant's comment: ""you can add logic here."" No one ever did.

Unity Catalog was dismissed because the team ""already started with Hive,"" as if a single line in a config file was an immovable object. The decision was made by someone who does not understand the difference and passed down without question.

SQL logic is copied across pipelines with minor changes and no documentation. There is no source control. Notebooks are overwritten. Errors are silent, and no one except me understands how the pieces connect.

The manager responsible continues to block adoption of better practices while pushing out work that appears complete. The team follows because the system still runs and the dashboards still load. On paper, it looks like progress.

It is not progress. It is technical debt disguised as delivery.

And eventually someone else will be asked to explain why it all failed.

#DataEngineering #TechnicalDebt #UnityCatalog #LeadershipAccountability #DataIntegrity",2,2025-06-09 18:06:13
"I started this role for quite some time now, and the management would like me to develop KPIs and KRAs. I took some time to create it and needed AI to help me as well. However, the CIO of that company told me during my evaluation that I had made the needed list incorrectly.

Example KRA with KPI and Metric below. Take note, I have the metric as well:

KRA 1: Cybersecurity Risk Management and Risk Assessment

KPI 1: Implement comprehensive data security assessments for 100% of critical systems containing \[product\] identification numbers (VINs), customer financial data, and connected \[product\] data within 1 year.  
Metric: % of critical data systems that have undergone a complete security assessment

KPI 2: Reduce security vulnerabilities in dealership management systems (DMS) by 40% through enhanced validation controls that prevent SQL injection and unauthorized access to customer and vehicle records.  
Metric: % reduction in identified security vulnerabilities

KPI 3: Implement role-based access controls for dealership data systems with quarterly recertification, reducing unauthorized access to customer financial information by 50%.  
Metric: % reduction in unauthorized access attempts

That KRA is non-negotiable, as the organization mandates it. There is no direct link as a DE, but it is one of my dimensions to take care of.







",2,2025-06-03 13:03:40
"To be exact, this requirement was raised by one of my financial clients. He felt that there were too many data engineers (100 people) and he hoped to reduce the number to about 20-30. I think this is feasible. We have not yet tapped into the capabilities of Gen AI. I think it will be easier to replace data engineers with AI than to replace programmers. We are currently developing Agents. I will update you if there is any progress.",2,2025-04-27 09:37:28
"Hi everyone,

My team is working on some tooling to build some user friendly ways to do things in Databricks. Our initial focus is around entity resolution, creating a simple tool that can evaluate the data in unity catalog and deduplicate tables, create identity graphs, etc.

I'm trying to get some insights from people who use Databricks day-to-day to figure out what other kinds of capabilities we'd want this thing to have if we want users to try it out. 

Some examples I have gotten from other venues so far:

* Cost optimization
* Annotating or using advanced features of Unity Catalog can't be done from the UI and users would like being able to do it without having to write a bunch of SQL
* Figuring out which libraries to use in notebooks for a specific use case

This is just an open call for input here. If you use Databricks all the time, what kind of stuff annoys you about it or is confusing?

For the record, this tool are building will be open source and this isn't an ad. The eventual tool will be free to use, I am just looking for broader input into how to make it as useful as possible.

Thanks!",2,2025-04-14 18:37:22
"I‚Äôm hosting a panel discussion with 3 AI experts at the Snowflake Summit. They are from Siemens, TS Imagine and ZeroError.

They‚Äôve all built scalable AI apps on Snowflake Cortex for different use cases.

What questions do you have for them?!",2,2025-06-05 17:26:07
"I‚Äôve stuck to the chat interfaces so far, but the OAI codex demo and now Claude Code release has peaked my interests in utilizing agentic frameworks for tasks in a dbt project.

Do you have experience using Cursor, Windsurf, or Claude Code with a data engineering repository? I haven‚Äôt seen any examples/feedback on this use case. ",2,2025-05-27 04:09:01
Does anyone have experience with improving & maintaining data quality of SAP data? Do you know of any tools or approaches in that regard? ,2,2025-04-28 18:23:14
"I didn‚Äôt ask to create a metastore. I just needed a Unity Catalog so I could register some tables properly.

I sent the documentation. Explained the permissions. Waited.

No one knew how to help.

Eventually the domain admin asked if the Data Platforms manager could set it up. I said no. His team is still on Hive. He doesn‚Äôt even know what Unity Catalog is.

Two minutes later I was a Databricks Account Admin.

I didn‚Äôt apply for it. No approvals. No training. Just a message that said ‚ÄúI trust you.‚Äù

Now I can take ownership of any object in any workspace. I can drop tables I‚Äôve never seen. I can break production in regions I don‚Äôt work in.

And the only way I know how to create a Unity Catalog is by seizing control of the metastore and assigning it to myself. Because I still don‚Äôt have the CLI or SQL permissions to do it properly.  And for some reason even as an account admin, I can't assign the CLI and SQL permissions I need to myself either. But taking over the entire metastore is not outside of the permissions scope for some reason. 

So I do it quietly. Carefully. And then I give the role back to the AD group.

No one notices. No one follows up.

I didn‚Äôt ask for power. I asked for a checkbox.

Sometimes all it takes to bypass governance is patience, a broken process, and someone who stops replying.",2,2025-06-07 03:16:32
So what are the daily tasks and responsibilities of a data collective officer?,2,2025-04-21 14:53:20
"Hey all!¬†

Over the years, I‚Äôve worked at companies as small as a team of 10 and at organizations with thousands of data engineers, and I‚Äôve seen wildly different philosophies around analytical data.

Some organizations go with the ""build it and they will come"" data lake approach, broadly ingesting data without initial structure, quality checks, or governance, and later deriving value via a medallion architecture.

Others embed governed analytical data directly into their user-facing or internal operations apps. These companies tend to treat their data like core backend services managed with a focus on getting schemas, data quality rules, and governance right from the start. Similar to how transactional data is managed in a classic web app.

I‚Äôve found that most data engineering frameworks today are designed for the former state, Airflow, Spark, and DBT really shine when there‚Äôs a lack of clarity around how you plan on leveraging your data.¬†

I‚Äôve spent the past year building an open-source framework around a data stack that's built for the latter case (clickhouse, redpanda, duckdb, etc)‚Äîwhen companies/teams know what they want to do with their data and need to build analytical backends that power user-facing or operational analytics quickly.

The framework has the following core principles behind it:

1. Derive as much of the infrastructure as possible from the business logic to minimize the amount of boilerplate
2. Enable a local developer experience so that I could build my analytical backends right alongside my Frontend (in my office, in the desert, or on plane)
3. Leverage data validation standards‚Äî like types and validation libraries such as pydantic or typia‚Äîto enforce data quality controls and make testing easy
4. Build in support for the best possible analytical infra while keeping things extensible to incrementally support legacy and emerging analytical stacks
5. Support the same languages we use to build transactional apps. I started with Python and TypeScript but I plan to expand to others

The framework is still in beta and it‚Äôs now used by teams at big and small companies to build analytical backends. I‚Äôd love some feedback from this community

You can take it for a spin by starting from a boilerplate starter project:[ https://docs.fiveonefour.com/moose/quickstart](https://docs.fiveonefour.com/moose/quickstart)

Or you can start from a pre-built project template for a more realistic example:[ https://docs.fiveonefour.com/templates](https://docs.fiveonefour.com/templates)",2,2025-04-30 22:15:45
"I'm trying to deeply understand the data stack that supports AI Agents or LLM-based products. Specifically, I'm interested in what tools, databases, pipelines, and architectures are typically used ‚Äî from data collection, cleaning, storing, to serving data for these systems.

I'd love to know how the data engineering side connects with model operations (like retrieval, embeddings, vector databases, etc.).

Any explanation of a typical modern stack would be super helpful!",2,2025-04-27 06:19:53
"Research Topic: I am researching a topic on the impact on data team when they are building a RAG Model or supporting a vertical Agent (for Customer Success, HR or sales) that was just bought in the organization. I am not sure sure if this is the right community. As a data engineer, I was always dealing with cleaning data and getting data ready for dashboard. Are we seeing the same issue supporting these agents and ensuring they have access to right data, specially around data in Sharepoint and in unstructured format?",2,2025-05-28 19:41:05
"Hi everyone, it's great to connect. I'm driven by a passion for using AI to tackle complex technical challenges, particularly in data engineering where I believe we can massively simplify how businesses unlock value from their data. That's what led me to create **Nova**, an AI-powered ecosystem I'm building to make data engineering as straightforward as a conversation ‚Äì you literally describe what you need in plain English, and Nova handles the intricate pipeline construction and execution without needing deep coding expertise. We've already got a functional core that successfully translates these natural language requests into live, operational cloud data pipelines, and I'm really eager to connect with forward-thinking people who are excited about building the next generation of data tools and exploring how we can scale transformative ideas like this.",2,2025-06-03 13:14:56
"Hello everyone!¬†

I am not a data engineer, but I am trying to help other people within my organization (as well as myself) get a better understanding of what an overall data strategy looks like.¬† So, I figured I would ask the experts.¬† ¬†¬†

**Do you have a go-to high-level diagram you use that simplifies the complexities of an overall data solution and helps you communicate what that should look like to non-technical people like myself?**¬†

I‚Äôm a very visual learner so seeing something that shows what the journey of data should look like from beginning to end would be extremely helpful.¬† I‚Äôve searched online but almost everything I see is created by a vendor trying to show why their product is better.¬† I‚Äôd much rather see an unbiased explanation of what the overall process should be and then layer in vendor choices later.

I apologize if the question is phrased incorrectly or too vague.¬† If clarifying questions/answers are needed, please let me know and I‚Äôll do my best to answer them.¬† Thanks in advance for your help.",2,2025-04-01 16:58:49
"Have any of you recently had an interviews for a data engineering role at a company highly focused on GenAI, or with leadership who strongly push for it? Are the interviews much different from regular DE interviews for supporting analysts and traditional data science?

I assume I would need to talk about data quality, prepping data products/datasets for training, things like that as well as how I‚Äôm using or have plans to use Gen AI currently. 

What about agentic AI?",2,2025-04-23 01:41:14
"Hi everyone,

I‚Äôm currently evaluating options for a metadata catalog and came across Palantir Foundry. While I know Foundry is a full-featured data platform, I‚Äôm specifically interested in hearing from anyone who has experience using it \*\*solely or primarily as a metadata catalog\*\*‚Äînot for data transformation, pipeline orchestration, or analysis.

If you‚Äôve used Foundry in this more focused way, I‚Äôd love to hear about:

* How well it functions as a metadata catalog
* Ease of integration with external tools/sources
* Governance, lineage, and discovery capabilities
* Pros/cons compared to other dedicated metadata tools (e.g., DataHub, Collibra, Atlan, Amundsen, etc.)
* Any limitations or unexpected benefits

Any insight or lessons learned would be much appreciated!",2,2025-06-03 12:32:34
Question for any DEs working with Nielsen data. How is your company sourcing the data? Is the discover tool really the usual option. I'm in awe (in a bad way) that the large CPMG I work for has to manually pull data every time we want to update our Nielsen pipelines. Suggestions welcome,2,2025-04-30 15:46:30
"And why my coleagues were able to present outputs more eagerly than I do:

I am trying to deliver a 'perfect data set', which is too much to expect from a fully on-prem DW/DS filled with couple of thousands of tables with zero data documentation and governance in all 30 years of operation...

I am not even a perfectionist myself so IDK what lead me to this point. Probably I trusted myself way too much? Probably I am trying to prove I am ""one of the best data engineers they had""? (I am still on probation and this is my 4th month here)

The company is fine and has continued to prosper over the decades without much data engineering. They just looked at the big numbers and made decisions based of it intuitively. 

Then here I am, just spent hours today looking for the excess 0.4$ from a total revenue of 40Million$ from a report I broke down to a FactTable. Mathematically, this is just peanuts. I should have let it go and used my time effectively on other things.

I am letting go of this perfectionism. 

I want to get regularized in this company. I really, really want to.",2,2025-03-31 03:47:38
"Hi everyone,

I'm in the process of setting up a dbt project on Databricks and planning to leverage Unity Catalog to implement a medallion architecture. I am not sure the correct approach.  I am considering a dev/test/prod catalog, with a bronze/silver/gold schema:

* dev.bronze
* test.bronze
* prod.bronze

However, this takes 2 of the namespaces so all of the other information has to live in a single namespace such as table type (dim/fact), department (hr/finance), and data source and table description.  It seems like a lot to cram in there.

I have used the medallion architecture as a guide, but never used it in the naming, but the current team I am on really wants it to be in the name.  Just wondering what approaches people have taken.

Thanks",2,2025-03-31 14:27:44
"qwen and gpt 4 are pretty bad at polars. (i assume due to a paucity of training data?)

what‚Äôs the best ai model for polars?

two particular use cases in mind:
- generating boilerplate code, which i then edit myself
- suggesting ways to optimize/improve existing code

thanks all!
",2,2025-05-01 15:53:05
Question for any DEs working with Nielsen data. How is your company sourcing the data? Is the discover tool really the usual option. I'm in awe (in a bad way) that the large CPMG I work for has to manually pull data every time we want to update our Nielsen pipelines. Suggestions welcome,2,2025-04-30 15:46:30
"I'm wondering how many people have tried to integrate an RAG agent to their business data and get on-demand analysis from it?

  
What was the biggest challenge? What tech stack did you use? 

I'm asking because i'm in the same journey",2,2025-05-28 11:16:33
"AI can automate tasks like pipeline creation and data transformation in data engineering, but it doesn‚Äôt always explain the reasoning behind design choices or best practices.",2,2025-05-01 05:01:40
"the bronze, silver, gold of the medallion architecture is kind of confusing, how about we start calling it Smelting, Casting, and Machining instead? I think it makes so much more sense.",2,2025-04-18 03:29:43
"What are them? I heard all the time that the role is a very strategic/ high demand role, future proof since is not easy to automate.

Just started a role as a DG Specialist and the tasks are very few. Building and maintaining a data catalog is very manual, and also don‚Äôt think is a task that takes 40 hours a week during many months. Ensuring data quality?
There are very fancy AI tools that search for anomalies and evaluate data quality metrics throughout the entire pipeline. What else we do?",2,2025-05-02 17:49:34
"This is a question I've wondered for a while - simply put, given a data warehouse several facts, dimensions etc. 

How does your company decide who gets access to what data? 

If someone from Finance requests data which is typically used for Marketing - just because they say they need it. 

What are your processes like? How do you decide? 


At least to me it seems completely arbitrary with my boss just deciding depending on how much pressure he has for a project.",2,2025-05-23 00:42:12
"Read this article about the Agent2Agent Protocol   
[https://medium.com/everyday-ai/understanding-google-clouds-agent2agent-a2a-protocol-81d0d9bcfd91](https://medium.com/everyday-ai/understanding-google-clouds-agent2agent-a2a-protocol-81d0d9bcfd91)",2,2025-04-17 10:04:27
"Have any of you recently had an interviews for a data engineering role at a company highly focused on GenAI, or with leadership who strongly push for it? Are the interviews much different from regular DE interviews for supporting analysts and traditional data science?

I assume I would need to talk about data quality, prepping data products/datasets for training, things like that as well as how I‚Äôm using or have plans to use Gen AI currently. 

What about agentic AI?",2,2025-04-23 01:41:14
"In my company we are looking to incorporate an AI tool that could identify errors in data automatically. Do you have any recommendations? I was looking into Azure‚Äôs Anomaly Detector but it looks like it will be discontinued next year. If you have any good recommendations I‚Äôd appreciate it, thanks",2,2025-04-16 14:59:21
"What do you, fellow DEs think of applying DORA metrics to our work? does it make sense, and if so, whould it need rewording or adjustments?",2,2025-05-25 09:56:56
"Hi all, I'm evaluating metadata management solutions for our data platform and would appreciate any thoughts from folks who've actually implemented these tools in production.

We're currently running into scaling issues with our in-house data catalog and I think we need something more robust for governance and lineage tracking.

I've narrowed it down to Acryl (DataHub) and Collate (openmetadata) as the main contenders. I know I should look at Collibra and Alation and maybe Unity Catalog?

For context, we're a mid-sized fintech (\~500 employees) with about 30 data engineers and scientists. We're AWS with Snowflake, Airflow for orchestration, and a growing number of ML models in production.

My question list is: 

1. How these tools handle machine-scale operations 
2. How painful was it to get set up?
3. For DataHub and openmetadata specifically - is the open source version viable or is the cloud version necessary?
4. Any unexpected limitations you've hit with any of these platforms?
5. Do you feel like these grow with you as we increasingly head into AI governance? 
6. How well they integrate with existing tools (Snowflake, dbt, Looker, etc.)

If anyone has switched from one solution to another, I'd love to hear why you made the change and whether it was worth it.

Sorry for the pick list of questions - the last post on this was years ago and I was hoping for some more insights. Thanks in advance for anyone's thoughts. ",2,2025-04-12 23:05:00
"We have created an AI model and algorithms that enable us to map an organisations data landscape. This is because we found all data catalogs fell short of context to be able to enable purpose-based governance.

Effectively, it enables us to map and validate all data purposes, processing activities, business processes, data uses, data users, systems and service providers automatically without stakeholder workshops - but we are struggling with the last hurdle. 

We are attempting to use the data context to infer (with help from scans of core environments) data fields, document types, business logic, calculations and metrics. We want to create an anchor ""data asset"". 

The difficulty we are having is how do we define the data assets. We need that anchor definition to enable cross-functional utility, so it can't be linked to just one concept (ie purpose, use, process, rights). This is because the idea is that:
- lawyers can use it for data rights and privacy
- technology can use it for AI, data engineering and cyber security 
- commercial can use it for data value, opportunities, decision making and strategy 
- operations can use it for efficiency and automation


We are thinking we need a ""master definition"" that clusters related fields / key words / documents and metrics to uses, processes etc. and then links that to context, but how do we create the names of the clusters! 

Everything we try falls flat, semantic, contextual, etc. All the data catalogs we have tested don't seem to help us actually define the data assets - it assumes you have done this! 

Can anyone tell me how they have done this at thier organisation? Or how you approached defining the data assets you have? 

",2,2025-04-16 12:47:52
"Hi all, I'm evaluating metadata management solutions for our data platform and would appreciate any thoughts from folks who've actually implemented these tools in production.

We're currently running into scaling issues with our in-house data catalog and I think we need something more robust for governance and lineage tracking.

I've narrowed it down to Acryl (DataHub) and Collate (openmetadata) as the main contenders. I know I should look at Collibra and Alation and maybe Unity Catalog?

For context, we're a mid-sized fintech (\~500 employees) with about 30 data engineers and scientists. We're AWS with Snowflake, Airflow for orchestration, and a growing number of ML models in production.

My question list is: 

1. How these tools handle machine-scale operations 
2. How painful was it to get set up?
3. For DataHub and openmetadata specifically - is the open source version viable or is the cloud version necessary?
4. Any unexpected limitations you've hit with any of these platforms?
5. Do you feel like these grow with you as we increasingly head into AI governance? 
6. How well they integrate with existing tools (Snowflake, dbt, Looker, etc.)

If anyone has switched from one solution to another, I'd love to hear why you made the change and whether it was worth it.

Sorry for the pick list of questions - the last post on this was years ago and I was hoping for some more insights. Thanks in advance for anyone's thoughts. ",2,2025-04-12 23:05:00
"Ok ok ok I know, we need data engineers. I‚Äôve been hearing some buzz on data engineers getting laid off because someone in management got told that a new SaaS product can speed up and improve the data process by reducing the need for DEs (we‚Äôre seeing the same thing in ai and software dev). Many data consumers may view DE teams as a bottleneck who slow down the data delivery process, instead the people managing the natural bottlenecks in enterprise data management. It almost sounds like companies go back and forth between long procedures and good data (hello DEs) or really fast access to sh*t data (goodbye DE) why is it that companies can‚Äôt have both? I have faith that some SaaS products can complement a DE the way a calculator compliments an accountant, but it doesn‚Äôt seem like anyone has found that silver bullet. This is not my area of expertise please don‚Äôt flame me in the comments.

So, to get the point:
1. What are the natural bottlenecks that DEs have to address? Data quality (large scale), governance procedures? Lineage? Why can‚Äôt that get automated effectively?

2. Are there any interesting articles you have read and would like to share regarding the importance of the data engineer? Maybe anything that supports why the SaaS solutions fail to replace them effectively?",2,2025-05-26 21:19:14
Just curious if anyone has any tales of having incorrect data anywhere at some point and how it went over when they told their boss or stakeholders,2,2025-04-27 23:05:11
"Hi, I've been testing out [https://github.com/Snowflake-Labs/orchestration-framework](https://github.com/Snowflake-Labs/orchestration-framework) which enables you to create an actual AI Agent (not just a workflow). I added my notes about the testing and created an blog about it:   
[https://www.recordlydata.com/blog/snowflake-ai-agent-orchestration](https://www.recordlydata.com/blog/snowflake-ai-agent-orchestration)   
  
or 

at Medium [https://medium.com/@mika.h.heino/ai-agents-snowflake-hands-on-native-agent-orchestration-agent-gateway-recordly-53cd42b6338f](https://medium.com/@mika.h.heino/ai-agents-snowflake-hands-on-native-agent-orchestration-agent-gateway-recordly-53cd42b6338f)

Hope you enjoy it as much it testing it out

Currently the tools supports and with those tools I created an AI agent that can provide me answers regarding Volkswagen T2.5/T3. Basically I have scraped web for old maintenance/instruction pdfs for RAG, create an Text2SQL tool that can decode a VINs and finally a Python tool that can scrape part prices.

Basically now I can ask ‚ÄúXXX is broken. My VW VIN is following XXXXXX. Which part do I need for it, and what are the expected costs?‚Äù

1. Cortex Search Tool: For unstructured data analysis, which requires a standard RAG access pattern.
2. Cortex Analyst Tool: For structured data analysis, which requires a Text2SQL access pattern.
3. Python Tool: For custom operations (i.e. sending API requests to 3rd party services), which requires calling arbitrary Python.
4. SQL Tool: For supporting custom SQL pipelines built by users.",2,2025-04-22 06:56:06
"Hi there, 

I've been thinking about the current generation of data catalogs like DataHub and OpenMetadata, and something doesn't add up for me. They do a great job tracking metadata, but stop short of doing what seems like the next obvious step, actually helping enforce data access policies.

Imagine a unified catalog that isn't just a metadata registry, but also the gatekeeper to data itself:

- Roles defined at the catalog level map directly to roles and grants on underlying sources through credential-vending.

- Every access, by a user or a pipeline, goes through the catalog first, creating a clean audit trail.

Iceberg‚Äôs REST catalog hints at this model: it stores table metadata and acts as a policy-enforcing access layer, managing credentials for the object storage underneath.

Why not generalize this idea to all structured and unstructured data? Instead of just listing a MySQL table or an S3 bucket of PDFs, the catalog would also vend credentials to access them. Instead of relying on external systems for access control, the catalog becomes the control plane.

This would massively improve governance, observability, and even simplify pipeline security models.

Is there any OSS project trying to do this today?

Are there reasons (technical or architectural) why projects like DataHub and OpenMetadata avoid owning the access control space?

Would you find it valuable to have a catalog that actually controls access, not just documents it?
",2,2025-04-26 18:24:38
"The real blocker in achieving good data quality is not only code, but ineffective teamwork (or no teamwork at all).

For example:

* Data producers (engineers) often don‚Äôt know the full downstream data needs.
* Data consumers (analysts, stakeholders) often *do*, but they can‚Äôt enforce it.
* Slack back-and-forths, manual QA, broken dashboards.



We can imagine a healthy alternative for data collaboration, where data issues can be **understood, defined,** and **detected** across a team, and then the actual collaboration friction can be significantly reduced.



So, what does this have to do with YAML? 

Well, by adapting YAML into a data quality checks framework, we will get a simple, accurate, and effective **data collaboration model** which allows us to have:

* A clean, declarative way to define data expectations.
* Non-engineers to contribute checks and proposals.
* Business logic flows more naturally in the data pipeline based on quantifiable data expectations.
* Actual data accountability that is visible and traceable.



That's what a properly structured and designed YAML-based framework can achieve. 

As a result, we will have more people involved in testing ‚Üí issues being caught earlier, which leads to less friction, fewer surprises. What do you think?

FWIW, I work at Soda and we‚Äôve leaned pretty hard into YAML for this reason. Happy to share more examples if folks are interested.",2,2025-05-26 14:49:41
"What do you think about the progress into [agentic data stack](https://goagentdata.com/)?  
",2,2025-03-30 21:11:28
"The company I am working at is implementing their first ERP system. They easily took the ""promise"" that ERP will solve all of their analytics problem and that dashboards are just ""half ERP"".

Later on the implementation process they realized that the ERP cannot process the data by itself and needs third party tools like Power BI and Looker.

  
Do you have similar experience to me? 

How do you convince business users that ERP is just another source system to every data engineer?",2,2025-05-25 15:32:18
"Thoughtworks have published their latest Technology Radar: https://www.thoughtworks.com/radar

FWIW, here are a few of the 'blips' (as they call them) of note in the data space:

üü¢ Adopt: [Data product thinking](https://www.thoughtworks.com/radar/techniques/data-product-thinking)

üü¢ Adopt: [Trino](https://www.thoughtworks.com/radar/platforms/trino)

üëç Trial: [Databricks Delta Live Tables](https://www.thoughtworks.com/radar/tools/databricks-delta-live-tables)

üëç Trial: [Metabase](https://www.thoughtworks.com/radar/tools/metabase)

‚úã Hold: [Reverse ETL](https://www.thoughtworks.com/radar/techniques/reverse-etl)

On Reverse ETL they say: 

> we're seeing a growing trend where product vendors use Reverse ETL as an excuse to move increasing amounts of business logic into a centralized platform ‚Äî their product. This approach exacerbates many of the issues caused by centralized data architectures, and we suggest exercising extreme caution when introducing data flows from a sprawling, central data platform to transaction processing systems.",2,2025-04-02 10:17:58
"From a more technical perspective what's your opinion about Vertex AI.  
I am trying to deploy a machine learning pipeline and my data science colleges are real data scientists and I do not trust them to bring everything into production.  
What's your experience with vertex ai?",2,2025-04-01 15:49:03
I recently came across a NeurIPS paper that created benchmark for AI models trying to mimic data engineering/analytics work. The results show that the AI models are not there yet (14% success rate) and maybe will need some more time. Let me know what you guys think.,2,2025-04-25 18:00:56
"Hi folks,
I know there have been some discussions on this topic; but given we had lot of development in technology and business space; would like to get your input on
1. How much is this still a problem?
2. Do agentic workflows open up some new challenges?
3. Is there any need to convert large excel files into  SQL tables?

",2,2025-03-30 23:48:51
"  
**Connecting GPT with Zapier doesn‚Äôt mean you‚Äôve created a smart agent**

A lot of people think they have ""smart agents"" just by linking GPT with Zapier, but the truth is, what you have is simply¬†**automation**. It follows a set sequence of steps, doesn't understand the objective, and doesn‚Äôt interact with any complex context.

But there's another level of¬†**AI Agents**, and it‚Äôs something entirely different.¬†**Real AI Agents**¬†understand the environment they‚Äôre working in. They don't just follow orders; they comprehend intent, make decisions based on variables, and operate like you‚Äôve hired a smart person who works autonomously.

**What can AI Agents do?**  
They‚Äôre more than just automation tools. They‚Äôre ‚Äúinternet robots‚Äù that can perform ANY task you need without mistakes, tirelessly working around the clock. Here‚Äôs what they can do:

* Build massive email lists
* Manage social media accounts
* Create and maintain websites
* Perform any service you need and deliver money!

**Does this sound too good to be true?**  
It‚Äôs not. AI Agents are real, and they‚Äôre changing everything.¬†**Do you need tech skills?**  
No! The great thing is that these agents can work efficiently without the need for advanced technical knowledge.

**Are we at the beginning of something big?**  
Yes! This is the perfect time to get involved before this technology becomes mainstream and disrupts the job market. In the future, these agents will become an integral part of working online. If you're curious about exploring this game-changing technology, you can start by checking out the¬†**AI Agents**¬†capabilities through¬†[this link](https://aieffects.art/ai-agents), where you'll get everything you need to start",2,2025-04-23 23:50:02
"Hi!

Sharing my latest article from the Data Tech Stack series, I‚Äôve revamped the format a bit, including the image, to showcase more technologies, thanks to feedback from readers.

  
I am still keeping it very high level, just covering the 'what' tech are used, in separate series I will dive into 'why' and 'how'. Please visit the link, to fine more details and also references which will help you dive deeper.

  
Some metrics gathered from several place.

* Ingesting \~2 trillions of events per day using Google Cloud Platform.
* Ingesting 4+ TB of data into BQ per day.
* Ingesting 1.8 trillion events per day at peak.
* Datawarehouse contains more than 200 PB of data in 30k GCS bucket.
* Snapchat receives 5 billions Snaps per day.
* Snapchat has 3,000 Airflow DAGS with 330,000 tasks.



Let me know in the comments, any feedback and suggests.

  
Thanks",3,2025-06-07 16:49:55
"ChatGPT can now remember all conversations you've had across all chat sessions. Google Gemini, I think, also implemented a similar feature about two months ago with *Personalization*‚Äîwhich provides help based on your search history.  

I‚Äôd like to hear from database engineers, database administrators, and other CS/IT professionals (as well as actual humans): What kind of database do you think they use? Relational, non-relational, vector, graph, data warehouse, data lake?  

*P.S. I know I could just do deep research on ChatGPT, Gemini, and Grok‚Äîbut I want to hear from Redditors.",3,2025-04-14 21:51:54
"I‚Äôm working with a nonprofit, supporting 17 veteran communities. The communities aren‚Äôt brick-and-mortar ‚Äî they meet at churches and community spaces, and track attendance manually. There‚Äôs very little technology ‚Äî no computers, mostly just phones and Facebook.

They want to understand:
	‚Ä¢	What services are being offered at the community level
	‚Ä¢	Who‚Äôs attending (recurring vs new)
	‚Ä¢	No-show rates
	‚Ä¢	Cost per veteran for services

The challenge: no digital systems or staff capacity for manual data entry.

What tech-light solutions or data collection flows would you recommend to gather this info and make it analyzable? Bonus if it can integrate later with HubSpot or a simple PostgreSQL DB.",3,2025-04-02 13:01:43
"Ever wondered why some Airbnb listings are way more expensive? My project explores price trends, demand drivers, and traveler-friendly insights based on NYC Airbnb data! üè†üìà

\#dezoomcamp #dataanalysis #airbnb",3,2025-03-31 00:54:57
"What is the best way to collect like >10 years old news articles from the mainstream media and newspapers?
 ",3,2025-03-30 18:39:53
I‚Äôve been looking for something to unify our data and found Semaphore. Anyone have this in their company and how are they using it? Like it? Is there an alternative? Want to get some data before I engage the sales vultures ,3,2025-04-01 00:29:49
We have 5 developers and none of them are data scientists. We need to be able to create interactive dashboards for management.,3,2025-04-01 21:23:16
"How I made my Airbnb analysis efficient:  
üîπ **Staging layer**: Standardized & cleaned raw data  
üîπ **Core layer**: Built fact & dimension tables  
üîπ **Analytics dataset**: Ready for insights!  
\#dezoomcamp #analyticsengineering",3,2025-03-31 00:57:17
Is it possible to install Knime on Anaconda Navigator? ,3,2025-04-02 02:25:22
"We have a python integration set up where we pull data from Google Ads and Facebook Marketing into our data warehouse. We're pulling data about all 3 hierarchy tiers and some daily metrics:

1. Campaigns (id, name, start time, stop time)
2. Ad Groups/Ad Sets (id, name)
3. Ads (id, name, URL)
4. Metrics (clicks, impressions, spend) for the previous day

For the Google Ads API, you basically send a SQL query and the return time is like a tenth of a second.

For Facebook, we see returns times in the minutes, especially on the Ads piece. Was hoping to get an idea of how others might have successfully set up a process to get this data from Facebook in a more timely fashion, and possibly without hitting the rate limiting threshold.

Not the exact code we're using - I can get it off my work system tomorrow - but the gist:

    from facebook_business.adobjects.adaccount import AdAccount
    from facebook_business.adobjects.campaign import Campaign
    from facebook_business.adobjects.ad import AdSet
    from facebook_business.adobjects.ad import Ad
    from facebook_business.adobjects.adcreative import AdCreative
    campaigns = AdAccount('act_123456789').get_campaigns(
        params={},
        fields=[Campaign.Field.id,Campaign.Field.name,Campaign.Field.start_time,Campaign.Field.stop_time]
    )
    adsets= AdAccount('act_123456789').get_ad_sets(
        params={},
        fields=[AdSet.Field.id,AdSet.Field.name]
    )
    ads = AdAccount('act_123456789').get_ads(
        params={},
        fields=[Ad.Field.id,Ad.Field.name,Ad.Field.creative]
    )
    object_urls = AdAccount('act_123456789').get_ad_creatives(
        params={},
        fields=[AdCreative.Field.object_story_spec]
    )
    asset_urls = AdAccount('act_123456789').get_ad_creatives(
        params={},
        fields=[AdCreative.Field.asset_feed_spec]
    )

We then have to do some joining between ads/object\_urls/asset\_urls to match the Ad with the destination URL if the ad is clicked on.

The performance is so slow, that I hope we are doing it wrong. I was never able to get the batch call to work and I'm not sure how to improve things.

Sincerely a data analyst who crosses over into data engineering because our data engineers don't know python.",3,2025-04-02 02:17:27
"We have a python integration set up where we pull data from Google Ads and Facebook Marketing into our data warehouse. We're pulling data about all 3 hierarchy tiers and some daily metrics:

1. Campaigns (id, name, start time, stop time)
2. Ad Groups/Ad Sets (id, name)
3. Ads (id, name, URL)
4. Metrics (clicks, impressions, spend) for the previous day

For the Google Ads API, you basically send a SQL query and the return time is like a tenth of a second.

For Facebook, we see returns times in the minutes, especially on the Ads piece. Was hoping to get an idea of how others might have successfully set up a process to get this data from Facebook in a more timely fashion, and possibly without hitting the rate limiting threshold.

Not the exact code we're using - I can get it off my work system tomorrow - but the gist:

    from facebook_business.adobjects.adaccount import AdAccount
    from facebook_business.adobjects.campaign import Campaign
    from facebook_business.adobjects.ad import AdSet
    from facebook_business.adobjects.ad import Ad
    from facebook_business.adobjects.adcreative import AdCreative
    campaigns = AdAccount('act_123456789').get_campaigns(
        params={},
        fields=[Campaign.Field.id,Campaign.Field.name,Campaign.Field.start_time,Campaign.Field.stop_time]
    )
    adsets= AdAccount('act_123456789').get_ad_sets(
        params={},
        fields=[AdSet.Field.id,AdSet.Field.name]
    )
    ads = AdAccount('act_123456789').get_ads(
        params={},
        fields=[Ad.Field.id,Ad.Field.name,Ad.Field.creative]
    )
    object_urls = AdAccount('act_123456789').get_ad_creatives(
        params={},
        fields=[AdCreative.Field.object_story_spec]
    )
    asset_urls = AdAccount('act_123456789').get_ad_creatives(
        params={},
        fields=[AdCreative.Field.asset_feed_spec]
    )

We then have to do some joining between ads/object\_urls/asset\_urls to match the Ad with the destination URL if the ad is clicked on.

The performance is so slow, that I hope we are doing it wrong. I was never able to get the batch call to work and I'm not sure how to improve things.

Sincerely a data analyst who crosses over into data engineering because our data engineers don't know python.",3,2025-04-02 02:17:27
"Fishing for advice as I'm sure many have been here before. I came from DE at a SaaS company where I was more focused on the infra but now I'm in a role much close to the business and currently working with marketing. I'm sure this could make the Top-5 all time repeated DE tasks. A daily marketing report showing metrics like Spend, cost-per-click, engagement rate, cost-add-to-cart, cost-per-traffic... etc. These are per campaign based on various data sources like GA4, Google Ads, Facebook Ads, TikTok etc. Data updates once a day.

It should be obvious I'm not writing API connectors for a dozen different services. I'm just one person doing this and have many other things to do. I have Fivetran up and running getting the data I need but MY GOD is it ever expensive for something that seems like it should be simple, infrequent & low volume. It comes with a ton of build in reports that I don't even need sucking rows and bloating the bill. I can't seem to get what I need without pulling millions of event rows which costs a fortune to do.

Are there other similar but (way) cheaper solutions are out there? I know of others but any recommendations for this specific purpose?",3,2025-04-04 20:45:27
"Hey all,
I‚Äôm working on a project that involves building a comprehensive overview of all therapist-related businesses in my country. I‚Äôve found a public online source that lists approximately 16,000 such businesses, spread across many paginated result pages.

Each entry links to a detail page with information such as:

Business name
Business owner (person or legal entity)
Registration number (similar to a company ID)
Location (optional)
No consistent link to a website, but it's often listed in the details

What I need help with:

(1) Scrape all business data into a structured list (CSV, JSON or database).
This involves crawling through all paginated pages and collecting each business profile‚Äôs content.

(2) Automatically search for a homepage/website for each business.
The source doesn't always list websites, so for those missing, I'd like to auto-search Google (or use a business API if necessary) to find the most likely company homepage.
(3) If a homepage is found: scrape relevant data from the website itself.

Goal:
To build a clean, filterable dataset that can be used for matching clients with therapists (via a separate platform I'm developing).

Questions I‚Äôd like help with:

Is this technically feasible using open tools or affordable APIs? What/who exactly would I be looking for? I have tried navigating Fiverr, but I am simply not sure what I need to be frank...

Thanks in advance!",3,2025-04-02 12:12:24
Is it possible to install Knime on Anaconda Navigator? ,3,2025-04-02 02:25:22
"Hey fellow data folks üëã  
I just published a short video demo of [SQLPage](https://sql-page.com/) ‚Äî an open-source framework that lets you build full web apps and dashboards using only SQL.

Think: internal tools, dashboards, user forms or lightweight data apps ‚Äî all created directly from your SQL queries.

üìΩÔ∏è Here's the video if you're curious ‚ñ∂Ô∏è [Video link](https://www.youtube.com/watch?v=88Jir5KVnHM)  
(We built it for our YC demo but figured it might be useful for others too.)

If you're a data engineer or analyst who's had to hack internal tools before, I‚Äôd love your feedback. Happy to answer any questions or show real use cases we‚Äôve built with it!",3,2025-06-05 13:28:25
"I am looking for existing website taxonomy / categorization data sources or at least some kind of closest approximation raw data for at least top 1000 most visited sites.

I suppose some of this data can be extracted from content filtering rules (e.g. office network ""allowlists"" / ""whitelists""), but I'm not sure what else can serve as a data source. Wikipedia? Querying LLMs? Parsing search engine results? SEO site rankings (e.g. so called ""top authority"")?

There is [`https://en.wikipedia.org/wiki/Lists_of_websites`](https://en.wikipedia.org/wiki/Lists_of_websites), but it's very small.

The goal is to assemble a simple static website taxonomy for many different uses, e.g. automatic bookmark categorisation, category-based network traffic filtering, network statistics analysis per category, etc.

Examples for a desired category tree branches:

    Categories
    ‚îú‚îÄ‚îÄ Engineering
    ‚îÇ   ‚îî‚îÄ‚îÄ Software
    ‚îÇ       ‚îî‚îÄ‚îÄ Source control
    ‚îÇ           ‚îú‚îÄ‚îÄ Remotes
    ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ Codeberg
    ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ GitHub
    ‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ GitLab
    ‚îÇ           ‚îî‚îÄ‚îÄ Tools
    ‚îÇ               ‚îî‚îÄ‚îÄ Git
    ‚îú‚îÄ‚îÄ Entertainment
    ‚îÇ   ‚îî‚îÄ‚îÄ Media
    ‚îÇ       ‚îú‚îÄ‚îÄ Audio
    ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ Books
    ‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Audible
    ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ Music
    ‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ Spotify
    ‚îÇ       ‚îî‚îÄ‚îÄ Video
    ‚îÇ           ‚îî‚îÄ‚îÄ Streaming
    ‚îÇ               ‚îú‚îÄ‚îÄ Disney Plus
    ‚îÇ               ‚îú‚îÄ‚îÄ Hulu
    ‚îÇ               ‚îî‚îÄ‚îÄ Netflix
    ‚îú‚îÄ‚îÄ Personal Info
    ‚îÇ   ‚îú‚îÄ‚îÄ Gmail
    ‚îÇ   ‚îî‚îÄ‚îÄ Proton
    ‚îî‚îÄ‚îÄ Socials
        ‚îú‚îÄ‚îÄ Facebook
        ‚îú‚îÄ‚îÄ Forums
        ‚îÇ   ‚îî‚îÄ‚îÄ Reddit
        ‚îú‚îÄ‚îÄ Instagram
        ‚îú‚îÄ‚îÄ Twitter
        ‚îî‚îÄ‚îÄ YouTube
    
    // probably should be categorized as a graph by multiple hierarchies,
    // e.g. GitHub could be
    // ""Topic: Engineering/Software/Source control/Remotes""
    // and
    // ""Function: Social network, Repository"",
    // or something like this.

Surely I am not the only one trying to find a website categorisation solution? Am I missing some sort of an obvious data source?

---

Will accumulate mentioned sources here:

+ [`schema.org`][schema.org] - content mapping and tagging system produced by collaboration of Google, Yandex, Yahoo and Bing.
+ [Semantic Web][wp/Semantic_Web.en]
+ [Upper Ontology][wp/Upper_ontology.en]
+ [Olog][wp/Olog.en]
+ [Semagrams][algebraicjulia/Semagrams]

[schema.org]: https://schema.org
[wp/Semantic_Web.en]: https://en.wikipedia.org/wiki/Semantic_Web
[wp/Upper_ontology.en]: https://en.wikipedia.org/wiki/Upper_ontology
[wp/Olog.en]: https://en.wikipedia.org/wiki/Olog
[algebraicjulia/Semagrams]: https://algebraicjulia.github.io/Semagrams.jl

---

Special thanks to u/Operadic for an introduction to these topics.",3,2025-06-05 14:44:27
"First of all thanks. A company response to me with this technical task . This is my first dashboard btw 


So iam trying to do my best so idk why i feel this dashboard is newbie look like not like the perfect dashboards i see on LinkedIn. 


",3,2025-04-26 18:26:36
"With all of the bells and whistles that these modern data platforms have I'd expect them all to have basic IDE style pop-up documentation tooltips when querying from a table or joining on another. I'm only really familiar with a handful of these platforms but even just selecting a column I normally have to go and dig up it's data type from some other interface, let alone getting any of the engineers' documentation on it.

Snowflake for instance allows us to create `comments` pinned to tables, views, schemas , columns. The lot basically. Why are these comments so hidden to our users whilst they're actually writing the queries that make use of these tables, columns, etc?

Our team goes to a decent amount of effort to build useful and readable documentation around each table but is it any use if the end users have to pull up the docs in a separate tab before they understand that they're using the wrong column for their joins?

This feels like something that's not too hard to implement, I know having objects tagged with a comment or description is already a nice to have in the data world but surely we can do better? Please tell me that I've just been unlucky and most solutions do this cleanly out of the box. Is there a platform or at least some DBM software out there that's doing this that I'm just unaware of?",3,2025-05-01 15:34:58
NL2SQL is also included in their system.,3,2025-04-21 15:18:35
"We just launched **Seda.** You can connect your data and ask questions in plain English, write and fix SQL with AI, build dashboards instantly, ask about data lineage, and auto-document your tables and metrics. We‚Äôre opening up early access now at [seda.ai](https://www.seda.ai/). It works with Postgres, Snowflake, Redshift, BigQuery, dbt, and more.",3,2025-04-16 00:40:23
"Not sure if this is the right place to ask, but this is my favorite and most helpful data sub... so here we go

What's your go to tool for product review and customer sentiment data? Primarily looking for Amazon and [Chewy.com](http://Chewy.com) reviews, customer sentiment from blogs, forums, and social media, but would love a tool that could also gather reviews from additional online retailers as requested.

Ideally I'd love a tool that's plug and play and will work seamlessly with Snowflake, Azure BLOB storage, or Google Analytics",3,2025-06-05 15:11:51
"Hello everyone, first time poster here and would like to ask for help building a econometric model.

Some background, I am the admin for a discord server where we have beginner traders and investors learning from tested mentors that help them make money in the finacial markets.  What we do is free and is aimed at helping beginners not lose money to the institutions play the game.

One of the ideas we would like to action would be to build a econometric model to see how institutional vs retail investors/traders are positioned on a weekly bases and have predictive validity for the following week. 

We figured having a data professional would be our best bet to make this a reality, so that is why I'm posting here. 

Let me know if this would be possible or if you would be interested in helping us. ",3,2025-04-28 00:22:49
"Hey everyone, I‚Äôm new here and found this subreddit while digging around online trying to find help with a pretty specific problem. I came across a few tips that kinda helped, but I‚Äôm still feeling a bit stuck.

I‚Äôm working on building an automated cold email outreach system that realtors can use to find and warm up leads. I‚Äôve done this before for B2B using big data sources, where I can just filter and sort to target the right people.

Where I‚Äôm getting stuck is figuring out what kind of audience actually makes sense for real estate. I‚Äôve got a few ideas, like using filters for job changes, relocations, or other life events that might mean someone is about to buy or sell. After that, it‚Äôs mostly just about sending the right message at scale.

But I‚Äôm also wondering if there are better data sources or other ways to find high signal leads. I‚Äôve heard of scraping real estate sites for certain types of listings, and that could work, but I‚Äôm not totally sure how strong that data would be. If anyone here has tried something similar or has any ideas, even if it‚Äôs just a different perspective on my approach, I‚Äôd really appreciate it.",3,2025-04-30 00:52:41
"We built this dashboard to visualize cannabis sales in real time across North America during 4/20. The data updates live from thousands of dispensary POS transactions as the day unfolds.

Under the hood, we‚Äôre using Estuary for data streaming and Tinybird to power super fast analytical queries. The charts are made in Tremor and the map is D3.",3,2025-04-20 19:13:01
"The gaming industry is insanely fast-paced‚Äîand unforgiving. Most games are expected to break even within six months, or they get sidelined. That means every click, every frame, every in-game action needs to be tracked and analyzed almost instantly to guide monetization and retention decisions.

From a data standpoint, we‚Äôre talking hundreds of thousands of events per second, producing tens of TBs per day. And yet‚Ä¶ most of the teams I‚Äôve worked with are still stuck in spreadsheet hell.

Some real pain points we‚Äôve faced:
- Engineers writing ad hoc SQL all day to generate 30+ Excel reports per person. Every. Single. Day.
- Dashboards don‚Äôt cover flexible needs, so it‚Äôs always a back-and-forth of ‚Äúcan you pull this?‚Äù
- Game telemetry split across client/web/iOS/Android/brands‚Äîeach with different behavior and screen sizes.
- Streaming rewards and matchmaking in real time sounds cool‚Äîuntil you‚Äôre debugging Flink queues and job delays at 2AM.
- Our big data stack looked ‚Äúsimple‚Äù on paper but turned into a maintenance monster: Kafka, Flink, Spark, MySQL, ZooKeeper, Airflow‚Ä¶ all duct-taped together.

We once worked with a top-10 game where even a 50-person data team took 2‚Äì3 days to handle most requests. 

And don‚Äôt even get me started on security. With so many layers, if something breaks, good luck finding the root cause before business impact hits.

So my question to you:
Has anyone here actually simplified their data pipeline for gaming workloads?
What worked, what didn‚Äôt?
Any experience moving away from the Kafka-Flink-Spark model to something leaner?",3,2025-04-23 14:22:37
"
I have the feedback/comments given by managers from the past two years (all levels).

My organization already has an LLM model. They want me to analyze these feedbacks/comments and come up with a framework containing dimensions such as clarity, specificity, and areas for improvement. The problem is how to create the logic from these subjective things to train the LLM model (the idea is to create a dataset of feedback). How should I approach this?

I have tried LIWC (Linguistic Inquiry and Word Count), which has various word libraries for each dimension and simply checks those words in the comments to give a rating. But this is not working.

Currently, only word count seems to be the only quantitative parameter linked with feedback quality (longer comments = better quality).

Any reading material on this would also be beneficial.",3,2025-04-24 21:39:01
"Hey everyone, I‚Äôm new here and found this subreddit while digging around online trying to find help with a pretty specific problem. I came across a few tips that kinda helped, but I‚Äôm still feeling a bit stuck.

I‚Äôm working on building an automated cold email outreach system that realtors can use to find and warm up leads. I‚Äôve done this before for B2B using big data sources, where I can just filter and sort to target the right people.

Where I‚Äôm getting stuck is figuring out what kind of audience actually makes sense for real estate. I‚Äôve got a few ideas, like using filters for job changes, relocations, or other life events that might mean someone is about to buy or sell. After that, it‚Äôs mostly just about sending the right message at scale.

But I‚Äôm also wondering if there are better data sources or other ways to find high signal leads. I‚Äôve heard of scraping real estate sites for certain types of listings, and that could work, but I‚Äôm not totally sure how strong that data would be. If anyone here has tried something similar or has any ideas, even if it‚Äôs just a different perspective on my approach, I‚Äôd really appreciate it.",3,2025-04-30 00:52:41
"First of all thanks . Iam looking for opinions how to better this dashboard because it's a task sent to me . this was my old dashboard :¬†[https://www.reddit.com/r/dataanalytics/comments/1k8qm31/need\_opinion\_iam\_newbie\_to\_bi\_but\_they\_sent\_me/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/dataanalytics/comments/1k8qm31/need_opinion_iam_newbie_to_bi_but_they_sent_me/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)

what iam trying to asnwer :¬†**Analyzing Sales**

1. Show the total sales in dollars in different granularity.
2. Compare the sales in dollars between 2009 and 2008 (Using Dax formula).
3. Show the Top 10 products and its share from the total sales in dollars.
4. Compare the forecast of 2009 with the actuals.
5. Show the top customer(Regarding the amount they purchase) behavior & the products they buy across the year span.

¬†Sales team should be able to filter the previous requirements by country & State.

¬†

1. **Visualization:**

* This is should be one page dashboard
* Choose the right chart type that best represent each requirement.
* Make sure to place the charts in the dashboard in the best way for the user to be able to get the insights needed.
* Add drill down and other visualization features if needed.
* You can add any extra charts/widgets to the dashboard to make it more informative.

¬†",3,2025-04-28 15:20:13
"YouTube released some interesting metrics for their 20 year celebration and their data environment is just insane.

- Processing infrastructure handling 20+ million daily video uploads 
- Storage and retrieval systems managing 20+ billion total videos
- Analytics pipelines tracking 3.5+ billion daily likes and 100+ million daily comments
- Real-time processing of engagement metrics (creator-hearted comments reaching 10 million daily)
- Infrastructure supporting multimodal data types (video, audio, comments, metadata)

From an analytics point of view, it would be extremely difficult to validate anything you build in this environment, especially if it's something that is very obscure. 
Supposed they calculate a ""Content Stickiness Factor"" (a metric which quantifies how much a video prevents users from leaving the platform), 
how would anyone validate that a factor of 0.3 is correct for creator X? That is just for 1 creator in one segment, there are different segments which all have different behaviors eg podcasts which might be longer vs shorts

I would assume training ml models, or basic queries would be either slow or very expensive which punishes mistakes a lot. You either run 10 computer for 10 days or or 2000 computers for 1.5 hours, and if you forget that 2000 computer cluster running, for just a few minutes for lunch maybe, or worse over the weekend, you will come back to regret it.

Any mistakes you do are amplified by the amount of data, you omitting a single ""LIMIT 10"" or use a ""SELECT * "" in the wrong place and you could easy cost the company millions of dollars.
""Forgot a single cluster running, well you just lost us $10 million dollars buddy""

And because of these challenges, l believe such an environment demands excellence, not to ensure that no one makes mistakes, but to prevent obvious ones and reduce the probability of catastrophic ones.

l am very curious how such an environment is managed and would love to see it someday.

I have gotten to a point in my career where l have to start thinking about things like this, so can anyone who has worked in this kind of environment share tips of how to design an environment like this to make it ""safer"" to work in.



[YouTube article](https://blog.youtube/news-and-events/happy-birthday-youtube-20/)",3,2025-04-26 18:52:12
"Cipher42 is a ""Cursor for data"" which works by connecting to your database/data warehouse, indexing things like schema, metadata, recent used queries and then using it to provide better answers and making data analysts more productive. It took a lot of inspiration from cursor but for data related app cursor doesn't work as well as data analysis workloads are different by nature.",3,2025-04-13 21:58:22
"Hi everyone,

I‚Äôm currently working on my final project titled¬†**‚ÄúThe Evolution of Social Media Engagement: Trends Before, During, and After the COVID-19 Pandemic.‚Äù**

I‚Äôm specifically looking for¬†**free datasets**¬†that align with this topic, but I‚Äôve been having trouble finding ones that are accessible without high costs ‚Äî especially as a full-time college student. Ideally, I need to be able to¬†**download the data as CSV files**¬†so I can import them into¬†**Tableau**¬†for visualizations and analysis.

Here are a few research questions I‚Äôm focusing on:

1. How did engagement levels on major social media platforms change between the early and later stages of the pandemic?
2. What patterns in user engagement (e.g., time of day or week) can be observed during peak COVID-19 months?
3. Did social media engagement decline as vaccines became widely available and lockdowns began to ease?

I‚Äôve already found a couple of datasets on¬†**Kaggle**¬†(linked below), and I may use some information from¬†**gs.statcounter**, though that data seems a bit too broad for my needs.

If anyone knows of any other relevant¬†**free data sources**, or has suggestions on where I could look, I‚Äôd really appreciate it!

[Kaggle dataset 1¬†](https://www.kaggle.com/datasets/michau96/social-media-popularity-2009-2023?resource=download&select=social_media_3.csv)

[Kaggle Dataset 2](https://www.kaggle.com/datasets/arunavakrchakraborty/covid19-twitter-dataset)",3,2025-04-16 23:16:11
"Hi everyone! I'm looking for opinions on the best dashboard for a non-profit that rescues food waste and redistributes it. Here are some insights:

\- I am the only person on the team capable of filtering an Excel table and reading/creating a pivot table,  and I only work very part-time on data management --> the platform must not bug often and must have a veryyyyy user-friendly interface (this takes PowerBI out of the equation)

\- We have about 6 different Excel files on the cloud to integrate, all together under a GB of data for now. Within a couple of years, it may pass this point.

\- Non-profit pricing or a free basic version is best!

\- The ability to display 'live' (from true live up to weekly refreshes) major data points on a public website is a huge plus.

\- I had an absolute nightmare of a time getting a Tableau Trial set up and the customer service was unable to fix a bug on the back end that prevented my email from setting up a demo, so they're out.",3,2025-06-05 19:14:18
"Hi, I‚Äôm working on a task as described in the title. I planned to use an AI model (model that can run using CPU) to help fix performance issues in the queries. Tkprof is similar to performance report.

And I‚Äôm thinking to connect sqldeveloper which contain informations for the tables data so that the model gets more information.

Open to any suggestions related to this taskü•π

Ps: currently working in a small company and this is my first task, no one guilds me so I‚Äôm not sure if my ideas are wrong.

Thanks",3,2025-06-05 07:36:02
"""Fully Managed Graph Database Service | Neo4j AuraDB"" https://neo4j.com/product/auradb/?utm_source=GSearch&utm_medium=PaidSearch&utm_campaign=Evergreen&utm_content=AMS-Search-SEMCE-DSA-None-SEM-SEM-NonABM&utm_term=&utm_adgroup=DSA&gad_source=1&gclid=Cj0KCQjwna6_BhCbARIsALId2Z27LAb-nD-42tRRF5viybJfBVull8EeBvj46w_V7OCs1RdtbR7hqBQaAuObEALw_wcB",3,2025-04-01 17:03:16
"Hi All, 

I am working on a personal project to combine the transactions from my brokerage accounts and create a dashboard that will allow me to:

1. View portfolio performance over time

2. Drill down the holdings by brokerage account, asset type, geography, etc. 

3. Performe performance attribution

On the backend, I am using sqlalchemy in python to create database models. As part of the database, I will be creating my own transaction types so that I can map differently name transactions from various brokerage to same type. I want to build a dashboard that will allow me to upload my monthly brokerage statements on the UI and also let me edit some fields in the database such as transaction types. 

I am mainly using python and sql. What is the industry standard tool/language used for creating dashboards and allow CRUD operations? 

  
Thank you in advance! ",3,2025-05-26 19:04:03
"After months of work from the community, Apache Airflow 3.0 has officially landed and it marks a major shift in how we think about orchestration! 

This release lays the foundation for a more modern, scalable Airflow. Some of the most exciting updates:

* **Service-Oriented Architecture**¬†‚Äì break apart the monolith and deploy only what you need
* **Asset-Based Scheduling**¬†‚Äì define and track data objects natively
* **Event-Driven Workflows**¬†‚Äì trigger DAGs from events, not just time
* **DAG Versioning**¬†‚Äì maintain execution history across code changes
* **Modern React UI**¬†‚Äì a completely reimagined web interface

I've been working on this one closely as a product manager at Astronomer and Apache contributor. It's been incredible to see what the community has built!

üëâ Learn more:¬†[https://airflow.apache.org/blog/airflow-three-point-oh-is-here/](https://airflow.apache.org/blog/airflow-three-point-oh-is-here/)

üëá Quick visual overview:

[A snapshot of what's new in Airflow 3.0. It's a big one!](https://preview.redd.it/3vwrpl0d1fwe1.png?width=425&format=png&auto=webp&s=337f49a51fa473ef8d872c9bde604526f5b70243)

",4,2025-04-22 16:50:13
"I would like to understand how you manage your DWW in day-to-day basis, solution, tools, architecture, workflows, ETL, serving...",4,2025-04-02 07:17:28
"**üì£ Apache Airflow 3.0.0 has just been released!**

After months of work and contributions from 300+ developers around the world, we‚Äôre thrilled to announce the **official release of Apache Airflow 3.0.0** ‚Äî the most significant update to Airflow since 2.0.



This release brings:

* ‚öôÔ∏è A new Task Execution API (run tasks anywhere, in any language)
* ‚ö° Event-driven DAGs and native data asset triggers
* üñ•Ô∏è A completely rebuilt UI (React + FastAPI, with dark mode!)
* üß© Improved backfills, better performance, and more secure architecture
* üöÄ The foundation for the future of AI- and data-driven orchestration



You can read more about what 3.0 brings in [https://airflow.apache.org/blog/airflow-three-point-oh-is-here/](https://airflow.apache.org/blog/airflow-three-point-oh-is-here/).

https://preview.redd.it/orp1w81r2fwe1.jpg?width=3840&format=pjpg&auto=webp&s=f9fcb81ff8c99f1eb889e44a17d94845c95932e1

üì¶ PyPI: [https://pypi.org/project/apache-airflow/3.0.0/](https://pypi.org/project/apache-airflow/3.0.0/)

üìö Docs: [https://airflow.apache.org/docs/apache-airflow/3.0.0](https://airflow.apache.org/docs/apache-airflow/3.0.0)

üõ†Ô∏è Release Notes: [https://airflow.apache.org/docs/apache-airflow/3.0.0/release\_notes.html](https://airflow.apache.org/docs/apache-airflow/3.0.0/release_notes.html)

ü™∂ Sources: [https://airflow.apache.org/docs/apache-airflow/3.0.0/installation/installing-from-sources.html](https://airflow.apache.org/docs/apache-airflow/3.0.0/installation/installing-from-sources.html)

This is the result of 300+ developers within the Airflow community working together tirelessly for many months! A huge thank you to all of them for their contributions.",4,2025-04-22 16:56:29
"Hi all,
We‚Äôre working on an enterprise data pipeline where we ingest property data from ATTOM, perform some basic transformations (mostly joins with dimension tables), and load it into a BigQuery star schema. Later, selected data will be pushed to MongoDB for downstream services.
We‚Äôre currently evaluating whether to use Apache Beam (Python SDK) running on Dataflow, orchestrated via Cloud Composer, for this flow. However, given that:
The data is batch-based (not streaming)
Joins and transformations are relatively straightforward
Much of the logic can be handled via SQL or Python
There are no real-time or ML workloads involved
I‚Äôm wondering if using Beam might be overkill in this scenario ‚Äî both in terms of operational complexity and cost.
Would it be more relevant to use something like:
Cloud Functions / Run for extraction
BigQuery SQL / dbt for transformation and modeling
Composer just for orchestration
Also, is there any cost predictability model enterprises follow (flat-rate or committed use) for Beam + Composer setups?
Would love to hear thoughts from others who‚Äôve faced a similar build-vs-simplify decision in GCP.",4,2025-06-09 17:52:43
"Hello everyone, I don't have experience in data engineering, only data analysis, but currently I'm creating an ELT data pipeline to extract data from MySQL (18 tables) and load it to Google BigQuery using Airflow and then transform it using DBT.

There are too many ways to do this, and I don't know which one is better. Should I use MySQLOperator, MySQLHook or pandas and SQLAlchemy
+
How to only extract the newly data not the whole table (daily scheduled)
+
How to loop over the 18 table
+
For the DBT part, should I run the SQL file inside the airflow DAG?

I don't want the way that's will do the job; I want the most efficient way. ",4,2025-06-04 23:43:19
"My company has a few clients and I am tasked with organizing our schemas so that each client has their own schema. I am mostly the only one working on ETL pipelines, but there are 1-2 devs who can split time between data and software, and our CTO who is mainly working on admin stuff but does help out with engineering from time to time. We deal with highly sensitive healthcare data. Our apps right now use mongo for our backend db, but a separate database for analytics. In the past we only required ETL pipelines for 2 clients, but as we are expanding analytics to our other clients we need to create ETL pipelines at scale. That also means making changes to our current dev process.

  
Right now both our production and preproduction data is stored in one single instance. Also, we only have one EC2 instance that houses our ETL pipeline for both clients AND our preproduction environment. My vision is to have two database instances (one for production data, one for preproduction data that can be used for testing both changes in the products and also our data pipelines) which are both HIPAA compliant. Also, to have two separate EC2 instances (and in the far future K8s); one for production ready code and one for preproduction code to test features, new data requests, etc.

  
My question is what is best practice: keep ALL ETL code for each client in one single repo and separate out in folders based on clients, or have separate repos, one for core ETL that loads parent tables and shared tables and then separate repos for each client? The latter seems like the safer bet, but just so much overhead if I'm the only one working on it. But I also want to build at scale seeing that we may be experiencing more growth than we imagine.

  
If it helps, right now our ETL pipelines are built in Python/SQL and scheduled via cron jobs. Currently exploring the use of dagster and dbt, but I do have some other client-facing analytics projects I gotta get done first.",4,2025-06-05 20:15:27
"In a recent blog, the team at La¬†Poste (France‚Äôs postal service) shared how they redesigned their real-time package tracking pipeline from a monolithic app into a modular microservice architecture. The goal was to provide more accurate ETA predictions for deliveries while making the system easier to scale and monitor in production. They describe splitting the pipeline into multiple decoupled stages (using Pathway ‚Äì an open-source streaming ETL engine) connected via Delta Lake storage and Kafka. This revamped design not only improved performance and reliability, but also significantly cut costs (the blog cites a 50% reduction in total cost of ownership for the IoT data platform and a projected 16% drop in fleet capital expenditures, which is huge). Below I‚Äôll outline the architecture, key decisions, and trade-offs from the blog in an engineering-focused way.

From Monolith to Microservices: Originally, a single streaming pipeline handled everything: data cleansing, ETA calculation, and maybe some basic monitoring. That monolith worked for a prototype, but it became hard to extend ‚Äì for instance, adding continuous evaluation of prediction accuracy or integrating new models would make the one pipeline much more complex and fragile. The team decided to decouple the concerns into separate pipelines (microservices) that communicate through shared data layers. This is analogous to breaking a big application into microservices ‚Äì here each Pathway pipeline is a lightweight service focused on one part of the workflow.

They ended up with four main pipeline components:

1. Data Acquisition & Cleaning: Ingest raw telemetry from delivery vehicles and clean it. IoT devices on trucks emit location updates (latitude/longitude, speed, timestamp, etc.) to a Kafka topic. This first pipeline reads from Kafka, applies a schema, and filters out bad data (e.g. GPS (0,0) errors, duplicates, out-of-order events). The cleaned, normalized data is then written to a Delta Lake table as the ‚Äúprepared data‚Äù store. Delta Lake was used here to persist the stream in a queryable table format (every incoming event gets appended as a new row). This makes the downstream processing simpler and the intermediate data reusable. (Notably, they chose Delta Lake over something like chaining another Kafka topic for the clean data ‚Äì a design choice we‚Äôll discuss more below.)

2. ETA Prediction: This stage consumes two things ‚Äì the cleaned vehicle data (from that Delta table) and incoming ETA requests. ETA request events come as another stream (Kafka topic) containing a delivery request ID, the target destination, the assigned vehicle ID, and a timestamp. The topic is partitioned by vehicle ID so all requests for the same vehicle are ordered (ensuring the sequence of stops is handled correctly). The Pathway pipeline joins each request with the latest state of the corresponding vehicle from the clean data, then computes an estimated arrival time. The blog kept the prediction logic straightforward (e.g., basically using current location to estimate travel time to the destination), since the focus was architecture. The important part is that this service is stateless with respect to historical data ‚Äì it relies on the up-to-date clean data table as its source of truth for vehicle positions. Once an ETA is computed for a request, the result is written out to two places: a Kafka topic (so that whoever requested the ETA gets the answer in real-time) and another Delta Lake table storing all predictions (for later analysis).

3. Ground Truth Extraction: This pipeline waits for deliveries to actually be completed, so they can record the real arrival times (‚Äúground truth‚Äù data for model evaluation). It reads the same prepared data table (vehicle telemetry) and the requests stream/table to know what destinations were expected. The logic here tracks each vehicle‚Äôs journey and identifies when a vehicle has reached the delivery location for a request (and has no further pending deliveries for that request). When it detects a completed delivery, it logs the actual time of arrival for that specific order. Each of these actual arrival records is written to a ground-truth Delta Lake table. This component runs asynchronously from the prediction one ‚Äì an order might be delivered 30 minutes after the prediction was made, but by isolating this in its own service, the system can handle that naturally without slowing down predictions. Essentially, the ground truth job is doing a continuous join between live positions and the list of active delivery requests, looking for matches to signal completion.

4. Evaluation & Monitoring: The final stage joins the predictions with their corresponding ground truths to measure accuracy. It reads from the predictions Delta table and the ground truths table, linking records by request ID (each record pairs a predicted arrival time with the actual arrival time for a delivery). The pipeline then computes error metrics. For example, it can calculate the difference in minutes between predicted and actual delivery time for each order. These per-delivery error records are extremely useful for analytics ‚Äì the blog mentions calculating overall Mean Absolute Error (MAE) and also segmenting error by how far in advance the prediction was made (predictions made closer to the delivery tend to be more accurate). Rather than hard-coding any specific aggregation in the pipeline, the approach was to output the raw prediction-vs-actual data into a PostgreSQL database (or even just a CSV file), and then use external tools or dashboards for deeper analysis and alerting. By doing so, they keep the streaming pipeline focused and let data analysts iterate on metrics in a familiar environment. (One cool extension: because everything is modular, they can add an alerting microservice that monitors this error data stream in real-time ‚Äì e.g. trigger a Slack alert if error spikes ‚Äì without impacting the other components.)

Key Architectural Decisions:

Decoupling via Delta Lake Tables: A standout decision was to connect these microservice pipelines using Delta Lake as the intermediate store. Instead of passing intermediate data via queues or Kafka topics, each stage writes its output to a durable table that the next stage reads. For example, the clean telemetry is a Delta table that both the Prediction and Ground Truth services read from. This has several benefits in a data engineering context:

Data Reusability & Observability: Because intermediate results are in tables, it‚Äôs easy to query or snapshot them at any time. If predictions look off, engineers can examine the cleaned data table to trace back anomalies. In a pure streaming hand-off (e.g. Kafka topic chaining), debugging would be harder ‚Äì you‚Äôd have to attach consumers or replay logs to inspect events. Here, Delta gives a persistent history you can query with Spark/Pandas, etc.

Multiple Consumers: Many pipelines can read the same prepared dataset in parallel. The La¬†Poste use case leveraged this to have two different processes (prediction and ground truth) independently consuming the prepared\_data table. Kafka could also multicast to multiple consumers, but those consumers would each need to handle data cleaning or maintaining state. With the Delta approach, the heavy lifting (cleaning) is done once and all consumers get a consistent view of the results.

Failure Recovery: If one pipeline crashes or needs to be redeployed, the downstream pipelines don‚Äôt lose data ‚Äì the intermediate state is stored in Delta. They can simply pick up from the last processed record by reading the table. There‚Äôs less worry about Kafka retention or exactly-once delivery mechanics between services, since the data lake serves as a reliable buffer and single source of truth.

Of course, there are trade-offs. Writing to a data lake introduces some latency (micro-batch writes of files) compared to an in-memory event stream. It also costs storage ‚Äì effectively duplicating data that in a pure streaming design might be transient. The blog specifically calls out the issue of many small files: frequent Delta commits (especially for high-volume streams) create lots of tiny parquet files and transaction log entries, which can degrade read performance over time. The team mitigated this by partitioning the Delta tables (e.g. by date) and periodically compacting small files. Partitioning by a day or similar key means new data accumulates in a separate folder each day, which keeps the number of files per partition manageable and makes it easier to run vacuum/compaction on older partitions. With these maintenance steps (partition + compact + clean old metadata), they report that the Delta-based approach remains efficient even for continuous, long-running pipelines. It‚Äôs a case of trading some complexity in storage management for a lot of flexibility in pipeline design.

Schema Management & Versioning: With data passing through tables, keeping schemas in sync became an important consideration. If the schema of the cleaned data table changes (say they add a new column from the IoT feed), then the downstream Pathway jobs reading that table must be updated to expect that schema. The blog notes this as an increased maintenance overhead compared to a monolith. They likely addressed it by versioning their data schemas and coordinating deployments ‚Äì e.g. update the writing pipeline to add new columns in parallel with updating readers, or use schema evolution features of Delta Lake. On the plus side, using Delta Lake made some aspects of schema handling easier: Pathway automatically stores each table‚Äôs schema in the Delta log, so when a job reads the table it can fetch the schema and apply it without manual definitions. This reduces code duplication and errors. Still, any intentional schema changes require careful planning across multiple services. This is just the nature of microservices ‚Äì you gain modularity at the cost of more coordination.

Independent Scaling & Fault Isolation: A big reason for the microservice approach was scalability and reliability in production. Each pipeline can be scaled horizontally on its own. For example, if ETA requests volume spikes, they could scale out just the Prediction service (Pathway supports parallel processing within a job as well, but logically separating it is an extra layer of scalability). Meanwhile, the data cleaning service might be CPU-bound and need its own scaling considerations, separate from the evaluation service which might be lighter. In a monolithic pipeline, you‚Äôd have to scale the whole thing as one unit, even if only one part is the bottleneck. By splitting them, only the hot spots get more resources. Likewise, if the evaluation pipeline fails due to, say, a bug or out-of-memory error, it doesn‚Äôt bring down the ingestion or prediction pipelines ‚Äì they keep running and data accumulates in the tables. The ops team can fix and redeploy the evaluation job and catch up on the stored data. This isolation is crucial for a production system where you want to minimize downtime and avoid one component‚Äôs failure cascading into an outage of the whole feature.

Pipeline Extensibility: The modular design also opened up new capabilities with minimal effort. The case study highlights a few:

They can easily plug in an anomaly detection/alerting service that reads the continuous error metrics (from the evaluation stage) and sends notifications if something goes wrong (e.g., if predictions suddenly become very inaccurate, indicating a possible model issue or data drift).

They can do offline model retraining or improvement by leveraging the historical data collected. Since they‚Äôre storing all cleaned inputs and outcomes, they have a high-quality dataset to train next-generation models. The blog mentions using the accumulated Delta tables of inputs and ground truths to experiment with improved prediction algorithms offline.

They can perform A/B testing of prediction strategies by running two prediction pipelines in parallel. For example, run the current model on half the vehicles and a new model on a subset of vehicles (perhaps by partitioning the Kafka requests by transport\_unit\_id hash). Because the infrastructure supports multiple pipelines reading the same input and writing results, this is straightforward ‚Äì you just add another Pathway service, maybe writing its predictions to a different topic/table, and compare the evaluation metrics in the end. In a monolithic system, A/B testing could be really cumbersome or require building that logic into the single pipeline.

  
Operational Insights: On the operations side, the team did have to invest in coordinated deployments and monitoring for multiple services. There are four Pathway processes to deploy (plus Kafka, plus maybe the Delta Lake storage on S3 or HDFS, and the Postgres DB for results). Automated deploy pipelines and containerization likely help here (the blog doesn‚Äôt go deep into it, but it‚Äôs implied that there‚Äôs added complexity). Monitoring needs to cover each component‚Äôs health as well as end-to-end latency. The payoff is that each component is simpler by itself and can be updated or rolled back independently. For instance, deploying a new model in the Prediction service doesn‚Äôt require touching the ingestion or evaluation code at all ‚Äì reducing risk. The scaling benefits were already mentioned: Pathway allows configuring parallelism for each pipeline, and because of the microservice separation, they only scale the parts that need it. This kind of targeted scaling can be more cost-efficient.

The La¬†Poste case is a compelling example of applying software engineering best practices (modularity, fault isolation, clear data contracts) to a streaming data pipeline. It demonstrates how breaking a pipeline into microservices can yield significant improvements in maintainability and extensibility for data engineering workflows. Of course, as the authors caution, this isn‚Äôt a silver bullet ‚Äì one should adopt such complexity only when the benefits (scaling, flexibility, etc.) outweigh the overhead. In their scenario of continuously improving an ETA prediction service, the trade-off made sense and paid off.

I found this architecture interesting, especially the use of Delta Lake as a communication layer between streaming jobs ‚Äì it‚Äôs a hybrid approach that combines real-time processing with durable data lake storage. It raises some great discussion points: e.g., would you have used message queues (Kafka topics) between each stage instead, and how would that compare? How do others handle schema evolution across pipeline stages in production? The post provides a concrete case study to think about these questions. If you want to dive deeper or see code snippets of how Pathway implements these connectors (Kafka read/write, Delta Lake integration, etc.), I recommend checking out the original blog and the Pathway GitHub. Links below. Happy to hear others‚Äô thoughts on this design!",4,2025-05-23 14:18:34
"Hi all, wanted to share the blog post about Volga (feature calculation and data processing engine for real-time AI/ML - https://github.com/volga-project/volga), focusing on performance numbers and real-life benchmarks of it's On-Demand Compute Layer (part of the system responsible for request-time computation and serving).

In this post we deploy Volga with Ray on EKS and run a real-time feature serving pipeline backed by Redis, with Locust generating the production load. Check out the post if you are interested in running, scaling and testing custom Ray-based services or in general feature serving architecture. Happy to hear your feedback!¬†

[https://volgaai.substack.com/p/benchmarking-volgas-on-demand-compute](https://volgaai.substack.com/p/benchmarking-volgas-on-demand-compute)",4,2025-04-28 05:33:31
"Hi everyone!

I am working in a small company (we're 3/4 in the tech department), with a lot of integrations to make with external providers/consumers (we're in the field of telemetry).

I have set up an Airflow that works like a charm in order to orchestrate existing scripts (as a replacement of old crontabs basically).

However, we have a lot of data processing to setup, pulling data from servers, splitting xml entries, formatting, conversion into JSON, read/Write into cache, updates with DBs, API calls, etc...

I have tried running Nifi on a single container, and it took some time before I understood the approach but I'm starting to see how powerful it is.

  
However, I feel like it's a real struggle to maintain:  
   - I couldn't manage to have it run behind an nginx so far (SNI issues) in the docker-compose context
   - I find documentation to be really thin
   - Interface can be confusing, naming of processors also
   - Not that many tutorials/walkthrough, and many stackoverflow answers aren't 

I wanted to try it in order to replace old scripts and avoid technical debt, but I am feeling like NiFi might not be super easy to maintain.

I am wondering if keeping digging into Nifi is worth the pain, if managing the flows can be easy to integrate on the long run or if Nifi is definitely made for bigger teams with strong processes?
Maybe we should stick to Airflow as it has more support and is more widespread?
Also, any feedback on NifiKop in order to run it in kubernetes?

I am also up for any suggestion!

Thank you very much!

",4,2025-05-27 15:58:27
"Hey!

I recently built a Python library called [reaktiv](https://github.com/buiapp/reaktiv) that implements reactive computation graphs with automatic dependency tracking. I come from IoT and web dev (worked with Angular), so I'm definitely not an expert in data science workflows.

This is my first attempt at creating something that might be useful outside my specific domain, and I'm genuinely not sure if it solves real problems for folks in your field. I'd love some honest feedback - even if that's ""this doesn't solve any problem I actually have.""

The library creates a computation graph that:

* Only recalculates values when dependencies actually change
* Automatically detects dependencies at runtime
* Caches computed values until invalidated
* Handles asynchronous operations (built for asyncio)

While it seems useful to me, I might be missing the mark completely for actual data science work. If you have a moment, I'd appreciate your perspective.

Here's a simple example with pandas and numpy that might resonate better with data science folks:

    import pandas as pd
    import numpy as np
    from reaktiv import signal, computed, effect
    
    # Base data as signals
    df = signal(pd.DataFrame({
        'temp': [20.1, 21.3, 19.8, 22.5, 23.1],
        'humidity': [45, 47, 44, 50, 52],
        'pressure': [1012, 1010, 1013, 1015, 1014]
    }))
    features = signal(['temp', 'humidity'])  # which features to use
    scaler_type = signal('standard')  # could be 'standard', 'minmax', etc.
    
    # Computed values automatically track dependencies
    selected_features = computed(lambda: df()[features()])
    
    # Data preprocessing that updates when data OR preprocessing params change
    def preprocess_data():
        data = selected_features()
        scaling = scaler_type()
        
        if scaling == 'standard':
            # Using numpy for calculations
            return (data - np.mean(data, axis=0)) / np.std(data, axis=0)
        elif scaling == 'minmax':
            return (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))
        else:
            return data
    
    normalized_data = computed(preprocess_data)
    
    # Summary statistics recalculated only when data changes
    stats = computed(lambda: {
        'mean': pd.Series(np.mean(normalized_data(), axis=0), index=normalized_data().columns).to_dict(),
        'median': pd.Series(np.median(normalized_data(), axis=0), index=normalized_data().columns).to_dict(),
        'std': pd.Series(np.std(normalized_data(), axis=0), index=normalized_data().columns).to_dict(),
        'shape': normalized_data().shape
    })
    
    # Effect to update visualization or logging when data changes
    def update_viz_or_log():
        current_stats = stats()
        print(f""Data shape: {current_stats['shape']}"")
        print(f""Normalized using: {scaler_type()}"")
        print(f""Features: {features()}"")
        print(f""Mean values: {current_stats['mean']}"")
    
    viz_updater = effect(update_viz_or_log)  # Runs initially
    
    # When we add new data, only affected computations run
    print(""\nAdding new data row:"")
    df.update(lambda d: pd.concat([d, pd.DataFrame({
        'temp': [24.5], 
        'humidity': [55], 
        'pressure': [1011]
    })]))
    # Stats and visualization automatically update
    
    # Change preprocessing method - again, only affected parts update
    print(""\nChanging normalization method:"")
    scaler_type.set('minmax')
    # Only preprocessing and downstream operations run
    
    # Change which features we're interested in
    print(""\nChanging selected features:"")
    features.set(['temp', 'pressure'])
    # Selected features, normalization, stats and viz all update

I think this approach might be particularly valuable for data science workflows - especially for:

* Building exploratory data pipelines that efficiently update on changes
* Creating reactive dashboards or monitoring systems that respond to new data
* Managing complex transformation chains with changing parameters
* Feature selection and hyperparameter experimentation
* Handling streaming data processing with automatic propagation

As data scientists, would this solve any pain points you experience? Do you see applications I'm missing? What features would make this more useful for your specific workflows?

I'd really appreciate your thoughts on whether this approach fits data science needs and how I might better position this for data-oriented Python developers.

Thanks in advance!",4,2025-04-27 21:43:13
"Hey guys,
we‚Äôre currently using self-hosted Airflow for our internal ETL and data workflows. It gets the job done, but I never really liked it. Feels too far away from actual Python, gets overly complex at times, and local development and testing is honestly a nightmare.

I recently stumbled upon Prefect and gave the self-hosted version a try. Really liked what I saw. Super Pythonic, easy to set up locally, modern UI - just felt right from the start.

But the problem is: the open-source version doesn‚Äôt offer user management or logging, so we‚Äôd need the Cloud version. Pricing would be around 30k USD per year, which is way above what we pay for Airflow. Even with a discount, it would still be too much for us.

Is there any way to make the community version work for a small team? Usermanagement and Audit-Logs is definitely a must for us. Or is Prefect just not realistic without going Cloud?

Would be a shame, because I really liked their approach.

If not Prefect, any tips on making Airflow easier for local dev and testing?",4,2025-03-31 10:26:08
"The problem: outputs looked fine, but missed org-specific language and structure. Too generic.

The fix: feed in actual user docs, support guides, policies, and internal wikis as grounding.

Now it generates:

* Domain-aligned data
* Context-aware responses
* Better results in compliance + support-heavy workflows

Small change, big gain.

Anyone else experimenting with grounded generation for domain-specific tasks? What's worked (or broken) for you?",4,2025-04-23 12:46:18
"I am a part time data engineer/integrator who is in school at the moment. I work using Dagster, AWS, Snowflake, and Docker.

I was hoping Dagster would have roles where I lived but it seems everyone prefers Airflow.

Is it worth exploring data engineering internships that use Airflow at the expense of losing my current role? Do you guys see any growth in Dagster?",4,2025-03-30 01:25:17
"I've just started a data engineering project where I‚Äôm building a data pipeline using DuckDB and DBT, but I‚Äôm a bit unsure whether to go with Airflow or Prefect for orchestration. Any suggestions?",4,2025-04-15 18:44:36
"I am a part time data engineer/integrator who is in school at the moment. I work using Dagster, AWS, Snowflake, and Docker.

I was hoping Dagster would have roles where I lived but it seems everyone prefers Airflow.

Is it worth exploring data engineering internships that use Airflow at the expense of losing my current role? Do you guys see any growth in Dagster?",4,2025-03-30 01:25:17
"When implementing a large and highly scalable ETL pipeline, I want to know what tools you are using in each step of the way. I will be doing my work primarily in Google Cloud Platform, so I will be expecting to use tools such as BigQuery for the data warehouse, Dataflow, and Airflow for sure. If any of you work with GCP, what would the full stack for the pipeline look like for each individual level of the ETL pipeline? For those who don't work in GCP, what tools do you use and why do you find them beneficial?",4,2025-06-09 21:26:10
"Orchestrating **dbt runs with Airflow** ensures transformations only happen when new data arrives. No wasted compute, no stale dashboards‚Äîjust efficient workflows! üî•  
\#dezoomcamp #dbtcore #airflow",4,2025-03-31 00:56:49
"I'm **completely new to Data Engineering**. Went from never touched Docker, Terraform, Airflow, DBT  ->to->  just **finished the DataTalks DE Zoomcamp (**[capstone](https://github.com/MichaelSalata/compare-my-biometrics)**)**.  After struggling so much with Airflow, I looked at the Astronomer Fundamentals Cert and feel I **have \~70% of the knowledge** off the top of my head and **could learn the rest in about a week**.

Job wise, I figure **companies would still use Airflow 2** a while until Airflow 3 is very stable. That or I might be able to **find work helping migrating to Airflow 3**.",4,2025-05-02 16:17:05
"Hi folks. I'm wondering how can I create a local deployment of our AWS native data stack using s3, athena, glue catalog, and dagster as orchestrator?

It's getting harder and not economical to test new pipelines and data assets in our aws staging environment so hoping there's a good way to have a local deployment wherein you can perform intial testing",4,2025-04-22 14:16:45
"# ‚úÖ Introduction: What If Software Just Worked‚Ä¶ Everywhere?

Have you ever built something that ran perfectly on your computer‚Äîonly to watch it crash on someone else‚Äôs machine?  
That age-old problem has a modern solution: **containers**.

In this post, you'll discover **what containers are**, **how they work**, and **why they're revolutionizing software development, cloud computing, and DevOps workflows**. Whether you're a curious beginner, a developer, or a tech leader, this is your ultimate beginner-to-pro guide.

# üß† What Are Containers? (In One Clear Sentence)

**A container** is a lightweight, portable unit that packages your application **along with everything it needs to run**‚Äîso it behaves exactly the same across different computers, servers, and environments.

# üîÅ Think of it as:

>

# üîç Why Do Containers Matter in 2025?

Containers are at the **heart of modern app development**. They power everything from **microservices** to **cloud-native architectures** and **CI/CD pipelines**. Giants like Netflix, Google, Spotify, and Airbnb use containers to deploy faster, scale instantly, and recover from failures in seconds.

# üìà Real-world impact:

* **Faster deployments**
* **Lower costs**
* **Zero configuration issues**
* **Massive scalability**

# üì¶ How Containers Actually Work (Simplified)

Let‚Äôs break it down:

1. You write an app.
2. You create a **container image** using tools like **Docker**.
3. This image includes your app **+ libraries, code, settings, runtime, dependencies**.
4. You launch the image ‚Üí it becomes a **container** running independently on any system.

‚úÖ The secret sauce?

>

# ‚öñÔ∏è Containers vs. Virtual Machines (VMs)

|Feature|Containers üê≥|Virtual Machines üñ•Ô∏è|
|:-|:-|:-|
|OS Overhead|Minimal|Full OS per VM|
|Startup Time|Seconds|Minutes|
|Portability|Extremely portable|Less portable|
|Isolation|Process-level|Full hardware-level|
|Performance|High|Moderate|

>

# üõ†Ô∏è Popular Tools You Should Know

* **Docker** ‚Äì The most popular container platform
* **Kubernetes** ‚Äì Manages containers at scale
* **Podman** ‚Äì A secure alternative to Docker
* **Containerd** ‚Äì A high-performance container runtime

# üßô Real-Life Analogy: Cooking with Containers

Imagine you're a chef. You have a recipe that works perfectly in *your* kitchen.

But when you travel to a new city, you're in someone else's kitchen:

* No garlic?
* Oven too hot?
* Tools missing?

ü§Ø The result: your food is inconsistent.

Now imagine carrying **your entire kitchen inside a suitcase**‚Äîwith your spices, knives, and perfect oven. You unpack it anywhere, cook your signature dish, and it **tastes exactly the same.**

That‚Äôs what containers do for software.  
**Portable, repeatable, reliable**.

# üéØ Benefits of Using Containers

‚úî **Run anywhere** ‚Äì Cloud, local, Windows, Linux‚Äîit just works.  
‚úî **Speed up development** ‚Äì Spin up dev environments in seconds.  
‚úî **Improve collaboration** ‚Äì ‚ÄúWorks on my machine‚Äù becomes ‚ÄúWorks everywhere.‚Äù  
‚úî **Simplify deployment** ‚Äì One artifact to deploy across all environments.  
‚úî **Scale effortlessly** ‚Äì Run hundreds of containers in parallel with tools like Kubernetes.

# ‚ö†Ô∏è Common Myths & Mistakes

|‚ùå Myth|‚úÖ Truth|
|:-|:-|
|""Containers are like VMs""|They share the OS kernel, not the entire OS|
|""Containers are secure by default""|You must harden container environments|
|""Containers can store app data""|**statelessvolumes**Containers are   unless you use  |

# üí° Real-World Use Cases

* **Microservices architecture** ‚Äì Break large apps into small, independently deployable services.
* **CI/CD pipelines** ‚Äì Use containers to test and deploy code faster.
* **Hybrid cloud** ‚Äì Move workloads across clouds seamlessly.
* **Dev environments** ‚Äì Onboard new developers in minutes.

# üöÄ Final Takeaway

**Containers have become the building blocks of modern software.**  
They‚Äôre not just a trend‚Äîthey‚Äôre a necessity for teams that want to move fast, stay agile, and build reliable systems in today‚Äôs cloud-first world.

If you're not using containers yet, you're leaving speed, consistency, and innovation on the table.

# üîó Want to Go Deeper?

üìò Learn Docker: [https://docker.com/get-started]()  
üìò Kubernetes basics: [https://kubernetes.io/docs/home/]()  
üìò Free labs & practice: [https://play-with-docker.com](https://play-with-docker.com)

# ‚ú® Bonus: Quick SEO FAQ

**Q: What is a container in simple terms?**  
A: It's a portable unit that packages your app and its dependencies to run consistently on any system.

**Q: How are containers different from virtual machines?**  
A: Containers share the host OS kernel and are faster and more lightweight than VMs.

**Q: What are the benefits of containerization?**  
A: Portability, speed, scalability, environment consistency, and cost efficiency.

**Q: Is Docker the same as a container?**  
A: Docker is a **tool** used to create and manage containers.",4,2025-05-26 14:49:54
"I‚Äôm looking for open source options for orchestration tools that are more event driven rather than batch that ideally have a native NATS connector to pin/sub to NATS streams. 

My use case is when a message comes in I need to trigger some ETL pipelines incl REST api calls and then publish a result back out to a different NATS stream. While I could do all this in code, it would be great to have the logging, ui, etc of an orchestration tool

I‚Äôve seen Kestra has a native NATS connector (https://kestra.io/plugins/plugin-nats), does anyone have any other alternatives? ",4,2025-04-28 14:37:33
"In my understanding Airflow is for orchestrating transformations. 

And dbt is for orchestrating transformations as well.

Typically Airflow calls dbt, but typically dbt doesn't call Airflow.

It seems to me that when you use both, you will use Airflow for ingestion, and then call dbt to do all transformations (e.g. bronze > silver > gold)

Are these assumptions correct?

How does this work with Airflow's concept of running DAGs per day?

Are there complications when backfilling data?

I'm curious what people's setups look like in the wild and what are their lessons learned.",4,2025-04-26 03:44:02
What do people use here for airflow observability needs besides the UI?,4,2025-05-27 04:41:08
"I recently completed a real-time ETL pipeline project as part of my data engineering portfolio, and I‚Äôd love to share it here and get some feedback from the community.

# What it does:

* Streams transactional data using **Amazon Kinesis**
* Backs up raw data in **S3** (Parquet format)
* Processes and transforms data with **Apache Spark**
* Loads the transformed data into **Redshift Serverless**
* Orchestrates the pipeline with **Apache Airflow (Docker)**
* Visualizes insights through a **QuickSight dashboard**

# Key Metrics Visualized:

* Total Revenue
* Orders Over Time
* Average Order Value
* Top Products
* Revenue by Category (donut chart)

I built this to practice real-time ingestion, transformation, and visualization in a scalable, production-like setup using AWS-native services.

# GitHub Repo:

[https://github.com/amanuel496/real-time-ecommerce-etl-pipeline](https://github.com/amanuel496/real-time-ecommerce-etl-pipeline)

If you have any thoughts on how to improve the architecture, scale it better, or handle ops/monitoring more effectively, I‚Äôd love to hear your input.

Thanks!",4,2025-04-04 02:55:36
"
I tried making edits to the config file but that doesn‚Äôt get picked up. Using airflow 2. Surely there must be a way to reload without restarting the pod? ",4,2025-06-11 20:02:06
Basically the title. I am interested in understanding what Airflow Operators are you using in you companies?,4,2025-06-12 16:54:01
"By outside, I mean the orchestrator runs an external script or docker image. Something like BashOperator or KubernetesPodsOperator in Airflow.

Any experiences on both approach? Pros and Cons?

Some that I can think of for writing inside the orchestrator.

Pros:

\- Easier to manage since everything is in one place.

\- Able to use the full features of the orchestrator.

\-  Variables, Connections and Credentials are easier to manage.

Cons:

\- Tightly coupled with the orchestrator. Migrating your code might be annoying if you want to use different orchestrator.

\- Testing your code is not really easy.

\- Can only use python.

For writing code outside the orchestrator, it is pretty much the opposite of the above.

Thoughts?

  
",4,2025-06-04 10:35:43
"I was wondering how do data engineers in different company group their pipelines together ?

Usually tables need to be refreshed at some specific refresh rates. This means that some table upstream might require 1h refresh while downstream table might require daily.

I can see people grouping things by domain and running domain one after each other sequentially, but then this break the concept of having different refresh rate per table or domain. I can see table configure with multiple corn but then I see issues with needing to schedule offset in cron jobs. 

Like most of the domain are very close to each other so when creating them I might be mixing a lot of stuff together which would impact downstream.

What‚Äôs your experience in structuring pipeline? Or any good reference I can read ?
",4,2025-04-08 11:13:44
"Hey everyone,
We're a small data team (3 data engineers + 1 data analyst). Two of us are strong in Python, and all of us are good with SQL.
We're considering setting up a stack composed of Airflow (for orchestration) and SQLMesh (for transformations and environment management).

We'd like to handle multiple projects (different domains, data products, etc.) and are wondering:

How would you organize your SQLMesh and Airflow setup for multiple projects?

Would you recommend one Airflow instance per project or a single shared instance?

Would you create separate SQLMesh repositories, or one monorepo with clear separation between projects?

Any tips for keeping things scalable and manageable for a small but fast-moving team?


Would love to hear from anyone who has worked with SQLMesh + Airflow together, or has experience managing multi-project setups in general!

Thanks a lot!",4,2025-04-26 11:31:28
"Openflow integrates Apache NiFi and Arctic LLMs to simplify data ingestion, transformation, and observability.",4,2025-06-05 18:06:34
"Hey guys , i justed started learning airflow . The thing that concerns me is that i often tend to use chatgpt or for giving me code for like writing etl  .  I understand the process and how things work . But is it fine to use LLms for helo or should i become expert at writing this scripts. I have had made few porject but each of them seems to use differnt logic for fetching and all . ",4,2025-06-05 09:37:41
"üöÄ Wanted to share that my team open-sourced Heimdall (Apache 2.0) ‚Äî a lightweight data orchestration tool built to help manage the complexity of modern data infrastructure, for both humans and services.

This is our way of giving back to the incredible data engineering community whose open-source tools power so much of what we do.

üõ†Ô∏è GitHub: [https://github.com/patterninc/heimdall](https://github.com/patterninc/heimdall)

üê≥ Docker Image: [https://hub.docker.com/r/patternoss/heimdall](https://hub.docker.com/r/patternoss/heimdall)

If you're building data platforms / infra, want to build data experiences where engineers can build on their devices using production data w/o bringing shared secrets to the client, completely abstract data infrastructure from client, want to use Airflow mostly as a scheduler, I'd appreciate you checking it out and share any feedback -- we'll work on making it better! I'll be happy to answer any questions.",4,2025-06-07 03:05:47
"Hi!

This is a problem I am facing in my current job right now. We have a lot of RPA requirements and 300's of CSV's and Excel files are manually obtained from some interfaces and mail and customer only works with excels including reporting and operational changes are being done manually by hand.

The thing is we don't have any data. We plan to implement Power Automate to grab these files from the said interfaces. But as some of you know, PowerAutomate has SQL Connectors. 

Do you think it is ok to write files directly to a database with PowerAutomate? Have any of you experience in this? Thanks.",5,2025-04-15 07:42:17
"Hi, I'm a bit outdated when it comes to all new cloud based solutions and request navigation on what architecture might be useful to start with (should be rather simple and not too much overhead to set up) while still be prepared for more data sources and more analysis requirements.

I'm using Azure

My use-case:
I have a time-series dataset coming from an API on which we perform a Python analysis. We would like to perform the Python analysis on a weekly basis, store the data and provide the output as a power bi dashboard. The dataset consists of like 500 000 rows each week, the analysis scripts processes a many to many calculation and I might be interested in adding more data sources as well as perform more KPI calculations pre-processed in data storage (i.e. not in power bi).",5,2025-04-01 08:03:41
"Hei! I‚Äôm designing a solution to pull daily survey data from an external API and load it into Power BI Service in a secure and automated way. Here‚Äôs the main idea:

	‚Ä¢	Use an Azure Function to fetch paginated API data and store it in Azure Blob Storage (daily-partitioned .json files).

	‚Ä¢	Power BI connects to the Blob container, dynamically loads the latest file/folder, and refreshes on schedule.

	‚Ä¢	No API calls happen inside Power BI Service (to avoid dynamic data source limitations). I was trying to do normal built-in GET API from Power BI Service but it doesn't accept dynamic data sources (Power BI Desktop works well, no issues) as API usually does.

	‚Ä¢	Everything is designed with data protection and scalability in mind ‚Äî future-compatible with Fabric Lakehouse.
P/S: The reason we are forced to go with this solution without using Fabric architecture because it requires cost-effective solution and Fabric integration is planning to be deployed in our organization (potentially project starts from November)

Looking for feedback on:

	‚Ä¢	Anything I might be missing?

	‚Ä¢	Any more robust or elegant approaches?

	‚Ä¢	Would love to hear if anyone‚Äôs done something similar.",5,2025-06-04 16:28:16
"i am building data pipelines. i use azure vms for experimentation on sample data. when im not using them, i need to shut them off (working at bootstrapped startup).

when restarting my vm, it randomly fails. it says an allocation failure occurred due to capacity in the region (usually us-east). the only solution ive found is moving the resource to a new region, which takes 30‚Äì60 mins.

how do i prevent this issue in a cost-effective manner? can azure just allocate my vm to whatever region is available?

i‚Äôve tried to troubleshoot this issue for weeks with azure support, but to no avail.

thanks all! :)
",5,2025-03-30 17:26:28
"Hiiiii I have to build a data warehouse by Jan/Feb and I kind of have no idea where to start. For context, I am one of one for all things tech (basic help desk, procurement, cloud, network, cyber) etc (no MSP) and now handling all (some) things data. I work for a sports team so this data warehouse is really all sports code footage, the files are .JSON I am likely building this in the Azure environment because that‚Äôs our current ecosystem but open to hearing about AWS features as well. I‚Äôve done some YouTube and ChatGPT research but would really appreciate any advice. I have 9 months to learn & get it done, so how should I start? Thank so much!

Edit: 
Thanks so far for the responses! As you can see I‚Äôm still new to this which is why I didn‚Äôt have enough information to provide but ‚Ä¶. 
In a season we have 3TB of video footage hoooweeveerr this is from all games in our league so even the ones we don‚Äôt play in. I can prioritize all our games only and that should be 350 GB data (I think) now ofcourse it wouldn‚Äôt be uploaded all at once but based off of last years data I have not seen a singular game file over 11.5 GB. I‚Äôm unsure how much practice footages we have but I‚Äôll see. 

Oh also I put our files in ChatGPT and it‚Äôs ‚Äú.SCTimeline , stream.json , video.json and package meta‚Äù Chat game me a hopefully this information helps. ",5,2025-06-03 05:51:12
"i am building data pipelines. i use azure vms for experimentation on sample data. when im not using them, i need to shut them off (working at bootstrapped startup).

when restarting my vm, it randomly fails. it says an allocation failure occurred due to capacity in the region (usually us-east). the only solution ive found is moving the resource to a new region, which takes 30‚Äì60 mins.

how do i prevent this issue in a cost-effective manner? can azure just allocate my vm to whatever region is available?

i‚Äôve tried to troubleshoot this issue for weeks with azure support, but to no avail.

thanks all! :)
",5,2025-03-30 17:26:28
"I'm working in a mid-sized FMCG company, I utilize Azure Data Factory (ADF).  The current ADF environment includes 1,310 pipelines and 243 datasets.  Maintaining this volume will become increasingly challenging.  How can we reduce the number of pipelines without impacting functionality?Any advice on this ?",5,2025-05-26 12:54:31
"We're migrating a bunch of geography data from local SQL Server to Azure Databricks.  Locally, we use ArcGIS to match latitude/longitude to city,state locations, and pay a fixed cost for the subscription.  We're looking for a way to do the same work on Databricks, but are having a tough time finding a cost effective ""all-you-can-eat"" way to do it.  We can't just install ArcGIS there to use or current sub.

Any ideas how to best do this geocoding work on Databricks, without breaking the bank?",5,2025-04-14 14:33:42
"Hi. 

Im looking to moving the computer to an Azure Function being orchestrated by ADF and merge into SQL. 

I need to pick which plan to go with and estimate my usage. I know I'll need VNET.

Im ingesting data from adls2 coming down a synapse link pipeline from d365fo.

Unoptimised ADF pipelines sink to an unoptimised Azure SQL Server.

I need to run the pipeline every 15 minutes with Max 1000 row updates on 150 tables. By my research 1 vCPU should easily cover this on the premium subscription.

Appreciate any assistance.

",5,2025-04-30 06:00:39
"Thank you everyone with all of your helpful insights from my initial post! Just as the title states, I'm an intern looking to weigh the pros and cons of using Dataverse vs an Azure SQL Database (After many back and forths with IT, we've landed at these two options that were approved by our company). 

Our team plans to use Microsoft Power Apps to collect data and are now trying to figure out where to store the data. Upon talking with my supervisor, they plan to have data exported from this database to use for data analysis in SAS or RStudio, in addition to the Microsoft Power App.

What would be the better or ideal solution for this? Thank you!
Edit: Also, they want to store images as well. Any ideas on how and where to store them?",5,2025-04-15 03:35:41
"Hello all,

We are Azure based. One of our vendors recently moved over to Redshift and I'm having a hell of a time trying to figure out how to run stored procedures (either call with a temp return or some database function) from ADF, logic apps or PowerBI. Starting to get worried I'm going to have to spin up a EC2 or lambda or some other intermediate to run the stored procedures, which will be an absolute pain training my junior analysts on how to maintain.

Is there a simple way to call Redshift SP from Azure stack?",5,2025-04-30 17:40:53
"Hi All! I am working on trying to automate a data extraction from a SaaS that displays a data table that I want to push into my database hosted on Azure. Unfortunately the CSV export requires me to sign-in with an email 2FA and then request it on the UI, and then download it after about 1min or so. The email log-in has made it difficult to scrape with headless browser and they do not have a read-only API, and they do not email the CSV export either. Am I out of luck here? Any avenues to automatically extract this data? ",5,2025-06-09 21:37:43
"Hi all,

I currently am working on a new structure to save sensor data coming from Azure Iot Hub in Azure to store it into Azure Blob Storage for historical data, and Clickhouse for hot data with TTL (around half year). The sensor data is coming from different entities (e.g building1, boat1, boat2) and should be partioned by entity. The data we‚Äôre processing daily is around 300-2 million records per day.

I know Azure Iot Hub is essentially a built-in Azure Hub. I had a few questions since I‚Äôve tried multiple solutions. 

1. Normal message routing to Azure Blob
Issue: no custom partitioning on file structure (e.g entityid/timestamp_sensor/) it requires you to use the enqueued time. And there is no dead letter queue for fallback

2. IoT hub -> Azure Functions -> Blob Storage & Clickhouse
Issue: this should work correctly but I have not that much experience in Azure Functions, I tried creating a function with the IoT Hub template but it seems I need to also have an Event Hubs namespace which is not what I want. HTTP trigger is also not what I want. I don‚Äôt find any good documentation on it aswell. I know I can maybe use Event Hubs trigger and use the Iot Hub connection string but I didn‚Äôt manage to do this yet.

3. IoT hub -> Event Grid 
Someone suggested using Event Grid, however to my knowledge Event Grid is not used for telemetry data despite there being an option for. Is this beneficial? I don‚Äôt really know what the flow would be since you can‚Äôt use Event Grid to send data to Clickhouse. You would still need an Azure Functions.

4. IoT Hub -> Event Grid -> Event Hubs -> Azure Functions -> Azure Blob & Clickhouse
This one seemed the most appealing to me but I don‚Äôt know if it‚Äôs the smartest, it can get expensive (maybe).
But the idea here is that we use Event Grid for batching the data and to have a dead letter queue.
Arrived in Event Hubs we use an Azure Function to send the data to blob storage and clickhouse.

The only problem is I might need some delay to sending to Clickhouse & Blob Storage (around maybe every 15 minutes) to reduce the risks of memory usage in Clickhouse and to reduce costs.

Can someone help me out? Am I forgetting something crucial? I am a graduated data scientist, however I have no in depth experience with Azure.


",5,2025-04-08 17:18:34
"Hello all,

We are Azure based. One of our vendors recently moved over to Redshift and I'm having a hell of a time trying to figure out how to run stored procedures (either call with a temp return or some database function) from ADF, logic apps or PowerBI. Starting to get worried I'm going to have to spin up a EC2 or lambda or some other intermediate to run the stored procedures, which will be an absolute pain training my junior analysts on how to maintain.

Is there a simple way to call Redshift SP from Azure stack?",5,2025-04-30 17:40:53
"Curious if anyone is brave enough to leave Azure/AWS Databricks for SAP Databricks? Or if you are an SAP shop would you choose that over pure Databricks. From past experiences with SAP I‚Äôve never been a fan of anything they do outside ERP. Personally, I believe you should separate yourself as much as possible for future contract negotiations. Also the risk of limited people singing up and you have a bunch of half baked integrations.",5,2025-04-15 23:21:01
"Hi. I work for the state and some of the tools we have are limited. Each week I go to AWS QuickSight to download a CSV file back to our NAS drive where it feeds my Power BI dashboard. I have a gateway setup for cloud to talk to my on-premise NAS drive so auto refresh works. 

Now, my next task: I want to automate the AWS data directly from Power BI so I don‚Äôt have to log into their website each week but how do I accomplish this without a programming background? (I majored in Asian History so I don‚Äôt know much about data engineering/setting up pipelines)

I read some articles and it seems to indicate that using API can accomplish this but I don‚Äôt know Python/SDKs nor do I use CLI (I did some Powershell) and even if I do what services should I use to run CLI for me behind the scenes? Can Power BI make API calls and handle JSON? 

Thanks üôè ",5,2025-04-02 08:02:21
"Hello Data Engineers,   
  
I am interested in getting your review of the Databricks Academy Labs?   
  
Please if you work for or affiliated to Databricks you aren't invited to provide feedback/review.",5,2025-05-25 16:34:39
"Hello Data Engineers,   
  
For those of you who have enrolled, I am interested in getting your review of the Databricks Blended Learning?   
  
Please if you work for or affiliated to Databricks you aren't invited to provide feedback/review.",5,2025-05-25 16:37:46
"Databricks announces LakeBase - Am I missing something here ? This is just their version of PostGres that they're charging us for ? 

I mean we already have this in AWS and Azure. 
Also, after telling us that Lakehouse is the future, are they now saying build a Kimball style Warehouse on PostGres ? ",5,2025-06-12 00:43:29
"I just want to know why isnt databricks going public ?   
They had so many chances so good market conditions what the hell is stopping them ? ",5,2025-04-30 05:55:33
"Anyone have any experience or ideas getting Databricks data into Excel aside from the ODBC spark driver or whatever? 

I've seen an uptick for requests for raw data for other teams to do data discovery and scoping out future PBI dashboards but it has been a little cumbersome to get them set up with the driver, connected to compute clusters, added to Unity Catalog, etc. Most of them are not SQL experienced so in the past when we had regular Azure SQL we would create views or tables for them to pull into Excel to do their work.

I have a few instances where I drop a csv file to a storage account and then shuffle those around to SharePoint or other locations using a logic app but was wondering if anyone had better ideas before I got too committed to that method.

We also considered backloading some data into a downsized Azure SQL instance because it plays better with Excel but it seems like a step backwards.

Frustrating that PBI has has bunch of direct connectors but Excel (and power automate/logic apps to a lesser extent) seems left out, considering how commonplace it is...",5,2025-04-17 21:13:03
"Hi! I‚Äôm working on a FinOps initiative to improve cloud cost visibility and attribution across departments and projects in our data platform. We do tagging production workflows on department level and can get a decent view in Azure Cost Analysis by filtering on tags like department: X. But I am struggling to bring Databricks into that picture ‚Äî especially when it comes to SQL Serverless Warehouses.

My goal is to be able to print out: total project cost = azure stuff + sql serverless.

**Questions**:

**1. Tagging Databricks SQL Warehouses for Attribution**

Is creating a separate SQL Warehouse per department/project the only way to track department/project usage or is there any other way? 



**2. Joining Azure + Databricks Costs**

Is there a clean way to join usage data from Azure Cost Analysis with Databricks billing data (e.g., from system.billing.usage)?

I'd love to get a unified view of total cost per department or project ‚Äî Azure Cost has most of it, but not SQL serverless warehouse usage or Vector Search or Model Serving. 



**3. Sharing Cost** 

For those of you doing this well ‚Äî how do you present project-level cost data to stakeholders like departments or customers?",5,2025-04-16 11:36:33
"In Azure ADF, how can I invoke a Python scripts on an Azure VM (behind a VPN), if the script can run for several hours and I need the success/failure status returned to the pipeline?",5,2025-04-17 15:04:11
"Hey all,

I‚Äôm in the design phase for a lightweight, map‚Äëcentric web app and would love a sanity check before I start provisioning Azure resources.

Proposed architecture:
- Front‚Äëend: Streamlit container in an Azure Web‚ÄØApp Service. It plots store/parking locations on a Leaflet/folium map.
- Back‚Äëend: FastAPI wrapped in an Azure Function (Linux custom container). DuckDB runs inside the function.
- Data: A ~200‚ÄØMB GeoParquet file in Azure Blob Storage (hot tier).
- Networking: Web‚ÄØApp ‚Üî‚ÄØFunction over VNet integration and Private Endpoints; nothing goes out to the public internet.
- Data flow: User input ‚Üí Web‚ÄØApp calls /locations ‚Üí Function queries DuckDB ‚Üí returns payloads.

Open questions

	1.	Function vs. always‚Äëon container: Is a serverless Azure Function the right choice, or would something like Azure Container Apps (kept warm) be simpler for DuckDB workloads? Cold‚Äëstart worries me a bit.

	2.	Payload format: For ‚â§‚ÄØ200‚ÄØk rows, is it worth the complexity of sending Arrow/Polars over HTTP, or should I stick with plain JSON for map markers? Any real‚Äëworld gains?

	3.	Pre‚Äëprocessing beyond ‚Äúquery from Blob‚Äù: I might need server‚Äëside clustering, hexbin aggregation, or even vector‚Äëtile generation to keep the payload tiny. Where would you put that logic‚Äîinside the Function, a separate batch job, or something else?

	4.	Gotchas: Security, cost surprises, deployment quirks? Anything you wish you‚Äôd known before launching a similar setup?

Really appreciate any pointers, war stories, or blog posts you can share. üôè",5,2025-04-20 13:09:51
"Hi. 

Im looking to moving the computer to an Azure Function being orchestrated by ADF and merge into SQL. 

I need to pick which plan to go with and estimate my usage. I know I'll need VNET.

Im ingesting data from adls2 coming down a synapse link pipeline from d365fo.

Unoptimised ADF pipelines sink to an unoptimised Azure SQL Server.

I need to run the pipeline every 15 minutes with Max 1000 row updates on 150 tables. By my research 1 vCPU should easily cover this on the premium subscription.

Appreciate any assistance.

",5,2025-04-30 06:00:39
"I‚Äôm currently trying to use powerbi‚Äôs native query function to return the result of a stored procedure that returns a temp table. Something like this:

Call dbo.storedprocedure(‚Äòtest‚Äô);
Select * from test;

When run in workbench, I get two results:
-the temp table
-the results of the temp table

However, powerbi stops with the first result, just giving me the value ‚Äòtest‚Äô

Is there any way to suppress the first result of the call function via sql?
",5,2025-04-30 20:40:20
"I have done these two projects:

  
**Real Time Azure Data Lakehouse Pipeline (Netflix Analytics) | Databricks, Synapse Mar. 2025**

‚Ä¢ Delivered a real time medallion architecture using Azure data factory, Databricks, Synapse, and Power BI.

‚Ä¢ Built parameterized ADF pipelines to extract structured data from GitHub and ADLSg2 via REST APIs, with

validation and schema checks.

‚Ä¢ Landed raw data into bronze using auto loader with schema inference, fault tolerance, and incremental loading.

‚Ä¢ Transformed data into silver and gold layers using modular PySpark and Delta Live Tables with schema evolution.

‚Ä¢ Orchestrated Databricks Workflows with parameterized notebooks, conditional logic, and error handling.

‚Ä¢ Implemented CI/CD to automate deployment of notebooks, pipelines, and configuration across environments.

‚Ä¢ Integrated with Synapse and Power BI for real-time analytics with 100% uptime during validation.

**Enterprise Sales Data Warehouse | SQL¬∑ Data Modeling¬∑ ETL/ELT¬∑ Data Quality¬∑ Git Apr. 2025**

‚Ä¢ Designed and delivered a complete medallion architecture (bronze, silver, gold) using SQL over a 14 days.

‚Ä¢ Ingested raw CRM and ERP data from CSVs (>100KB) into bronze with truncate plus insert batch ELT,

achieving 100% record completeness on first run.

‚Ä¢ Standardized naming for 50+ schemas, tables, and columns using snake case, resulting in zero naming conflicts across 20 Git tracked commits.

‚Ä¢ Applied rule based quality checks (nulls, types, outliers) and statistical imputation resulting in 0 defects.

‚Ä¢ Modeled star schema fact and dimension tables in gold, powering clean, business aligned KPIs and aggregations.

‚Ä¢ Documented data dictionary, ER diagrams, and data flow



**QUESTION: What would be a step up from this now?**   
**I think I want to focus on Azure Data Engineering solutions.** ",5,2025-04-20 19:09:01
"I work as Data Engineer in manufacturing company. I deal with databricks on Azure + SAP Datasphere. Big data? I don't thinks so, 10 GB most of the times loaded once per day, mostly focusing on easy maintenance/reliability of pipeline. Data mostly ends up as OLAP / reporting data in BI for finance / sales / C level suite.
Could you let me know what dangers you see for my position? I feel like not working with streaming / extremely hard real time pipelines makes me less competitive on job market in the long run. 
Any words of wisdom guys?",5,2025-05-01 09:55:46
"**TL;DR:** Open source ""dbt for API integration"" - SQL-centric, git-friendly, no vendor lock-in. Code-first approach to API workflows.

Hey r/dataengineering,

We built Sequor to solve a recurring problem: choosing between two bad options for API/app integration:

1. Proprietary black-box SaaS connectors with vendor lock-in
2. Custom scripts that are brittle, opaque, and hard to maintain

As data engineers, we wanted a solution that followed the principles that made dbt so powerful (code-first, git-based version control, SQL-centric), but designed specifically for API integration workflows.

**What Sequor does:**

* Connects APIs to your databases with an iterator model
* Uses SQL for all data transformations and preparation
* Defines workflows in YAML with proper version control
* Adds procedural flow control (if-then-else, for-each loops)
* Uses Python and Jinja for dynamic parameters and response mapping

**Quick example:** 

* Data acquisition: Pull Salesforce leads ‚Üí transform with SQL ‚Üí push to HubSpot ‚Üí all in one declarative pipeline.
* Data activation (Reverse ETL): Pull customer behavior from warehouse ‚Üí segment with SQL ‚Üí sync personalized offers to Klaviyo/Mailchimp
* App integration: Pull new orders from Amazon ‚Üí join with SQL to identify new customers ‚Üí create the customers and sales orders in NetSuite
* App integration: Pull inventory levels from NetSuite ‚Üí filter with SQL for eBay-active SKUs ‚Üí update quantities on eBay

**How it's different from other tools:**

Instead of choosing between rigid and incomplete prebuilt integration systems, you can easily build your own custom connectors in minutes using just two basic operations (**transform** for SQL and **http\_request** for APIs) and starting from prebuilt examples we provide.

The project is open source and we welcome any feedback and contributions.

**Links:**

* Website: [https://sequor.dev/](https://sequor.dev/) (includes code examples)
* Quickstart: [https://docs.sequor.dev/getting-started/quickstart](https://docs.sequor.dev/getting-started/quickstart)
* GitHub: [https://github.com/paloaltodatabases/sequor](https://github.com/paloaltodatabases/sequor)
* Examples of prebuilt integrations: [https://github.com/paloaltodatabases/sequor-integrations](https://github.com/paloaltodatabases/sequor-integrations)

**Questions for the community:**

* What's your current approach to API integrations?
* What business apps and integration scenarios do you struggle with most?
* Are there specific workflows that have been particularly challenging to implement?",6,2025-05-28 18:11:03
"I have had eyes on dbt for years. I think it helps with well-organized processes and clean code. I have never used it further than a PoC though because my company uses a lot of Python for data processing. Some of it could be replaced with SQL but some of it is text processing with Python NLP libraries which I wouldn‚Äôt know how to do in SQL. And dbt Python models are only available for some cloud database services while we use Postgres on-prem, so no go here.

Now finally for the question: can you point me to software/frameworks that
- allow Python code execution
- build a DAG like dbt and only execute what is required
- offer versioning where you could ‚Äûgo back in time‚Äú to obtain the state of data like it was half a year before
- offer a graphical view of the DAG
- offer data lineage 
- help with project structure and are not overly complicated 

It should be open source software, no GUI required. If we would use dbt, we would be dbt-core users.

Thanks for hints!",6,2025-05-28 18:04:28
"My company is considering Coalesce.io and dbt. I used dbt at my last job and loved it, so I'm already biased. I haven't tried Coalesce yet. Anybody tried both? 

I'd like to know how well coalesce does version control - can I see at a glance how transformations changed between one version and the next? Or all the changes I'm committing?
",6,2025-04-25 23:14:13
"I‚Äôd like to implement jinja templated SQL for a project. But I don‚Äôt want or need DBT‚Äôs extra bells and whistles. I just need/want to write macros, templated .sql files, then on execution (from python application), render the SQL at runtime.

What‚Äôs the solution here? Pure jinja? (What‚Äôre some resources for that?) Are there OSS libraries I can use? Or, do I just use DBT, but only use it from a python driver?",6,2025-04-01 16:26:46
Has anyone migrated their dbt cloud to sqlmesh? If so what tools did you use? How many models? How much time did take? Biggest pain points?,6,2025-04-14 08:38:03
"Hi. 

Just trying to use the new VSCode extension from dbt. Requires dbt Fusion which I‚Äôve setup but when trying to view lineage I keep getting the extension complaining about ‚Äúdbt language server is not running in this workspace‚Äù.

Anyone else getting this?

",6,2025-06-05 06:09:44
"My company uses DBT in the transform/silver layer of our quasi-medallion architecture. It's a very small DE team (I'm the second guy they hired) with a historic reliance on low-code tooling I'm helping to migrate us off for scalability reasons.

Previously, we moved data into the report layer via the webhook notification generated by our DBT build process. It pinged a workflow in N8n which ran an ungainly web of many dozens of nodes containing copy-pasted and slightly-modified SQL statements executing in parallel whenever the build job finished. I went through these queries and categorized them into general patterns and made Jinja templates for each pattern. I am also in the process of modifying these statements to use materialized views instead, which is presenting other problems outside the scope of this post.

I've been wondering about ways to manage templated SQL. I had an idea for a Python package that worked with a YAML schema that organized the metadata surrounding the various templates, handled input validation, and generated the resulting queries. By metadata I mean parameter values, required parameters, required columns in the source table, including/excluding various other SQL elements (e.g. a where filter added to the base template), etc. Something like this:

    default_params: 
      distinct: False 
      query_type: default 
    
    ## The Jinja Templates 
    query_types: 
      active_inactive: 
        template: |
          create or replace table `{{ report_layer }}` as 
          select {%if distinct%}distinct {%-endif}*
          from `{{ transform_layer }}_inactive`
          union all 
          select {%if distinct%}distinct {%-endif}*
          from `{{ transform_layer }}_active`
      master_report_vN_year: 
        template: | 
          create or replace table `{{ report_layer }}` AS 
          select *
          from `{{ transform_layer }}`
          where project_id in (
              select distinct project_id
              from `{{ transform_layer }}`
              where delivery_date between `{{ delivery_date_start }}` and `{{ delivery_date_end }}`
          )
        required_columns: [
          ""project_id"",
          ""delivery_date""
        ]
        required_parameters: [
          ""delivery_date_start"", 
          ""delivery_date_end""
        ]
    
    ## Describe the individual SQL models here 
    materialization_blocks: 
      mz_deliveries: 
        report_layer: ""<redacted>""
        transform_layer: ""<redacted>""
        params:
          query_type: active_inactive
          distinct: True

Would be curious to here if something like this exists already or if there's a better approach.",6,2025-04-29 19:14:12
"My company is considering Coalesce.io and dbt. I used dbt at my last job and loved it, so I'm already biased. I haven't tried Coalesce yet. Anybody tried both? 

I'd like to know how well coalesce does version control - can I see at a glance how transformations changed between one version and the next? Or all the changes I'm committing?
",6,2025-04-25 23:14:13
"I know that dbt announced a Power Bi Semantic Layer connector recently but I'm finding it hard to understand how this operates or how beneficial it might be in practice. I don't currently have a dbt project set up so I can't test it myself right now, but I'm curious to learn more as I might be suggesting either dbt or SQLMesh for a POC in my place of work.

Are any of you actively using this connector?

If so, can you let me know what it looks like in action? For example:

- how did you configure your metrics?
- are they shared across reports?
- is this a feasible solution?
- what works and what doesn't?

Thanks.",6,2025-05-01 05:41:52
"Company has the setup in the title. Why would our data engineering team use amundsen for documentation  and another program that‚Äôs tied to a Google sheet (the name which escapes me) and not just use dbt documentation and tests? Especially with the dbt power user VS Code extension? Am I missing something? I asked around and folks can only say ‚Äúit is what it is.‚Äù It‚Äôs frustrating too at times when I can‚Äôt even run dbt commands because docker doesn‚Äôt like to play nice with my mandated vpn. 
What‚Äôs the purpose of not using dbt to its fullest extent? 

Edit: I meant dbt Power User for VS Code. Not dbt hero. ",6,2025-05-24 00:53:03
"Hiya. The duckdb's Ducklake is just fresh out of the oven. The ducklake uses a special type of 'attach' that does not use the standard 'path' (instead ' data_path'), thus making dbt and sqlmesh incompatible with this new extension. At least that is how I currently perceive this.

However, I am not an expert in dbt or sqlmesh so I was hoping there is a smart trick i dbt/sqlmesh that may make it possible to use ducklake untill an update comes along.

Are there any dbt / sqlmesh experts with some brilliant approach to solve this?

EDIT:
Is it possible to handle the attach ducklake with macros before each model?",6,2025-05-28 12:43:17
"I just published a practical breakdown of a method I call **Observe & Fix** ‚Äî a simple way to manage data quality in DBT without breaking your pipelines or relying on external tools.

It‚Äôs a self-healing pattern that works entirely within DBT using native tests, macros, and logic ‚Äî and it‚Äôs ideal for fixable issues like duplicates or nulls.

Includes examples, YAML configs, macros, and even when to alert via Elementary.

Would love feedback or to hear how others are handling this kind of pattern.

üëâ[Read the full post here ](https://medium.com/@baruchjacob/self-healing-pipelines-with-dbt-the-observe-fix-method-9d6b2da4eae3)",6,2025-04-13 19:41:47
"I have been using dbt for over 1 year now i moved to a new company and while there is a lot of documentation for DBT, what I have found is that it's not particularly well laid out unlike documentation for many python packages like pandas, for example, where you can go to a particular section and get an exhaustive list of all the options available to you.

 I find that Google is often the best way to parse my way through DBT documentation. It's not clear where to go to find an exhaustive list of all the options for yml files is so I keep stumbling across new things in dbt which shouldn't be the case. I should be able to read through documentation and find an exhaustive list of everything I need does anybody else find this to be the case? Or have any tips",6,2025-04-20 20:39:58
"Hi all,

I'm working at a company that uses three main branches:¬†`development`,¬†`testing`, and¬†`production`.

I created a feature branch called¬†`feature/streaming-pipelines`, which is based off the¬†`development`¬†branch. Currently, my feature branch is¬†**3 commits behind**¬†and¬†**2 commits ahead**¬†of¬†`development`.

I want to update my feature branch with the latest changes from¬†`development`¬†**without risking anything in the shared repo**. This repo includes not just code but also other important objects.

What Git commands should I use to safely bring my branch up to date? I‚Äôve read various things online , but I‚Äôm not confident about which approach is safest in a shared repo.

I really don‚Äôt want to mess things up by experimenting. Any guidance is much appreciated!

Thanks in advance!",6,2025-06-09 18:33:27
"Hey dbt folks,

I'm a data engineer and use dbt on a day-to-day basis, my team and I were struggling to find a good open-source tool for user-friendly column-level lineage visualization that we could use daily, similar to what commercial solutions like dbt Cloud offer. So, I decided to start building one...

https://reddit.com/link/1jnh7pu/video/wcl9lru6zure1/player

You can find the repo [here](https://github.com/Fszta/dbt-column-lineage), and the package on [pypi](https://pypi.org/project/dbt-col-lineage/0.1.1/)

**Under the hood**

Basically, it works by combining dbt's manifest and catalog with some compiled SQL parsing magic (big shoutout to sqlglot!).

I've built it as a CLI, keeping the syntax similar to dbt-core, with upstream and downstream selectors.

    dbt-col-lineage --select stg_transactions.amount+ --format html

Right now, it supports:

* Interactive HTML visualizations
* DOT graph images
* Simple text output in the console

**What's next ?**

* Focus on compatibility with more SQL dialects
* Improve the parser to handle complex syntax specific to certain dialects
* Making the UI less... basic. It's kinda rough right now, plus some information could be added such as materialization type, col typing etc

Feel free to drop any feedback or open an issue on the [repo](https://github.com/Fszta/dbt-column-lineage/tree/main)! It's still super early, and any help for testing on other dialects would be awesome. It's only been tested on projects using Snowflake, DuckDB, and SQLite adapters so far.",6,2025-03-30 17:11:12
"Hello guys,
Lately I‚Äôve been using dbt in a project and I feel like it‚Äôs some pretty simple stuff, just a bunch of models that I need to modify or fix based on business feedback, some SCD and making sure the tests are passed. 
For those using dbt, how ‚Äúcomplex‚Äù your projects get? How difficult you find it?

Thank you!

",6,2025-06-03 14:27:08
"Hey dbt folks,

I'm a data engineer and use dbt on a day-to-day basis, my team and I were struggling to find a good open-source tool for user-friendly column-level lineage visualization that we could use daily, similar to what commercial solutions like dbt Cloud offer. So, I decided to start building one...

https://reddit.com/link/1jnh7pu/video/wcl9lru6zure1/player

You can find the repo [here](https://github.com/Fszta/dbt-column-lineage), and the package on [pypi](https://pypi.org/project/dbt-col-lineage/0.1.1/)

**Under the hood**

Basically, it works by combining dbt's manifest and catalog with some compiled SQL parsing magic (big shoutout to sqlglot!).

I've built it as a CLI, keeping the syntax similar to dbt-core, with upstream and downstream selectors.

    dbt-col-lineage --select stg_transactions.amount+ --format html

Right now, it supports:

* Interactive HTML visualizations
* DOT graph images
* Simple text output in the console

**What's next ?**

* Focus on compatibility with more SQL dialects
* Improve the parser to handle complex syntax specific to certain dialects
* Making the UI less... basic. It's kinda rough right now, plus some information could be added such as materialization type, col typing etc

Feel free to drop any feedback or open an issue on the [repo](https://github.com/Fszta/dbt-column-lineage/tree/main)! It's still super early, and any help for testing on other dialects would be awesome. It's only been tested on projects using Snowflake, DuckDB, and SQLite adapters so far.",6,2025-03-30 17:11:12
"This problem exists for most Data tooling, not just DBT.

  
Like a really basic thing would be how can we do proper incident management from log to alert to tracking to resolution. ",6,2025-04-21 08:29:00
"Hey all,

I‚Äôm using Snowflake for our data warehouse and just recently got our team set up with Git/source control. Now we‚Äôre looking to roll out either dbt or SQLMesh for transformations (I've been able to sell the team on its value as it's something I've seen work very well in another company I worked at).

One of the biggest unknowns (and requirements the team has) is tracking column-level lineage across dbt/SQLMesh and Power BI.

Essentially, I want to find a way to use a DAG (and/or testing on a pipeline) to track dependencies so that we can assess how upstream database changes might impact reports in Power BI.

For example: if an employee opens a pull/merge request in GIT to modify TABLE X (change/delete a column), running a command like 'dbt run' (crude example, I know) would build everything downstream and trigger a warning that the column they removed/changed is used in a Power BI report.

Important: it has to be at a column level. Model level is good to start but we'll need both.

Has anyone found good ways to manage this?

I'd love to hear about any tools, workflows, or best practices that are relevant.

Thanks!",6,2025-04-25 04:39:42
"Implemented **SCD Type 2** in dbt for tracking historical host changes. Now I can analyze how hosts evolve over time and their impact on reviews and pricing! üîÑ  
\#dezoomcamp #dbt #datawarehousing",6,2025-03-31 00:55:39
"Hi Data folks,

A few weeks ago, I got some validation:

* This is a real need¬†(thanks u/\[PrincipalEngineer\])
* Add BigQuery or GTFO

So, After nights of coffee-fueled coding, we‚Äôve got an¬†**imperfect**¬†version of Tesser that now has some additional features:

* Support for Bigquery as a source
* Trace a column¬†from Snowflake ‚Üí BigQuery ‚Üí Looker in 2 clicks
* Find who broke revenue¬†by tracking ad-hoc queries (Slack, notebooks, etc.)
* Shows lineage for ALL SQL¬†‚Äì not just your 'proper' dbt models

Disclaimer: The UI‚Äôs still ugly & WIP, but the core works.

**need to hear your perspective**:

* ‚ÄúWould you use this daily if we added \[X\]?‚Äù
* ‚ÄúWhat‚Äôs the dumbest lineage issue you‚Äôve faced?‚Äù¬†(I‚Äôll fix it next.)

If this isn‚Äôt useful, tell us why‚Äî we'll pivot fast.",6,2025-06-07 18:18:56
"Would like to learn more about experiences while using dbt with glue as it was primarily used in data warehouses and then with popularity growing , more connectors were built such as for glue. 

",6,2025-05-25 20:19:28
"This might be an open-ended question, but I recently spoke with someone who had migrated an old ETL process‚Äîoriginally built with stored procedures‚Äîover to DBT. It was running on Oracle, by the way. He mentioned that using DBT led to the creation of many more steps or models, since best practices in DBT often encourage breaking large SQL scripts into smaller, modular ones. However, he also said this made the process slower overall, because the Oracle query optimizer tends to perform better with larger, consolidated SQL queries than with many smaller ones.

Is there some truth to what he said, or is it just a case of him not knowing how to use the tools properly",6,2025-05-28 07:33:44
"Wondering if doing a hard delete in fivetran is possible without a dbt connector. I did my initial sync, go to transformations and can't figure out how to just add a sql statement to run after each sync.",6,2025-04-15 19:02:28
"According to the DBT Cloud [api](https://docs.getdbt.com/dbt-cloud/api-v2#/operations/Retrieve%20Run%20Failure%20Details), I can only tell that a job has failed and retrieve the failure details. 

There's no way for me to know when a job is hung.

Yesterday, an issue with our Fivetran replication and several of our DBT jobs hung for several hours.

Any idea how to monitor for hung DBT jobs?",6,2025-04-08 19:45:39
"I am a new learner and have recently learned more about tools such as DuckDB and DBT. 

As suggested by the title, I have some questions as to why DBT is used when you can quite possibly handle most transformations in DuckDB itself using SQL queries or pandas.

Additionally, I also want to know what tradeoff there would be if I use DBT on DuckDB before loading into the data warehouse, versus loading into the warehouse first before handling transformation with DBT? ",6,2025-05-02 03:24:22
"One read-only for my BI tool, and one read-write for dbt/sqlmesh

Then I'd use it for almost every project",6,2025-06-03 15:07:46
"I'm setting up a Glue (Spark) to Redshift pipeline with incremental SQL loads, and while fact tables are straightforward (just append new records), dimension tables are more complex to be honest - I have a few questions regarding the practical implementation of a star schema data warehouse model ?   
  
First, avoiding duplicates, transactional facts won't have this issue because they will be unique, but for dimensions it is not the case,  do you pre-filter in Spark (reads existing Redshift dim tables and ensure new chunks of dim tables are new records) or just dump everything to Redshift and let it deduplicate (let Redshift handle upinserts)?   
  
Second, surrogate keys, they have to be globally unique across all the table because they will serve as primary keys, do you generate them in Spark (risk collisions across job runs) or use Redshift IDENTITY for example?   
  
Third, SCD Type 2: implement change detection in Spark (comparing new vs old records) or handle it in Redshift (with MERGE/triggers)? Would love to hear real-world experiences on what actually scales, especially for large dimensions (10M+ rows) - how do you balance the Spark vs Redshift work while keeping everything consistent?

Last but not least I want to know how to ensure fact tables are properly pointing to dimension tables, do we fill the foreign key column in spark before loading to redshift? 

PS: if you have any learning resources with practical implementations and best practices in place please provide them, because I feel the majority of the info on the web is theoretical.   
Thank you in advance.",7,2025-04-17 12:51:24
"
Hi everyone,
I‚Äôm working as a data engineer on a project that follows a Medallion Architecture in Synapse, with bronze and silver layers on Spark, and the gold layer built using Serverless SQL.

For a specific task, the requirement is to replicate multiple source views exactly as they are ‚Äî without applying transformations or modeling ‚Äî directly from the source system into the gold layer. In this case, the silver layer is being skipped entirely, and the gold layer will serve as a 1:1 technical copy of the source views.

While working on the development, I noticed that some of these source views contain duplicate records. I recommended introducing logical business keys to ensure uniqueness and preserve data quality, even though we‚Äôre not implementing dimensional modeling. However, the team responsible for the source system insists that the views should be replicated as-is and that it‚Äôs unnecessary to define any keys at all.

I‚Äôm not convinced this is a good approach, especially for a layer that will be used for downstream reporting and analytics.

What would you do in this case?
Would you still enforce some form of business key validation in the gold layer, even when doing a simple pass-through replication?

Thanks in advance.",7,2025-06-05 21:12:17
"I have a provider and customer dimensions, the ids for these dimensions were created through a mapping table, however each provider or customer can have multiple ids per source or across sources so including these ‚Äúsource ids‚Äù into my final dimensions would kinda deflect the purpose of the deduplication and mapping done previously. Do you guys think it‚Äôs necessary to include these ids for a basic sales analysis?",7,2025-04-25 21:35:02
"**Question:**

I am currently learning Google Cloud Platform for data engineering. I learned that there are three types of schemas that I can use when constructing tables in BigQuery: 1) Normalized relational schema, 2) Nested & Repeating Schema, 3) Denormalized schema. I am trying to understand when I will realistically use ""Nested & Repeating Schema"" instead of ""normalized relational schema"" for the tables that I construct in BigQuery.



Please answer both of these questions below:

1. When do you use ""Nested & Repeating Schema"" over ""normalized relational schema"" when you construct tables in BigQuery?

2. When constructing tables within BigQuery data warehouses, how often do you use ""Nested & Repeating Schema""? How often do you use ""normalized relational schema""? If possible, please provide me a ballpark percentage (Ex. 40% Nested & Repeating Schema vs. 60% normalized relational schema).

  


**My Current Rationale:**

I understand that BigQuery is a columnar oriented database. I learned that ""Nested & Repeating Schema"" is a more cost-effective for querying and more efficient than ""normalized relational schema"". However, even after researching it, I do not fully understand the real life advantages of ""Nested & Repeating Schema"" over a ""normalized relational schema"".

Although ""Nested & Repeating Schema"" is more efficient and cost-effective for querying, I think a ""normalized relational schema"" makes more sense because it allows you to update records more easily like a traditional SQL RDBMS.

I understand that columnar oriented databases are great when your historical data within the BigQuery table does not change. However, from my experience on working as a data analyst, historical data frequently needs to change. For example, lets say you have an external OLTP RDBMS that feeds into BigQuery daily. This external OLTP RDBMS contains a table named sales data. This table contains a column named ""Member Status"" and returns either one of two outputs: ""Active"" or ""Inactive"". ""Member ID"" 123456 has a ""Member Status"" of ""Active"". The data for that daily load is sent from the external OLTP RDBMS to the BigQuery table containing the data of ""Member ID"" 123456 with a ""Member Status"" of ""Active"". Three months later, the ""Member Status"" of ""Member ID"" 123456 changes to ""Inactive"" within the external OLTP RDBMS.

From my understanding, now I cannot change that data easily within the BigQuery table if it has ""Nested & Repeating Schema"" . If my BigQuery table had ""normalized relational schema"", it should be able to update the ""Member Status"" of ""Member ID"" 123456 very easily.

This is my rationale on why I think a ""normalized relational schema"" is better than ""Nested & Repeating Schema"" for the majority of real world use cases. 

Please let me know if you agree, disagree, etc. I would love to hear your thoughts. I am still learning GCP and data engineering.

  
Thank you for reading.    :)",7,2025-05-25 20:07:28
"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",7,2025-03-30 13:12:50
"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",7,2025-03-30 13:12:50
"Apologies if this is a silly question, but I'm trying to understand how clustering actually works  / processes, when it's applied / how it's applied in BigQuery.

Reason being I'm trying to help myself answer questions like, if we have an incremental model with a merge strategy then does clustering get applied when the merge is looking to find a row match on the unique key defined, and updates the correct attributes? Or is clustering only beneficial for querying and not ever for table generation?",7,2025-04-26 02:22:43
"Hi everyone, I'm currently using an NVIDIA Tesla V100  32GB with CUDF to do som transformation on a dataset. The response  time for the operations I'm doing is good, however, I'm wondering what is the best approach to do some grouping operations in some SQL database.  Assuming I'm allowed to create a DB architecture from scratch, what is my best option? Is Indexing a good idea or is there something else (better) for my use case?

Thanks in advance.

  
EDIT: Thank you very much for the response to all of you,  I tried Clickhouse as many of you suggested and holy cow, it is insane what it does. I didn't bulk all the data into the DB yet, but I tried with a subset of 145 GB, and got the following metrics: 

465 rows in set. Elapsed: 4.333 sec. **Processed 1.23 billion rows**, 47.36 GB (284.16 million rows/s., 10.93 GB/s.). Peak memory usage: 302.26 KiB.

  
I'm not sure if there is any way to even improve the response time, but I think I'm good with what I got. By the way, the database is pretty simple:

  
| DATE | COMPANY\_ID | FIELD 1 | ..... | .... | ......| .... | ..... | FIELD 7 | 

  
The query I was: 

  
SELECT FIELD 1, FIELD 2, COUNT(\*) FROM test\_table GROUP BY FIELD 1, FIELD 2;",7,2025-05-27 00:08:27
"Hey all, 

1. I have two tables which are about 20-30 gbs and I created a backfill for them as I noticed that two days data was missing, now after an hour the backfill completed, now I am seeing some items in the streaming buffer, I need to update my seniors when the data is ready for analysis, so when can I safely say the data is present?

2. Also, one more question, if I insert a row manually into Bigquery and then create a backfill for it to fetch the data again from transactional database, will the entry I added manually (which doesn't exist in transactional database) be erased?

3. Is there a way to track the ingestion of data into BigQuery?

",7,2025-04-02 08:57:23
"Hi,

Our main OLTP database is an RDS Aurora Postgres database and it's working well but we need to perform some analytics queries that we currently do on a read replica but some of those queries are quite slow and we want to offload all of this to an OLAP or OLAP-like database. Most of our data is similar to a time-series so we thought of going with another Postgres instance but with Timescale installed to create aggregate functions. We mainly need to keep sums / averages / of historical data and timescale seems like a good fit for this.

The problem I have is how can I keep RDS -> Postgres in sync? Our use-case cannot really have batched data because our services need this analytics data to perform domain decisions (has a user reached his daily transactions limit for example) and we also want to offload all of our grafana dashboards from the main database to Timescale.

What do people usually use for this? Debezium? Logical Replication? Any other tool?

We would really like to keep using RDS as a source of truth but offload all analytics to another DB that is more suited for this, if possible.

If so, how do you deal with an evolving DDL schema over time, do you just apply your DB migrations to both DBs and call it a day? Do you keep a completely different schema for the second database?

Our Timescale instance would be hosted in K8s through the CNPG operator.

I want to add that we are not 100% set on Timescale and would be open to other suggestions. We also looked at Starrocks, a CNCF project, which looks promising but a bit complex to get up and running.",7,2025-06-12 15:49:31
"I run an analytics team at a mid sized company. We currently use redshift as our primary data warehouse. I see all the time arguments about how redshift is slower, not as feature rich, has bad concurrency scaling etc. etc. I've discussed these points with leadership but they, i think understandably push back on the idea of a large migration which will take our team out of commission. 

  
I was curious to hear from other folks what they've seen in terms of business cases for a major migration like this? Has anyone here ever successfully convinced leadership that a migration off of redshift or something similar was necessary?",7,2025-06-11 21:39:53
"I have to come clean: I am an ML Engineer always lurking in this community.  
  
We have a fraud detection model that depends on many time based aggregations e.g. `customer_number_transactions_last_7d`.

We have to compute these in real-time and we're on GCP, so I'm about to redesign the schema in BigTable as we are p99ing at 6s and that is too much for the business. We are currently on a combination of BigTable and DataFlow.

  
So, I want to ask the community: what do you use?  
  
I for one am considering a timeseries DB but don't know if it will actually solve my problems.

If you can point me to legit resources on how to do this, I also appreciate.",7,2025-04-23 16:46:48
"# Our Current Data Pipeline

* PostgreSQL OLTP database as source
* Data pipeline moves data to BigQuery at different frequencies:
   * Critical tables: hourly
   * Less critical tables: daily
* Two datasets in BigQuery:
   * **Raw dataset**: Always appends new data (similar to SCD Type 2 but without surrogate keys, current flags, or valid\_to dates)
   * **Clean dataset**: Only contains latest data from raw dataset

# Our Planned Revamp

We're implementing dimensional modeling to create proper OLAP tables.

**Original plan:**

1. Create DBT snapshots (SCD Type 2) from raw dataset
2. Build dimension and fact tables from these snapshots

**Problem:**

* SCD Type 2 implementation is resource-intensive
* Causes full table scans in BigQuery (expensive)
* Requires complex joins and queries

# The Reality of Our Analytics Needs

* Analytics team **only uses latest data** for insights
* Historical change tracking isn't currently used
* Raw dataset already exists if historical analysis is needed in rare cases

# Our Potential Solution

Instead of creating snapshots, we plan to:

* Skip the SCD Type 2 snapshot process entirely
* Build dimension tables (SCD Type 1) directly from our raw tables
* Leverage the fact that our raw tables already implement a form of SCD Type 2 (they contain historical data through append-only inserts)
* Update dimensions with latest data only

This approach would:

* Reduce complexity
* Lower BigQuery costs
* Match current analytics usage patterns
* Still allow historical access via raw dataset if needed

# Questions

1. Is our approach to implement SCD Type 1 reasonable given our specific use case?
2. What has your experience been if you've faced similar decisions?
3. Are there drawbacks to this approach we should consider?

Thanks for any insights you can share!",7,2025-04-26 18:40:10
"We're considering moving from a dated ETL system to dbt with data being ingested via AWS Glue.

We have a data warehouse which uses a Kimball dimensional model, and I am wondering how we would migrate the dimension load processes.

We don't have access to all historic data, so it's not a case of being able to look across all files and then pull out the dimensions. Would it make sense fur the dimension table to be bothered a source and a dimension?

I'm still trying to pivot my way of thinking away from the traditional ETL approach so might be missing something obvious.",7,2025-04-08 15:09:02
"In a database, how di you manage to keep memory of changes in the rows. I am thinking about user info that changes, contracts type, payments type and so on but that it is important that one has the ability to track hitorical beahviour in case of backtests or kpis history.

How do you get it?  ",7,2025-04-22 19:58:37
"Hey yall,

  
I stumbled upon this linkedin post today and thought it was really insightful and well written, but I'm getting tripped up on the idea that wide tables are inherently bad within the silver layer. I'm by no means an expert and would like to make sure I'm understanding the concept first. 

Is this article claiming that if I have, say, a dim\_customers table, that to widen that table with customer attributes like location, sign up date, size, etc. that I will create a brittle architecture? To me this seems like a standard practice, as long as you are maintaining the grain of the table (1 customer per record). I also might use this table to join in all of the ids from various source systems. This makes it easy to investigate issues and increases the tables reusability IMO.

Am I misunderstanding the article maybe, or is there a better, more scalable approach than what I'm currently doing in my own work?

  
Thanks!",7,2025-06-03 22:29:41
"Im looking for suggestions on what infrastructure and techniques to use to achieve these requirements. I want to keep it simple, easy to maintain and understand. I dont need scalability at this time.

I have a requirement to design a data warehouse in redshift that supports the ability to query past data states similarly to temporal tables in MS SQL Server. (if an update query is made, I need to be able to query for what the table looked like before the update) this is sometimes called ""time travel query"" or ""point in time architecture"" depending on your background. The data sources do not retain this historical data, and are not in an ideal data warehouse schema, so Ill need to transform the data either before or after loading it, and maintain the historical records. Redshift seems to lack a direct solution for this problem.

a second requirement is to ingest the data using streaming technology such as kafka. though the data warehouse does not have to be updated in real time. that is optional.

I have looked at redshift's ""history mode"" but its quite new and it looks like all the data would need to go into RDS first, which has tradeoffs. but one of the main data sources is already on RDS, so that seems promising.

total data volume is low, no need for cluster computing if we can save some complexity.

I would prefer to lean toward python and sql for programming.

I would prefer to do things in real-time, but would accept batches if a particularly elegant solution is available.

thanks for considering :D",7,2025-06-03 19:38:21
"I had a small ‚Äúargument‚Äù at the office today. I am building a fact table to aggregate session metrics from our Google Analytics environment. One of the columns is the of course the session‚Äôs datetime. There are multiple reports and dashboards that do analysis at hour granularity. Ex : ‚ÄúWhat hour are visitors from this source more likely to buy hour product?‚Äù

To address this, I creates a date and time dimention. Today, the Data Specialist had an argument with me and said this is suboptimal and a single timestamp dimention should have been created. I though this makes no sense since it would result in extreme redudancy : you would have multiple minute rows for a single day for example. 

Now I am questioning my skills as he is a specialist and teorically knows better. I am failing to understand how a single timestamp table is better than seperates time and date dimentions",7,2025-04-15 22:39:58
"I have to come up with a database design working on postgres. I have to migrate at the end almost trillions volumes of data into a postgres DB wherein CRUD operations can be run most efficiently. The data present is in the form of a many to many relationship. How the data looks is:  
  
In my old data base i have a value T1 which is connected to on average 700 values (like x1,x2,x3...x700). Here in the old DB we are saving 700 records of this connection. Similarly other values like T2,T3,T100 all have multiple connections each having a separate row

Use case:  
We need to make updates,deletions and inserts to both values of T and values of X  
for example,  
I am given That value T1 instead of 700 connections of X has now 800 connections...so i must update or insert all the new connections corresponding to T1  
And like wise if I am given , we need to update all T values X1 (say X1 has 200 connection of T) i need to insert/update or delete T values associated with X1.

  
for now, I was thinking of aggregating my data in the form of a jsonb column  
where  
Column T             Column X (jsonb)  
T1                           {""value"":\[X1,X2,X3.....X700\]}

But i will have to create another similar table where i keep column T as jsonb. Since any updates in one table needs to be synced to the other any errors may cause it to be out of sync.

Also the time taken to read and update a jsonb row will be high

Any other suggestions on how i should think about creating schema for my problem?",7,2025-04-15 09:41:51
"**Context:**

I am currently learning data engineering and Google Cloud Platform (GCP).

I am currently constructing an OLAP data warehouse within BigQuery so data analysts can create Power BI reports.

The example OLAP table is:  
\* Member ID (Not repeating. Primary Key)

\* Member Status (Can repeat. Is an array)

\* Date Modified (Can repeat. Is an array)

\* Sold Date (Can repeat. Is an array)

I am facing a rookie dilemma - I highly prefer to use ""nested & repeating"" schema because I like how everything is organized with this schema. However, I should also consider partitioning and clustering the data because it will reduce query execution costs. It seems like I can only partition and cluster the data if I use a ""denormalized"" schema. I am not a fan of ""denormalized"" schema because I think it can duplicate some records, which will confuse analysts and inflate data. (Ex. The last thing I want is for a BigQuery table to inflate revenue per Member ID.).

**Question:**

My questions are this:

1) In your data engineering job, when constructing OLAP data warehouse tables for data analysis, do you ever use partitioning and clustering?

2) Do you always use ""nested & repeating"" schema, or do you sometimes use ""denormalized schema"" if you need to partition and cluster columns? I want my data warehouse tables to have proper schema for analysis while being cost-effective.",7,2025-05-28 12:53:05
"
Hey everyone,

I‚Äôm working as a data engineer at a large marketplace company. We process over 3 million transactions per month and receive more than 20 million visits to our website monthly.

We‚Äôre currently trying to integrate data from Google Analytics 4 (GA4) and BigQuery into our AWS-based architecture, where we use S3, Redshift, dbt, and Tableau for analytics and reporting.

However, we‚Äôre running into some issues with the ETL process ‚Äî especially when dealing with the semi-structured NoSQL-like GA4 data in BigQuery. We‚Äôve successfully flattened the arrays into a tabular model, but the resulting tables are huge ‚Äî both in terms of columns and rows ‚Äî and we can‚Äôt run dbt models efficiently on top of them.

We attempted to create intermediate, smaller tables in BigQuery to reduce complexity before loading into AWS, but this introduced an extra transformation layer that we‚Äôd rather avoid, as it complicates the pipeline and maintainability.

I‚Äôd like to implement an incremental model in dbt, but I‚Äôm not sure if that‚Äôs going to be effective given the way the GA4 data is structured and the performance bottlenecks we‚Äôve hit so far.

Has anyone here faced similar challenges with integrating GA4 data into an AWS ecosystem?

How did you handle the schema explosion and performance issues with dbt/Redshift?

Any thoughts on best practices or architecture patterns would be really appreciated.

Thanks in advance!",7,2025-05-28 21:20:48
"Hi everyone, 

I‚Äôm trying to build a Streamlit app that, among other things, uses polygons to highlight areas on a map. My plan was to store them in BigQuery and pull them from there. However, the whole table is 1GB, with one entry per polygon, and there‚Äôs no way to cluster it.

This means that every time I pull a single entry, BigQuery scans the entire table. I thought about loading them into memory and selecting from there, but it feels like a duct-taped solution.

Anyway, this is my first time dealing with this format, and I‚Äôm not a data engineer by trade, so I might be missing something really obvious. I thought I‚Äôd ask.

Cheers :)",7,2025-05-24 09:34:40
"So first of all, it's not ""urgent"", this happened in a non prod environment, while running tests.

I don't know if DBT did this or if it was me hitting the task on airflow to run a model two times in a row.

We have an airflow connected to DBT using astronomer. One of our models is a simple materialized = table model.

I was testing some changes in it and hit the clear task in airflow, maybe I did it twice, I don't know. But checking the BigQuery job logs, I can see that the query was tried two times, within 20 seconds of each other, this particular query takes around 1 minute to run, but this change I made had a partitioning error, so it failed very quickly with error **\[PARTITION BY expression must be DATE(<timestamp\_column>) etc...\]**. timestamps for creation and end time for both queries were:

|| || |2025-05-25 13:00:35.436000 UTC|2025-05-25 13:00:36.598000 UTC |

|| || |2025-05-25 13:00:55.847000 UTC|2025-05-25 13:00:56.810000 UTC |

The table definitely existed previous to 2025-05-25 13:00:35.436000 UTC, we test on it all the time. I went to check the table in stg today and it doesn't exist anymore.

I don't know if it matters but the SA that uses DBT was also running other jobs/queries in bigquery, non related to dbt, but those other jobs started 1 minute after my DBT queries were tried, probably doesn't affect this whole thing.

My questions are:

1. **how on earth did this happen? Is it possible that during CREATE OR REPLACE TABLE the table was dropped and not recreated, because of the invalidQuery? How does CREATE OR REPLACE TABLE work under the hood?**
2. **Can I recover a deleted table like this in the future? I worry about this happening in PRD environment - I don't need to recover this one, I can just re run the DBT model with the correct partition and it's fine, after all it's a materialized = table model and I have the data to repopulate it. I worry about this happening on incremental models in the future, though.**

Thanks in advance!

EDIT: I found out that, a few milliseconds before the CREATE OR REPLACE TABLE took place, there was a delete request (saw it in the log explorer), by the same service account that DBT uses, I'm assuming DBT did this? I don't know.",7,2025-05-26 11:49:24
"I am currently implementing a Data Warehouse using Glue and Redshift, a star schema dimensional model to be exact. 

  
And I think of the data transformations, that need to be done before having the clean fact and dimension tables in the data warehouse, as two types:

  
\* Transformations related to the logic or business itself, eg. drop irrelevant columns, create new columns etc,   
 \* Transformations that are purely related to the structure of a table, eg. the surrogate key column, the foreign key columns that we need to add to fact tables, etc  
For the second type, from what I understood from mt research, it can be done in Glue or Redshift, but apparently it will be more complicated to do it in Glue?  

Take the example of surrogate keys, they will be Primary keys later on, and therefore if we will generate them in Glue, we have to ensure their uniqueness, this is feasible for the same job run, but if you want to ensure uniqueness across the entire table, you need to load the entire surrogate key column from Redshift and ensure that the newly generated ones in the job are unique.  


I find this type of question recurrent in almost everything related to the structure of the data warehouse, from surrogate keys, to foreign keys, to SCD type 2.

Please if you have any thoughts or suggestions feel free to comment them.  
Thanks :)",7,2025-04-23 15:13:10
"Hi All,

Im learning Dimensional modelling. Im working on the NYC taxi dataset ( [here is the data dictionary](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf) ).

Im struggling to model Datetime columns: tpep\_pickup\_datetime, tpep\_dropoff\_datetime.  
Does these columns should be in Dimensions table or in Fact table? 

What I understand from the Kimball datawarehouse toolkit book is to have a DateDim table populated with dates from start\_date to end\_date with details like month, year, quarter, day of week etc. but what about timestamp?

Lets say if I want to see the data for certain time of the day like nights? In this case, do I need to split the columns: tpep\_pickup\_datetime, tpep\_dropoff\_datetime into date, hour, mins in fact table and join to a dim table with the timestamp details like hour, mins etc? ( so two dim tables - date and timestamp )

It would be great someone can help me here?",7,2025-04-01 15:21:53
"Serious question to those who have done some data warehousing where Spark/Glue is the transformation engine, bonus if the data warehouse is Redshift.

This is my first time putting a data warehouse in place, and , I am doing so with AWS Glue and Redshift. The data load is incremental.

While in theory dimensional modeling ( star schemas to be exact ) is not hard, I am finding a hard time implementing the actual model.

I want to know how are these dimensional modeling concepts are actually implemented, the following is  my thoughts about how I understand some theoretical concepts and the way I find gaps between them and the actual practice.

**Avoiding duplicates in both fact and dimension tables**¬†‚Äìdoes this happen in the Spark job or Redshift itself? 

I feel like for transactional fact tables it is not a problem, but for dimensions, it is not straight forward: you need to insure uniqueness of entries for all the table not just the chunk you loaded during this run and this raises the above question, whether it is done in Spark, and in this case we will need to somehow load the dimension table  in dataframes so that we can filter new data loads, or in redshidt, and in this case we just load everything new to Redshift and delegate upserts and duplication checks to Redshift.

  
And speaking of uniqueness of entries in dimension tables ( I know it is getting long, bear with me, we are almost there xD) , we have to also allow exceptions, because when dealing with **SCD type 2,** we must allow duplicate entries and update the old ones to be depricated, so again how is this exception implemented practically? 

**Surrogate keys**¬†‚Äì Generate in Spark (eg. UUIDs/hashes?) or rely on Redshift¬†`IDENTITY` for example?

Surrogate keys are going to serve as primary keys for both our fact and dimension tables, so they have to be unique, again do we generate them in Spark then load to, Redshift or do we just make Redshift handle these for us and not worry about uniqueness? 

**Fact-dim integrity**¬†‚Äì Resolve FKs in Spark or after loading to Redshift?

Another concern arises when talking about surrogate keys, each fact table has to point to its dimensions with FKs, which in reality will be the surrogate keys of the dimensions, so these columns need to be filled with the right values, I am wondering whether this is done in Spark, and in this case we will have to again load the dimensions from Redshift in Spark dataframes and extract the right values of FKs, or can this be done in Reshift????

If you have any thoughts or insights please feel free to share them, litterally anything can help at this point xD",7,2025-04-17 22:02:05
"As a data engineer, I really like the control and customization that Azure offers.
At the same time, I can see how Fabric is more business-friendly and leans toward a low/no-code experience.

But with all the content and comparisons floating around the internet, why is no one talking about how insanely expensive Fabric is?! Seriously‚Äîam I missing something here?


-
",8,2025-04-09 01:20:51
"So I recently appeared for the DP-203 certification by Microsoft and want to share my learnings and strategy that I followed to crack the exam.

As you all must already be knowing that this exam is labelled as ‚Äú**Intermediate**‚Äù by¬†Microsoft themselves which is perfect in my opinion. This exam does test you in the various concepts that are required for a data engineer to¬† master in his/her career. 

Having said that, it is not too hard to crack the exam but at the same time also not as easy as appearing for AZ-900. 

DP-203 is aimed at testing the understanding of data related concepts and various tools Microsoft has offered in its suite to make your life easier. Some topics include SQL, Modern Data Warehousing, Python, PySpark, Azure Data Factory, Azure Synapse Analytics, Azure Stream Analytics, Azure EventHubs, Azure Data Lake Storage and last but not the least Azure Databricks. You can go through the complete set of topics this exam focuses on here - [https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/?practice-assessment-type=certification#certification-take-the-exam](https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/?practice-assessment-type=certification#certification-take-the-exam)



**Courses:**

I had just taken this one course for DP-203 by Alan Rodrigues *(This is not a paid promotion. I just thought that these resources were good to refer to)* and this is a 24 hour long course which has covered all the important and core concepts clearly and precisely. What I loved the most about this course is that it is a complete hands-on course. One more thing is that the instructor very rarely mentions anything as ‚Äúthis has already been covered in the previous sections‚Äù. If there is anything that we are using in the current section he makes sure to give a quick background on what has been covered in the earlier sections. Why this is so important is because we tend to forget some things and by just getting a refresher in a couple of sentences we are up to speed. 

For those of you who don‚Äôt know, Microsoft offers access to majority resources if not all for FREE credit worth $200 for 30 days. So you simply have to sign up on their portal (insert link) and get access to all of them for 30 days. If you are residing in another country then convert dollars to your local currency. That is how much worth of free credit you will get for 30 days. 

**For example -** 

I live in India. 

1 $ = 87.789 INR 

So I got FREE credits worth 87.789 X 200 = Rs 17,557

Even when I appeared for the exam (Feb 8th, 2025) I hardly got 3-4 questions from the mock tests. But don‚Äôt get disheartened. Be sure you are consistent with your learning path and take notes whenever required. As I mentioned earlier, the exam is not very hard.

**Link -** [https://www.udemy.com/course/data-engineering-on-microsoft-azure/learn/lecture/44817315?start=40#overview](https://www.udemy.com/course/data-engineering-on-microsoft-azure/learn/lecture/44817315?start=40#overview)



**Mock Tests Resources:**

So I had referred a couple of resources for taking the mocks which I have mentioned below. *(This is not a paid promotion. I just thought that these resources were good to refer to.)*



1. **Udemy Practice Tests -** [https://www.udemy.com/course/practice-exams-microsoft-azure-dp-203-data-engineering/?couponCode=KEEPLEARNING](https://www.udemy.com/course/practice-exams-microsoft-azure-dp-203-data-engineering/?couponCode=KEEPLEARNING)
2. **Microsoft Practice Assessments -** [https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/practice/assessment?assessment-type=practice&assessmentId=49&practice-assessment-type=certification](https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/practice/assessment?assessment-type=practice&assessmentId=49&practice-assessment-type=certification)
3. [https://www.examtopics.com/exams/microsoft/dp-203/](https://www.examtopics.com/exams/microsoft/dp-203/)





**DO‚Äôs:**



1. Make sure that if and whenever possible you do hands-on for all the sections and videos that have been covered in the Udemy course as I am 100% sure that you will encounter certain errors and would have to explore and solve the errors by yourself. This will build a sense of confidence and achievement after being able to run the pipelines or code all by yourself. (Also don‚Äôt forget to delete or pause resources whenever needed so that you get a hang of it and don‚Äôt lose out on money. The instructor does tell you when to do so.)
2. Let‚Äôs be very practical, nobody remembers all the resolutions or solutions to every single issue or problem faced in the past. We tend to forget things over time and hence it is very important to document everything that you think is useful and would be important in the future. Maintain an excel sheet and create two columns ‚Äú**Errors‚Äù and ‚ÄúLearnings/Resolution**‚Äù so that next time you encounter the same issue you already have a solution and don‚Äôt waste time. 
3. Watch and practice at least 5-10 videos daily. This way you can complete all the videos in a month and then go back and rewatch lessons you thought were hard. Then you can start giving practice tests. 





**DON'Ts:**



1. By heart all the MCQs or answers to the questions. 
2. Refer to many resources so much so that you will get overwhelmed and not be able to focus on preparation.
3. Even refer to multiple courses from different websites.





**Conclusion:**

All in all, just make sure you do your hands on, practice regularly, give a timeline for yourself, don‚Äôt mug up things, don‚Äôt by heart things, make sure you use limited but quality resources for learning and practice. I am sure that by following these things you will be able to crack the exam in the first attempt itself. ",8,2025-04-15 18:34:58
"# FREE Azure Course for Beginners | Learn Azure & Data Bricks in 1 Hour

[https://www.youtube.com/watch?v=8XH2vTyzL7c](https://www.youtube.com/watch?v=8XH2vTyzL7c)",8,2025-04-09 02:36:56
"I have completed Microsoft Azure Data Engineering (DP 203) certification which has given me a solid foundation of data engineering on Azure. 

Next, I followed along and did this project by Ansh Lamba: [https://www.youtube.com/watch?v=uc-u\_juRg-w&t=16941s&ab\_channel=AnshLamba](https://www.youtube.com/watch?v=uc-u_juRg-w&t=16941s&ab_channel=AnshLamba) 

  
What should be my next step to enhance my skills? Any recommendation? 4 weeks ago I didn't know anything about data engineering :p",8,2025-04-02 20:21:01
"Hey r/dataengineering community,  

I‚Äôm deep into prepping for the Google Cloud Professional Data Engineer cert and want to transition from theory to real-world projects. To ace the exam and build job-ready skills, I‚Äôm looking for:  

- Hands-on opportunities (pro bono!) to work with GCP tools like BigQuery, Dataflow, Pub/Sub, Cloud Composer, etc.  
- Mentorship or collaboration on data pipelines, workflow optimization, or cloud architecture projects.  
- Open-source/community projects needing an extra pair of hands.  

Why me? I‚Äôm motivated, detail-oriented, and eager to learn. I‚Äôll treat your project like my own!  

If you‚Äôre working on anything data-related in GCP - or know someone who is - I‚Äôd hugely appreciate a chance to contribute (or even just advice on where to start). Comment/DM me, and thanks for being an awesome community!  

P.S. Upvotes for visibility help a ton! üôè",8,2025-04-04 01:33:00
"Feeling really down as my data engineer professional exam got suspended one hour into the exam.

Before that, I got a warning that I am not allowed to close my eyes. I didn't. Those questions are long and reading them from top to bottom might look like I'm closing my eyes. I can't help it. 

They then had me show the entire room and suspended the exam without any explanantion.

I prefer Microsoft exams to this. At least, the virtual tour happens before the exam begins and there's an actual person constantly proctoring. Not like Kryterion where I think they are using some kind of software to detect eye movement.

",8,2025-05-25 07:40:31
"Hi guys 

I‚Äôve studied some Azure DE job descriptions and would like to know - what are the best resources to learn Data Factory / Azure Databricks and Azure Synapses?

Microsoft documentation? Udemy? YouTube? Books?

",8,2025-06-03 15:25:38
"Has anyone had much luck with finding roles in NZ or AU which have a heavy reliance on the types of orchestration frameworks above?

I understand most businesses will always just go for the out of the box, click and forget approach, or the option from the big providers like Azure, Aws, Gcp, etc.

However, I'm more interested in finding a company building it open source or at least managed outside of a big platform. 

I've found d it really hard to crack into those roles, they seem to just reject anyone without years of experience using the tool in question, so I've been building my own projects while using little bits of them at various jobs like managed airflow in azure or GCP.

I just find data engineering tasks within the big platforms, especially azure, a bit stale, it'll get much worse with fabric too. GCP isn't to bad, I've not used much in aws besides S3 with snowflake or glue and redshift.",8,2025-04-30 09:15:57
"Has anyone had much luck with finding roles in NZ or AU which have a heavy reliance on the types of orchestration frameworks above?

I understand most businesses will always just go for the out of the box, click and forget approach, or the option from the big providers like Azure, Aws, Gcp, etc.

However, I'm more interested in finding a company building it open source or at least managed outside of a big platform. 

I've found d it really hard to crack into those roles, they seem to just reject anyone without years of experience using the tool in question, so I've been building my own projects while using little bits of them at various jobs like managed airflow in azure or GCP.

I just find data engineering tasks within the big platforms, especially azure, a bit stale, it'll get much worse with fabric too. GCP isn't to bad, I've not used much in aws besides S3 with snowflake or glue and redshift.",8,2025-04-30 09:15:57
"Yes i know the skills are transferable. I want to know from a recruiters side. I‚Äôve posted something similar about this before where Reddit has said they‚Äôll always prefer someone with the other cloud stack than someone that doesn‚Äôt. 

I‚Äôm more keen on AWS because of people from this Reddit have stated it‚Äôs much cleaner and easier to use. 

Onto my question: Will i be employable for AWS if I‚Äôm on Azure as my FIRST job? I wanna switch to AWS, what are ways i could do that (i know nothing can beat experience so what‚Äôs the second best for me to be a worthwhile competitor?)",8,2025-05-02 19:33:17
Data engineering on azure cloud easier or aws? which one would you say? im currently learning azure :p,8,2025-04-02 20:18:15
"If anyone on this sub has worked for Amazon as a Data engineer, preferably entry level or early careers, how has your experience been working at amazon at Amazon? 

I‚Äôve heard their work culture is very startup like, and their is an abundance of poor managers. The company just cars about share holder value, instead of caring for their customers and employees.

I wanted to hear on this sub, how has your experience been? How was the hiring process like? What all skills I should develop to work for Amazon?",8,2025-04-08 20:43:31
"Hey everyone,  

I'm planning to apply for a Data Engineer position at Docker Inc. and was wondering if anyone here has experience with their application process. I have a solid background in Azure, Databricks, and data pipeline development, but I‚Äôd love to hear from others who have interviewed there or work there.

- What kind of technical questions should I expect?  
- Do they focus more on SQL, Python, or cloud architecture?  
- Any tips on system design or behavioral interviews?  

Would really appreciate any insights or advice!",8,2025-03-30 16:03:52
"Hi everyone,

I‚Äôve been working in data for almost three years, mainly with on-prem technologies like SQL, SSIS, and Power BI, plus some experience with SSRS, datastage, Microstrategy and pl/SQL.

Lately, I‚Äôve been looking for new opportunities, but most roles require Spark, Python, Databricks, Snowflake, and cloud experience, which I don‚Äôt have. My company won‚Äôt move me to a cloud-related project, but they do pay for some certifications (mainly related to Azure/Microsoft)‚ÄîI‚Äôve done Azure Data Fundamentals and I'm currently taking a Databricks course and plan to take the certification after.

What‚Äôs the best way to gain hands-on experience with cloud and these technologies? How did you make the transition?

Would love to hear your advice!",8,2025-04-02 16:44:10
"Some reviewers say Udacity's AWS Data Engineering nanodegree was a waste of money, but what about the Azure nanodegree?",8,2025-05-26 10:00:12
"I'm currently on my data engineering journey using AWS as my cloud platform. However, I‚Äôve come across the Microsoft Fabric data engineering challenge. Should I pause my AWS learning to take the Fabric challenge? Is it worth switching focus?
",8,2025-04-13 06:48:20
"i took the Azure DP-203 last week ‚Äî of course, it‚Äôs retiring literally tomorrow. But I figured it is indeed a very broad certification and so it can give a ""grounding"" scope in Azure D.E.

Also, I think it's still super early to go full Fabric (DP-600 or even DP-700), because the job demand is still not really there. Most jobs still demand strong grounding in Azure services even in the wake of Fabric adoption (POCing‚Ä¶).

So of course here, it‚Äôs retiring literally tomorrow unfortunately. I have passed the exam with a high score (900+). Also, I have worked (during internship) directly with MS Fabric only. So I would say some skills actually transfer quite nicely (ex: ADF ~ FDF).

---

### Some notes on resources for future exams:

I have relied primarily on [@tybulonazure](https://www.youtube.com/@tybulonazure)‚Äôs excellent YouTube channel (DP-203 playlist). It‚Äôs really great (watch on 1.8x ‚Äì 2x speed).  
Now going back to Fabric, I have seen he has pivoted to Fabric-centric content ‚Äî also a great news!

I also used the official ‚ÄúGuide‚Äù book (2024 version), which I found to be a surprisingly good way of structuring your learning. I hope equivalents for Fabric will be similar (TBS‚Ä¶).

---

The labs on Microsoft Learn are honestly **poorly designed** for what they offer.  
**Tip:** @tybul has video labs too ‚Äî *use these*.  
And for the exams, always focus on **conceptual understanding**, not rote memorization.

Another **important (and mostly ignored)** tip:  
Focus on the **‚Äúbest practices‚Äù** sections of Azure services in Microsoft Learn ‚Äî I‚Äôve read a lot of MS documentation, and those parts are often more helpful on the exam than the main pages.

---

**Examtopics** is obviously very helpful ‚Äî but **read the comments**, they‚Äôre essential!

---

Finally, I do think it‚Äôs a shame it‚Äôs retiring ‚Äî because the ‚Äútraditional‚Äù Azure environment knowledge seems to be a sort of industry standard for companies. Also, the Fabric pricing model seems quite aggressive.

So for juniors, it would have been really good to still be able to have this background knowledge as a base layer.",8,2025-03-30 19:48:14
"**Hello everyone,**

I‚Äôm currently a Data Engineer with 2 years of experience, mostly working in the Azure stack ‚Äî Databricks, ADF, etc. I‚Äôm proficient in Python and SQL, and I also have some experience with Terraform.

I recently got an offer for a DataOps role that looks really interesting, but I‚Äôm wondering if this is a good path for growth compared to staying on the traditional data engineering track.

Would love to hear any advice or experiences you might have!

Thanks in advance.",8,2025-05-28 16:06:55
"Hey everyone, I've been a working a few years as a data engineer, I'd say I'm very comfortable in python (databricks), sql and git and have mostly worked in Azure. I would like to get comfortable with devops, setting up proper ci/cd, iac etc.

What resources would you recommend?

Where I work we 2 repos set up, an infratsructure repo that I am totally clueless about that is mostly terraform and another repo where we make changes to notebooks and pipelines etc whose structure makes more sense to me.

The whole thing was initially set up by consultants. My goal is really to understand how it was set up, why 2 different repos, how to change the ci/cd pipeline to add testing etc.

Thanks!",8,2025-04-08 05:33:41
"Hey , I was working on on premise data engineering and recently started to use google cloud data services like data form, BigQuery, cloud storage etc. 
I am trying to switch my position to gcp data engineer. Any better suggestions on job market demands on gcp data engineers especially like when having comparison with azure, and aws?",8,2025-04-17 21:53:21
"Hi Folks,

  
14+ years into data engineering with Onprem for 10 and 4 years into Azure DE with mainly expertise on python and Azure databricks.

Now trying to shift job but 4 out of 5 jobs i see are asking for  AWS (i am targeting only product companies or  GCC) . Is self learning AWS for DE possible.

Has anyone shifted from Azure stack DE to AWS ?

What services to focus .

any paid courses that you have taken like udemy etc

Thanks",8,2025-03-31 10:23:27
Had anyone taken Cloud Pandit Azure Data Engg course. just wanted to know !!,8,2025-03-31 13:08:18
"Hey everyone,

I‚Äôm a beginner and really want to start learning cloud, but I‚Äôm confused about which Azure certification to start with: DP-900 or DP-203. 

I recently came across a post where people were talking that 900 is irrelevant now..I have no prior experience in cloud. Should I go for DP-900 first to build my basics, or is it better to jump straight into DP-203 if my goal is to become a data engineer? Would love to hear your advice and experiences, especially from those who started from scratch! Cheers!",8,2025-06-07 22:05:15
"Microsoft DP-203 exam English language is retired on March 31, 2025, other languages are also available to take.

[DP-203 available langauges](https://preview.redd.it/74o3tjzuecwe1.png?width=1091&format=png&auto=webp&s=cc82f598d41cbadb933ab4da8ec447476c80b9b4)

**Note: There is no direct replacement for the DP-203 exam. But DP-700 is indeed the recommendation to take from this retirement.**

Hope the above information can help people who are preparing for this test.

[https://www.reddit.com/r/dataengineer/comments/1k50lhv/dp203\_exam\_english\_language\_is\_retired\_dp700\_is/](https://www.reddit.com/r/dataengineer/comments/1k50lhv/dp203_exam_english_language_is_retired_dp700_is/)",8,2025-04-22 07:59:11
Only because I‚Äôve seen lots of big companies on AWS platform and I‚Äôm seriously considering learning it. Should i? ,8,2025-04-22 22:35:02
"Hi All,

i have 3 years of exp in service based Org. I have been in Azure project were im Azure platform engineer and little bit data engineering work i do. im well versed with Databricks, ADF, ADLS Gen2, SQL Server, Git but begineer in python. I want to switch to DE Role. I know Azure cloud inside out, ETL process. What you guys suggest how should i move forward or what all difficulties i will be facing.",8,2025-04-27 18:03:55
"I'm doing a bit of a trade study. 

I built a prototype pipeline that takes data from DDS topics, writes that data to Kafka, which does some processing and then inserts the data into MariaDB. 

I'm now exploring RTI Connext DDS native tools for processing and storing data. I have found that RTI has a library roughly equivalent to Kafka Streams, and also has an adapter API roughly equivalent to Kafka Connect. 

Does anyone have any experience with both Kafka Streams and RTI Connext Processor? How about both Kafka Connect and RTI Routing Service Adapters? What are your thoughts?",9,2025-06-04 19:59:15
" **Setup:** Kafka compacted topic, multiple partitions, need to trigger analysis after processing each batch per partition.

Note - This kafka recieves updates continuously at a product level... 
 
 **Key Questions:**
 1. **When to trigger?** Wait for consumer lag = 0? Use message count coordination? Poison pill?
 2. **During analysis:** Halt consumer or keep consuming new messages?
 
 **Options I'm considering:**
 - **Producer coordination:** Send expected message count, trigger when processed count matches for a product
 - **Lag-based:** Trigger when lag = 0 + timeout fallback  
 - **Continue consuming:** Analysis works on snapshot while new messages process
 
 **Main concerns:** Data correctness, handling failures, performance impact
 
 **What works best in production?** Any gotchas with these approaches...",9,2025-06-05 17:00:15
"""**Flink DataStream API - Scalable Event Processing for Supplier Stats**""!

Having explored the lightweight power of Kafka Streams, we now level up to a full-fledged distributed processing engine: **Apache Flink**. This post dives into the foundational DataStream API, showcasing its power for stateful, event-driven applications.

In this deep dive, you'll learn how to:

* Implement sophisticated event-time processing with Flink's native **Watermarks**.
* Gracefully handle late-arriving data using Flink‚Äôs elegant **Side Outputs** feature.
* Perform stateful aggregations with custom **AggregateFunction** and **WindowFunction**.
* Consume Avro records and sink aggregated results back to Kafka.
* Visualize the entire pipeline, from source to sink, using **Kpow** and **Factor House Local**.

This is post 4 of 5, demonstrating the control and performance you get with Flink's core API. If you're ready to move beyond the basics of stream processing, this one's for you!

Read the full article here: https://jaehyeon.me/blog/2025-06-10-kotlin-getting-started-flink-datastream/

In the final post, we'll see how Flink's Table API offers a much more declarative way to achieve the same result. Your feedback is always appreciated!

üîó **Catch up on the series**:
1. Kafka Clients with JSON
2. Kafka Clients with Avro
3. Kafka Streams for Supplier Stats",9,2025-06-09 22:08:42
"i've been working on project recently (Stock market monitoring and anomlies detection)  , the goal is tp provide a real time anaomalie detection for the stock prices (eg. significant drop in TSLA stock in one 1hour), first i simullate some real time data flow , by  reading from some csv files , then write the messages in Kafka topic , then there is a consumer reading from that topic and for each message/stock\_data assign a celery task , that will take the data point and performe the calculation to detect if its a an anomalie or not , the celery workers will store all the anomalies in an elasticseach index , also i need to keep both the anomalies and raw data log in elasticsearch for future analysis , finally i shoud make these anomalies accessible via soem FastApi endpoints to get anamlies in specific time range , or even generate a pdf report for a list of anomalies  , 

I know that was a long introduction and u probaly wondering what has this to with the title :

i want to prensent/demo this end of year project , but usual projects are web dev related so they are preetty straightforward presents the full stack app , but now and this my first data project , i dont how to preseesnt this , i run this project by some commads , and the whole process done in thebackgund , i can maybe log things in the terminal , but still i dont think it a good  a idea to present this , maybe some visualisation tools locally that show the process of data being processed ,   
  
So if u have an idea how to visualise this and or how you usally demonstrate this kinda of projets that would be helpful .

",9,2025-06-03 09:55:11
"I always heard about Kafka in the context of ingesting streaming data, maybe with some in-transit transformation, to be passed off to applications and storage. 

But, I just watched this video introduction to Kafka, and the speaker talks bout using Kafka to persist and query data indefinitely: [https://www.youtube.com/watch?v=vHbvbwSEYGo](https://www.youtube.com/watch?v=vHbvbwSEYGo)

I'm wondering how viable storage and query of data using Kafka is and how it scales. Does anyone know?",9,2025-04-16 18:00:02
"I didn‚Äôt work a lot with streaming concept, did mostly batch.

I‚Äôm wondering how do you define when a data will be done?

For example you count the sums of multiple blockchain wallets. You have the transactions and end up doing sum over a time period. Let‚Äôs say you do this per 15 min periods. How do you know you period is finished ? Like you define that arbitrary like 30min and hope for the best ?

Can you reprocess the same period later if some system fail badly ?  

I except a very generic answer here. I just don‚Äôt understand the concept. Like do you need to have data that if you miss some records it‚Äôs fine to deliver Half the response or can you have precise data there too where every records count ?

TLDR; how do you validate that you have all your data before letting the downstream module consume an aggregated topic or flush the period of aggregation from the stream ?",9,2025-04-21 01:05:40
"I want to replicate a collection and sync in real time.
The CDC events are streamed to Kafka and I‚Äôll be listening to it and based on operationType I‚Äôll have to process the document and load it in delta table. I have all the columns possible in my table in case of schema change in fullDocument.

I am working with PySpark in Databricks. I have tried couple of different approaches -

1. using forEachBatch, clusterTime for ordering but this requires me to do a collect and process event, this was too slow
2. Using SCD kind of approach where Instead of deleting any record I was marking them inactive -
This does not give you a proper history tracking because for an `_id` I am taking the latest change and processing it. What issue I am facing with this is - I have been told by the source team that I can get an insert event for an `_id` after a delete event of the same `_id` so if in my batch for an `_id` there are events - ‚Äúupdate ‚Üí delete, ‚Üí insert‚Äù then based on latest change I‚Äôll pick the insert and this will cause a duplicate record in my table.
What will be the best way to handle this?",9,2025-06-09 15:53:39
"Hi everyone,

  
I'm working on a small project where I'm taking some stock ticker data, and streaming it into GCP BigQuery using DataFlow. I'm completely new to Apache Beam so I've been wrapping my head around the programming model and windowing system and have some queries about how best to implement what I'm going for. At source I'm recieving typical OHLC (open, high, low, close) data every minute and I want to compute various rolling metrics on the close attribute for things like rolling averages etc. Currently the only way I see forward is to use sliding windows to calculate these aggregated metrics. The problem is that a rolling average of a few days being updated every minute for each new incoming row would result in shedloads of sliding windows being held at any given moment which feels like a horribly inefficient load of duplication of the same basic data. 

I'm also curious about attributes which you don't neccessarily want to aggregate and how you reconcile that with your rolling metrics. It feels like everything leans so heavily into using windowing that the only way to get the unaggregated attributes such as open/high/low is by sorting the whole window by timestamp and then finding the latest entry, which again feels like a rather ugly and inefficient way of doing things. Is there not some way to leave some attributes out of the sliding window entirely since they're all going to be written at the same frequency anyways? I understand the need for windowing when data can often be unordered but it feels like things get exceedingly complicated if you don't want to use the same aggregation window for all your attributes.

  
Should I stick with my current direction, is there a better way to do this sort of thing in Beam or should I really be using Spark for this sort of job? Would love to hear the thoughts of people with more of a clue than myself.",9,2025-05-28 18:49:49
"Are you curious about building real-time streaming pipelines from popular streaming platforms like Azure Event Hubs? In this tutorial, I explain key Event Hubs concepts and demonstrate how to build Spark Structured Streaming pipelines interacting with Event Hubs. Check it out here:¬†[https://youtu.be/wo9vhVBUKXI](https://youtu.be/wo9vhVBUKXI)",9,2025-04-12 15:43:22
"Hey everyone,

I'm designing a system to process and analyze a continuous stream of data with a focus on both high throughput and low latency. I wanted to share my proposed architecture and get your insights.

1. The core components are: **Kafka:** Serving as the central nervous system for ingesting a massive amount of data reliably.
2. **Go Processor:** A consumer application written in Go, placed directly after Kafka, to perform initial, low-latency processing and filtering of the incoming data.
3. **Intermediate Queue (Redis Streams/NATS JetStream):** To decouple the low-latency processing from the potentially slower analytics and to provide buffering for data that needs further analysis.
4. **Analytics Consumer:** Responsible for the more intensive analytical tasks on the filtered data from the queue.
5. **WebSockets:** For pushing the processed insights to a frontend in real-time.

The idea is to leverage Kafka's throughput capabilities while using Go for quick initial processing. The queue acts as a buffer and allows us to be selective about the data sent for deeper analytics. Finally, WebSockets provide the real-time link to the user.

I built this keeping in mind these three principles

* **Separation of Concerns:** Each component has a specific responsibility.
* **Scalability:** Kafka handles ingestion, and individual consumers can be scaled independently.
* **Resilience:** The queue helps decouple processing stages.

Has anyone implemented a similar architecture? What were some of the challenges and lessons learned? Any recommendations for improvements or alternative approaches?

Looking forward to your feedback!",9,2025-04-13 13:35:41
"Hi folks. My company is trying to switch some batch jobs to streaming. The current method is that the data are streaming data through Kafka, then there's a Spark streaming job that consumes the data and appends them to a raw table (with schema defined, so not 100% raw). Then we have some scheduled batch jobs (also Spark) that read data from the raw table, transform the data, load them into destination tables, and show them in the dashboards. We use Databricks for storage (Unity catalog) and compute (Spark), but use something else for dashboards.

Now we are trying to switch these scheduled batch jobs into streaming, since the incoming data are already streaming anyway, why not make use of it and turn our dashboards into realtime. It makes sense from business perspective too.

However, we've been facing some difficulty in rewriting the transformation jobs from batch to streaming. Turns out, Spark streaming doesn't support some imporant operations in batch. Here are a few that I've found so far:

1. Spark streaming doesn't support window function (e.g. : ROW\_NUMBER() OVER (...)). Our batch transformations have a lot of these.
2. Joining streaming dataframes is more complicated, as you have to deal with windows and watermarks (I guess this is important for dealing with unbounded data). So it breaks many joining logic in the batch jobs.
3. Aggregations are also more complicated. For example you can't do this: raw\_df -> get aggregated df from raw\_df -> join aggregated\_df with raw\_df

So far I have been working around these limitations by using Foreachbatch and using intermediary tables (Databricks delta table). However, I'm starting to question this approach, as the pipelines get more complicated. Another method would be refactoring the entire transformation queries to conform both the business logic and streaming limitations, which is probably not feasible in our scenario.

Have any of you encountered such scenario and how did you deal with it? Or maybe do you have some suggestions or ideas? Thanks in advance.",9,2025-04-16 19:31:32
"Hi,
I want to start apache Kafka. I have some idea of it coz I am little exposed to Google Cloud Pub/Sub. 
Could anyone pls help me with the good youtube videos or courses for learning ?",9,2025-04-26 18:41:53
"We‚Äôre currently indexing blockchain data using our Golang services, sending it into Redpanda, and from there into ClickHouse via the Kafka engine. This data is then exposed to consumers through our GraphQL API.

However, we‚Äôve run into issues with real-time ingestion. Pushing data into ClickHouse at high frequency is causing too many merge parts and system instability ‚Äî to the point where insert blocks are occasionally being rejected. This is especially problematic since some of our data (like blocks and transactions) needs to be available in real-time, with query latency under 100ms.

To manage this better, we‚Äôre considering separating our ingestion strategy: keeping batch ingestion into ClickHouse for historical and analytical needs, while finding a way to access fresh data in real-time when needed ‚Äî particularly for the GraphQL layer.

Would love to get thoughts on how we can approach this ‚Äî especially around managing real-time queryability while keeping ingestion efficient and stable.",9,2025-04-24 10:52:14
We‚Äôre debating between Kafka and something simpler (like AWS SQS or Pub/Sub) for a project that has low data volume but high reliability requirements. When is it truly worth the overhead to bring in Kafka?,9,2025-06-12 12:54:37
"Id there is someone familiar with Apache Flink, how to set up exactly once message processing to handle gailure? When the flink job fails between two checkpoints, some messages are processed but not included in the checkpoint, so when the job starts again it starts from the checkpoint and repeat some messages? I want to disable that and make sure each message is processed exactly once. I am worling with Kafka source.",9,2025-04-22 17:03:52
"The second installment, ""Kafka Clients with Avro - Schema Registry and Order Events,"" is now live and takes our event-driven journey a step further.

In this post, we level up by:

* Migrating from JSON to Apache Avro for robust, schema-driven data serialization.
* Integrating with Confluent Schema Registry for managing Avro schemas effectively.
* Building Kotlin producer and consumer applications for Order events, now with Avro.
* Demonstrating the practical setup using Factor House Local and Kpow for a seamless Kafka development experience.

This is post 2 of 5 in the series. Next up, we'll dive into Kafka Streams for real-time processing, before exploring the power of Apache Flink!

Check out the full article: https://jaehyeon.me/blog/2025-05-27-kotlin-getting-started-kafka-avro-clients/",9,2025-05-26 20:06:47
"Create a pipeline named 'realtime\_session\_analysis'. Add a Kafka source named 'clickstream\_kafka\_source'. It should read from the topic 'user\_clickstream\_events'. Ensure the message format is JSON. Create a stream named 'user\_sessions'. This stream should take data from 'clickstream\_kafka\_source'. Modify the 'user\_sessions' stream. Add a sliding window operation. The window should be of type sliding, with a duration of ""30.minutes()"" and a step of ""5.minutes()"". The timestamp field for windowing is 'event\_timestamp'. For the 'user\_sessions' stream, after the window operation, add an aggregate operation. This aggregate should define three output fields: 'session\_start' using window\_start, 'user' using the 'user\_id' field directly (this implies grouping by user\_id in aggregation later if possible, or handling user\_id per window output), and 'page\_view\_count' using count\_distinct on the 'page\_url' field. Create a PostgreSQL sink named 'session\_summary\_pg\_sink'. This sink should take data from the 'user\_sessions' stream. Configure it to connect to host 'localhost', database 'nova\_db', user 'nova\_user', and password 'nova\_password'. The target table should be 'user\_session\_analytics\_output'. Use overwrite mode for writing.

The DSL is working very well, check it below:

pipeline realtime\_session\_analysis {

source clickstream\_kafka\_source {

type: kafka;

topic: ""user\_clickstream\_events"";

format: json;

}

stream user\_sessions {

from: clickstream\_kafka\_source;

|> window(

type: sliding,

duration: ""30.minutes()"",

step: ""5.minutes()"",

timestamp\_field: ""event\_timestamp""

);

|> aggregate {

group\_by: user\_id;

session\_start: window\_start;

user: user\_id;

page\_view\_count: count\_distinct(page\_url);

}

}

sink session\_summary\_pg\_sink {

type: postgres;

from: user\_sessions;

host: ""localhost"";

database: ""nova\_db"";

user: ""nova\_user"";

password: ""${POSTGRES\_PASSWORD}""; // Environment variable

table: ""user\_session\_analytics\_output"";

write\_mode: overwrite;

}

}",9,2025-06-03 14:00:38
"My manager told me that I might get a new project of building a data pipeline on real time data ingestion and processing using Apache Kafka, flink and snowflake. I am new to Flink, and I wanted to learn it, but I haven't found any good resource to learn flink",9,2025-05-01 09:23:45
"Hey guys, In the past couple of years I've ended up writing quite a few data generation *scripts*. I work mainly with streaming data / events data and none of the existing frameworks were really designed for generating real world steaming data.

What I needed was a flexible data generation that can create data with a dynamic schema and has the ability to send that data to a destination (csv, kafka).We all have used Faker and its a great library but in itself doesn't finish the job. All my*scriptsl* were using Faker but always extended with some additional usecase. This is how I ended up writing [glassgen](https://pypi.org/project/glassgen/). It generates synthetic data, sends it to a sink and is simply configured by a json config. It can also generate duplicates in the data (if you want) and can send at a defined rps (best effort). 

Happy to hear your feedback and hope you find the library useful. Thanks",9,2025-04-29 19:28:15
"Hey all!I hope everyone here is doing great.I'm running some performance benchmarks for the Mongo connector and comparing it against another tool that I'm already using. Given my limited experience with Debezium's Mongo connector, I thought I'd ask for some ideas around tuning it.:)

The test is set up so that Kafka Connect, Mongo and Kafka are run as containers. Once a connector (or generally a pipeline) is created, the Kafka destination topic is monitored for throughput. This particular test focuses on CDC (there's another one for snapshots) and is using Kafka Connect 7.8 and Mongo connector 3.1.

I went through all the properties in the Mongo connector and tuned those that I thought made sense tuning. Those are:

  
`""key.converter.schemas.enable"":¬†false,`  
`""value.converter.schemas.enable"":¬†false,`  
  
`""key.converter"":¬†""org.apache.kafka.connect.json.JsonConverter"",`  
`""value.converter"":¬†""org.apache.kafka.connect.json.JsonConverter"",`  
  
`""max.batch.size"":¬†64000,`  
`""max.queue.size"":¬†128000,`

`""producer.override.batch.size"":¬†1000000`

The full configuration can be found¬†[here](https://github.com/ConduitIO/streaming-benchmarks/blob/df8dc6f5dc05a48eb15ee3e9518d2080cf90210e/benchmarks/mongo-kafka-cdc/kafka-connect-dbz/data/connector.json).

Additionally I've set the Kafka Connect worker's heap to 10 GB. The whole test is run on EC2 (on an instance with 8 vCPUs and 32 GiB of memory).

Any comments on whether this makes sense or how to tune it even more are greatly appreciated.:)  
  
Thanks!",9,2025-04-25 15:09:27
"Ready to explore the world of Kafka, Flink, data pipelines, and real-time analytics without the headache of complex cloud setups or resource contention?

üöÄ Introducing the **NEW Factor House Local Labs** ‚Äì your personal sandbox for building and experimenting with sophisticated data streaming architectures, all on your local machine!

We've designed these hands-on labs to take you from foundational concepts to building complete, reactive applications:

üîó **Explore the Full Suite of Labs Now:**
[https://github.com/factorhouse/examples/tree/main/fh-local-labs](https://github.com/factorhouse/examples/tree/main/fh-local-labs)

**Here's what you can get hands-on with:**

*   üíß **Lab 1 - Streaming with Confidence:**
    *   Learn to produce and consume Avro data using Schema Registry. This lab helps you ensure data integrity and build robust, schema-aware Kafka streams.

*   üîó **Lab 2 - Building Data Pipelines with Kafka Connect:**
    *   Discover the power of Kafka Connect! This lab shows you how to stream data from sources to sinks (e.g., databases, files) efficiently, often without writing a single line of code.

*   üß† **Labs 3, 4, 5 - From Events to Insights:**
    *   Unlock the potential of your event streams! Dive into building real-time analytics applications using powerful stream processing techniques. You'll work on transforming raw data into actionable intelligence.

*   üèûÔ∏è **Labs 6, 7, 8, 9, 10 - Streaming to the Data Lake:**
    *   Build modern data lake foundations. These labs guide you through ingesting Kafka data into highly efficient and queryable formats like Parquet and Apache Iceberg, setting the stage for powerful batch and ad-hoc analytics.

*   üí° **Labs 11, 12 - Bringing Real-Time Analytics to Life:**
    *   See your data in motion! You'll construct reactive client applications and dashboards that respond to live data streams, providing immediate insights and visualizations.

**Why dive into these labs?**
*   **Demystify Complexity:** Break down intricate data streaming concepts into manageable, hands-on steps.
*   **Skill Up:** Gain practical experience with essential tools like Kafka, Flink, Spark, Kafka Connect, Iceberg, and Pinot.
*   **Experiment Freely:** Test, iterate, and innovate on data architectures locally before deploying to production.
*   **Accelerate Learning:** Fast-track your journey to becoming proficient in real-time data engineering.

Stop just dreaming about real-time data ‚Äì start *building* it! Clone the repo, pick your adventure, and transform your understanding of modern data systems.
",9,2025-06-11 21:33:39
"As a Cursor and VSCode user, I am always disappointed with their performance on Notebooks. They loose context, don't understand the notebook structure etc. 

I built an open source AI copilot specifically for Jupyter Notebooks. Docs [here](https://docs.trymito.io/mito-ai/data-copilot). You can directly pip install it to your Jupyter IDE. 

Some example of things you can do with it that other AIs struggle with:

1. Ask the agent to add markdown cells to document your notebook 

2. Iterate cell outputs, our AI can read the outputs of your cells

3. Turn your notebook into a streamlit app -- try the ""build app"" button, and the AI will turn your notebook into a streamlit app. 

Here is a [demo environment](https://launch.trymito.io/) to try it as well.",10,2025-06-04 18:56:13
"Hey there :)

  
I hope I find myself in the right subreddit for this as I am trying to **engineer** my computer to push around some **data** ;) 

I'm currently working on a project to fully automate the processing of test results for a scientific study with students. 

The workflow consists of several stages:

1. **Data Extraction:** The test data is extracted from a local SQL database.
2. **SPSS Processing:** The extracted data is then processed using SPSS with a custom-built syntax (legacy). This step generates multiple files from the data. I have been looking into how I can transition this syntax to a python script, so this step might be cut later.
3. **Python Automation:** A Python script takes over the further processing. It reads the files, splits the data per class, inserts it into pre-designed Excel reporting templates.
4. **File Upload:** The files are then automatically uploaded to a self-hosted Nextcloud instance.
5. **Notification:** Once the workflow is complete, a notification  

I have been thinking about different ways to implement this. Right now the inputs and outputs for the different steps are still done manually. 

At work I have been using Jenkins lately and I think it feels natural  to do it in Jenkins and just describe the whole workflow in a pipeline with different stages to run. Besides that I have some experience with AWS Lambda and n8n but I am not sure if they would be helpful with this task.

I¬¥m not that experienced setting up such workflows as my work background is more in Infosec, so please forgive my uneducated guesses about how I best go about this :D Just trying not to take decisions that will be problematic later.



Greetings from Germany",10,2025-04-02 11:06:05
"Our company is implementing data quality testing and we are interested in borrowing from the Great Expectations suite of open source tests. I've read mostly negative reviews of the initial implementation of Great Expectations, but am curious if anyone else set up a much more lightweight configuration?

Ultimately, we plan to use the GX python code to run tests on data in Snowflake and then make the results available in Snowflake. Has anyone done something similar to this?",10,2025-04-04 14:18:53
"Hey r/dataengineering,

I‚Äôm 6 months into learning Python, SQL and DE. 

For my current work (non-related to DE) I need to process an Excel file with 10k+ rows of product listings (boats, ATVs, snowmobiles) for a classifieds platform (like Craigslist/OLX). 

I already have about 10-15 scripts in Python I often use on that Excel file which made my work tremendously easier. And I thought it would be logical to make the whole process automated in a full pipeline with Airflow, normalization, validation, reporting etc.

Here‚Äôs my plan:

1. Extract:  
- load Excel (local or cloud) using pandas

2. Transform:  
- create a 3NF SQL DB
- validate data, check unique IDs, validate years columns, check for empty/broken data, check constency, data types fix invalid addresses etc)
- run obligatory business-logic scripts (validate addresses, duplicate rows if needed, check for dealerships and many more)
- query final rows via joins, export to data/transformed.xlsx

3. Load
   - upload final Excel via platform‚Äôs API
   - archive versioned files on my VPS

4. Report
   - send Telegram message with row counts, category/address summaries, Matplotlib graphs, and attached Excel.  
   - error logs for validation failures

5. Testing
   - pytest unit tests for each stage (e.g., Excel parsing, normalization, API uploads).  

Planning to use Airflow to manage the pipeline as a DAG, with tasks for each ETL stage and retries for API failures but didn‚Äôt think that through yet.

As experienced data engineers what strikes you first as bad design or bad idea here? How can I improve it as a project for my portfolio?

Thanks in advance!",10,2025-04-22 18:43:19
"I've created a small tool to normalize(split) columns of a DataFrame with low cardinality, to be more focused on data engineering than LabelEncoder. The idea is to implement more grunt work tools, like a quick report of the tables looking for cardinality. I am a Novice in this area so every tip will be kindly received.  
The github link is [https://github.com/tekoryu/pychisel](https://github.com/tekoryu/pychisel) and you can just pip install it.",10,2025-06-11 20:52:38
"I‚Äôve been tasked at work with automating some processes ‚Äî things like scraping data from emails with attached CSV files, or running a script that currently takes a couple of hours every few days.

I‚Äôm seeing this as a great opportunity to dive into some new tools and best practices, especially with a long-term goal of becoming a Data Engineer. That said, I‚Äôm not totally sure where to start, especially when it comes to automating multi-step processes ‚Äî like pulling data from an email or an API, processing it, and maybe loading it somewhere maybe like a PowerBi Dashbaord or Excel.

I‚Äôd really appreciate any recommendations on tools, workflows, or general approaches that could help with automation in this kind of context!",10,2025-04-20 15:48:21
"Hi everyone , 

I have a software that writes live data to a CSV file in realtime. I want to be able to import this data every second, into Excel or a another spreadsheet program, where I can use formulas to mirror cells and manipulate my data. I then want this to export to another live CSV file in realtime. Is there any easy way to do this? 

I have tried Google sheets (works for json but not local CSV, and requires manual updates)

I have used macros in VBA in excel to save and refresh data every second and it is unreliable. 

Any help much appreciated.. possibly create a database?",10,2025-04-20 14:03:02
"Hello, everyone. 

I'm having a hard time designing for ETL and would like your opinion on the best way to extract this information from my business. 

I have 27 databases (PostgreSQL) that have the same modeling (Column, attributes, etc.). For a while I used Python+PsycoPg2 to extract information in a unified way from customers, vehicles and others. All this I've done at report level, no ETL jobs so far. 

Now, I want to start a Datawarehouse modeling process and unifying all these databases is my priority. I'm thinking of using Airflow to manage all the Postgresql connections and using Python to perform the transformations (SCD dimension and new columns). 

Can anyone shed some light on the best way to create these DAGs? A DAG for each database? or a DAG with all 27 databases knowing that the modeling of all banks are the same? ",10,2025-04-28 11:42:22
"Hey data engineers,

For client implementations I thought it was a pain to write python scripts over and over, so I built a tool on top of Pandas to solve my own frustration and as a personal hobby. The goal was to make it so I didn't have to start from the ground up and rewrite and keep track of each script for each data source I had.

**What I Built:**  
A visual transformation tool with some features I thought might interest this community:

1. **Python execution on a row-by-row basis**¬†\- Write Python once per field, save the mapping, and process. It applies each field's mapping logic to each row and returns the result without loops
2. **Visual logic builder**¬†that generates Python from the drag and drop interface. It can re-parse the python so you can go back and edit form the UI again
3. **AI Co-Pilot**¬†that can write Python logic based on your requirements
4. **No environment setup**¬†\- just upload your data and start transforming
5. **Handles nested JSON**¬†with a simple dot notation for complex structures

Here's a screenshot of the logic builder in action:

https://preview.redd.it/znh4fom8y9se1.png?width=2690&format=png&auto=webp&s=2daf229aab2f5de272c4f5668a782d8011ff3207

I'd love some feedback from people who deal with data transformations regularly. If anyone wants to give it a try feel free to shoot me a message or comment, and I can give you lifetime access if the app is of use. Not trying to sell here, just looking for some feedback and thoughts since I just built it.

**Technical Details:**

* Supports CSV, Excel, and JSON inputs/outputs, concatenating files, header & delimiter selection
* Transformations are saved as editable mapping files
* Handles large datasets by processing chunks in parallel
* Built on Pandas. Supports Pandas and re libraries

[DataFlowMapper.com](http://DataFlowMapper.com)",10,2025-04-01 19:28:00
"Hello,

I‚Äôm not a DE but i work for a small company as a BI analyst and I‚Äôm tasked to pull together the right resources to make this happen.

In a nutshell - Looking to pull ad data from the company‚Äôs FB / insta ads and load into postgresql staging so i can make views / pull into tableau.

Want to extract and load this data by writing a python script using the fast api framework. Want to orchestrate using dagster.

Regarding how and where to set all this up, im lost. Is it best to spin up a vm and write these scripts in there? What other tools and considerations do i need to make? We have AWS S3. Do i need docker?

I need to conceptually understand whats needed so i can convince my manager to invest in the right resources.

Thank you in advance.

",10,2025-06-04 02:19:30
"Hi everyone 

Full disclosure. I‚Äôm a data engineer for 3 years and now I‚Äôm facing a challenge.
Most of my prior needs were develop my pipeline using DBT and Fivetran as the data ingestion tool. But the company I‚Äôm working no longer approves the use of both tools and now I need to implement these two layers (ingestion and transformation) using GCP environment 
The basic architecture of the application I have approved, it will be :
- cloud Run generating csv. One per table/day
- cloud composer calling sql files to run the transformations

The difficult part (for me) is the Python development.
This is my first actual python development, so I‚Äôm pretty new to this part, even having some theoretical knowledge of python concepts

So far I was able to create a python app that
- connect with Shopify session
- runs a graphQL query 
- generate a csv file
- upload to a gcs bucket

My current challenge is to implement a date filter into the graphQL query and creates one file for each day.

Has anyone implemented something like this ?
",10,2025-05-01 11:42:59
"Okay. I've started working on a new business in a new country I just moved to.

I need to cold call companies via email giving them my company's introduction and telling them what we do and Yada Yada Yada. 


I have a list the registered name of about 16000 companies. 

Process 1: So, If I Google ""contact email _company x_"", 7 out of 10 times Google comes up with the email I need. 

Process 2: I then go on to copy paste that email into my outlook and send them the introduction.


Is there any way we can automate either/both of these processes? 


Its been 10 days since I started working on my project at I'm still only 10% through. :/

Any kind of advice would go a long way in helping me. Thanks! 

",10,2025-05-02 22:48:03
"Hi,
I have a Notebook A containing multiple SQL scripts in multiple cells.  I am trying to use the output of specific cells of Notebook_A in another notebook. Eg: count of records returned in cell2 of notebook_a in the python Notebook_B.

Kindly suggest on the feasible ways to implement the above.",10,2025-04-15 09:24:21
"Hi, 

I'm a data analyst with 2 years of experience slowly making progress towards using SSIS and Python to move data around.

Recently, I've found myself sending requests to the Microsoft Partner Center APIs using Python scripts in order to get that information and send it to tables on a SQL Server, and for this purpose I need to run these data flows on a schedule, so I've been using the Windows Task Scheduler hosted on a VM with Windows Server to run them, are there any other better options to run the Python scripts on a schedule?

Thank you.

",10,2025-06-07 02:56:38
"Hello everyone,

I've been working as a Data Engineer for a while, mainly on GCP: BigQuery, GCS, Cloud Functions, Cloud SQL.
I have set up quite a few batch pipelines to process and expose business data.
I structured the code in Python with object-oriented logic, automated processing via Cloud Scheduler, optimized BigQuery queries, built tables at the right level for business analysis (product, country, etc.), set up quality tests, benchmarks, etc.

I also work regularly with business lines to understand their needs, structure the data, and present the results in Postgres databases or GCS exports.

But despite all that... I don't find my experience very rewarding given that it's a project that lasted 4 years.

I don‚Äôt do real-time processing, no AI, no ‚Äúfancy‚Äù stuff.
Even unit testing, I do very little if at all, because everything happens in BigQuery and I've never really seen the point of testing Python scripts that just execute SQL queries that have already been tested manually.

Sometimes I feel like I'm just getting data from point A to point B, cleanly.
And I wonder: is this ‚Äújust that‚Äù, the job? Or have I missed another level?

Do you feel this too?
Are we underestimating this work, even though it is essential?
And above all, how do you find meaning or progress in this kind of context?

Thank you in advance for your feedback.",10,2025-04-29 19:20:08
"Example: I want to import 100 CSVs into 100 SSMS tables (that are not pre-created). The datatypes can be varchar for all (unless it could autoassign some).

I'd like to just point the process to a folder with the CSVs and read that into a specific database + schema. Then the table name just becomes the name of the file (all lower case).

What's the simplest solution here? I'm positive it can be done in either SSIS or Python. But my C skill for SSIS are lacking (maybe I can avoid a C script?). In python, I had something kind of working, but it takes way too long (10+ hours for a csv thats like 1gb).

Appreciate any help!",10,2025-04-16 14:56:55
"TLDR: how do I run ~25 scripts that must be run on my local company server instance but allow for tracking through an easy UI since prefect hobby tier (free) only allows server-less executions. 
 
Hello everyone! 

I was looking around this Reddit and thought it would be a good place to ask for some advice. 

Long story short I am a dashboard-developer who also for some reason does programming/pipelines for our scripts that run only on schedule (no events). I don‚Äôt have any prior background on data engineering but on our 3 man team I‚Äôm the one with the most experience in Python. 

We had been using Prefect which was going well before they moved to a paid model to use our own compute. Previously I had about 25 scripts that would launch at different times to my worker on our company server using prefect. It sadly has to be on my local instance of our server since they rely on something called Alteryx which our two data analysts use basically exclusively. 

I liked prefects UI but not the 100$ a month price tag. I don‚Äôt really have the bandwidth or good-will credits with our IT to advocate for the self-hosted version. I‚Äôve been thinking of ways to mimic what we had before but I‚Äôm at a loss. I don‚Äôt know how to have something ‚Äòtalk‚Äô to my local like prefect was when the worker was live. 

I could set up windows task scheduler but tbh when I first started I inherited a bunch of them and hated the transfer process/setup. My boss would also like to be able to see the ‚Äòfailures‚Äô if any happen. 

We have things like bitbucket/s3/snowflake that we use to host code/data/files but basically always pull them down to our local/ inside Alteryx.  

Any advice would be greatly appreciated and I‚Äôm sorry for any incorrect terminology/lack of understanding. Thank you for any help!",10,2025-06-09 07:25:51
"Hey r/dataengineering,

I‚Äôm 6 months into learning Python, SQL and DE.

For my current work (non-related to DE) I need to process an Excel file with 10k+ rows of product listings (boats, ATVs, snowmobiles) for a classifieds platform (like Craigslist/OLX).

I already have about 10-15 scripts in Python I often use on that Excel file which made my work tremendously easier. And I thought it would be logical to make the whole process automated in a full pipeline with Airflow, normalization, validation, reporting etc.

Here‚Äôs my plan:

**Extract**

- load Excel (local or cloud) using pandas

**Transform**

- create a 3NF SQL DB

- validate data, check unique IDs, validate years columns, check for empty/broken data, check constency, data types fix invalid addresses etc)

- run obligatory business-logic scripts (validate addresses, duplicate rows if needed, check for dealerships and many more)

- query final rows via joins, export to data/transformed.xlsx

**Load**

- upload final Excel via platform‚Äôs API
- archive versioned files on my VPS

**Report**

- send Telegram message with row counts, category/address summaries, Matplotlib graphs, and attached Excel
- error logs for validation failures

**Testing**

- pytest unit tests for each stage (e.g., Excel parsing, normalization, API uploads).

Planning to use Airflow to manage the pipeline as a DAG, with tasks for each ETL stage and retries for API failures but didn‚Äôt think that through yet.

As experienced data engineers what strikes you first as bad design or bad idea here? How can I improve it as a project for my portfolio?

Thank you in advance!",10,2025-04-23 12:10:30
"Hello everyone,

I recently joined a project that uses BigQuery for data storage, dbt for transformations, and Tableau for dashboarding. I'd like some advice on improving our current setup.

# Current Architecture

* Data pipelines run transformations using dbt
* Data from BigQuery is synchronized to Google Sheets
* Tableau reports connect to these Google Sheets (not directly to BigQuery)
* Users can modify tracking values directly in Google Sheets

# The Problems

1. **Manual Process**: Currently, the Google Sheets and Tableau connections are created manually during development
2. **Authentication Issues**: In development, Tableau connects using the individual developer's account credentials
3. **Orchestration Concerns**: We have Google Cloud Composer for orchestration, but the Google Sheets synchronization happens separately

# Questions

1. What's the best way to automate the creation and configuration of Google Sheets in this workflow? Is there a Terraform approach or another IaC solution?
2. How should we properly manage connection strings in tableau between environments, especially when moving from development (using personal accounts) to production?

Any insights from those who have worked with similar setups would be greatly appreciated!",10,2025-04-15 15:24:07
Is anyone using Alteryx and able to make scheduled runs without the scheduler they are discontinuing?  They have moved to a server option but at $80k that is cost prohibitive for our company in order to just schedule automated runs.,10,2025-05-26 15:44:51
"Hello,

I‚Äôm not a DE but i work for a small company as a BI analyst and I‚Äôm tasked to pull together the right resources to make this happen.

In a nutshell - Looking to pull ad data from the company‚Äôs FB / insta ads and load into postgresql staging so i can make views / pull into tableau.

Want to extract and load this data by writing a python script using the fast api framework. Want to orchestrate using dagster.

Regarding how and where to set all this up, im lost. Is it best to spin up a vm and write these scripts in there? What other tools and considerations do i need to make? We have AWS S3. Do i need docker?

I need to conceptually understand whats needed so i can convince my manager to invest in the right resources.

Thank you in advance.

",10,2025-06-04 02:19:30
"anyone that needed to create a cloud inventory (for cloud resources such as EC2, RDS, etc), using some kind of an ETL (hand written or by using a paid product or opensource) - how did you build it?

I have been using CloudQuery and very happy about it - concurrent requests, schemas and a lot more is taken care for you - but its price is too unpredictable especially looking forward.  
SteamPipe s mode ad-hoc and feels less suited for production workloads, at least not without substantial effort.",11,2025-05-26 14:37:56
"__Disclaimer: I‚Äôm one of the creators of PortalJS.__

Hi everyone, I wanted to share why we built this service:

**Our mission:**

Open data publishing shouldn‚Äôt be hard. We want local governments, academics, and NGOs to treat publishing their data like any other SaaS subscription: sign up, upload, update, and go.

**Why PortalJS?**

- Small teams need a simple, affordable way to get their data out there.
- Existing platforms are either extremely expensive or require a technical team to set up and maintain.
- Scaling an open data portal usually means dedicating an entire engineering department‚Äîand we believe that shouldn‚Äôt be the case.

Happy to answer any questions!",11,2025-05-02 12:18:52
"If your current setup involves an DWH on-prem (ETL Tool and Database) and you are planning to migrate it in cloud, is it 'mandatory' to migrate the ETL Tool and the Database at the same time or is it - regarding expenses - even.
From what factory does it depend on? 

Thx!",11,2025-04-25 19:40:18
"Hey everyone,

I'm currently a **Data Engineering intern + final-year CS student** with a strong passion for building real-world DE systems.

Over the past few weeks, I‚Äôve been diving deep into ETL, orchestration, cloud platforms (Azure, Databricks, Snowflake), and data architecture. Inspired by some great Substacks and events like **OpenXData**, I‚Äôm thinking of starting a **public learning repository** focused on :

I‚Äôve structured it into **three project levels** each one more advanced and realistic than the last:

Basic -> 2 projects -> Python, SQL, Airflow, PostgreSQL, basic ETL| 

Intermediate -> 2 projects -> Azure Data Factory, Databricks (batch), Snowflake, dbt

Advanced -> 2 projects -> Streaming pipelines, Kafka + PySpark, Delta Lake, CI/CD, monitoring

* Not just dashboards or small-scale analysis
* Projects designed to **scale from 100 rows ‚Üí 1 billion rows**
* Focus on **workflow orchestration**, **data modeling**, and **system design**
* Learning-focused but aligned with **production-grade design principles**
* Built to **learn, practice, and showcase** for real interviews & job prep

Feedback on project ideas, structure, or tech stack, Suggestions for **realistic use cases** to build, Tips from experienced engineers who‚Äôve built at scale, Anyone who wants to follow or contribute you're welcome!

Would love any thoughts you all have thanks for reading üôè",11,2025-05-25 03:30:07
"Hello, 

i've recently joined a company that works with a home made ETL solution (Python for scripts, node-red as an orchestrator, the whole in Linux environment). 

  
We're starting to consider moving this app to AWS (aws itself is new to the company). As i don't have any idea about what AWS offers , is it a good idea to shift to AWS ? maybe it's an overkill ?  i mean what could be the ROI of this project? a on daily basis , i'm handling support of the home made ETL, and evolution. The solution as a whole is not monitored and depends on few people that could understand it and eventually provide support in case of problem. 

  
Your opinions / retex are highly appreciated. 

  
Thanks ",11,2025-04-23 13:56:34
"My current org builds all ETL in-house. The AWS bill for is a few hundred USD a month (more context on this number at the end), and it's a lot cheaper to hire more engineers in our emerging market than it is to foot 4 or 5 digit monthly payments in USD. Are any of you in the opposite situation? 

For some data sources that we deal with, afaik there isn't any product available that would even do what's needed, e.g. send a GET request to endpoint E with payload P if conditions C1 or C2 or ... or Cn are met, schedule that with cronjob T, and then write the response to the DW. Which I imagine is a very normal situation.

I keep seeing huge deals in the ETL space (fivetran just acquired census btw), and I wonder who's making the procurement decisions that culminate in the tens of thousands of six or seven digit monthly ETL bills that justify these valuations.

Context: Our DW grows at about 2-3 GB/ month, and we have ~120GB in total. We ingest data from a bit over a dozen different sources, and it's all regular Joe kinds of data, like production system transactional dbs, event streams, commercial partner's APIs, some event data stuck in dynamoDB, some CDC logs.",11,2025-05-02 17:33:25
"Hi all ! I'm an analytics engineer not DE but felt it would be relevant to ask this here.

When you're taking on a new project, how do you think about balancing turning something around asap vs really digging in and understanding and possibly delivering something better? 

For example, I have a report I'm updating and adding to. On one extreme, I could probably ship the thing in like a week without much of an understanding outside of what's absolutely necessary to understand to add what needs to be added. 

On the other hand, I could pull the thread and work my way all the way from source system to queries that create the views to the transformations done in the reporting layer and understanding the business process and possibly modeling the data if that's not already done etc 

I know oftentimes I hear leaders of data teams talk about balancing short versus long-term investments, but even as an IC I wonder how y'all do it?

In a previous role, I aired on the side of understanding everything super deeply from the ground up on every project, but that means you don't deliver things quickly.",11,2025-04-20 15:54:48
"My current responsibility is databricks + power bi. Now don't get me wrong, our scrum process is not correct scrum and we have our super benevolent rules for POs and we are planning everything for 2 upcoming quarters (?!!!), but even without this stupid future planning I found out we are doing anything but agile. Scrum turned to: give me estimation for everything, Dev or PO can change task during sprint because BI development is pretty much unpredictable. And mostly how the F\*\*\* I can give estimate in hours for something I have no clue! Every time developer needs to be in defend position AKA why we are always underestimate, lol. BI development takes lots of exploration and prototyping and specially with tool like Power BI. In the end we are not delivering according to plan but our team is always overcommitted. I don't know any person who is actually enjoying scrum including devs, manegers and POs. What's your attitude towards scrum? cheers",11,2025-05-26 13:10:30
"Just found out our IT department contracted a pipeline build that moves 500MB daily. They're pretending to manage data (insert long story about why they shouldn't). It's costing our business $10,000 per year. 

Granted that comes with theoretical support and maintenance. I'd estimate the vendor spends maybe 1-6 hours per year doing support. 

They don't know what value the company derives from it so they ask me every year about it. It does generate more value than it costs.

I'm just wondering if this is even reasonable? We have over a hundred various systems that we need to incorporate as topics into the ""warehouse"" this IT team purchased from another vendor (it's highly immutable so really any ETL is just filling other databases in the same server). They did this stuff in like 2021-2022 and have yet to extend further, including building pipelines for the other sources. At this rate, we'll be paying millions of dollars to manage the full suite (plus whatever custom build charges hit upfront) of ETL, no even compute or storage. The $10k isn't for cloud, it's all on prem on our computer and storage.

There's probably implementation details I'm leaving out. Just wondering if this is reasonable. 
",11,2025-05-27 20:33:38
"A small win I‚Äôm proud of.

The marketing team I work with was spending a lot on SaaS tools for basic data pipelines.

Instead of paying crazy fees, I deployed Airbyte self-hosted on Kubernetes.
	‚Ä¢	Pulled data from multiple marketing sources (ads platforms, CRMs, email tools, etc.)
	‚Ä¢	Wrote all raw data into S3 for later processing (building L2 tables)
	‚Ä¢	Some connectors needed a few tweaks, but nothing too crazy

Saved around $30,000 USD annually.
Gained more control over syncs and schema changes.
No more worrying about SaaS vendor limits or lock-in.

Just sharing in case anyone‚Äôs considering self-hosting ETL tools. It‚Äôs absolutely doable and worth it for some teams.

Happy to share more details if anyone‚Äôs curious about the setup.

I don‚Äôt know want to share the name of the tool which marketing team was using. 
",11,2025-04-27 11:54:17
"Hi everyone,

I'm part of a team that's increasingly using multiple LLM platforms (OpenAI, Anthropic, Cohere, etc.) across different departments and projects. As our usage grows, we're struggling to effectively track and manage billing across these services.

**Current challenges:**

* Fragmented spending across multiple provider accounts
* Difficulty attributing costs to specific teams/projects
* No centralized dashboard for monitoring total LLM expenditure
* Inconsistent billing cycles between providers
* Unexpected cost spikes that are hard to trace back to specific usage

**I'd love to hear from others:**

1. What tools or systems do you use to track LLM spending across platforms?
2. How do you handle cost allocation to departments/projects?
3. Are there any third-party solutions you'd recommend for unified billing management?
4. What reporting and alerting systems work best for monitoring usage?
5. Any best practices for forecasting future LLM costs as usage scales?

We're trying to avoid building something completely custom if good solutions already exist. Any insights from those who've solved this problem would be incredibly helpful!",11,2025-04-17 00:39:48
"Looking for guidance on learning an end-to-end data pipeline using the Lambda architecture.

I‚Äôm specifically interested in the following areas:
	‚Ä¢	Real-time streaming: Using Apache Flink with Kafka or Kinesis
	‚Ä¢	Batch processing: Using Apache Spark (PySpark) on AWS EMR
	‚Ä¢	Data ingestion and modeling: Ingesting data into Snowflake and building transformations using dbt

I‚Äôm open to multiple resources‚Äîincluding courses or YouTube channels‚Äîbut looking for content that ties these components together in practical, real-world workflows.

Can you recommend high-quality YouTube channels or courses that cover these topics?",11,2025-06-12 06:50:15
"Hey Everyone

I'm a self-taught solo engineer/developer (with university + multi-year professional software engineer experience) developing a solution for a growing problem I've noticed many organizations are facing: managing and optimizing spending across multiple AI and LLM platforms (OpenAI, Anthropic, Cohere, Midjourney, etc.).

# The Problem I'm Research / Attempting to Address:

From my own research and conversations with various teams, I'm seeing consistent challenges:

* No centralized way to track spending across multiple AI providers
* Difficulty attributing costs to specific departments, projects, or use cases
* Inconsistent billing cycles creating budgeting headaches
* Unexpected cost spikes with limited visibility into their causes
* Minimal tools for forecasting AI spending as usage scales

# My Proposed Solution

Building a platform-agnostic billing management solution that would:

* Provide a unified dashboard for all AI platform spending
* Enable project/team attribution for better cost allocation
* Offer usage analytics to identify optimization opportunities
* Include customizable alerts for budget management
* Generate forecasts based on historical usage patterns

# I Need Your Input:

Before I go too deep into development, I want to make sure I'm building something that genuinely solves problems:

1. What features would be most valuable for your organization?
2. What platforms beyond the major LLM providers should we support?
3. How would you ideally integrate this with your existing systems?
4. What reporting capabilities are most important to you?
5. How do you currently handle this challenge (manual spreadsheets, custom tools, etc.)?

Seriously would love your insights and/or recommendations of other projects I could build because I'm pretty good at launching MVPs extremely quickly (few hours to 1 week MAX).",11,2025-04-18 02:37:18
"I am a data/functional analyst looking to get a better grasp on 'end-to-end data management and processing' (in quotations as that's the term my direct lead used when we aligned regarding my career objectives for the year, which definitely matches with my current interests).

Given this objective, my own interest in gaining a better grasp on general principles and functions of Data Engineering (and later on, Data Science), as well as a personal project I'm looking to finish for myself/my own portfolio by the end of the year, I'm looking for a cloud-based data pipeline solution for me to spend my role's training stipend on -- though I'm willing (and allowed) to use some of my personal funds on this as well since it's both for career progression and personal fun.

I started looking for small-scale subscriptions for some of the platforms we've previously used at work (AWS, Databricks), but the consensus I've gotten is that cost-wise, I'm potentially better off with multiple smaller platforms that I can just weave together, instead of going for platforms normally designed for enterprise use. And so I come here for help!

*(Though if this is not the right community for me to ask this, please let me know where instead)*

# Budget and Scope

* Base budget is a $200 stipend from our department, which refreshes at the end of the year
* I'm willing to add another $200-300/year, if necessary
* I'm from South-East Asia (converting our local currency, in case regional pricing matters)
* I prefer platforms/solutions that come with upfront costs instead of per-pull/use costs, so that's it's easier to manage the finances involved
* Languages I'm comfortable with are Python and SQL, since these are what I mainly use for work, but willing to learn basics of new languages where necessary 
* Since I will be using my personal computer, which is not that powerful currently, I am looking for a pipeline/platform that I can use completely via cloud (including running training models) 

# Learning Objectives 

*(aka things I want my platform/s to be able to do)*

**1. Data Gathering/Ingestion/Validation/Transformation/Storage**

* Obviously not too familiar with this step currently, so I ended up lumping a bunch of them together, but essentially ETL
* In terms of data I intend to use, I have three main sources I intend to work my way through, from simple to complex: **(1)** will be starting with ingesting CSV files I get from third-party free data providers like Kaggle, **(2)** then moving on to structured databases from sports stat sites such as BBREF, before  eventually work my way to **(3)** automating data collection from publicly available video games I normally play (Dota 2, Path of Exile -- if it matters)
* I want to be able to load these into SQL tables that I can regularly query form

**2. Data Querying and Extract Cloud Storage**

* I want to then be able to set it up so that I can easily query and then store-via-cloud what I need.
* To make my intention clear, most of my work experience is with AWS Athena and S3, so I hope to get platforms that function similarly.

**3. Cloud-Based Data Processing, Machine Learning, and Visualization**

* I currently do majority of my post-query data processing and modelling locally on my work laptop on Jupyter (via the Anaconda distribution), but one of my key objectives is learning to do all of these on cloud (especially since my peronal computer I'll be using would obviously not be as powerful as what I use for work)
* I definitely prefer Notebook-like environments, so perhaps something like AWS EMR
* My main experience at this level is mainly with Python (using specific packages such as Pandas, Numpy, Matplotlib, Numpy, Sklearn, etc.), but I'm looking to do more PySpark as well

**4. EXTRA/OPTIONAL: Dashboard Creation and Hosting**

* If I can get a platform/pipeline that will allow me to host interactive dashboards for me to just embed in my portfolio, that would be a plus, but I am very easily willing to drop this should it not fit my budget

#  Final Remarks

* Want to learn ETL, and cloud-based data processing on a personal data pipeline and processing platform/solution that also has SQL capabilities -- in line with my career and personal learning objectives for the year 
* Tried looking into personal subscriptions for 360 solutions like Databricks and AWS Sagemaker Unified Studio, but was told that for what I want I might be better off with patching together T2 solutions or something along those lines -- but I imagine this would be much more tricky to set up ",11,2025-03-30 14:20:16
"I am working on a website whose job is to serve data from MongoDb. Just textual data in row format nothing complicated. 

This is my current setup: client sends a request to cloudfront that manages the cache and triggers a lambda for a cache miss to query from MongoDB. I also use signedurl for security purposes for each request. 

I am not an expert that but I think cloud front can handle DDoS attacks etc. Does this setup work or do I need to bring in API Gateway into the fold? I don‚Äôt have any user login etc. and no form on the website (no sql injection risk I guess). I don‚Äôt know much about network security etc but have heard horror stories of websites getting hacked etc. Hence am a bit paranoid before launching the website. 

Based on some reading, I came to the conclusion that I need to use AWS WAF + API Gateway for dynamic queries and AWS + cloud front for static pages. And lambda should be associated with API Gateway to connect with MongoDB and API Gateway does rate limiting and caching (user authentication is no big a problem here). I wonder if cloudfront is even needed or should just stick with the current architecture I have. 

Need your suggestions. 




",11,2025-03-29 23:56:02
"My team is involved in Project development work that fits perfectly in the agile framework, but we also have some ongoing tasks related to platform administration, monitoring support, continuous enhancement of security, etc. These tasks do not fit well in the agile process. How do others track such tasks and measure progress on them? Do you use specific tools for this?",11,2025-04-15 03:21:03
"Hi everyone, I‚Äôm building / running a business services company where we provide strategic development planning along with implementing and improving their operations with these core operating systems:

	‚Ä¢	CRM (Customer Relationship Management)
	‚Ä¢	ERP (Financial/Operations Core)
	‚Ä¢	Workflow Automation (iPaaS/RPA)
	‚Ä¢	Business Intelligence (BI)
	‚Ä¢	Project Management/OKR Tracking

I‚Äôm trying to figure out the most practical, streamlined way to actually implement these systems for clients, especially for startups and small to mid-sized businesses.

For anyone who knows about business, these operating systems, or any software expertise:


How would you go about implementing these tools in order to provide value to your clients?

Do you believe this business model can work as a useful service or should I workshop?

Lastly, if this model does work what mistakes should I avoid early on?

Appreciate any insights you can share!",11,2025-04-26 20:44:47
"I am a data/functional analyst looking to get a better grasp on 'end-to-end data management and processing' (in quotations as that's the term my direct lead used when we aligned regarding my career objectives for the year, which definitely matches with my current interests).

Given this objective, my own interest in gaining a better grasp on general principles and functions of Data Engineering (and later on, Data Science), as well as a personal project I'm looking to finish for myself/my own portfolio by the end of the year, I'm looking for a cloud-based data pipeline solution for me to spend my role's training stipend on -- though I'm willing (and allowed) to use some of my personal funds on this as well since it's both for career progression and personal fun.

I started looking for small-scale subscriptions for some of the platforms we've previously used at work (AWS, Databricks), but the consensus I've gotten is that cost-wise, I'm potentially better off with multiple smaller platforms that I can just weave together, instead of going for platforms normally designed for enterprise use. And so I come here for help!

*(Though if this is not the right community for me to ask this, please let me know where instead)*

# Budget and Scope

* Base budget is a $200 stipend from our department, which refreshes at the end of the year
* I'm willing to add another $200-300/year, if necessary
* I'm from South-East Asia (converting our local currency, in case regional pricing matters)
* I prefer platforms/solutions that come with upfront costs instead of per-pull/use costs, so that's it's easier to manage the finances involved
* Languages I'm comfortable with are Python and SQL, since these are what I mainly use for work, but willing to learn basics of new languages where necessary 
* Since I will be using my personal computer, which is not that powerful currently, I am looking for a pipeline/platform that I can use completely via cloud (including running training models) 

# Learning Objectives 

*(aka things I want my platform/s to be able to do)*

**1. Data Gathering/Ingestion/Validation/Transformation/Storage**

* Obviously not too familiar with this step currently, so I ended up lumping a bunch of them together, but essentially ETL
* In terms of data I intend to use, I have three main sources I intend to work my way through, from simple to complex: **(1)** will be starting with ingesting CSV files I get from third-party free data providers like Kaggle, **(2)** then moving on to structured databases from sports stat sites such as BBREF, before  eventually work my way to **(3)** automating data collection from publicly available video games I normally play (Dota 2, Path of Exile -- if it matters)
* I want to be able to load these into SQL tables that I can regularly query form

**2. Data Querying and Extract Cloud Storage**

* I want to then be able to set it up so that I can easily query and then store-via-cloud what I need.
* To make my intention clear, most of my work experience is with AWS Athena and S3, so I hope to get platforms that function similarly.

**3. Cloud-Based Data Processing, Machine Learning, and Visualization**

* I currently do majority of my post-query data processing and modelling locally on my work laptop on Jupyter (via the Anaconda distribution), but one of my key objectives is learning to do all of these on cloud (especially since my peronal computer I'll be using would obviously not be as powerful as what I use for work)
* I definitely prefer Notebook-like environments, so perhaps something like AWS EMR
* My main experience at this level is mainly with Python (using specific packages such as Pandas, Numpy, Matplotlib, Numpy, Sklearn, etc.), but I'm looking to do more PySpark as well

**4. EXTRA/OPTIONAL: Dashboard Creation and Hosting**

* If I can get a platform/pipeline that will allow me to host interactive dashboards for me to just embed in my portfolio, that would be a plus, but I am very easily willing to drop this should it not fit my budget

#  Final Remarks

* Want to learn ETL, and cloud-based data processing on a personal data pipeline and processing platform/solution that also has SQL capabilities -- in line with my career and personal learning objectives for the year 
* Tried looking into personal subscriptions for 360 solutions like Databricks and AWS Sagemaker Unified Studio, but was told that for what I want I might be better off with patching together T2 solutions or something along those lines -- but I imagine this would be much more tricky to set up ",11,2025-03-30 14:20:16
"Would love to hear how you guys handle lightweight ETL, are you all-in on serverless, or sticking to more traditional pipelines? Full code walkthrough of what I did¬†[here](https://betaacid.co/blog/a-hands-on-guide-to-serverless-etl-with-aws?utm_source=reddit&utm_medium=social&utm_campaign=blog_2025)",11,2025-04-29 19:48:42
"Title simple as that. What techniques and tools do you use to tie value to specific engineering tasks and projects? I'm talking beginning development and evolves to support all the way through the whole process from API to a platinum mart.  If you're using Jira, is there a simpler way? How would you present a DEs teams value to those upstairs? Our team's efforts support several specific mature data products for analytics and more for other segments. The green manager is struggling on quantifying our value add (development  and ongoing support ) to be able to request more people. There's now a renewed push towards overusing Jira. I have a good sense on how it would be calculated but the several layer abstraction seems to muddy the waters?",11,2025-05-28 02:05:33
"Hello,
I‚Äôm facing a philosophical question at work and I can‚Äôt find an answer that would put my brain at ease. 

Basically we work with Databricks and Pyspark for ingestion and transformation.

We have a new data provider that sends crypted and zipped files to an s3 bucket. There are a couple of thousands of files (2 years of historic).

We wanted to use dataloader from databricks. It‚Äôs basically a spark stream that scans folders, finds the files that you never ingested (it keeps track in a table) and reads the new files only and write them.
The problem is that dataloader doesn‚Äôt handle encrypted and zipped files (json files inside).

We can‚Äôt unzip files permanently. 

My coworker proposed that we use the autoloader to find the files (that it can do) and in that spark stream use the for each batch method to apply a lambda that does:
- get the file name (current row)
-decrypt and unzip
-hash the files (to avoid duplicates in case of failure)
-open the unzipped file using spark
-save in the final table using spark 

I argued that it‚Äôs not the right place to do all that and since it‚Äôs not the use case of autoloader it‚Äôs not a good practice, he argues that spark is distributed and that‚Äôs the only thing we care since it allows us to do what we need quickly even though it‚Äôs hard to debug (and we need to pass the s3 credentials to each executor using the lambda‚Ä¶)

I proposed a homemade solution which isn‚Äôt the most optimal, but it seems better and easier to maintain which is:
- use boto paginator to find files
- decrypt and unzip each file 
- write then json in the team bucket/folder
-create a monitoring table in which we save the file name, hash, status (ok/ko) and exceptions if there are any

He argues that this is not efficient since it‚Äôll only use one single node cluster and not parallelised. 

I never encountered such use case before and I‚Äôm kind of stuck, I read a lot of literature but everything seems very generic. 

Edit: we only receive 2 to 3 files daily per data feed (150mo per file on average) but we have 2 years of historical data which amounts to around 1000 files. So we need 1 run for all the historic then a daily run. 
Every feed ingested is a class instantiation (a job on a cluster with a config) so it doesn‚Äôt matter if we have 10 feeds. 

Edit2: 1000 files roughly summed to 130go after unzipping. Not sure of average zip/json file though. 

What do you people think of this? Any advices ?
Thank you ",12,2025-04-15 18:15:12
"Long story short we are processing 40M records from a input file in s3 by directly streaming each line by line we used ray architecture to submit each line as tasks and parallelize them across available cores in the cluster(ray rakes care of scheduling based on config)

We did poc for 6M records in a small machine 16core cpu catering towards the worst case (if it can work on a small machine will work in bigger resource pool) now he had successfully ran it for without any memory overload by using ray wait and get to constantly clear memory.

Problem with bigger resources is the stream reading we are doing is still single threaded python smart open package while processing is a Ferrari car with parallelization based on bigger cores available so we are not submitting enough tasks to make use of the full cores available which causes a discrepancy in the cost and time projection we did based on poc 

Any ideas to parallelize the streaming using python smartopen without any duplication? To increase read throughput and submit more tasks in parallel to parallel processing",12,2025-04-24 07:16:11
"I'm working on some data pipelines for a new source of data for our data lake, and right now we really only have one path to get the data up to the cloud. Going to do some hand-waving here only because I can't control this part of the process (for now), but a process is extracting data from our mainframe system as text (csv), and then compressing the data, and then copying it out to a cloud storage account in S3.

Why compress it? Well, it does compress well; we see around \~30% space saved and the data size is not small; we're going from roughly 15GB per extract to down to 4.5GB. These are averages; some days are smaller, some are larger, but it's in this ballpark. Part of the reason for the compression is to save us some bandwidth and time in the file copy.

So now, I have a spark job to ingest the data into our raw layer, and it's taking longer than I \*feel\* it should take. I know that there's some overhead to reading compressed .gzip (I feel like I read somewhere once that it has to read the entire file on a single thread first). So the reads and then ultimately the writes to our tables are taking a while, longer than we'd like, for the data to be available for our consumers.

The debate we're having now is where do we want to ""eat"" the time:

* Upload uncompressed files (vs compressed) so longer times in the file transfer
* Add a step to decompress the files before we read them
* Or just continue to have slower ingestion in our pipelines

My argument is that we can't beat physics; we are going to have to accept some length of time with any of these options. I just feel as an organization, we're over-indexing on a solution. So I'm curious which ones of these you'd prefer? And for the title: ",12,2025-06-07 11:42:44
"**Context:** I joined a finance consultancy a few years ago and noticed that most people in my department are frustrated with the current ""software"" our engineering team has built over decades (yes - not years, decades). The issue is that the software consists of a bundle of Python scripts that repeatedly read large CSV files whenever a user interacts with it. The master CSV file ranges in size from 20MB to 1GB.

For example, if a user wants to select an option from a dropdown menu, clicking the dropdown triggers the reading and aggregation of a 1GB file, after which the frontend is ""returned"" 20 strings (the dropdown options). By ""returned,"" I mean that a new CSV file is created somewhere on the user's local file system, and the ""frontend"" picks it up. I tested a similar functionality using a Flask REST API, and once the CSV file is loaded into virtual memory, the process takes only 100ms‚Äîcompared to the current design, which takes a full minute (some of it is due to scripts needing to sort out dependencies, validation, etc.). However, our engineering team refuses to adopt web-based communication, arguing that it's not worth the effort. The idea of using a cloud-based relational database is essentially taboo; it has to be either CSV files or Python‚Äôs pickle dumps on each user's local system.

I have some experience in software engineering, so I made it my mission to redesign this legacy monster‚Äîwith the blessing of a senior manager. So far, the transition has gone incredibly well. Last year, I did a soft launch of a small subset of features, and within days, every person in my department was using it.

**Question:** My current design requires users to set up a virtual environment and run an installation script that sorts out any environment variables, dependencies, etc. Each time they want to start the software, they must run a local Flask API, which interacts with a React TypeScript frontend. When the Flask API starts, it loads all necessary files into memory, does validation and other things (takes around a minute). After that, every subsequent request is easy and takes on average 100 to 200 ms. However, I dislike that each user needs a fully configured environment. Version control is also a headache since every user must manually run an update script.

I‚Äôd like to move my Flask API to the cloud so that either:

1. **A single server serves all colleagues**, or
2. **Each colleague gets a dedicated node/pod.**

The problem with a single server is that it would quickly run out of virtual memory if 100+ colleagues loaded large datasets simultaneously. The problem with one node per colleague is the complexity‚Äîit would require Kubernetes (K8s) or AWS Fargate, along with an orchestrator to manage node creation and termination, which is a significant engineering effort.

I then considered making my Flask API stateless: storing large datasets in S3, using DynamoDB for file mapping, and loading data into virtual memory on every request. I converted some sample datasets to Parquet, reducing their size significantly (down to \~10MB), but I worry about added latency. Repeatedly reading the same data (given that each user makes 1‚Äì10 requests per minute) seems highly inefficient.

Am I missing any alternatives? Based on this, a local Flask API still seems like the best option‚Äîunless I want to pay for an expensive 64GB vRAM EC2 instance or invest significant time in building a node-per-user architecture.

Thanks!",12,2025-03-31 21:17:37
"I have an application that does some simple pre-processing to batch time series data and feeds it to another system. This downstream system requires data to be split into daily files for consumption. The way we do that is with Hive partitioning while processing and writing the data.

The problem is data processing tools cannot deal with this stupid partitioning system, failing with OOM; sometimes we have 3 years of daily data, which incurs in over a thousand partitions.

Our current data processing tool is Polars (using LazyFrames) and we were studying migrating to DuckDB. Unfortunately, none of these can handle the larger data we have with a reasonable amount of RAM. They can do the processing and write to disk without partitioning, but we get OOM when we try to partition by day. I've tried a few workarounds such as partitioning by year, and then reading the yearly files one at a time to re-partition by day, and still OOM.

Any suggestions on how we could implement this, preferably without having to migrate to a distributed solution?",12,2025-04-28 14:46:24
"What is the best possible solution to process the files in an S3 bucket in a sequential order. 

Use case is that an external systems generates CSV files and dump them on to S3 buckets. These CSV files consists of data from few  Oracle tables. These files needs to be processed in a sequential order in order to maintain the referential integrity of the data while loading into the Postgres RDS. If the files are not processed in an order, the load errors out with the reference data doesn't exist. What is a best solution to process the files on a S3 bucket in an order? ",12,2025-04-14 16:47:37
"I need to create a replica of a Dropbox folder on S3, including its folder structure and files, and ensure that when a file is uploaded or deleted in Dropbox, S3 is updated automatically to reflect the change. 



Is this possible? Can someone please tell me how to do this?",12,2025-04-22 12:12:36
"Guys, 

My pipeline on airflow was blowing memory and failing. I decide to read files in batches (50k collections per batch - mongodb - using cursor) and the memory problem was solved. The problem is now one file has around 100 partitioned JSON. Is this a problem?  Is this not recommended? It‚Äôs working but I feel it‚Äôs wrong. lol 
",12,2025-05-01 18:34:51
"Pardon the noob question. I'm building a simple ETL process using Airflow on a remote Linux server and need a way for users to upload input files and download processed files.

I would prefer a method that is easy to use for users like a shared drive (like Google Drive).

I've considered Syncthing, and in the worst case, SFTP access. What solutions do you typically use or recommend for this? Thanks!",12,2025-04-04 22:25:11
"Hey¬†r/dataengineering ,  
Google¬†BigQuery¬†recently¬†released¬†job-level¬†reservation¬†assignments‚Äîa¬†feature¬†that¬†lets you¬†choose¬†on-demand or¬†reserved capacity¬†for¬†each¬†query, not¬†just¬†at¬†the project level. This is a¬†huge¬†deal¬†for¬†anyone¬†trying¬†to optimize cloud¬†costs¬†or¬†manage¬†complex workloads. I¬†wrote¬†a¬†blog¬†post¬†breaking¬†down:

* What¬†this¬†new¬†feature¬†actually¬†means¬†(with¬†practical¬†SQL¬†examples)

* How to¬†decide¬†which¬†pricing model¬†to¬†use¬†for each¬†job

* How¬†we¬†use¬†the Rabbit¬†BQ Job Optimizer to¬†automate¬†these¬†decisions¬†

If¬†you‚Äôre interested¬†in¬†smarter¬†BigQuery cost¬†management, check it¬†out:

üëâ¬†[https://followrabbit.ai/blog/unlock-bigquery-savings-with-dynamic-job-level-optimization](https://followrabbit.ai/blog/unlock-bigquery-savings-with-dynamic-job-level-optimization?utm_source=reddit&utm_medium=dataengineering&utm_campaign=thoughtleadershipcsabi)  
Curious¬†to¬†hear¬†how¬†others¬†are approaching this‚Äîanyone already using job-level assignments? Any¬†tips or¬†gotchas¬†to¬†share?  
\#bigquery¬†#dataengineering¬†#cloud¬†#finops",12,2025-05-28 01:36:54
"Hi everyone,

I have a microservices architecture where I have a lambda function that takes an ID, sends it to an API for enrichment, and then resultant response is recorded in an S3 Bucket. My issue is that over \~200 concurrent lambdas and in effort to keep memory usage low, I am getting 1000's of small 30 - 200kb compressed ndjson files that make downstream computation a little challenging.

I tried to use Firehose but quickly get throttled and getting ""Slow Down."" error. Is there a tool or architecture decision I should consider besides just a downstream process that might consolidate these files perhaps in Glue?",12,2025-04-02 21:21:25
"
Subject: Seeking Feedback: CrossLink - Faster Data Sharing Between Python/R/C++/Julia via Arrow & Shared Memory

Hey r/dataengineering


I've been working on a project called CrossLink aimed at tackling a common bottleneck: efficiently sharing large datasets (think multi-million row Arrow tables / Pandas DataFrames / R data.frames) between processes written in different languages (Python, R, C++, Julia) when they're running on the same machine/node.
Mainly given workflows where teams have different language expertise.

The Problem:
We often end up saving data to intermediate files (CSVs are slow, Parquet is better but still involves disk I/O and serialization/deserialization overhead) just to pass data from, say, a Python preprocessing script to an R analysis script, or a C++ simulation output to Python for plotting. This can dominate runtime for data-heavy pipelines.

CrossLink's Approach:
The idea is to create a high-performance IPC (Inter-Process Communication) layer specifically for this, leveraging:
Apache Arrow: As the common, efficient in-memory columnar format.
Shared Memory / Memory-Mapped Files: Using Arrow IPC format over these mechanisms for potential minimal-copy data transfer between processes on the same host.


DuckDB: To manage persistent metadata about the shared datasets (unique IDs, names, schemas, source language, location - shmem key or mmap path) and allow optional SQL queries across them.


Essentially, it tries to create a shared data pool where different language processes can push and pull Arrow tables with minimal overhead.


Performance:
Early benchmarks on a 100M row Python -> R pipeline are encouraging, showing CrossLink is:
Roughly 16x faster than passing data via CSV files.
Roughly 2x faster than passing data via disk-based Arrow/Parquet files.

It also now includes a streaming API with backpressure and disk-spilling capabilities for handling >RAM datasets.


Architecture:
It's built around a C++ core library (libcrosslink) handling the Arrow serialization, IPC (shmem/mmap via helper classes), and DuckDB metadata interactions. Language bindings (currently Python & R functional, Julia building) expose this functionality idiomatically.


Seeking Feedback:
I'd love to get your thoughts, especially on:
Architecture: Does using Arrow + DuckDB + (Shared Mem / MMap) seem like a reasonable approach for this problem? 

Any obvious pitfalls or complexities I might be underestimating (beyond the usual fun of shared memory management and cross-platform IPC)?


Usefulness: Is this data transfer bottleneck a significant pain point you actually encounter in your work? Would a library like CrossLink potentially fit into your workflows (e.g., local data science pipelines, multi-language services running on a single server, HPC node-local tasks)?


Alternatives: What are you currently using to handle this? (Just sticking with Parquet on shared disk? Using something like Ray's object store if you're in that ecosystem? Redis? Other IPC methods?)


Appreciate any constructive criticism or insights you might have! Happy to elaborate on any part of the design.

I built this to ease the pain of moving across different scripts and languages for a single file. Wanted to know if it useful for any of you here and would be a sensible open source project to maintain.

It is currently built only for local nodes, but looking to add support with arrow flight across nodes as well. ",12,2025-03-31 08:43:18
"https://preview.redd.it/d3i71nv0kq2f1.jpg?width=3584&format=pjpg&auto=webp&s=25f84c3295ddb2b3ae06de97a0f28697c31073bd

I built a tool called SftpSync that lets you spin up an SFTP server with a dedicated user in one click.  
You can set how uploaded files should be processed, transformed, and validated ‚Äî and then get the final result via API or webhook.

Main features:

* SFTP server with user access
* File transformation and mapping
* Schema validation
* Webhook when processing is done
* Clean output available via API

Would love to hear what you think ‚Äî do you see value in this? Would you try it?

[sftpsync.io](http://sftpsync.io)",12,2025-05-24 13:47:01
"Hello, I am trying to make a data pipeline that fetches a huge amount of pdf files online and processes them and then uploads them back as csv rows into cloud. I am doing this on Python.  
I have 2 questions:  
1-Is it possible to process these pdf/docx files directly in memory without having to do an ""intermediate write"" on disk when I download them? I think that would be much more efficient and faster since I plan to go with batch processing too.  
2-I don't think the operations I am doing are complicated, but they will be time consuming so I want to do concurrent batch processing. I felt that using job queues would be unneeded and I can go with simpler multi threading/processing for each batch of files. Is there design pattern or architecture that could work well with this?

I already built an Object-Oriented code but I want to optimize things and also make it less complicated as I feel that my current code looks too messy for the job, which is definitely in part due to my inexperience in such use cases.",12,2025-04-30 12:51:53
"Can you imagine a world where no more huge price to pay or determine data ingestion frequency so it won't be costly to move data raw files like CSV to target data warehouse like SQL server. That is pay per compute.. am paying to run 15 threads aka Spark Pool compute always so I can move 15 tables delta data to target..Now here comes DataPig.. They say can move 200 tables delta less than 10 seconds..

How according benchmark it takes 45 min to write 1 million rows data to target tables using Azure Synapse spark pool.. but DataPig does it 8 sec to stage data into SQL server for same data. With leveraging only target compute power eliminating pay to play on compute side of spark and they implemented multithreaded parallel processing aka parallel 40 threads processing 40 tables changes at same time. Delta ingestion to milliseconds from seconds. Persevering both CDC and keeping only latest data for data warehouse for application like D365 is bang for money.

Let me know what you guys think. I build the engine so any feedback is valuable. We took one use case but with preserving base concept we can make both source Dataverse,SAP HANA, etc.. and target it can be SQL server, Snowflake,etc plug and play. So will industry ingest this shift in Big Data batch processing?",12,2025-04-17 06:42:59
"...the joys of memory and compute resources seems to be a neverending suck üò≠

We're building ETL pipelines, using Airflow in one K8s namespace and Spark in another (the latter having dedicated hardware). Most data workloads aren't really Spark-worthy as files are typically <20GB, and we keep hitting pain points where processes struggle in Airflow's memory (workers are 6Gi and 6 CPU, with a limit of 10GI; no KEDA or HPA). We are looking into more efficient data structures like DuckDB, Polars, etc or running ""mid-tier"" processes as separate K8s jobs but then we hit constraints like tools/libraries relying on Pandas use so we seem stuck with eager processes.

Case in point, I just learned that our teams are having to split files into smaller files of 125k records so Pydantic schema validation won't fail on memory. I looked into GX Core and see the main source options there again appear to be Pandas or Spark dataframes (yes, I'm going to try DuckDB through SQLAlchemy). I could bite the bullet and just say to go with Spark, but then our pipelines will be using Spark for QA and not for ETL which will be fun to keep clarifying. 

Sisyphus is the patron saint of Data Engineering... just sayin'

[Make it stoooooooooop!](https://preview.redd.it/qwikfhcpihue1.png?width=503&format=png&auto=webp&s=6565d874d8d2213835c172a8ed449b14cff8214a)

(there may be some internal sobbing/laughing whenever I see posts asking ""should I get into DE..."")",12,2025-04-12 23:08:19
"I am building a pipeline for the first time, using dlt, and it's kind of... janky. I feel like an imposter, just copying and pasting stuff into a zombie.

Ideally: SFTP (.csv) -> AWS S3 (.csv) -> Snowflake

Currently: I keep getting a JSONL file in the s3 bucket, which would be okay if I could get it into Snowflake table

* SFTP -> AWS: this keeps giving me a JSONL file
* AWS S3 -> Snowflake: I keep getting errors, where it is not reading the JSONL file deposited here

Other attempts to find issue:

* Local CSV file -> Snowflake: I am able to do this using read\_csv\_duckdb(), but not read\_csv()
* CSV manually moved to AWS -> Snowflake: I am able to do this with read\_csv()
* so I can probably do it directly SFTP -> Snowflake, but I want to be able to archive the files in AWS, which seems like best practice?

There are a few clients, who periodically drop new files into their SFTP folder. I want to move all of these files (plus new files and their file date) to AWS S3 to archive it. From there, I want to move the files to Snowflake, before transformations.

When I get the AWS middle point to work, I plan to create one table for each client in Snowflake, where new data is periodically appended / merged / upserted to existing data. From here, I will then transform the data.",12,2025-04-18 01:30:28
"...the joys of memory and compute resources seems to be a neverending suck üò≠

We're building ETL pipelines, using Airflow in one K8s namespace and Spark in another (the latter having dedicated hardware). Most data workloads aren't really Spark-worthy as files are typically <20GB, and we keep hitting pain points where processes struggle in Airflow's memory (workers are 6Gi and 6 CPU, with a limit of 10GI; no KEDA or HPA). We are looking into more efficient data structures like DuckDB, Polars, etc or running ""mid-tier"" processes as separate K8s jobs but then we hit constraints like tools/libraries relying on Pandas use so we seem stuck with eager processes.

Case in point, I just learned that our teams are having to split files into smaller files of 125k records so Pydantic schema validation won't fail on memory. I looked into GX Core and see the main source options there again appear to be Pandas or Spark dataframes (yes, I'm going to try DuckDB through SQLAlchemy). I could bite the bullet and just say to go with Spark, but then our pipelines will be using Spark for QA and not for ETL which will be fun to keep clarifying. 

Sisyphus is the patron saint of Data Engineering... just sayin'

[Make it stoooooooooop!](https://preview.redd.it/qwikfhcpihue1.png?width=503&format=png&auto=webp&s=6565d874d8d2213835c172a8ed449b14cff8214a)

(there may be some internal sobbing/laughing whenever I see posts asking ""should I get into DE..."")",12,2025-04-12 23:08:19
"Hello, I am trying to make a data pipeline that fetches a huge amount of pdf files online and processes them and then uploads them back as csv rows into cloud. I am doing this on Python.  
I have 2 questions:  
1-Is it possible to process these pdf/docx files directly in memory without having to do an ""intermediate write"" on disk when I download them? I think that would be much more efficient and faster since I plan to go with batch processing too.  
2-I don't think the operations I am doing are complicated, but they will be time consuming so I want to do concurrent batch processing. I felt that using job queues would be unneeded and I can go with simpler multi threading/processing for each batch of files. Is there design pattern or architecture that could work well with this?

I already built an Object-Oriented code but I want to optimize things and also make it less complicated as I feel that my current code looks too messy for the job, which is definitely in part due to my inexperience in such use cases.",12,2025-04-30 12:51:53
"Hi. I have a Databricks PySpark notebook that takes 20 minutes to run as opposed to one minute in on-prem Linux + Pandas. How can I speed it up?

It's not a volume issue. The input is around 30k rows. Output is the same because there's no filtering or aggregation; just creating new fields. No collect, count, or display statements (which would slow it down).¬†

The main thing is a bunch of mappings I need to apply, but it depends on existing fields and there are various models I need to run. So the mappings are different depending on variable and model. That's where the for loops come in.¬†

Now I'm¬†***not***¬†iterating over the dataframe itself; just over 15 fields (different variables) and 4 different mappings. Then do that 10 times (once per model).

The worker is m5d 2x large and drivers are r4 2x large, min/max workers are 4/20. This should be fine.¬†

I attached a pic to illustrate the code flow. Does anything stand out that you think I could change or that you think Spark is slow at, such as json.load or create\_map?¬†",13,2025-04-28 14:17:28
"Hi everyone, I was wondering if you know about a publicly available dataset large enough so that it can be used to practice spark and be able to appreciate the impact of optimised queries. I believe it is harder to tell in smaller datasets",13,2025-05-01 08:53:22
"PySpark 4 is out on PyPi and I also found this link: [https://dlcdn.apache.org/spark/spark-4.0.0/spark-4.0.0-bin-hadoop3.tgz](https://dlcdn.apache.org/spark/spark-4.0.0/spark-4.0.0-bin-hadoop3.tgz), which means we can expect Spark 4 soon ?

What are you mostly excited bout in Spark 4 ?",13,2025-05-27 16:24:50
"[https://mr3docs.datamonad.com/blog/2025-04-18-performance-evaluation-2.0](https://mr3docs.datamonad.com/blog/2025-04-18-performance-evaluation-2.0)



In this article, we report the results of evaluating the performance of the following systems using the 10TB TPC-DS Benchmark.

1. Trino 468 (released in December 2024)
2. Spark 4.0.0-RC2 (released in March 2025)
3. Hive 4.0.0 on Tez (built in February 2025)
4. Hive 4.0.0 on MR3 2.0 (released in April 2025)",13,2025-04-21 09:15:57
"Hey Data Engineers üëã

I recently launched [**DEtermined**](https://determinedeng.com/) ‚Äì an open platform focused on *real-world Data Engineering prep* and hands-on learning.

It‚Äôs built for the community, by the community ‚Äì designed to cover the **6 core categories** that every DE should master:

* SQL
* ETL/ELT
* Big Data
* Data Modeling
* Data Warehousing
* Distributed Systems

Every day, I break down a DE question or a real-world challenge on my **Substack newsletter** ‚Äì [DE Prep](https://deprep.substack.com/) ‚Äì and walk through the entire solution like a mini masterclass.

üîç **Latest post:**  
**‚ÄúDecoding Spark Query Plans: From Black Box to Bottlenecks‚Äù**  
‚Üí I dove into how Spark's query execution works, why your joins are slow, and how to interpret the physical plan like a pro.  
[Read it here](https://deprep.substack.com/p/prep-20-understanding-spark-query)

This week‚Äôs focus? **Spark Performance Tuning.**

If you're prepping for DE interviews, or just want to sharpen your fundamentals with real-world examples, I think you‚Äôll enjoy this.

Would love for you to check it out, subscribe, and let me know what you'd love to see next!  
And if you're working on something similar, I‚Äôd love to collaborate or feature your insights in an upcoming post!

You can also follow me on [LinkedIn](https://www.linkedin.com/in/nvkanirudh/), where I share **daily updates** along with **visually-rich infographics** for every new Substack post.  
  
Would love to have you join the journey! üöÄ

Cheers üôå  
*Data Engineer | Founder of DEtermined*",13,2025-05-28 20:03:34
"Hi guys, i am moving to on-prem managed Spark applications with Kuberenetes. I am wondering what do u use for logging? I am talking about Python and PySpark. Do u setup log4j? Or just use Python's logging library for application? What is the standard here? I have not seen much about log4j within PySpark.",13,2025-04-04 15:01:43
"Hey folks,  
I‚Äôve worked with Spark for years and tried using PyDeequ for data quality ‚Äî but ran into too many blockers:

* No row-level visibility
* No custom checks
* Clunky config
* Little community activity

So I built **üöÄ SparkDQ** ‚Äî a lightweight, plugin-ready DQ framework for PySpark with Python-native and declarative config (YAML, JSON, etc.).

Still early stage, but already offers:

* Row + aggregate checks
* Fail-fast or quarantine logic
* Custom check support
* Zero bloat (just PySpark + Pydantic)

If you're working with Spark and care about data quality, I‚Äôd love your thoughts:

‚≠ê [GitHub ‚Äì SparkDQ](https://github.com/sparkdq-community/sparkdq)  
‚úçÔ∏è [Medium: Why I moved beyond PyDeequ](https://medium.com/@marcel.kennert/goodbye-pydeequ-time-to-upgrade-your-data-quality-stack-d86fe9cdc5be)

Any feedback, ideas, or stars are much appreciated. Cheers!",13,2025-05-01 10:30:06
"In pyspark 3.4 you can write sql as 

spark.sql(SELECT * FROM {df_input}, df_input = df_input) 

The popular sql linters I tried SQL Formatter and and Prettier SQL Vscode currently does not accommodate{}. Does anyone know of any linters that does? Thank you",13,2025-04-04 14:54:19
"Hi,

  
I'm trying to make parallel API calls using pyspark RDD.  
I have list of tuples like : (TableName, URL, Offset) . I'm making RDD out of it. So the structure looks like something like this :  


|TableName|URL|Offset|
|:-|:-|:-|
|Invoices|[https://api.example.com/invoices](https://api.example.com/invoices)|0|
|Invoices|[https://api.example.com/invoices](https://api.example.com/invoices)|100|
|Invoices|[https://api.example.com/invoices](https://api.example.com/invoices)|200|
|PurchaseOrders|[https://api.example.com/purchaseOrders](https://api.example.com/purchaseOrders)|0|
|PurchaseOrders|[https://api.example.com/purchaseOrders](https://api.example.com/purchaseOrders)|150|
|PurchaseOrders|[https://api.example.com/purchaseOrders](https://api.example.com/purchaseOrders)|300|

 For each RDD, a function is called to extract data from API and returns a dictionary of data.

Later on I want to filter RDD based on table name and create separate dataframes out of it. Each table has a different schema, so I'm avoiding creating a data frame that could include extra irrelevant schemas for my tables

    rdd = spark.sparkContext.parallelize(offset_tuple_list)
    fetch_rdd = rdd.flatMap(lambda tuple:get_data(tuple,extraction_date,token)).cache()
    
    ## filter RDD per table
    invoices_rdd = fetch_rdd.filter(lambda row: row[""table""] == ""Invoices"")
    purchaseOrders_rdd = fetch_rdd.filter(lambda row: row[""table""] == ""PurchaseOrders"")
    
    ## convert it to json for automatic schema inference by read.json
    invoices_json_rdd = invoices_rdd.map(lambda row: json.dumps(row))
    purchaseOrders_json_rdd = purchaseOrders_rdd.map(lambda row: json.dumps(row))
    
    invoices_df = spark.read.json(invoices_json_rdd)
    purchaseOrders_df = spark.read.json(purchaseOrders_json_rdd)

I'm using cache() to avoid multiple API calls and do it only once.  
My problem is that caching won't work for me if **invoices\_df and purchaseOrders\_df**  are running by different executors. If they are run on the same executor then one takes 3 min and the other a few seconds, since it uses the cache(). If not both take 3 min + 3 min = 6min calling API twice.  
  
This behaviour is random, sometimes it runs on separate executors and I can see locality becomes RACK\_LOCAL instead of PROCESS\_LOCAL  


Any idea how I can make all executors use the same cached RDD? ",13,2025-05-23 18:06:20
"In this opinionated article I am going to explain why I believe we have reached peak Spark usage and why it is only downhill from here.

# Before Spark

Some will remember that 12 years ago [Pig](https://pig.apache.org), [Hive](https://hive.apache.org), [Sqoop](https://sqoop.apache.org), [HBase](https://hbase.apache.org) and MapReduce were all the rage. Many of us were under the spell of [Hadoop](https://hadoop.apache.org) during those times.

# Enter Spark

The brilliant [Matei Zaharia](https://www.linkedin.com/in/mateizaharia/) started working on Spark sometimes before 2010 already, but adoption really only began after 2013.  
The lazy evaluation and memory leveraging as well as other [innovative features](https://www.youtube.com/watch?v=w0Tisli7zn4&t=97s) were a huge leap forward and I was dying to try this new promising technology.  
My then CTO was visionary enough to understand the potential and for years since, I, along with many others, ripped the benefits of an only improving Spark.

# The Loosers

How many of you recall companies like [Hortonworks](https://hortonworks.com/wp-content/uploads/2013/11/Webinar.HDP2_.20131112.pdf) and [Cloudera](https://www.cloudera.com/about.html)? Hortonworks and Cloudera merged after both becoming public, only to be taken private a few years later. Cloudera still exists, but not much more than that.

Those companies were yesterday‚Äôs [Databricks](https://www.databricks.com) and they bet big on the Hadoop ecosystem and not so much on Spark.

# Hunting decisions

In creating Spark, Matei did what any pragmatist would have done, he piggybacked on the existing Hadoop ecosystem. This allowed Spark not to be built from scratch in isolation, but integrate nicely in the Hadoop ecosystem and supporting tools.

There is just one problem with the Hadoop ecosystem‚Ä¶it‚Äôs exclusively **JVM based**. This decision has fed and made rich thousands of consultants and engineers that have fought with the [GC](https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)) and inconsistent memory issues for years‚Ä¶and still does. The JVM is a solid choice, safe choice, but despite more than 10 years passing and Databricks having the plethora of resources it has, some of Spark's core issues with managing memory and performance just can't be fixed.

# The writing is on the wall

Change is coming, and few are noticing it ([some do](https://nolanlawson.com/2024/10/20/why-im-skeptical-of-rewriting-javascript-tools-in-faster-languages/?utm_source=chatgpt.com)). This change is happening in all sorts of supporting tools and frameworks.

What do [uv](https://docs.astral.sh/uv/), [Pydantic](https://docs.pydantic.dev/latest/), [Deno](https://deno.com), [Rolldown](https://rolldown.rs) and the Linux kernel all have in common that no one cares about...for now? They all have a Rust backend or have an increasingly large Rust footprint. These handful of examples are just the tip of the iceberg.

Rust is the most prominent example and the forerunner of a set of languages that offer performance, a completely different memory model and some form of usability that is hard to find in market leaders such as C and C++. There is also Zig which similar to Rust, and a bunch of other languages that can be found in TIOBE's top 100.

The examples I gave above are all of tools for which the primary target are not Rust engineers but Python or JavaScipt. Rust and other languages that allow easy interoperability are increasingly being used as an efficient reliable backend for frameworks targeted at completely different audiences.

There's going to be less of ""by Python developers for Python developers"" looking forward.

# Nothing is forever

Spark is here to stay for many years still, hey, Hive is still being used and maintained, but I belive that peak adoption has been reached, there's nowhere to go from here than downhill. Users don't have much to expect in terms of performance and usability looking forward.

On the other hand, frameworks like [Daft](https://www.getdaft.io) offer a completely different experience working with data, no strange JVM error messages, no waiting for things to boot, just bliss. Maybe it's not Daft that is going to be the next best thing, but it's inevitable that Spark will be overthroned.

# Adapt

Databricks better be ahead of the curve on this one.  
Instead of using scaremongering marketing gimmicks like labelling the use of engines other than Spark as *Allow External Data Access*, it better ride with the wave.",13,2025-04-30 05:45:51
"When i am testing things often i need to run some counts  in databricks.

What is the prefered way?

I am creating a pyspark.dataframe  using spark.sql statements and later
DF.count().

Further information can be provided.",13,2025-03-31 17:06:54
"I‚Äôve benchmarked it. For use cases in my specific industry it‚Äôs something like x5, x7 more efficient in computation. It looks like it‚Äôs pretty revolutionary in terms of cost savings. It‚Äôs faster and cheaper.

The problem is PySpark is like using a missile to kill a worm. In what I‚Äôve seen, it‚Äôs totally overpowered for what‚Äôs actually needed. It starts spinning up clusters and workers and all the tasks. 

I‚Äôm not saying it‚Äôs not useful. It‚Äôs needed and crucial for huge workloads but most of the time huge workloads are not actually what‚Äôs needed.

Spark is perfect with big datasets and when huge data lake where complex computation is needed. It‚Äôs a marvel and will never fully disappear for that.

Also Polars syntax and API is very nice to use. It‚Äôs written to use only one node. 

By comparison Pandas syntax is not as nice (my opinion). 

And it‚Äôs computation is objectively less efficient.  It‚Äôs simply worse than Polars in nearly every metric in efficiency terms.

I cant publish the stats because it‚Äôs in my company enterprise solution but search on open Github other people are catching on and publishing metrics.

Polars uses Lazy execution, a Rust based computation (Polars is a Dataframe library for Rust). Plus Apache Arrow data format.

It‚Äôs pretty clear it occupies that middle ground where Spark is still needed for 10GB/ terabyte / 10-15 million row+ datasets. 

Pandas is useful for small scripts (Excel, Csv) or hobby projects but Polars can do everything Pandas can do and faster and more efficiently.

Spake is always there for the those use cases where you need high performance but don‚Äôt need to call in artillery. 

Its syntax means if you know Spark is pretty seamless to learn.

I predict as well there‚Äôs going to be massive porting to Polars for ancestor input datasets.

You can use Polars for the smaller inputs that get used further on and keep Spark for the heavy workloads. The problem is converting to different data frames object types and data formats is tricky. Polars is very new.

Many legacy stuff in Pandas over 500k rows where costs is an increasing factor or cloud expensive stuff is also going to see it being used.

",13,2025-04-30 08:27:02
"I‚Äôve benchmarked it. For use cases in my specific industry it‚Äôs something like x5, x7 more efficient in computation. It looks like it‚Äôs pretty revolutionary in terms of cost savings. It‚Äôs faster and cheaper.

The problem is PySpark is like using a missile to kill a worm. In what I‚Äôve seen, it‚Äôs totally overpowered for what‚Äôs actually needed. It starts spinning up clusters and workers and all the tasks. 

I‚Äôm not saying it‚Äôs not useful. It‚Äôs needed and crucial for huge workloads but most of the time huge workloads are not actually what‚Äôs needed.

Spark is perfect with big datasets and when huge data lake where complex computation is needed. It‚Äôs a marvel and will never fully disappear for that.

Also Polars syntax and API is very nice to use. It‚Äôs written to use only one node. 

By comparison Pandas syntax is not as nice (my opinion). 

And it‚Äôs computation is objectively less efficient.  It‚Äôs simply worse than Polars in nearly every metric in efficiency terms.

I cant publish the stats because it‚Äôs in my company enterprise solution but search on open Github other people are catching on and publishing metrics.

Polars uses Lazy execution, a Rust based computation (Polars is a Dataframe library for Rust). Plus Apache Arrow data format.

It‚Äôs pretty clear it occupies that middle ground where Spark is still needed for 10GB/ terabyte / 10-15 million row+ datasets. 

Pandas is useful for small scripts (Excel, Csv) or hobby projects but Polars can do everything Pandas can do and faster and more efficiently.

Spake is always there for the those use cases where you need high performance but don‚Äôt need to call in artillery. 

Its syntax means if you know Spark is pretty seamless to learn.

I predict as well there‚Äôs going to be massive porting to Polars for ancestor input datasets.

You can use Polars for the smaller inputs that get used further on and keep Spark for the heavy workloads. The problem is converting to different data frames object types and data formats is tricky. Polars is very new.

Many legacy stuff in Pandas over 500k rows where costs is an increasing factor or cloud expensive stuff is also going to see it being used.

",13,2025-04-30 08:27:02
"Hello, I recently started learning spark.

I wanted to clear up this doubt, but couldn't find a clear answer, so please help me out.

Let's assume I have a large dataset of like 200 gb, with each data instance (like, lets assume a pdf) of 1 MB each.  
I read somewhere (mostly gpt) that I/O bottleneck can cause the performance to dip, so how can I really deal with this ? Should I try to combine these pdfs into like larger sizes, around 128 MB before asking spark to create partitions ? If I do so, can I later split this back into pdfs ?  
I kinda lack in both the language and spark department, so please correct me if i went somewhere wrong.

Thanks!",13,2025-04-24 07:21:24
"In this opinionated article I am going to explain why I believe we have reached peak Spark usage and why it is only downhill from here.

# Before Spark

Some will remember that 12 years ago [Pig](https://pig.apache.org), [Hive](https://hive.apache.org), [Sqoop](https://sqoop.apache.org), [HBase](https://hbase.apache.org) and MapReduce were all the rage. Many of us were under the spell of [Hadoop](https://hadoop.apache.org) during those times.

# Enter Spark

The brilliant [Matei Zaharia](https://www.linkedin.com/in/mateizaharia/) started working on Spark sometimes before 2010 already, but adoption really only began after 2013.  
The lazy evaluation and memory leveraging as well as other [innovative features](https://www.youtube.com/watch?v=w0Tisli7zn4&t=97s) were a huge leap forward and I was dying to try this new promising technology.  
My then CTO was visionary enough to understand the potential and for years since, I, along with many others, ripped the benefits of an only improving Spark.

# The Loosers

How many of you recall companies like [Hortonworks](https://hortonworks.com/wp-content/uploads/2013/11/Webinar.HDP2_.20131112.pdf) and [Cloudera](https://www.cloudera.com/about.html)? Hortonworks and Cloudera merged after both becoming public, only to be taken private a few years later. Cloudera still exists, but not much more than that.

Those companies were yesterday‚Äôs [Databricks](https://www.databricks.com) and they bet big on the Hadoop ecosystem and not so much on Spark.

# Hunting decisions

In creating Spark, Matei did what any pragmatist would have done, he piggybacked on the existing Hadoop ecosystem. This allowed Spark not to be built from scratch in isolation, but integrate nicely in the Hadoop ecosystem and supporting tools.

There is just one problem with the Hadoop ecosystem‚Ä¶it‚Äôs exclusively **JVM based**. This decision has fed and made rich thousands of consultants and engineers that have fought with the [GC](https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)) and inconsistent memory issues for years‚Ä¶and still does. The JVM is a solid choice, safe choice, but despite more than 10 years passing and Databricks having the plethora of resources it has, some of Spark's core issues with managing memory and performance just can't be fixed.

# The writing is on the wall

Change is coming, and few are noticing it ([some do](https://nolanlawson.com/2024/10/20/why-im-skeptical-of-rewriting-javascript-tools-in-faster-languages/?utm_source=chatgpt.com)). This change is happening in all sorts of supporting tools and frameworks.

What do [uv](https://docs.astral.sh/uv/), [Pydantic](https://docs.pydantic.dev/latest/), [Deno](https://deno.com), [Rolldown](https://rolldown.rs) and the Linux kernel all have in common that no one cares about...for now? They all have a Rust backend or have an increasingly large Rust footprint. These handful of examples are just the tip of the iceberg.

Rust is the most prominent example and the forerunner of a set of languages that offer performance, a completely different memory model and some form of usability that is hard to find in market leaders such as C and C++. There is also Zig which similar to Rust, and a bunch of other languages that can be found in TIOBE's top 100.

The examples I gave above are all of tools for which the primary target are not Rust engineers but Python or JavaScipt. Rust and other languages that allow easy interoperability are increasingly being used as an efficient reliable backend for frameworks targeted at completely different audiences.

There's going to be less of ""by Python developers for Python developers"" looking forward.

# Nothing is forever

Spark is here to stay for many years still, hey, Hive is still being used and maintained, but I belive that peak adoption has been reached, there's nowhere to go from here than downhill. Users don't have much to expect in terms of performance and usability looking forward.

On the other hand, frameworks like [Daft](https://www.getdaft.io) offer a completely different experience working with data, no strange JVM error messages, no waiting for things to boot, just bliss. Maybe it's not Daft that is going to be the next best thing, but it's inevitable that Spark will be overthroned.

# Adapt

Databricks better be ahead of the curve on this one.  
Instead of using scaremongering marketing gimmicks like labelling the use of engines other than Spark as *Allow External Data Access*, it better ride with the wave.",13,2025-04-30 05:45:51
"Hello all! etl4s is a tiny, zero-dep Scala lib: [https://github.com/mattlianje/etl4s](https://github.com/mattlianje/etl4s) (that plays great with Spark)

We are now using it heavily @ Instacart to turn Spark spaghetti into clean, config-driven pipelines

Your veteran feedback helps a lot!",13,2025-05-28 14:29:31
"I‚Äôm looking to move some of my teams etl away from redshift and on to AWS glue. 

I‚Äôm noticing that the spark sql data frames don‚Äôt sort back in the same order in the case of having nulls vs redshift. 

My hope was to port over the Postgres sql to spark sql and end up with very similar output. 

Unfortunately it‚Äôs looking like it‚Äôs off. 
For instance if I have a window function for row count, the same query assigns the numbers to different rows in spark. 

What is the best path forward to get the sorting the same?",13,2025-04-15 01:06:55
"please explain the key differences between using Aspects , Aspect Types and Tags , Tags Template in Dataplex Catalog.¬†  
  
\- We use Tags to define the business metadata for the an entry ( BQ Table ) using Tag Templates.¬†  
\- Why we also have aspect and aspect types which also are similar to Tags & Templates.¬†  
\- If Aspect and Aspect Types are modern and more robust version of Tags and Tag Templates will Tags will be removed from Dataplex Catalog ?  
\- I just need to understand why we have both if both have similar functionality.¬†",14,2025-04-23 03:19:22
"I'm starting a new job (a startup that is doubling in size every year) and the IT director has already warned me that they have a lot of problems with data structure changes, both due to new implementations in internally developed software and in those developed externally.

My question is whether I should prepare the central architecture using data warehouse or lakehouse, since the current data volume is still quite small <500 GB, but as I said, constant changes in data structure have been a problem.

By the way, I will be the first data engineer on the analytics team.",14,2025-03-29 19:27:40
"I have about 20 years worth of flat files stored in a folder on a network drive as a result of lackluster data practices. Essentially, three different flat files get printed to this folder on a nightly bases that represent three different types of data (think: person, sales, products). Essentially this data could exist as three separate long tables with date as key. 

I'd like to establish a proper data warehouse, but am unsure of how to best handle the process of warehousing these flats. I have been interfacing with the data through Python Pandas so far, but the company has a SQL server...It would probably be best to place the warehouse as a database on the server, then pull/manipulate the data from there? But what is tripping me up is the order of operations to perform in the warehousing procedure. I don't believe I would be able to dump into SQL server without profiling the data first as number of columns and the type of data stored in the flat files may have changed throughout the years.



I am essentially struggling with how to sequence the process of : network drive flats > sql server db:

  
My concerns are:

Best method to profile the data?

Best way to store the metadata?

Throw flats into SQL server and then query them from there to perform data transformations/validations? 

  \-- It seems without knowing the meta data, I should perform this step in Pandas first before loading into SQL server? What is the best practice for that? perform operations on each flat file separately or combine first (e.g., should I clean the data during the loop or after combining tables)?

   \-- Right now, I am creating a list of flat files, using that list to create a dictionary of dataframes, and then using that dictionary to create a dataframe of dataframes to group and concatenate into 3 long tables -- am I convoluting this process?

How to approach data cleaning/validation/and additional column calculations? e.g. -- Should I perform these procedures on each file separately before concatenating into a long table or perform these procedures after concatenation?-- Should I even concatenate into longs or keep them separate and define a relationship to their keys stored in a separate table?

How many databases for this process? One for raws? One for staging? A third as the datawarehouse to be queried?

When to stage and how much of the process to perform in RAM/behind the scenes before printing to a new table?

Should I consider compressing the data at any point in the process? (e.g. store as Parquet)



The data gets used for data analytics and to assemble reports/dashboards. Ideally, I would like to eliminate having to perform as many joins as possible during the querying for analysis process. I'd also like to orchestrate the warehouse so that adjustments only need to happen in a single place and propagate throughout the pipeline with a history of adjustments stored as record.",14,2025-04-24 18:18:34
"Checkout this amazing post about Kimball's Approach 

[https://medium.com/@adityasharmah27/kimballs-approach-the-sorcerer-s-stone-of-data-warehousing-9658f292eeb4](https://medium.com/@adityasharmah27/kimballs-approach-the-sorcerer-s-stone-of-data-warehousing-9658f292eeb4)",14,2025-04-15 10:05:17
"I might be asking naive question, but looking forward for some good discussion and experts opinion. Currently I'm working on a solution basically azure functions which extracts data from different sources and make the data available in snowflake warehouse for the users to write their own analytics model on top of it, currently both data model and users business model is sitting on top of same database and schema the downside of this is objects under schema started growing and also we started to see the responsibility of the user model started to be blurred like it is being pushed on to engineering team for maintaince which is creating kind of urgent user request to be addressed mid sprint. I'm sure we are not the only one had this issue just started this discussion on how others tackled this scenario and what are the pros and cons of each scenario. If we can separate both modellings it will be easy incase if other teams decide to use the data from warehouse.",14,2025-04-30 18:14:34
"So,

  
as always that's for the insight from other people, I find a lot of these discussions around points very entertaining and very helpful!

I'm having an argument with someone who is several levels above me. This might sound petty so I apologise in advance. It centres around the definition of a Mart. Our Mart is a single Fact with around 20 dimensions. The Fact is extremely wide and deep. Indeed we usually put it into a de normalised table for reporting. To me this isn't a MART as it isn't based on requirements but rather a star schema that supposedly servers multiple purposed or potential purposes. When engaged on requirements the person leans on there experience in the domain and says a user probable wants to do X, Y and Z. I've never seen anything written down. Constantly that report also defers to Kimball methodology and how this follows them closely. My take on the book is that these things need to be based of requirement, business requirements. 

My questions is, is it fair to say that a data mart needs to have requirements and ideally a business domain in mind or else its just a star schema?

Yes this  is  very theoretical... yes I probable need a hobby but look there hasn't been a decent RTS game in years and its friday!!!

Have a good weekend everyone",14,2025-04-04 13:41:31
"Higher ed institutions are under pressure to improve reporting, optimize funding efforts, and centralize siloed systems ‚Äî but most are still working with outdated or disconnected data infrastructure.

This blog breaks down how a modern data warehouse helps universities:

* Streamline compliance reporting
* Support grant/funding visibility
* Improve decision-making across departments

It‚Äôs a solid resource for anyone working in edtech, institutional research, or data architecture in education.

üîó Read it here:  
[Data Warehousing for Universities: Compliance & Funding](https://data-sleek.com/blog/data-warehousing-for-universities-meeting-compliance-funding-challenges/) 

I would love to hear from others working in higher education. What platforms or approaches are you using to integrate your data?",14,2025-04-16 16:20:34
"Hello folks, I recently joined a research center with a mission to manage data generated from our many labs. This is my first time building data infrastructure, I'm eager to learn from you in the industry.

We deal with a variety of data. Time-series from sensor data log, graph data from knowledge graph, and vector data from literature embedding. We also have relational data coming from characterization. Right now, each lab manages their own data, they are all saved as Excel for csv files in disperse places.

From initial discussion, we think that we should do the following:

A. Find databases to house the lab operational data.

B. Implement a data lake to centralize all the data from different labs

C. Turn all relational data to documents (JSON), as schema might evolve and we don't really do heave analytics or reporting, AI/ML modelling is more of the focus.

If you have any comments on the above points, they will be much appreciated.

I also have a question in mind:

1. For databases, is it better to find specific database for each type of data (neo4j for graph, Chroma for vector...etc), or we would be better of with a general purpose database (e.g. Cassandra) that houses all types of data to simplify managing processes but to lose specific computing capacity for each data type(for example, Cassandra can't do graph traversal)?
2. Cloud infrastructure seems to be the trend, but we have our own data center so we need to leverage it. Is it possible to use the managed solution from Cloud provides (Azure, AWS, we don't have a preference yet) and still work with our own storage and compute on-prem?

Thank you for reading, would love to hear from you.",14,2025-05-02 15:01:04
"Hi, I need help. I need a proper architecture for a department, and I am trying to get a data lake/warehouse.

Why: We have a lot of data sources from SaaS to manually created documents. We use a lot of SaaS products, but we have no centralised repository to store and stage the data, so we end up with a lot of workaround such as using SharePoint and csv stored in folders for reporting. We also change SaaS products quite frequently, so sources can change often. It is difficult to do advanced analytics. 

I prefer a lake & warehouse approach because (1) for SaaS users, they can can just drop the data to the lake and (2) transformation and processing can be done for reporting, and we could combine the datasets even when we change the SaaS software. 

My huge considerations are that (1) the data is to be accessible within the department only and (2) it has to be decent cost. Currently considered Azure Data Lake Storage Gen2 & DataBricks, or Snowflake (to have both the lake and warehouse). My previous experience was only with Data Lake Storage Gen2.

I'm willing to work my way up for my technical limitations, but at this stage I am exploring the software solutions to get the buy in to kickstart this project. 

Any sharing is much appreciated, and if you worked with such an environment, I appreciate your guidance and learnings as well. Thank you in advance.",14,2025-04-08 16:27:08
Check out my latest blog on data warehouses! Discover powerful insights and strategies that can transform your data management. Read it here: https://medium.com/@adityasharmah27/data-warehouse-essentials-guide-706d81eada07!,14,2025-03-31 13:13:37
"Hi everyone!

I‚Äôd like to ask for your advice on designing a relational data warehouse fed from our ERP system. We plan to use Power BI as our reporting tool, and all departments in the company will rely on it for analytics.

The challenge is that teams from different departments expect **the data to be fully related and ready** to use when building dashboards, minimizing the need for additional modeling. We‚Äôre struggling to determine the best approach to meet these expectations.

What would you recommend?

Should all dimensions and facts be pre-related in the data warehouse, even if it adds complexity?

Creating separate data models in Power BI for different departments/use cases?

Handling all relationships in the data warehouse and exposing them via curated datasets?

Should we empower Power BI users to create their own data models, or enforce strict governance with documented relationships?

Thanks in advance for your insights! ",14,2025-04-14 07:55:22
"I might be asking naive question, but looking forward for some good discussion and experts opinion. Currently I'm working on a solution basically azure functions which extracts data from different sources and make the data available in snowflake warehouse for the users to write their own analytics model on top of it, currently both data model and users business model is sitting on top of same database and schema the downside of this is objects under schema started growing and also we started to see the responsibility of the user model started to be blurred like it is being pushed on to engineering team for maintaince which is creating kind of urgent user request to be addressed mid sprint. I'm sure we are not the only one had this issue just started this discussion on how others tackled this scenario and what are the pros and cons of each scenario. If we can separate both modellings it will be easy incase if other teams decide to use the data from warehouse.",14,2025-04-30 18:14:34
"Hi everyone!

Looking for someone with a good experience in Informatica DEI/BDM. Currently I am trying to read binary data from Kafka topic that represents XML files.

I have created a mapping that is reading this topic, and enabled column projection on the data column while specifying the XSD schema for the file.

I then create the corresponding target on HDFS with same schema and mapped the columns.

The issue is that when running the mapping I am having a NullPointerException linked to a function called populateBooleans.

Have no idea what may be wrong.
Anyone has a potential idea or suggestions? How can I debug it further?",14,2025-06-04 07:00:51
"Between GPS logs, fuel cards, and maintenance reports, our fleet data used to live everywhere ‚Äî and nowhere at the same time.

We recently explored how cloud-based data warehousing can clean that up. Better asset visibility, fewer surprises, and way easier decision-making.

Here‚Äôs a blog that breaks it down if you're curious:  
üîó¬†[Fleet Management & Cloud-Based Warehousing](https://data-sleek.com/blog/fleet-management-cloud-based-data-warehousing/)

Curious how others are solving this ‚Äî are you centralizing your data or still working across multiple systems?",14,2025-05-23 04:41:29
"Books, articles, courses... what resources have been useful to you for learning how to develop production-ready APIs? Production-ready meaning robust, secure, performant, modular etc

Thanks!",15,2025-04-02 22:43:35
"Hey everyone, Databricks and Datapao are running a free Field Lab in London on April 29. It‚Äôs a full-day, hands-on session where you‚Äôll build an end-to-end data pipeline using streaming, Unity Catalog, DLT, observability tools, and even a bit of GenAI + dashboards. It‚Äôs very practical, lots of code-along and real examples. Great if you're using or exploring Databricks. [https://events.databricks.com/Datapao-Field-Lab-April](https://events.databricks.com/Datapao-Field-Lab-April) 

",15,2025-04-21 05:17:35
"Hello, 

I am not really a dataengineer but after looking at what I'm going to be working on I may be one soon. 

  
Ok to start with the project, I work for a clinical research company and we currently are pulling reports manually and working with them in excel (occasionally making visualizaitons).  We pull from two sources, one we own but can't access (we could probably ask but we want a proof of concept first) but the other source we can use their API to access our data on their system.  

I am looking for open-source (free) programs I can use to take the information given in the API break it into a full database (dataset tables) and keep in constantly updated in a gateway.  In this phase of the project I am more invested in being able to do an API call and automatically pulling the data to set it into the appropriate schema. 

I have a really good understanding of dataset creation put I am new to the scripting an API side.  

I don't really know what else to add but if you have any follow up questions please comment. 

  
I appreciate any help or advice you can give me. (I will be using our lord and savior youtube to learn as much as I can about whatever you suggest). ",15,2025-05-23 23:32:35
"Hey all, we're working on a group project and need help with the UI. It's an application to help data professionals quickly analyze datasets, identify quality issues and receive recommendations for improvements ( [https://github.com/Ivan-Keli/Data-Insight-Generator](https://github.com/Ivan-Keli/Data-Insight-Generator) )

1. Backend; Python with FastAPI
2. Frontend; Next.js with TailwindCSS
3. LLM Integration; Google Gemini API and DeepSeek API",15,2025-04-28 12:16:43
"I see this has been asked prior but I didn't see a clear answer. We have a smallish database (glorified spreadsheet) where one field contains text. It houses details regarding customers, etc calling in for various issues. For various reasons (in-house) they want to keep using the simple app (it's a SharePoint List). I can easily download the data to a CSV file, for example, but is there a fairly simple method (AI?) to make sense of this data and correlate it? Maybe a creative prompt? Or is there a tool for this? (I'm not a software engineer). Thanks!",15,2025-04-04 12:39:42
"My usual flow looked like:

1. Load CSV in a notebook
2. Write boilerplate to clean/inspect
3. Switch to another tool (or hack together Plotly) to visualize
4. Manually handle app hosting or sharing
5. Repeat for every new dataset

This reduces that to a chat interface + a real-time execution engine. Everything is transparent. no black box stuff. You see the code, own it, modify it 

  
btw if youre interested in trying some of the experimental features we're building, shoot me a DM. Always looking for feedback from folks who actually work with data day-to-day [https://app.preswald.com/](https://app.preswald.com/)

https://reddit.com/link/1k7elh2/video/y3mb2s4bhxwe1/player

  
",15,2025-04-25 06:53:46
"Hi guys,

I‚Äôve built a small tool called [**DataPrep**](https://data-prep.app) that lets you visually **explore and clean datasets** in your browser without any coding requirement.

You can try the live demo here (no signup required):  
 [demo.data-prep.app](https://demo.data-prep.app)

  
I work with data pipelines and I often needed a quick way to inspect raw files, test cleaning steps, and get some insights into my data without jumping into Python or SQL and for that I started working on **DataPrep**.  
The app is in its **MVP / Alpha** stage.

It'd be really helpful if you guys can try it out and provide some feedback on some topics like :

* Would this save time in your workflows ?
* What features would make it more useful ?
* Any integrations or export options that should be added to it ?
* How can the UI / UX be improved to make it more intuitive ?
* Bugs encountered

Thanks in advance for giving it a look. Happy to answer any questions regarding this.",15,2025-05-23 18:42:04
"Hi all,

Has anybody pulled cornerstone training data using their APIs or used anyother method to pull the data?",15,2025-04-08 15:54:25
"So I'm working on a project where we're building out an ETL pipeline to a Microsoft SQL Server database. But the managers want a UI to allow them to see the data that's been uploaded, make spot changes where necessary and have those changes go through a review process.

I've tested Directus, Appsmith and baserow. All are kind of fine, though I'd prefer the team and time to build out an app even in something like Shiny that would allow for more fine grained debugging when needed. 

What are you all using for this? It seems to be the kind of internal tool everyone is using in one way or another. Another small detail is the solution has to be available for on-prem use.",15,2025-06-09 17:26:46
"Hello everyone, I am working on this end-to-end process for processing Pitch-by-Pitch data with some inner workings for also enabling analytics to be done directly from the system with little set up. I began this project because I use different computers and it became an issue switching from device to device when it came to working on these projects, and I can use it as my school project to cut down on time spent. I have it posted on my GitHub here and would love for any feedback any of you could have on the overall direction of this project and ways I could improve this Thank you!

Github Link: [https://github.com/jwolfe972/mlb\_prediction\_app](https://github.com/jwolfe972/mlb_prediction_app)",15,2025-03-31 00:02:37
"Hello, 

I am not really a dataengineer but after looking at what I'm going to be working on I may be one soon. 

  
Ok to start with the project, I work for a clinical research company and we currently are pulling reports manually and working with them in excel (occasionally making visualizaitons).  We pull from two sources, one we own but can't access (we could probably ask but we want a proof of concept first) but the other source we can use their API to access our data on their system.  

I am looking for open-source (free) programs I can use to take the information given in the API break it into a full database (dataset tables) and keep in constantly updated in a gateway.  In this phase of the project I am more invested in being able to do an API call and automatically pulling the data to set it into the appropriate schema. 

I have a really good understanding of dataset creation put I am new to the scripting an API side.  

I don't really know what else to add but if you have any follow up questions please comment. 

  
I appreciate any help or advice you can give me. (I will be using our lord and savior youtube to learn as much as I can about whatever you suggest). ",15,2025-05-23 23:32:35
"We‚Äôre wrapping up the Metabase Data Stack Survey soon. If you haven‚Äôt shared your experience yet, now‚Äôs the time.

Join hundreds of data experts who are helping build an open, honest guide to what‚Äôs really working in data engineering (and you'll get exclusive access to the results üòâ)

* [Survey link](https://jjcp5mdurex.typeform.com/to/KBnYX8Xe)

*Thanks to everyone who‚Äôs already shared their experience!*",16,2025-06-11 20:21:11
"Hi everyone! I'm conducting a university research survey on commonly used Big Data tools among students and professionals. If you work in data or tech, I‚Äôd really appreciate your input ‚Äî it only takes 3 minutes! Thank you

[https://docs.google.com/forms/d/e/1FAIpQLScXK6CnNUHGR9UIEHUhX83kHoZGYuSunRE0foZgnew81nxxLg/viewform?usp=header](https://docs.google.com/forms/d/e/1FAIpQLScXK6CnNUHGR9UIEHUhX83kHoZGYuSunRE0foZgnew81nxxLg/viewform?usp=header)",16,2025-04-12 18:39:21
If you know anything about this topic please kindly reach out to me as I am struggling with this issue ,16,2025-04-17 00:48:22
"I'm diving deeper into Data Engineering and I‚Äôd love some help finding quality resources. I‚Äôm familiar with the basics of tools like SQL, PySpark, Redshift, Glue, ETL, Data Lakes, and Data Marts etc.

I'm specifically looking for:

* Platforms or websites that provide *real-world case studies*, *architecture breakdowns*, or *project-based learning*
* Blogs, YouTube channels, or newsletters that cover *practical DE problems* and how they‚Äôre solved in production
* Anything that can help me understand *how these tools are used together* in real scenarios

Would appreciate any suggestions! Paid or free resources ‚Äî all are welcome. Thanks in advance!",16,2025-04-27 19:28:38
"Hi Guys,

Do anyone use or tried to use clean architecture for data engineering projects? If yes, May I know, how did it go and any comments on it or any references on github if you have?

Please don't give negative comments/responses without reasons.

Best regards",16,2025-04-08 18:13:04
"Hey everyone,

 I'm currently working on strengthening my tech watch efforts around the data ecosystem and I‚Äôm looking for fresh ideas on recent features, tools, or trends worth diving into.

Any suggestions are welcome ‚Äî thanks in advance!",16,2025-04-20 19:29:12
"Hey everyone! I‚Äôm starting a beginner-friendly Data Engineering group to learn, share resources, and stay motivated together.

If you‚Äôre just starting out and want support, accountability, and useful learning materials, drop a comment or DM me! Let‚Äôs grow together.

Here's the whatsapp link to join:
https://chat.whatsapp.com/GfAh5OQimLE7uKoo1y5JrH",16,2025-04-02 05:28:15
"Hi, ive been practicing leetcode and stratascratch questions. I had coding assigment for a company and they asked me store procedures, create, update, delete, insert, functions etc.. I failed... 

I didnt remember the syntax of anything... 

  
Where i can practice these types of question? Installing database on my machine and inventing question my self is not an option...

Thanks",16,2025-04-17 03:14:29
"Hey everyone,  
I‚Äôm working on a Data Engineering project and I want to make sure I‚Äôm organizing everything properly from the start. I'm looking for **best practices, lessons learned, or even examples of folder structures** used in real-world data engineering projects.

Would really appreciate:

* Any **advice or personal experience** on what worked well (or didn‚Äôt) for you
* **Blog posts, GitHub repos, YouTube videos**, or other resources that walk through good project structure
* Recommendations for organizing things like ETL pipelines, raw vs processed data, scripts, configs, notebooks, etc.

Thanks in advance ‚Äî trying to avoid a mess later by doing things right early on!",16,2025-04-15 16:15:01
"I‚Äôm a recent MSCS graduate trying to navigate this tough U.S. job market. I have around 2.5 years of prior experience in data engineering, and I‚Äôm currently preparing for data engineering interviews. One of the biggest challenges I‚Äôm facing is the lack of structured, comprehensive resources‚Äîeverything I find feels scattered and incomplete.

If anyone could share resources or materials, especially around **data modeling case studies**, I‚Äôd be incredibly grateful. üôèüèºüò≠",16,2025-05-27 19:03:07
"Just launched the [Urban Data Dictionary](https://www.urbandatadictionary.com/) and to celebrate what what we actually do in data engineering. Hope you find it fun and like it too. 

Check it out and add your own definitions. What terms would you contribute?

Happy April Fools!",16,2025-04-01 14:15:10

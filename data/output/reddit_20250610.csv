id,title,selftext,score,num_comments,author,created_utc,url,upvote_ratio,over_18,edited,spoiler,stickied
1l760h3,Help with parsing a troublesome PDF format,I’m working on a tool that can parse this kind of PDF for shopping list ingredients (to add functionality). I’m using Python with pdfplumber but keep having issues where ingredients are joined together in one record or missing pieces entirely (especially ones that are multi-line). The varying types of numerical and fraction measurements have been an issue too. Any ideas on approach?,26,36,SpreadSmiles897,2025-06-09 14:18:45,https://i.redd.it/20vldtnguw5f1.jpeg,0,False,False,False,False
1l7at7h,How are we helping our non-technical colleagues to edit data in the database?,"So I'm working on a project where we're building out an ETL pipeline to a Microsoft SQL Server database. But the managers want a UI to allow them to see the data that's been uploaded, make spot changes where necessary and have those changes go through a review process.

I've tested Directus, Appsmith and baserow. All are kind of fine, though I'd prefer the team and time to build out an app even in something like Shiny that would allow for more fine grained debugging when needed. 

What are you all using for this? It seems to be the kind of internal tool everyone is using in one way or another. Another small detail is the solution has to be available for on-prem use.",25,33,ursamajorm82,2025-06-09 17:26:46,https://www.reddit.com/r/dataengineering/comments/1l7at7h/how_are_we_helping_our_nontechnical_colleagues_to/,0,False,False,False,False
1l73r5h,Understanding DuckLake: A Table Format with a Modern Architecture (video),"There have already been a few blog posts about this topic, but here’s a video that tries to do the best job of recapping how we first arrived at the table format wars with Iceberg and Delta Lake, how DuckLake’s architecture differs, and a pragmatic hands-on guide to creating your first DuckLake table.",15,2,TransportationOk2403,2025-06-09 12:40:56,https://www.youtube.com/watch?v=hrTjvvwhHEQ,0,False,False,False,False
1l7jhoa,How is everyone's organization utilizing AI?,"We recently started using Cursor, and it has been a hit internally. Engineers are happy, and some are able to take on projects in the programming language that they did not feel comfortable previously.

Of course, we are also seeing a lot of analysts who want to be a DE, building UI on top of internal services that don't need a UI, and creating unnecessary technical debt.  But so far, I feel it has pushed us to build things faster. 

What has been everyone's experience with it?",19,14,wxf140430,2025-06-09 23:10:45,https://www.reddit.com/r/dataengineering/comments/1l7jhoa/how_is_everyones_organization_utilizing_ai/,0,False,False,False,False
1l6yk2v,Advice for a clueless soul,"TLDR: how do I run ~25 scripts that must be run on my local company server instance but allow for tracking through an easy UI since prefect hobby tier (free) only allows server-less executions. 
 
Hello everyone! 

I was looking around this Reddit and thought it would be a good place to ask for some advice. 

Long story short I am a dashboard-developer who also for some reason does programming/pipelines for our scripts that run only on schedule (no events). I don’t have any prior background on data engineering but on our 3 man team I’m the one with the most experience in Python. 

We had been using Prefect which was going well before they moved to a paid model to use our own compute. Previously I had about 25 scripts that would launch at different times to my worker on our company server using prefect. It sadly has to be on my local instance of our server since they rely on something called Alteryx which our two data analysts use basically exclusively. 

I liked prefects UI but not the 100$ a month price tag. I don’t really have the bandwidth or good-will credits with our IT to advocate for the self-hosted version. I’ve been thinking of ways to mimic what we had before but I’m at a loss. I don’t know how to have something ‘talk’ to my local like prefect was when the worker was live. 

I could set up windows task scheduler but tbh when I first started I inherited a bunch of them and hated the transfer process/setup. My boss would also like to be able to see the ‘failures’ if any happen. 

We have things like bitbucket/s3/snowflake that we use to host code/data/files but basically always pull them down to our local/ inside Alteryx.  

Any advice would be greatly appreciated and I’m sorry for any incorrect terminology/lack of understanding. Thank you for any help!",14,12,AdditionMiserable161,2025-06-09 07:25:51,https://www.reddit.com/r/dataengineering/comments/1l6yk2v/advice_for_a_clueless_soul/,0,False,False,False,False
1l79z73,Databricks+SQLMesh,"My organization has settled on Databricks to host our data warehouse. I’m considering implementing SQLMesh for transformations.

1. Is it possible to develop the ETL pipeline without constantly running a Databricks cluster? My workflow is usually develop the SQL, run it, check resulting data and iterate, which on DBX would require me to constantly have the cluster running. 

2. Can SQLMesh transformations be run using Databricks jobs/workflows in batch?

3. Can SQLMesh be used for streaming?

I’m currently a team of 1 and mainly have experience in data science rather than engineering so any tips are welcome. I’m looking to have the least amount of maintenance points possible.",10,6,SRobo97,2025-06-09 16:54:56,https://www.reddit.com/r/dataengineering/comments/1l79z73/databrickssqlmesh/,0,False,False,False,False
1l779tc,Soda Data Quality Acquires AI Monitoring startup NannyML,,7,1,santiviquez,2025-06-09 15:09:17,https://siliconcanals.com/brussels-soda-acquires-nannyml/,0,False,False,False,False
1l6y3xv,"30 team healthcare company - no dedicated data engineers, need assistance on third party etl tools and cloud warehousing","We have no data engineers to setup a data warehouse. I was exploring etl tools like hevo and fivetran, but would like recommendations on which option has their own data warehousing provided.

My main objective is to have salesforce and quickbooks data ingested into a cloud warehouse, and i can manipulate the data myself with python/sql. Then push the manipulated data to power bi for visualization",7,10,tytds,2025-06-09 06:55:36,https://www.reddit.com/r/dataengineering/comments/1l6y3xv/30_team_healthcare_company_no_dedicated_data/,0,False,False,False,False
1l7axgs,I built a free “Analytics Engineer” course/roadmap for my community—Would love your feedback.,,4,0,Due_Dot9893,2025-06-09 17:31:13,https://www.figureditout.space/roadmaps/,0,False,False,False,False
1l70ccg,Future German Job Market ?,"Hi everyone,

I know this might be a repeat question, but I couldn't find any answers in all previous posts I read, so thank you in advance for your patience.

I'm currently studying a range of Data Engineering technologies—Airflow, Snowflake, DBT, and PySpark—and I plan to expand into Cloud and DevOps tools as well. My German level is B2 in listening and reading, and about B1 in speaking. I’m a non-EU Master's student in Germany with about one year left until graduation.

My goal is to build solid proficiency in both the tech stack and the German language over the next year, and then begin applying for jobs. I have no professional experience yet.

But to be honest—I've been pushing myself really hard for the past few years, and I’m now at the edge of burnout. Recently, I've seen many Reddit posts saying the junior job market is brutal, the IT sector is struggling, and there's a looming threat from AI automation.

I feel lost and mentally exhausted. I'm not sure if all this effort will pay off, and I'm starting to wonder if I should just enjoy my remaining time in the EU and then head back home.


My questions are:

1. Is there still a realistic chance for someone like me (zero experience, but good German skills and strong tech learning) to break into the German job market—especially in Data Engineering, Cloud Engineering, or even DevOps (I know DevOps is usually a mid-senior role, but still curious)?


2. Do you think the job market for Data Engineers in Germany will improve in the next 1–2 years? Or is it becoming oversaturated?


I’d really appreciate any honest thoughts or advice. Thanks again for reading.
",6,11,YesterdayNecessary27,2025-06-09 09:27:40,https://www.reddit.com/r/dataengineering/comments/1l70ccg/future_german_job_market/,0,False,False,False,False
1l6xmsr,"Apache Iceberg: how to SELECT on table ""PARTITIONED BY Truncate(L, col)"".","I have a iceberg table which is partitioned by truncate(10, requestedtime).

requestedtime column(partition column) is basically string data type in a datetime format like this: 2025-05-30T19:33:43.193660573. and I want the dataset to be partitioned like ""2025-05-30"", ""2025-06-01"", so I created table with this query CREATE TABLE table (...) PARTITIONED BY truncate(10, requestedtime)

In S3,  the iceberg table technically is partitioned by

requestedtime\_trunc=2025-05-30/

requestedtime\_trunc=2025-05-31/

requestedtime\_trunc=2025-06-01/

Here's a problem I have.

When I try below query from spark engine,

""SELECT count(\*) FROM table WHERE substr(requestedtime,1,10) = '2025-05-30'""

The spark engine look through whole dataset, not a requested partition (requestedtime\_trunc=2025-05-30).

What SELECT query would be appropriate to only look through selected partition?

p.s) In AWS Athena,  the query ""SELECT count(\*) FROM table WHERE substr(requestedtime,1,10) = '2025-05-30'"" worked fine and used only requested partition data.",5,7,Gold_Environment6248,2025-06-09 06:23:49,https://www.reddit.com/r/dataengineering/comments/1l6xmsr/apache_iceberg_how_to_select_on_table_partitioned/,0,False,2025-06-09 07:34:55,False,False
1l7hb43,Ideas to Automate Data Report from SaaS with no access to API,"Hi All! I am working on trying to automate a data extraction from a SaaS that displays a data table that I want to push into my database hosted on Azure. Unfortunately the CSV export requires me to sign-in with an email 2FA and then request it on the UI, and then download it after about 1min or so. The email log-in has made it difficult to scrape with headless browser and they do not have a read-only API, and they do not email the CSV export either. Am I out of luck here? Any avenues to automatically extract this data? ",3,2,onmywaytostealyagirl,2025-06-09 21:37:43,https://www.reddit.com/r/dataengineering/comments/1l7hb43/ideas_to_automate_data_report_from_saas_with_no/,1,False,False,False,False
1l78ec8,Custom mongoDB CDC handler in pyspark,"I want to replicate a collection and sync in real time.
The CDC events are streamed to Kafka and I’ll be listening to it and based on operationType I’ll have to process the document and load it in delta table. I have all the columns possible in my table in case of schema change in fullDocument.

I am working with PySpark in Databricks. I have tried couple of different approaches -

1. using forEachBatch, clusterTime for ordering but this requires me to do a collect and process event, this was too slow
2. Using SCD kind of approach where Instead of deleting any record I was marking them inactive -
This does not give you a proper history tracking because for an `_id` I am taking the latest change and processing it. What issue I am facing with this is - I have been told by the source team that I can get an insert event for an `_id` after a delete event of the same `_id` so if in my batch for an `_id` there are events - “update → delete, → insert” then based on latest change I’ll pick the insert and this will cause a duplicate record in my table.
What will be the best way to handle this?",3,1,iam_mahend,2025-06-09 15:53:39,https://www.reddit.com/r/dataengineering/comments/1l78ec8/custom_mongodb_cdc_handler_in_pyspark/,0,False,False,False,False
1l75n4d,"In Iceberg, Can we use multiple glue catalogs which is corresponding to each dev/stating/prod environment.","I'm trying to figure out what might be the best way to divide environment by dev/staging/prod in apache iceberg.

On my first thought, Using multiple catalogs corresponding to each environments(dev/staging/prod) would be fine.

    # prod catalog <> prod environment 
    
    SparkSession.builder \
        .config(""spark.sql.catalog.iceberg_prod"", ""org.apache.iceberg.spark.SparkCatalog"") \
        .config(""spark.sql.catalog.iceberg_prod.catalog-impl"", ""org.apache.iceberg.aws.glue.GlueCatalog"") \
        .config(""spark.sql.catalog.iceberg_prod.warehouse"", ""s3://prod-datalake/iceberg_prod/"")
    
    
    
    spark.sql(""SELECT * FROM client.client_log"")  # Context is iceberg_prod.client.client_log
    
    
    
    
    # dev catalog <> dev environment 
    
    SparkSession.builder \
        .config(""spark.sql.catalog.iceberg_dev"", ""org.apache.iceberg.spark.SparkCatalog"") \
        .config(""spark.sql.catalog.iceberg_dev.catalog-impl"", ""org.apache.iceberg.aws.glue.GlueCatalog"") \
        .config(""spark.sql.catalog.iceberg_dev.warehouse"", ""s3://dev-datalake/iceberg_dev/"")
    
    
    spark.sql(""SELECT * FROM client.client_log"")  # Context is iceberg_dev.client.client_log

I assume, using this way, I can keep my source code(source query) unchanged and use the code in different environment (dev, prod)

    # I don't have to specify certian environment in the code and I can keep my code unchanged regardless of environment.
    
    spark.sql(""SELECT * FROM client.client_log"")

If this isn't gonna work, what might be the reason?

I just wonder how do you guys set up and divide dev and prod environment using iceberg.",2,2,Gold_Environment6248,2025-06-09 14:03:37,https://www.reddit.com/r/dataengineering/comments/1l75n4d/in_iceberg_can_we_use_multiple_glue_catalogs/,0,False,2025-06-09 14:09:21,False,False
1l6stze,Is it premature to job hunt?,"So I was hoping to job hunt after finishing the [DataTalks.club](http://DataTalks.club) Zoomcamp but I ended up not fully finishing the curriculum (*Spark & Kafka*) because of a combination of RL issues. I'd say it'd take another personal project and about 4-8 weeks to learn the basics of them.

**I'm considering these options:**

* Do I apply to train-to-hire programs like Revature now and try to fill out those skills with the help of a mentor in a group setting.
* Or do I skill build and do the personal project first then try applying to DE and other roles (e.g. DA, DevOps, Backend Engineering) along side the train-to-hire programs?

I can think of a few reasons for either.

Any feedback is welcome, including things I probably hadn't considered.

P.S. [my final project](https://github.com/MichaelSalata/compare-my-biometrics) \- [qualifications](https://docs.google.com/document/d/1NlyR8epSti_MD31crqarEo3QWgZAz1en5BK-8ACnZWw/edit?usp=sharing)",2,1,RustyEyeballs,2025-06-09 01:46:30,https://www.reddit.com/r/dataengineering/comments/1l6stze/is_it_premature_to_job_hunt/,0,False,2025-06-09 02:16:22,False,False
1l7h0v9,ETL Pipeline Question,"When implementing a large and highly scalable ETL pipeline, I want to know what tools you are using in each step of the way. I will be doing my work primarily in Google Cloud Platform, so I will be expecting to use tools such as BigQuery for the data warehouse, Dataflow, and Airflow for sure. If any of you work with GCP, what would the full stack for the pipeline look like for each individual level of the ETL pipeline? For those who don't work in GCP, what tools do you use and why do you find them beneficial?",2,3,OliveBubbly3820,2025-06-09 21:26:10,https://www.reddit.com/r/dataengineering/comments/1l7h0v9/etl_pipeline_question/,0,False,False,False,False
1l7busw,Data Dysfunction Chronicles Part 2,"The hardest part of working in data isn’t the technical complexity. It’s watching poor decisions get embedded into the foundation of a system, knowing exactly how and when they will cause failure.

A proper cleanse layer was defined but never used. The logic meant to transform data was never written. The production script still contains the original consultant's comment: ""you can add logic here."" No one ever did.

Unity Catalog was dismissed because the team ""already started with Hive,"" as if a single line in a config file was an immovable object. The decision was made by someone who does not understand the difference and passed down without question.

SQL logic is copied across pipelines with minor changes and no documentation. There is no source control. Notebooks are overwritten. Errors are silent, and no one except me understands how the pieces connect.

The manager responsible continues to block adoption of better practices while pushing out work that appears complete. The team follows because the system still runs and the dashboards still load. On paper, it looks like progress.

It is not progress. It is technical debt disguised as delivery.

And eventually someone else will be asked to explain why it all failed.

#DataEngineering #TechnicalDebt #UnityCatalog #LeadershipAccountability #DataIntegrity",0,0,FunkybunchesOO,2025-06-09 18:06:13,https://www.reddit.com/r/dataengineering/comments/1l7busw/data_dysfunction_chronicles_part_2/,0,False,False,False,False
1l7bhym,Best tool to load data from azure sql to GCP - transactional db with star schema,"Hi all,
We’re working on an enterprise data pipeline where we ingest property data from ATTOM, perform some basic transformations (mostly joins with dimension tables), and load it into a BigQuery star schema. Later, selected data will be pushed to MongoDB for downstream services.
We’re currently evaluating whether to use Apache Beam (Python SDK) running on Dataflow, orchestrated via Cloud Composer, for this flow. However, given that:
The data is batch-based (not streaming)
Joins and transformations are relatively straightforward
Much of the logic can be handled via SQL or Python
There are no real-time or ML workloads involved
I’m wondering if using Beam might be overkill in this scenario — both in terms of operational complexity and cost.
Would it be more relevant to use something like:
Cloud Functions / Run for extraction
BigQuery SQL / dbt for transformation and modeling
Composer just for orchestration
Also, is there any cost predictability model enterprises follow (flat-rate or committed use) for Beam + Composer setups?
Would love to hear thoughts from others who’ve faced a similar build-vs-simplify decision in GCP.",0,2,Je_suis_belle_,2025-06-09 17:52:43,https://www.reddit.com/r/dataengineering/comments/1l7bhym/best_tool_to_load_data_from_azure_sql_to_gcp/,0,False,False,False,False
1l7037q,How to learn vertexAI and bqml?,Can someone plz tell me some resources for this. I need in way that i can learn it and apply it cross platform if need be. Thank you.,0,5,Mortified__,2025-06-09 09:10:43,https://www.reddit.com/r/dataengineering/comments/1l7037q/how_to_learn_vertexai_and_bqml/,0,False,False,False,False
1l7i2a9,"🚀 The journey continues! Part 4 of my ""Getting Started with Real-Time Streaming in Kotlin"" series is here:","""**Flink DataStream API - Scalable Event Processing for Supplier Stats**""!

Having explored the lightweight power of Kafka Streams, we now level up to a full-fledged distributed processing engine: **Apache Flink**. This post dives into the foundational DataStream API, showcasing its power for stateful, event-driven applications.

In this deep dive, you'll learn how to:

* Implement sophisticated event-time processing with Flink's native **Watermarks**.
* Gracefully handle late-arriving data using Flink’s elegant **Side Outputs** feature.
* Perform stateful aggregations with custom **AggregateFunction** and **WindowFunction**.
* Consume Avro records and sink aggregated results back to Kafka.
* Visualize the entire pipeline, from source to sink, using **Kpow** and **Factor House Local**.

This is post 4 of 5, demonstrating the control and performance you get with Flink's core API. If you're ready to move beyond the basics of stream processing, this one's for you!

Read the full article here: https://jaehyeon.me/blog/2025-06-10-kotlin-getting-started-flink-datastream/

In the final post, we'll see how Flink's Table API offers a much more declarative way to achieve the same result. Your feedback is always appreciated!

🔗 **Catch up on the series**:
1. Kafka Clients with JSON
2. Kafka Clients with Avro
3. Kafka Streams for Supplier Stats",0,3,jaehyeon-kim,2025-06-09 22:08:42,https://i.redd.it/ooyqves96z5f1.png,0,False,False,False,False
1l7cjul,How do I safely update my feature branch with the latest changes from development?,"Hi all,

I'm working at a company that uses three main branches: `development`, `testing`, and `production`.

I created a feature branch called `feature/streaming-pipelines`, which is based off the `development` branch. Currently, my feature branch is **3 commits behind** and **2 commits ahead** of `development`.

I want to update my feature branch with the latest changes from `development` **without risking anything in the shared repo**. This repo includes not just code but also other important objects.

What Git commands should I use to safely bring my branch up to date? I’ve read various things online , but I’m not confident about which approach is safest in a shared repo.

I really don’t want to mess things up by experimenting. Any guidance is much appreciated!

Thanks in advance!",0,19,Quantumizera,2025-06-09 18:33:27,https://www.reddit.com/r/dataengineering/comments/1l7cjul/how_do_i_safely_update_my_feature_branch_with_the/,0,False,False,False,False
1l7533w,Just tried Rakuten SixthSense for Data Observability Surprisingly Solid + Free Trial,"
Been messing around with different observability platforms lately and stumbled on Rakuten SixthSense. Didn’t expect much at first, but honestly… it’s pretty slick.


Full-stack observability

Works well with distributed tracing

Real-time insights on latency, failures, and anomalies

UI isn’t bloated like some of the others (looking at Dynatrace/NewRelic)

They offer a free trial and an interactive sandbox demo, no credit card required.

If you’re into tracing APIs, services, or debugging async failures, this is worth checking out.

Free Trial
Interactive Demo

Not affiliated. Just a dev who’s tired of overpriced tools with clunky UX.
This one’s lean, fast, and does the job.

Anyone else tried this?
",0,1,Adventurous_Okra_846,2025-06-09 13:40:17,https://sixthsense.rakuten.com/data-observability/,0,False,False,False,False
1l70bt5,Behind every clean dataset is a data engineer turning chaos into order! 🛠️,,0,5,victorviro,2025-06-09 09:26:38,https://i.redd.it/nzl56zh8ev5f1.png,0,False,False,False,False

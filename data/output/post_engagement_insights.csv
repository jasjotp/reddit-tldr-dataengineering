,title,body,score,num_comments,created_utc,upvote_ratio,over_18,edited,spoiler,stickied,created_dt,hour,weekday,word_count,char_count,flesch_reading_ease,sentence_count,syllable_count,smog_index,has_url,has_code,sentiment_label,sentiment_score,tokens,processed_body,engagement_level,dominant_cluster,cluster,keywords,avg_score,avg_comments,avg_word_count,avg_sentiment,avg_char_count,avg_readability,avg_sentence_count,avg_syllable_count,avg_smog_index,pct_has_url,pct_has_code,top_30%_post_count,middle_40%_post_count,bottom_30%_post_count,example_post,score_rank,comment_rank
0,SQLFlow: DuckDB for Streaming Data,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",69,16,2025-03-29 18:35:07,0,False,2025-03-29 18:44:17,False,False,2025-03-29 18:35:07,18,Saturday,121.0,1106,9.55,11,262,12.2,1,0,POSITIVE,0.980389416217804,"['httpsgithubcomturbolyticssqlflowhttpsgithubcomturbolyticssqlflow', 'goal', 'sqlflow', 'bring', 'simplicity', 'duckdb', 'streaming', 'data', 'sqlflow', 'highperformance', 'stream', 'processing', 'engine', 'simplifies', 'building', 'data', 'pipelines', 'enabling', 'define', 'using', 'sql', 'think', 'sqlflow', 'lightweight', 'modern', 'flink', 'sqlflow', 'models', 'streamprocessing', 'sql', 'queries', 'using', 'duckdb', 'sql', 'dialecthttpsduckdborgdocssqlintroductionhtml', 'express', 'entire', 'stream', 'processing', 'pipelineingestion', 'transformation', 'enrichmentas', 'single', 'sql', 'statement', 'configuration', 'file', 'process', 'thousands', 'events', 'per', 'secondhttpssqlflowcomdocstutorialsclickhousesink', 'single', 'machine', 'low', 'memory', 'overhead', 'using', 'python', 'duckdb', 'arrow', 'confluent', 'python', 'client', 'tap', 'duckdb', 'ecosystem', 'tools', 'libraries', 'build', 'stream', 'processing', 'applications', 'sqlflow', 'supports', 'parquethttpssqlflowcomdocstutorialssparquetsink', 'csv', 'json', 'iceberghttpssqlflowcomdocstutorialsicebergsink', 'read', 'data', 'kafka']",httpsgithubcomturbolyticssqlflowhttpsgithubcomturbolyticssqlflow goal sqlflow bring simplicity duckdb streaming data sqlflow highperformance stream processing engine simplifies building data pipelines enabling define using sql think sqlflow lightweight modern flink sqlflow models streamprocessing sql queries using duckdb sql dialecthttpsduckdborgdocssqlintroductionhtml express entire stream processing pipelineingestion transformation enrichmentas single sql statement configuration file process thousands events per secondhttpssqlflowcomdocstutorialsclickhousesink single machine low memory overhead using python duckdb arrow confluent python client tap duckdb ecosystem tools libraries build stream processing applications sqlflow supports parquethttpssqlflowcomdocstutorialssparquetsink csv json iceberghttpssqlflowcomdocstutorialsicebergsink read data kafka,High,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
1,Interactive Change Data Capture (CDC) Playground,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",51,4,2025-03-29 18:12:13,0,False,False,False,False,2025-03-29 18:12:13,18,Saturday,146.0,975,17.68,5,263,17.6,0,1,POSITIVE,0.8007277250289917,"['ive', 'built', 'interactive', 'demo', 'cdc', 'help', 'explain', 'works', 'app', 'currently', 'shows', 'transaction', 'logbased', 'querybased', 'cdc', 'approaches', 'change', 'data', 'capture', 'cdc', 'design', 'pattern', 'tracks', 'changes', 'inserts', 'updates', 'deletes', 'database', 'makes', 'changes', 'available', 'downstream', 'systems', 'realtime', 'near', 'realtime', 'cdc', 'super', 'useful', 'variety', 'use', 'cases', 'realtime', 'data', 'replication', 'operational', 'databases', 'data', 'warehouses', 'lakehouses', 'keeping', 'analytics', 'systems', 'date', 'without', 'full', 'batch', 'reloads', 'synchronizing', 'data', 'across', 'microservices', 'distributed', 'systems', 'feeding', 'eventdriven', 'architectures', 'turning', 'database', 'changes', 'event', 'streams', 'maintaining', 'materialized', 'views', 'derived', 'tables', 'fresh', 'data', 'simplifying', 'etlelt', 'pipelines', 'processing', 'changed', 'records', 'many', 'let', 'know', 'think', 'theres', 'functionality', 'missing', 'could', 'interesting', 'showcase']",ive built interactive demo cdc help explain works app currently shows transaction logbased querybased cdc approaches change data capture cdc design pattern tracks changes inserts updates deletes database makes changes available downstream systems realtime near realtime cdc super useful variety use cases realtime data replication operational databases data warehouses lakehouses keeping analytics systems date without full batch reloads synchronizing data across microservices distributed systems feeding eventdriven architectures turning database changes event streams maintaining materialized views derived tables fresh data simplifying etlelt pipelines processing changed records many let know think theres functionality missing could interesting showcase,High,4,4,"files, api, tables, query, models, read, google, json, file, logic, write, case, csv, store, datasets, across, sources, metadata, created, schema, systems, large, platform, custom, script, changes, dataset, row, reporting, parquet, issues, ads, load, fields, analysis, cases, product, values, design, view, idea, layer, search, import, suggest, language, excel, app, blob, form, request, params, postgres, ingestion, raw, mysql, fact, processed, structures, either, separate, structured, connection, core, method, downstream, scripts, extract, maintain, send, hub, records, transactional, overhead, lineage, environment, object, external, pull, iot, order, typically, grid, hubs, validation, requires, mapping, batch, mechanism, location, updates, keeping, checkout, operations, runs, convert, optimized, avoid, matching, views, formats, matters, vpc, bring, reduce, migrate, consists, starts, reads, refresh, relationships, bronze, historical, tracking, developers, natural",8.0,7.1,169.4,-0.679,1041.7,44.5,9.5,267.7,10.7,24.1,17.9,49,102,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,7
2,When to use a surrogate key instead of a primary key?,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",34,42,2025-03-30 13:12:50,0,False,False,False,False,2025-03-30 13:12:50,13,Sunday,165.0,980,50.57,8,264,14.4,0,0,NEGATIVE,-0.9980127811431885,"['reviewing', 'interviews', 'following', 'question', 'come', 'mind', 'surrogate', 'keys', 'supposed', 'unique', 'identifiers', 'dont', 'real', 'world', 'meaning', 'primary', 'keys', 'supposed', 'reliably', 'identify', 'distinguish', 'individual', 'record', 'also', 'dont', 'real', 'world', 'meaning', 'someone', 'use', 'surrogate', 'key', 'wouldnt', 'using', 'primary', 'keys', 'case', 'surrogate', 'keys', 'way', 'surrogate', 'primary', 'keys', 'auto', 'generated', 'right', 'understand', 'surrogate', 'key', 'doesnt', 'necessarily', 'primary', 'key', 'considering', 'real', 'meaning', 'outside', 'wonder', 'purpose', 'surrogate', 'keys', 'work', 'different', 'projects', 'mainly', 'use', 'natural', 'keys', 'analytical', 'workloads', 'primary', 'keys', 'uniquely', 'identifying', 'given', 'row', 'wondering', 'kind', 'casesprojects', 'surrogate', 'keys', 'fit']",reviewing interviews following question come mind surrogate keys supposed unique identifiers dont real world meaning primary keys supposed reliably identify distinguish individual record also dont real world meaning someone use surrogate key wouldnt using primary keys case surrogate keys way surrogate primary keys auto generated right understand surrogate key doesnt necessarily primary key considering real meaning outside wonder purpose surrogate keys work different projects mainly use natural keys analytical workloads primary keys uniquely identifying given row wondering kind casesprojects surrogate keys fit,High,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
3,Do I need to know software engineering to be a data engineer?,As title says ,27,61,2025-03-30 07:53:33,0,False,False,False,False,2025-03-30 07:53:33,7,Sunday,3.0,14,93.81,1,4,0.0,0,0,POSITIVE,0.981099545955658,"['title', 'says']",title says,High,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
4,Should I stay in part-time role that uses Dagster or do internships in roles that use Airflow,"I am a part time data engineer/integrator who is in school at the moment. I work using Dagster, AWS, Snowflake, and Docker.

I was hoping Dagster would have roles where I lived but it seems everyone prefers Airflow.

Is it worth exploring data engineering internships that use Airflow at the expense of losing my current role? Do you guys see any growth in Dagster?",12,18,2025-03-30 01:25:17,0,False,False,False,False,2025-03-30 01:25:17,1,Sunday,64.0,365,66.94,5,96,8.8,0,0,NEGATIVE,-0.9822868704795837,"['part', 'time', 'data', 'engineerintegrator', 'school', 'moment', 'work', 'using', 'dagster', 'aws', 'snowflake', 'docker', 'hoping', 'dagster', 'would', 'roles', 'lived', 'seems', 'everyone', 'prefers', 'airflow', 'worth', 'exploring', 'data', 'engineering', 'internships', 'use', 'airflow', 'expense', 'losing', 'current', 'role', 'guys', 'see', 'growth', 'dagster']",part time data engineerintegrator school moment work using dagster aws snowflake docker hoping dagster would roles lived seems everyone prefers airflow worth exploring data engineering internships use airflow expense losing current role guys see growth dagster,High,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
5,The classic problem of killing flies with a cannon? DW vs. LH,"I'm starting a new job (a startup that is doubling in size every year) and the IT director has already warned me that they have a lot of problems with data structure changes, both due to new implementations in internally developed software and in those developed externally.

My question is whether I should prepare the central architecture using data warehouse or lakehouse, since the current data volume is still quite small <500 GB, but as I said, constant changes in data structure have been a problem.

By the way, I will be the first data engineer on the analytics team.",9,7,2025-03-29 19:27:40,0,False,2025-03-29 19:41:38,False,False,2025-03-29 19:27:40,19,Saturday,100.0,576,46.14,3,154,14.6,0,0,NEGATIVE,-0.9990635514259338,"['starting', 'new', 'job', 'startup', 'doubling', 'size', 'every', 'year', 'director', 'already', 'warned', 'lot', 'problems', 'data', 'structure', 'changes', 'due', 'new', 'implementations', 'internally', 'developed', 'software', 'developed', 'externally', 'question', 'whether', 'prepare', 'central', 'architecture', 'using', 'data', 'warehouse', 'lakehouse', 'since', 'current', 'data', 'volume', 'still', 'quite', 'small', 'said', 'constant', 'changes', 'data', 'structure', 'problem', 'way', 'first', 'data', 'engineer', 'analytics', 'team']",starting new job startup doubling size every year director already warned lot problems data structure changes due new implementations internally developed software developed externally question whether prepare central architecture using data warehouse lakehouse since current data volume still quite small said constant changes data structure problem way first data engineer analytics team,High,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
6,A dbt column lineage visualization tool (with dynamic web visualization),"Hey dbt folks,

I'm a data engineer and use dbt on a day-to-day basis, my team and I were struggling to find a good open-source tool for user-friendly column-level lineage visualization that we could use daily, similar to what commercial solutions like dbt Cloud offer. So, I decided to start building one...

https://reddit.com/link/1jnh7pu/video/wcl9lru6zure1/player

You can find the repo [here](https://github.com/Fszta/dbt-column-lineage), and the package on [pypi](https://pypi.org/project/dbt-col-lineage/0.1.1/)

**Under the hood**

Basically, it works by combining dbt's manifest and catalog with some compiled SQL parsing magic (big shoutout to sqlglot!).

I've built it as a CLI, keeping the syntax similar to dbt-core, with upstream and downstream selectors.

    dbt-col-lineage --select stg_transactions.amount+ --format html

Right now, it supports:

* Interactive HTML visualizations
* DOT graph images
* Simple text output in the console

**What's next ?**

* Focus on compatibility with more SQL dialects
* Improve the parser to handle complex syntax specific to certain dialects
* Making the UI less... basic. It's kinda rough right now, plus some information could be added such as materialization type, col typing etc

Feel free to drop any feedback or open an issue on the [repo](https://github.com/Fszta/dbt-column-lineage/tree/main)! It's still super early, and any help for testing on other dialects would be awesome. It's only been tested on projects using Snowflake, DuckDB, and SQLite adapters so far.",5,0,2025-03-30 17:11:12,0,False,2025-03-30 17:15:59,False,False,2025-03-30 17:11:12,17,Sunday,216.0,1529,28.43,12,387,13.3,1,1,NEGATIVE,-0.9993751645088196,"['hey', 'dbt', 'folks', 'data', 'engineer', 'use', 'dbt', 'daytoday', 'basis', 'team', 'struggling', 'find', 'good', 'opensource', 'tool', 'userfriendly', 'columnlevel', 'lineage', 'visualization', 'could', 'use', 'daily', 'similar', 'commercial', 'solutions', 'like', 'dbt', 'cloud', 'offer', 'decided', 'start', 'building', 'one', 'httpsredditcomlinkjnhpuvideowcllruzureplayer', 'find', 'repo', 'herehttpsgithubcomfsztadbtcolumnlineage', 'package', 'pypihttpspypiorgprojectdbtcollineage', 'hood', 'basically', 'works', 'combining', 'dbts', 'manifest', 'catalog', 'compiled', 'sql', 'parsing', 'magic', 'big', 'shoutout', 'sqlglot', 'ive', 'built', 'cli', 'keeping', 'syntax', 'similar', 'dbtcore', 'upstream', 'downstream', 'selectors', 'dbtcollineage', 'select', 'stgtransactionsamount', 'format', 'html', 'right', 'supports', 'interactive', 'html', 'visualizations', 'dot', 'graph', 'images', 'simple', 'text', 'output', 'console', 'whats', 'next', 'focus', 'compatibility', 'sql', 'dialects', 'improve', 'parser', 'handle', 'complex', 'syntax', 'specific', 'certain', 'dialects', 'making', 'less', 'basic', 'kinda', 'rough', 'right', 'plus', 'information', 'could', 'added', 'materialization', 'type', 'col', 'typing', 'etc', 'feel', 'free', 'drop', 'feedback', 'open', 'issue', 'repohttpsgithubcomfsztadbtcolumnlineagetreemain', 'still', 'super', 'early', 'help', 'testing', 'dialects', 'would', 'awesome', 'tested', 'projects', 'using', 'snowflake', 'duckdb', 'sqlite', 'adapters', 'far']",hey dbt folks data engineer use dbt daytoday basis team struggling find good opensource tool userfriendly columnlevel lineage visualization could use daily similar commercial solutions like dbt cloud offer decided start building one httpsredditcomlinkjnhpuvideowcllruzureplayer find repo herehttpsgithubcomfsztadbtcolumnlineage package pypihttpspypiorgprojectdbtcollineage hood basically works combining dbts manifest catalog compiled sql parsing magic big shoutout sqlglot ive built cli keeping syntax similar dbtcore upstream downstream selectors dbtcollineage select stgtransactionsamount format html right supports interactive html visualizations dot graph images simple text output console whats next focus compatibility sql dialects improve parser handle complex syntax specific certain dialects making less basic kinda rough right plus information could added materialization type col typing etc feel free drop feedback open issue repohttpsgithubcomfsztadbtcolumnlineagetreemain still super early help testing dialects would awesome tested projects using snowflake duckdb sqlite adapters far,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
7,creating big query source node in aws glue,"i have to send data from bigquery using aws glue to rds, i need to understand how to create big query source node in glue that can access a view from big query , is it by selecting table or custom query option... also what to add in materialization dataset , i dont have that ??? i have tried using table option , added view details there but then i get an error that view is not enabled in data preview section.

",7,1,2025-03-29 21:14:57,1,False,False,False,False,2025-03-29 21:14:57,21,Saturday,82.0,414,62.01,3,112,10.5,0,0,NEGATIVE,-0.9995943903923035,"['send', 'data', 'bigquery', 'using', 'aws', 'glue', 'rds', 'need', 'understand', 'create', 'big', 'query', 'source', 'node', 'glue', 'access', 'view', 'big', 'query', 'selecting', 'table', 'custom', 'query', 'option', 'also', 'add', 'materialization', 'dataset', 'dont', 'tried', 'using', 'table', 'option', 'added', 'view', 'details', 'get', 'error', 'view', 'enabled', 'data', 'preview', 'section']",send data bigquery using aws glue rds need understand create big query source node glue access view big query selecting table custom query option also add materialization dataset dont tried using table option added view details get error view enabled data preview section,High,4,4,"files, api, tables, query, models, read, google, json, file, logic, write, case, csv, store, datasets, across, sources, metadata, created, schema, systems, large, platform, custom, script, changes, dataset, row, reporting, parquet, issues, ads, load, fields, analysis, cases, product, values, design, view, idea, layer, search, import, suggest, language, excel, app, blob, form, request, params, postgres, ingestion, raw, mysql, fact, processed, structures, either, separate, structured, connection, core, method, downstream, scripts, extract, maintain, send, hub, records, transactional, overhead, lineage, environment, object, external, pull, iot, order, typically, grid, hubs, validation, requires, mapping, batch, mechanism, location, updates, keeping, checkout, operations, runs, convert, optimized, avoid, matching, views, formats, matters, vpc, bring, reduce, migrate, consists, starts, reads, refresh, relationships, bronze, historical, tracking, developers, natural",8.0,7.1,169.4,-0.679,1041.7,44.5,9.5,267.7,10.7,24.1,17.9,49,102,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,7
8,Transitioning from DE to ML Engineer in 2025?,"I am a DE with 2 years of experience, but my background is mainly in statistics. I have been offered a position as an ML Engineer (de facto Data Scientist, but also working on deployment - it is a smaller IT department, so my scope of duties will be simply quite wide). 

The position is interesting, and there are multiple pros and cons to it (that I do not want to discuss in this post). However my question is a bit more general - in 2025, with all the LLMs performing quite well with code generation and fixing, which path would you say is more stable long-term - sticking to DE and becoming better and better at it, or moving more towards ML and doing data science projects?

Furthermore, I also wonder about growth in each field - in ML/DS, my fear is that I am not PhD nor excellent mathematician. In DE, on the other hand, my fear is lack of my solid CS/SWE foundations (as my background is more in statistics). 

Ultimately, it is just an honest question, as I am very curious of your perspective on the matter - does moving towards data science projects (XGBoost and other algorithms) in 2025 from DE (PySpark and Airflow) makes sense in 2025? Which path would you say is more reasonable, and what kind of growth I can expect for each position? Personally I am a bit reluctant to switch simply since I have already dedicated 2 years to growing as an DE, but on the other hand I also see how much more and more of my tasks can be automated. Thanks for tips and honest suggestions!",5,9,2025-03-30 17:47:04,1,False,False,False,False,2025-03-30 17:47:04,17,Sunday,278.0,1489,52.23,10,399,13.5,0,0,POSITIVE,0.9940673112869263,"['years', 'experience', 'background', 'mainly', 'statistics', 'offered', 'position', 'engineer', 'facto', 'data', 'scientist', 'also', 'working', 'deployment', 'smaller', 'department', 'scope', 'duties', 'simply', 'quite', 'wide', 'position', 'interesting', 'multiple', 'pros', 'cons', 'want', 'discuss', 'post', 'however', 'question', 'bit', 'general', 'llms', 'performing', 'quite', 'well', 'code', 'generation', 'fixing', 'path', 'would', 'say', 'stable', 'longterm', 'sticking', 'becoming', 'better', 'better', 'moving', 'towards', 'data', 'science', 'projects', 'furthermore', 'also', 'wonder', 'growth', 'field', 'mlds', 'fear', 'phd', 'excellent', 'mathematician', 'hand', 'fear', 'lack', 'solid', 'csswe', 'foundations', 'background', 'statistics', 'ultimately', 'honest', 'question', 'curious', 'perspective', 'matter', 'moving', 'towards', 'data', 'science', 'projects', 'xgboost', 'algorithms', 'pyspark', 'airflow', 'makes', 'sense', 'path', 'would', 'say', 'reasonable', 'kind', 'growth', 'expect', 'position', 'personally', 'bit', 'reluctant', 'switch', 'simply', 'since', 'already', 'dedicated', 'years', 'growing', 'hand', 'also', 'see', 'much', 'tasks', 'automated', 'thanks', 'tips', 'honest', 'suggestions']",years experience background mainly statistics offered position engineer facto data scientist also working deployment smaller department scope duties simply quite wide position interesting multiple pros cons want discuss post however question bit general llms performing quite well code generation fixing path would say stable longterm sticking becoming better better moving towards data science projects furthermore also wonder growth field mlds fear phd excellent mathematician hand fear lack solid csswe foundations background statistics ultimately honest question curious perspective matter moving towards data science projects xgboost algorithms pyspark airflow makes sense path would say reasonable kind growth expect position personally bit reluctant switch simply since already dedicated years growing hand also see much tasks automated thanks tips honest suggestions,Mid,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
9,Applying for a Data Engineer Role at Docker Inc,"Hey everyone,  

I'm planning to apply for a Data Engineer position at Docker Inc. and was wondering if anyone here has experience with their application process. I have a solid background in Azure, Databricks, and data pipeline development, but I’d love to hear from others who have interviewed there or work there.

- What kind of technical questions should I expect?  
- Do they focus more on SQL, Python, or cloud architecture?  
- Any tips on system design or behavioral interviews?  

Would really appreciate any insights or advice!",5,3,2025-03-30 16:03:52,0,False,False,False,False,2025-03-30 16:03:52,16,Sunday,88.0,538,50.73,7,143,11.5,0,0,NEGATIVE,-0.8351058959960938,"['hey', 'everyone', 'planning', 'apply', 'data', 'engineer', 'position', 'docker', 'inc', 'wondering', 'anyone', 'experience', 'application', 'process', 'solid', 'background', 'azure', 'databricks', 'data', 'pipeline', 'development', 'love', 'hear', 'others', 'interviewed', 'work', 'kind', 'technical', 'questions', 'expect', 'focus', 'sql', 'python', 'cloud', 'architecture', 'tips', 'system', 'design', 'behavioral', 'interviews', 'would', 'really', 'appreciate', 'insights', 'advice']",hey everyone planning apply data engineer position docker inc wondering anyone experience application process solid background azure databricks data pipeline development love hear others interviewed work kind technical questions expect focus sql python cloud architecture tips system design behavioral interviews would really appreciate insights advice,Mid,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
10,3 years into Devops Engineering trying to move to Data Engineering,"I came to know that most of the skillset are matching 
in this 2 fields, apart from learning SQL, pyspark.

so would this be a better switching career ?",3,10,2025-03-30 07:42:11,0,False,False,False,False,2025-03-30 07:42:11,7,Sunday,29.0,152,82.65,2,37,0.0,0,0,NEGATIVE,-0.9942536950111389,"['came', 'know', 'skillset', 'matching', 'fields', 'apart', 'learning', 'sql', 'pyspark', 'would', 'better', 'switching', 'career']",came know skillset matching fields apart learning sql pyspark would better switching career,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
11,Introducing AnuDB: A Lightweight Embedded Document Database,"**AnuDB** \- a lightweight, embedded document database.

# Key Features

* **Embedded & Serverless**: Runs directly within your application - no separate server process required
* **JSON Document Storage**: Store and query complex JSON documents with ease
* **High Performance**: Built on RocksDB's LSM-tree architecture for optimized write performance
* **C++11 Compatible**: Works with most embedded device environments that adopt C++11
* **Cross-Platform**: Supports both Windows and Linux (including embedded Linux platforms)
* **Flexible Querying**: Rich query capabilities including equality, comparison, logical operators and sorting
* **Indexing**: Create indexes on frequently accessed fields to speed up queries
* **Compression**: Optional ZSTD compression support to reduce storage footprint
* **Transactional Properties**: Inherits atomic operations and configurable durability from RocksDB
* **Import/Export**: Easy JSON import and export for data migration or integration with other systems

Checkout README for more info: [https://github.com/hash-anu/AnuDB](https://github.com/hash-anu/AnuDB)",3,0,2025-03-30 04:26:19,1,False,False,False,False,2025-03-30 04:26:19,4,Sunday,139.0,1107,-42.73,2,272,0.0,1,1,POSITIVE,0.9677327275276184,"['anudb', 'lightweight', 'embedded', 'document', 'database', 'key', 'features', 'embedded', 'serverless', 'runs', 'directly', 'within', 'application', 'separate', 'server', 'process', 'required', 'json', 'document', 'storage', 'store', 'query', 'complex', 'json', 'documents', 'ease', 'high', 'performance', 'built', 'rocksdbs', 'lsmtree', 'architecture', 'optimized', 'write', 'performance', 'compatible', 'works', 'embedded', 'device', 'environments', 'adopt', 'crossplatform', 'supports', 'windows', 'linux', 'including', 'embedded', 'linux', 'platforms', 'flexible', 'querying', 'rich', 'query', 'capabilities', 'including', 'equality', 'comparison', 'logical', 'operators', 'sorting', 'indexing', 'create', 'indexes', 'frequently', 'accessed', 'fields', 'speed', 'queries', 'compression', 'optional', 'zstd', 'compression', 'support', 'reduce', 'storage', 'footprint', 'transactional', 'properties', 'inherits', 'atomic', 'operations', 'configurable', 'durability', 'rocksdb', 'importexport', 'easy', 'json', 'import', 'export', 'data', 'migration', 'integration', 'systems', 'checkout', 'readme', 'info', 'httpsgithubcomhashanuanudbhttpsgithubcomhashanuanudb']",anudb lightweight embedded document database key features embedded serverless runs directly within application separate server process required json document storage store query complex json documents ease high performance built rocksdbs lsmtree architecture optimized write performance compatible works embedded device environments adopt crossplatform supports windows linux including embedded linux platforms flexible querying rich query capabilities including equality comparison logical operators sorting indexing create indexes frequently accessed fields speed queries compression optional zstd compression support reduce storage footprint transactional properties inherits atomic operations configurable durability rocksdb importexport easy json import export data migration integration systems checkout readme info httpsgithubcomhashanuanudbhttpsgithubcomhashanuanudb,Mid,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
12,how to deal with azure vm nightmare?,"i am building data pipelines. i use azure vms for experimentation on sample data. when im not using them, i need to shut them off (working at bootstrapped startup).

when restarting my vm, it randomly fails. it says an allocation failure occurred due to capacity in the region (usually us-east). the only solution ive found is moving the resource to a new region, which takes 30–60 mins.

how do i prevent this issue in a cost-effective manner? can azure just allocate my vm to whatever region is available?

i’ve tried to troubleshoot this issue for weeks with azure support, but to no avail.

thanks all! :)
",2,6,2025-03-30 17:26:28,1,False,False,False,False,2025-03-30 17:26:28,17,Sunday,106.0,610,59.6,9,164,9.7,0,0,NEGATIVE,-0.999656081199646,"['building', 'data', 'pipelines', 'use', 'azure', 'vms', 'experimentation', 'sample', 'data', 'using', 'need', 'shut', 'working', 'bootstrapped', 'startup', 'restarting', 'randomly', 'fails', 'says', 'allocation', 'failure', 'occurred', 'due', 'capacity', 'region', 'usually', 'useast', 'solution', 'ive', 'found', 'moving', 'resource', 'new', 'region', 'takes', 'mins', 'prevent', 'issue', 'costeffective', 'manner', 'azure', 'allocate', 'whatever', 'region', 'available', 'ive', 'tried', 'troubleshoot', 'issue', 'weeks', 'azure', 'support', 'avail', 'thanks']",building data pipelines use azure vms experimentation sample data using need shut working bootstrapped startup restarting randomly fails says allocation failure occurred due capacity region usually useast solution ive found moving resource new region takes mins prevent issue costeffective manner azure allocate whatever region available ive tried troubleshoot issue weeks azure support avail thanks,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
13,Building a Cloud-Based Data Pipeline for Personal Use/Learning/Projects,"I am a data/functional analyst looking to get a better grasp on 'end-to-end data management and processing' (in quotations as that's the term my direct lead used when we aligned regarding my career objectives for the year, which definitely matches with my current interests).

Given this objective, my own interest in gaining a better grasp on general principles and functions of Data Engineering (and later on, Data Science), as well as a personal project I'm looking to finish for myself/my own portfolio by the end of the year, I'm looking for a cloud-based data pipeline solution for me to spend my role's training stipend on -- though I'm willing (and allowed) to use some of my personal funds on this as well since it's both for career progression and personal fun.

I started looking for small-scale subscriptions for some of the platforms we've previously used at work (AWS, Databricks), but the consensus I've gotten is that cost-wise, I'm potentially better off with multiple smaller platforms that I can just weave together, instead of going for platforms normally designed for enterprise use. And so I come here for help!

*(Though if this is not the right community for me to ask this, please let me know where instead)*

# Budget and Scope

* Base budget is a $200 stipend from our department, which refreshes at the end of the year
* I'm willing to add another $200-300/year, if necessary
* I'm from South-East Asia (converting our local currency, in case regional pricing matters)
* I prefer platforms/solutions that come with upfront costs instead of per-pull/use costs, so that's it's easier to manage the finances involved
* Languages I'm comfortable with are Python and SQL, since these are what I mainly use for work, but willing to learn basics of new languages where necessary 
* Since I will be using my personal computer, which is not that powerful currently, I am looking for a pipeline/platform that I can use completely via cloud (including running training models) 

# Learning Objectives 

*(aka things I want my platform/s to be able to do)*

**1. Data Gathering/Ingestion/Validation/Transformation/Storage**

* Obviously not too familiar with this step currently, so I ended up lumping a bunch of them together, but essentially ETL
* In terms of data I intend to use, I have three main sources I intend to work my way through, from simple to complex: **(1)** will be starting with ingesting CSV files I get from third-party free data providers like Kaggle, **(2)** then moving on to structured databases from sports stat sites such as BBREF, before  eventually work my way to **(3)** automating data collection from publicly available video games I normally play (Dota 2, Path of Exile -- if it matters)
* I want to be able to load these into SQL tables that I can regularly query form

**2. Data Querying and Extract Cloud Storage**

* I want to then be able to set it up so that I can easily query and then store-via-cloud what I need.
* To make my intention clear, most of my work experience is with AWS Athena and S3, so I hope to get platforms that function similarly.

**3. Cloud-Based Data Processing, Machine Learning, and Visualization**

* I currently do majority of my post-query data processing and modelling locally on my work laptop on Jupyter (via the Anaconda distribution), but one of my key objectives is learning to do all of these on cloud (especially since my peronal computer I'll be using would obviously not be as powerful as what I use for work)
* I definitely prefer Notebook-like environments, so perhaps something like AWS EMR
* My main experience at this level is mainly with Python (using specific packages such as Pandas, Numpy, Matplotlib, Numpy, Sklearn, etc.), but I'm looking to do more PySpark as well

**4. EXTRA/OPTIONAL: Dashboard Creation and Hosting**

* If I can get a platform/pipeline that will allow me to host interactive dashboards for me to just embed in my portfolio, that would be a plus, but I am very easily willing to drop this should it not fit my budget

#  Final Remarks

* Want to learn ETL, and cloud-based data processing on a personal data pipeline and processing platform/solution that also has SQL capabilities -- in line with my career and personal learning objectives for the year 
* Tried looking into personal subscriptions for 360 solutions like Databricks and AWS Sagemaker Unified Studio, but was told that for what I want I might be better off with patching together T2 solutions or something along those lines -- but I imagine this would be much more tricky to set up ",2,1,2025-03-30 14:20:16,0,False,False,False,False,2025-03-30 14:20:16,14,Sunday,778.0,4571,1.95,11,1197,22.0,0,1,NEGATIVE,-0.9714420437812805,"['datafunctional', 'analyst', 'looking', 'get', 'better', 'grasp', 'endtoend', 'data', 'management', 'processing', 'quotations', 'thats', 'term', 'direct', 'lead', 'used', 'aligned', 'regarding', 'career', 'objectives', 'year', 'definitely', 'matches', 'current', 'interests', 'given', 'objective', 'interest', 'gaining', 'better', 'grasp', 'general', 'principles', 'functions', 'data', 'engineering', 'later', 'data', 'science', 'well', 'personal', 'project', 'looking', 'finish', 'myselfmy', 'portfolio', 'end', 'year', 'looking', 'cloudbased', 'data', 'pipeline', 'solution', 'spend', 'roles', 'training', 'stipend', 'though', 'willing', 'allowed', 'use', 'personal', 'funds', 'well', 'since', 'career', 'progression', 'personal', 'fun', 'started', 'looking', 'smallscale', 'subscriptions', 'platforms', 'weve', 'previously', 'used', 'work', 'aws', 'databricks', 'consensus', 'ive', 'gotten', 'costwise', 'potentially', 'better', 'multiple', 'smaller', 'platforms', 'weave', 'together', 'instead', 'going', 'platforms', 'normally', 'designed', 'enterprise', 'use', 'come', 'help', 'though', 'right', 'community', 'ask', 'please', 'let', 'know', 'instead', 'budget', 'scope', 'base', 'budget', 'stipend', 'department', 'refreshes', 'end', 'year', 'willing', 'add', 'another', 'year', 'necessary', 'southeast', 'asia', 'converting', 'local', 'currency', 'case', 'regional', 'pricing', 'matters', 'prefer', 'platformssolutions', 'come', 'upfront', 'costs', 'instead', 'perpulluse', 'costs', 'thats', 'easier', 'manage', 'finances', 'involved', 'languages', 'comfortable', 'python', 'sql', 'since', 'mainly', 'use', 'work', 'willing', 'learn', 'basics', 'new', 'languages', 'necessary', 'since', 'using', 'personal', 'computer', 'powerful', 'currently', 'looking', 'pipelineplatform', 'use', 'completely', 'via', 'cloud', 'including', 'running', 'training', 'models', 'learning', 'objectives', 'aka', 'things', 'want', 'platforms', 'able', 'data', 'gatheringingestionvalidationtransformationstorage', 'obviously', 'familiar', 'step', 'currently', 'ended', 'lumping', 'bunch', 'together', 'essentially', 'etl', 'terms', 'data', 'intend', 'use', 'three', 'main', 'sources', 'intend', 'work', 'way', 'simple', 'complex', 'starting', 'ingesting', 'csv', 'files', 'get', 'thirdparty', 'free', 'data', 'providers', 'like', 'kaggle', 'moving', 'structured', 'databases', 'sports', 'stat', 'sites', 'bbref', 'eventually', 'work', 'way', 'automating', 'data', 'collection', 'publicly', 'available', 'video', 'games', 'normally', 'play', 'dota', 'path', 'exile', 'matters', 'want', 'able', 'load', 'sql', 'tables', 'regularly', 'query', 'form', 'data', 'querying', 'extract', 'cloud', 'storage', 'want', 'able', 'set', 'easily', 'query', 'storeviacloud', 'need', 'make', 'intention', 'clear', 'work', 'experience', 'aws', 'athena', 'hope', 'get', 'platforms', 'function', 'similarly', 'cloudbased', 'data', 'processing', 'machine', 'learning', 'visualization', 'currently', 'majority', 'postquery', 'data', 'processing', 'modelling', 'locally', 'work', 'laptop', 'jupyter', 'via', 'anaconda', 'distribution', 'one', 'key', 'objectives', 'learning', 'cloud', 'especially', 'since', 'peronal', 'computer', 'ill', 'using', 'would', 'obviously', 'powerful', 'use', 'work', 'definitely', 'prefer', 'notebooklike', 'environments', 'perhaps', 'something', 'like', 'aws', 'emr', 'main', 'experience', 'level', 'mainly', 'python', 'using', 'specific', 'packages', 'pandas', 'numpy', 'matplotlib', 'numpy', 'sklearn', 'etc', 'looking', 'pyspark', 'well', 'extraoptional', 'dashboard', 'creation', 'hosting', 'get', 'platformpipeline', 'allow', 'host', 'interactive', 'dashboards', 'embed', 'portfolio', 'would', 'plus', 'easily', 'willing', 'drop', 'fit', 'budget', 'final', 'remarks', 'want', 'learn', 'etl', 'cloudbased', 'data', 'processing', 'personal', 'data', 'pipeline', 'processing', 'platformsolution', 'also', 'sql', 'capabilities', 'line', 'career', 'personal', 'learning', 'objectives', 'year', 'tried', 'looking', 'personal', 'subscriptions', 'solutions', 'like', 'databricks', 'aws', 'sagemaker', 'unified', 'studio', 'told', 'want', 'might', 'better', 'patching', 'together', 'solutions', 'something', 'along', 'lines', 'imagine', 'would', 'much', 'tricky', 'set']",datafunctional analyst looking get better grasp endtoend data management processing quotations thats term direct lead used aligned regarding career objectives year definitely matches current interests given objective interest gaining better grasp general principles functions data engineering later data science well personal project looking finish myselfmy portfolio end year looking cloudbased data pipeline solution spend roles training stipend though willing allowed use personal funds well since career progression personal fun started looking smallscale subscriptions platforms weve previously used work aws databricks consensus ive gotten costwise potentially better multiple smaller platforms weave together instead going platforms normally designed enterprise use come help though right community ask please let know instead budget scope base budget stipend department refreshes end year willing add another year necessary southeast asia converting local currency case regional pricing matters prefer platformssolutions come upfront costs instead perpulluse costs thats easier manage finances involved languages comfortable python sql since mainly use work willing learn basics new languages necessary since using personal computer powerful currently looking pipelineplatform use completely via cloud including running training models learning objectives aka things want platforms able data gatheringingestionvalidationtransformationstorage obviously familiar step currently ended lumping bunch together essentially etl terms data intend use three main sources intend work way simple complex starting ingesting csv files get thirdparty free data providers like kaggle moving structured databases sports stat sites bbref eventually work way automating data collection publicly available video games normally play dota path exile matters want able load sql tables regularly query form data querying extract cloud storage want able set easily query storeviacloud need make intention clear work experience aws athena hope get platforms function similarly cloudbased data processing machine learning visualization currently majority postquery data processing modelling locally work laptop jupyter via anaconda distribution one key objectives learning cloud especially since peronal computer ill using would obviously powerful use work definitely prefer notebooklike environments perhaps something like aws emr main experience level mainly python using specific packages pandas numpy matplotlib numpy sklearn etc looking pyspark well extraoptional dashboard creation hosting get platformpipeline allow host interactive dashboards embed portfolio would plus easily willing drop fit budget final remarks want learn etl cloudbased data processing personal data pipeline processing platformsolution also sql capabilities line career personal learning objectives year tried looking personal subscriptions solutions like databricks aws sagemaker unified studio told want might better patching together solutions something along lines imagine would much tricky set,Mid,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
14,"I am learning data engineering from a course. I am a fresher with no job experience, a commerce background, and a two-year gap.",Will any company hire me? What certificate could I obtain that would help me?,1,8,2025-03-30 11:12:06,0,False,False,False,False,2025-03-30 11:12:06,11,Sunday,14.0,77,64.37,2,22,0.0,0,0,NEGATIVE,-0.9991484880447388,"['company', 'hire', 'certificate', 'could', 'obtain', 'would', 'help']",company hire certificate could obtain would help,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
15,"Struggling with Career Path – Stuck in Java, Want to Return to Data Engineering (6.5 YOE)","I've been working in IT for the past 6.5 years. I started as a Java Developer for a year before transitioning into Data Engineering, where I worked with Airflow, GCP, Python, and SQL (BigQuery).

In June 2022, I joined my second company as a Data Engineer, but after just six months, the project was shelved, and I was moved to a Java-based project (Spring Boot, Kafka, etc.). This happened during a market downturn and layoffs, so I was grateful to still have a job.

Now, after two years in this role, I feel stuck. I struggle with coding, don’t enjoy Java, and constantly feel like an imposter. I know for sure that I don’t want to continue in Java and Spring Boot. However, I’ve stayed in this role because it’s high-paying, and I have family responsibilities (supporting a family of five).

I want to transition back into Data Engineering, but now the job market expects a higher level of expertise given my experience and salary range. I’m unsure about the best way to upskill and make this switch without a major setback.

How can I strategically transition back into Data Engineering while balancing financial stability? Would love advice from those who have made similar career shifts.

Thanks in advance!",2,1,2025-03-30 17:08:46,0,False,False,False,False,2025-03-30 17:08:46,17,Sunday,209.0,1214,55.13,13,324,11.5,0,0,POSITIVE,0.7942933440208435,"['ive', 'working', 'past', 'years', 'started', 'java', 'developer', 'year', 'transitioning', 'data', 'engineering', 'worked', 'airflow', 'gcp', 'python', 'sql', 'bigquery', 'june', 'joined', 'second', 'company', 'data', 'engineer', 'six', 'months', 'project', 'shelved', 'moved', 'javabased', 'project', 'spring', 'boot', 'kafka', 'etc', 'happened', 'market', 'downturn', 'layoffs', 'grateful', 'still', 'job', 'two', 'years', 'role', 'feel', 'stuck', 'struggle', 'coding', 'dont', 'enjoy', 'java', 'constantly', 'feel', 'like', 'imposter', 'know', 'sure', 'dont', 'want', 'continue', 'java', 'spring', 'boot', 'however', 'ive', 'stayed', 'role', 'highpaying', 'family', 'responsibilities', 'supporting', 'family', 'five', 'want', 'transition', 'back', 'data', 'engineering', 'job', 'market', 'expects', 'higher', 'level', 'expertise', 'given', 'experience', 'salary', 'range', 'unsure', 'best', 'way', 'upskill', 'make', 'switch', 'without', 'major', 'setback', 'strategically', 'transition', 'back', 'data', 'engineering', 'balancing', 'financial', 'stability', 'would', 'love', 'advice', 'made', 'similar', 'career', 'shifts', 'thanks', 'advance']",ive working past years started java developer year transitioning data engineering worked airflow gcp python sql bigquery june joined second company data engineer six months project shelved moved javabased project spring boot kafka etc happened market downturn layoffs grateful still job two years role feel stuck struggle coding dont enjoy java constantly feel like imposter know sure dont want continue java spring boot however ive stayed role highpaying family responsibilities supporting family five want transition back data engineering job market expects higher level expertise given experience salary range unsure best way upskill make switch without major setback strategically transition back data engineering balancing financial stability would love advice made similar career shifts thanks advance,Mid,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
16,Need help for a small website design choices,"I am working on a website whose job is to serve data from MongoDb. Just textual data in row format nothing complicated. 

This is my current setup: client sends a request to cloudfront that manages the cache and triggers a lambda for a cache miss to query from MongoDB. I also use signedurl for security purposes for each request. 

I am not an expert that but I think cloud front can handle DDoS attacks etc. Does this setup work or do I need to bring in API Gateway into the fold? I don’t have any user login etc. and no form on the website (no sql injection risk I guess). I don’t know much about network security etc but have heard horror stories of websites getting hacked etc. Hence am a bit paranoid before launching the website. 

Based on some reading, I came to the conclusion that I need to use AWS WAF + API Gateway for dynamic queries and AWS + cloud front for static pages. And lambda should be associated with API Gateway to connect with MongoDB and API Gateway does rate limiting and caching (user authentication is no big a problem here). I wonder if cloudfront is even needed or should just stick with the current architecture I have. 

Need your suggestions. 




",1,2,2025-03-29 23:56:02,0,False,False,False,False,2025-03-29 23:56:02,23,Saturday,215.0,1183,64.51,14,329,10.6,0,0,NEGATIVE,-0.9988116025924683,"['working', 'website', 'whose', 'job', 'serve', 'data', 'mongodb', 'textual', 'data', 'row', 'format', 'nothing', 'complicated', 'current', 'setup', 'client', 'sends', 'request', 'cloudfront', 'manages', 'cache', 'triggers', 'lambda', 'cache', 'miss', 'query', 'mongodb', 'also', 'use', 'signedurl', 'security', 'purposes', 'request', 'expert', 'think', 'cloud', 'front', 'handle', 'ddos', 'attacks', 'etc', 'setup', 'work', 'need', 'bring', 'api', 'gateway', 'fold', 'dont', 'user', 'login', 'etc', 'form', 'website', 'sql', 'injection', 'risk', 'guess', 'dont', 'know', 'much', 'network', 'security', 'etc', 'heard', 'horror', 'stories', 'websites', 'getting', 'hacked', 'etc', 'hence', 'bit', 'paranoid', 'launching', 'website', 'based', 'reading', 'came', 'conclusion', 'need', 'use', 'aws', 'waf', 'api', 'gateway', 'dynamic', 'queries', 'aws', 'cloud', 'front', 'static', 'pages', 'lambda', 'associated', 'api', 'gateway', 'connect', 'mongodb', 'api', 'gateway', 'rate', 'limiting', 'caching', 'user', 'authentication', 'big', 'problem', 'wonder', 'cloudfront', 'even', 'needed', 'stick', 'current', 'architecture', 'need', 'suggestions']",working website whose job serve data mongodb textual data row format nothing complicated current setup client sends request cloudfront manages cache triggers lambda cache miss query mongodb also use signedurl security purposes request expert think cloud front handle ddos attacks etc setup work need bring api gateway fold dont user login etc form website sql injection risk guess dont know much network security etc heard horror stories websites getting hacked etc hence bit paranoid launching website based reading came conclusion need use aws waf api gateway dynamic queries aws cloud front static pages lambda associated api gateway connect mongodb api gateway rate limiting caching user authentication big problem wonder cloudfront even needed stick current architecture need suggestions,Mid,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
17,Junior vs Senior role,"What is the difference between a junior and senior in this role? How much can you really know in data engineering; get the data, clean it, dump it somewhere with a cloud service. 

But what would take someone from a junior role to a senior role? Is it just the number years of experience? 

",0,15,2025-03-30 07:52:21,0,False,False,False,False,2025-03-30 07:52:21,7,Sunday,54.0,291,74.69,4,74,8.1,0,0,NEGATIVE,-0.9996652603149414,"['difference', 'junior', 'senior', 'role', 'much', 'really', 'know', 'data', 'engineering', 'get', 'data', 'clean', 'dump', 'somewhere', 'cloud', 'service', 'would', 'take', 'someone', 'junior', 'role', 'senior', 'role', 'number', 'years', 'experience']",difference junior senior role much really know data engineering get data clean dump somewhere cloud service would take someone junior role senior role number years experience,Low,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
18,When to use a surrogate key instead of a primary key?,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",54,47,2025-03-30 13:12:50,0,False,False,False,False,2025-03-30 13:12:50,13,Sunday,165.0,980,50.57,8,264,14.4,0,0,NEGATIVE,-0.9980127811431885,"['reviewing', 'interviews', 'following', 'question', 'come', 'mind', 'surrogate', 'keys', 'supposed', 'unique', 'identifiers', 'dont', 'real', 'world', 'meaning', 'primary', 'keys', 'supposed', 'reliably', 'identify', 'distinguish', 'individual', 'record', 'also', 'dont', 'real', 'world', 'meaning', 'someone', 'use', 'surrogate', 'key', 'wouldnt', 'using', 'primary', 'keys', 'case', 'surrogate', 'keys', 'way', 'surrogate', 'primary', 'keys', 'auto', 'generated', 'right', 'understand', 'surrogate', 'key', 'doesnt', 'necessarily', 'primary', 'key', 'considering', 'real', 'meaning', 'outside', 'wonder', 'purpose', 'surrogate', 'keys', 'work', 'different', 'projects', 'mainly', 'use', 'natural', 'keys', 'analytical', 'workloads', 'primary', 'keys', 'uniquely', 'identifying', 'given', 'row', 'wondering', 'kind', 'casesprojects', 'surrogate', 'keys', 'fit']",reviewing interviews following question come mind surrogate keys supposed unique identifiers dont real world meaning primary keys supposed reliably identify distinguish individual record also dont real world meaning someone use surrogate key wouldnt using primary keys case surrogate keys way surrogate primary keys auto generated right understand surrogate key doesnt necessarily primary key considering real meaning outside wonder purpose surrogate keys work different projects mainly use natural keys analytical workloads primary keys uniquely identifying given row wondering kind casesprojects surrogate keys fit,High,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
19,Do I need to know software engineering to be a data engineer?,As title says ,46,71,2025-03-30 07:53:33,0,False,False,False,False,2025-03-30 07:53:33,7,Sunday,3.0,14,93.81,1,4,0.0,0,0,POSITIVE,0.981099545955658,"['title', 'says']",title says,High,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
20,A dbt column lineage visualization tool (with dynamic web visualization),"Hey dbt folks,

I'm a data engineer and use dbt on a day-to-day basis, my team and I were struggling to find a good open-source tool for user-friendly column-level lineage visualization that we could use daily, similar to what commercial solutions like dbt Cloud offer. So, I decided to start building one...

https://reddit.com/link/1jnh7pu/video/wcl9lru6zure1/player

You can find the repo [here](https://github.com/Fszta/dbt-column-lineage), and the package on [pypi](https://pypi.org/project/dbt-col-lineage/0.1.1/)

**Under the hood**

Basically, it works by combining dbt's manifest and catalog with some compiled SQL parsing magic (big shoutout to sqlglot!).

I've built it as a CLI, keeping the syntax similar to dbt-core, with upstream and downstream selectors.

    dbt-col-lineage --select stg_transactions.amount+ --format html

Right now, it supports:

* Interactive HTML visualizations
* DOT graph images
* Simple text output in the console

**What's next ?**

* Focus on compatibility with more SQL dialects
* Improve the parser to handle complex syntax specific to certain dialects
* Making the UI less... basic. It's kinda rough right now, plus some information could be added such as materialization type, col typing etc

Feel free to drop any feedback or open an issue on the [repo](https://github.com/Fszta/dbt-column-lineage/tree/main)! It's still super early, and any help for testing on other dialects would be awesome. It's only been tested on projects using Snowflake, DuckDB, and SQLite adapters so far.",35,3,2025-03-30 17:11:12,0,False,2025-03-30 17:15:59,False,False,2025-03-30 17:11:12,17,Sunday,216.0,1529,28.43,12,387,13.3,1,1,NEGATIVE,-0.9993751645088196,"['hey', 'dbt', 'folks', 'data', 'engineer', 'use', 'dbt', 'daytoday', 'basis', 'team', 'struggling', 'find', 'good', 'opensource', 'tool', 'userfriendly', 'columnlevel', 'lineage', 'visualization', 'could', 'use', 'daily', 'similar', 'commercial', 'solutions', 'like', 'dbt', 'cloud', 'offer', 'decided', 'start', 'building', 'one', 'httpsredditcomlinkjnhpuvideowcllruzureplayer', 'find', 'repo', 'herehttpsgithubcomfsztadbtcolumnlineage', 'package', 'pypihttpspypiorgprojectdbtcollineage', 'hood', 'basically', 'works', 'combining', 'dbts', 'manifest', 'catalog', 'compiled', 'sql', 'parsing', 'magic', 'big', 'shoutout', 'sqlglot', 'ive', 'built', 'cli', 'keeping', 'syntax', 'similar', 'dbtcore', 'upstream', 'downstream', 'selectors', 'dbtcollineage', 'select', 'stgtransactionsamount', 'format', 'html', 'right', 'supports', 'interactive', 'html', 'visualizations', 'dot', 'graph', 'images', 'simple', 'text', 'output', 'console', 'whats', 'next', 'focus', 'compatibility', 'sql', 'dialects', 'improve', 'parser', 'handle', 'complex', 'syntax', 'specific', 'certain', 'dialects', 'making', 'less', 'basic', 'kinda', 'rough', 'right', 'plus', 'information', 'could', 'added', 'materialization', 'type', 'col', 'typing', 'etc', 'feel', 'free', 'drop', 'feedback', 'open', 'issue', 'repohttpsgithubcomfsztadbtcolumnlineagetreemain', 'still', 'super', 'early', 'help', 'testing', 'dialects', 'would', 'awesome', 'tested', 'projects', 'using', 'snowflake', 'duckdb', 'sqlite', 'adapters', 'far']",hey dbt folks data engineer use dbt daytoday basis team struggling find good opensource tool userfriendly columnlevel lineage visualization could use daily similar commercial solutions like dbt cloud offer decided start building one httpsredditcomlinkjnhpuvideowcllruzureplayer find repo herehttpsgithubcomfsztadbtcolumnlineage package pypihttpspypiorgprojectdbtcollineage hood basically works combining dbts manifest catalog compiled sql parsing magic big shoutout sqlglot ive built cli keeping syntax similar dbtcore upstream downstream selectors dbtcollineage select stgtransactionsamount format html right supports interactive html visualizations dot graph images simple text output console whats next focus compatibility sql dialects improve parser handle complex syntax specific certain dialects making less basic kinda rough right plus information could added materialization type col typing etc feel free drop feedback open issue repohttpsgithubcomfsztadbtcolumnlineagetreemain still super early help testing dialects would awesome tested projects using snowflake duckdb sqlite adapters far,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
21,Passed DP-203 -- some thoughts on its retiring,"i took the Azure DP-203 last week — of course, it’s retiring literally tomorrow. But I figured it is indeed a very broad certification and so it can give a ""grounding"" scope in Azure D.E.

Also, I think it's still super early to go full Fabric (DP-600 or even DP-700), because the job demand is still not really there. Most jobs still demand strong grounding in Azure services even in the wake of Fabric adoption (POCing…).

So of course here, it’s retiring literally tomorrow unfortunately. I have passed the exam with a high score (900+). Also, I have worked (during internship) directly with MS Fabric only. So I would say some skills actually transfer quite nicely (ex: ADF ~ FDF).

---

### Some notes on resources for future exams:

I have relied primarily on [@tybulonazure](https://www.youtube.com/@tybulonazure)’s excellent YouTube channel (DP-203 playlist). It’s really great (watch on 1.8x – 2x speed).  
Now going back to Fabric, I have seen he has pivoted to Fabric-centric content — also a great news!

I also used the official “Guide” book (2024 version), which I found to be a surprisingly good way of structuring your learning. I hope equivalents for Fabric will be similar (TBS…).

---

The labs on Microsoft Learn are honestly **poorly designed** for what they offer.  
**Tip:** @tybul has video labs too — *use these*.  
And for the exams, always focus on **conceptual understanding**, not rote memorization.

Another **important (and mostly ignored)** tip:  
Focus on the **“best practices”** sections of Azure services in Microsoft Learn — I’ve read a lot of MS documentation, and those parts are often more helpful on the exam than the main pages.

---

**Examtopics** is obviously very helpful — but **read the comments**, they’re essential!

---

Finally, I do think it’s a shame it’s retiring — because the “traditional” Azure environment knowledge seems to be a sort of industry standard for companies. Also, the Fabric pricing model seems quite aggressive.

So for juniors, it would have been really good to still be able to have this background knowledge as a base layer.",16,2,2025-03-30 19:48:14,0,False,False,False,False,2025-03-30 19:48:14,19,Sunday,344.0,2100,56.86,23,525,11.2,1,0,NEGATIVE,-0.9986817240715027,"['took', 'azure', 'last', 'week', 'course', 'retiring', 'literally', 'tomorrow', 'figured', 'indeed', 'broad', 'certification', 'give', 'grounding', 'scope', 'azure', 'also', 'think', 'still', 'super', 'early', 'full', 'fabric', 'even', 'job', 'demand', 'still', 'really', 'jobs', 'still', 'demand', 'strong', 'grounding', 'azure', 'services', 'even', 'wake', 'fabric', 'adoption', 'pocing', 'course', 'retiring', 'literally', 'tomorrow', 'unfortunately', 'passed', 'exam', 'high', 'score', 'also', 'worked', 'internship', 'directly', 'fabric', 'would', 'say', 'skills', 'actually', 'transfer', 'quite', 'nicely', 'adf', 'fdf', 'notes', 'resources', 'future', 'exams', 'relied', 'primarily', 'tybulonazurehttpswwwyoutubecomtybulonazures', 'excellent', 'youtube', 'channel', 'playlist', 'really', 'great', 'watch', 'speed', 'going', 'back', 'fabric', 'seen', 'pivoted', 'fabriccentric', 'content', 'also', 'great', 'news', 'also', 'used', 'official', 'guide', 'book', 'version', 'found', 'surprisingly', 'good', 'way', 'structuring', 'learning', 'hope', 'equivalents', 'fabric', 'similar', 'tbs', 'labs', 'microsoft', 'learn', 'honestly', 'poorly', 'designed', 'offer', 'tip', 'tybul', 'video', 'labs', 'use', 'exams', 'always', 'focus', 'conceptual', 'understanding', 'rote', 'memorization', 'another', 'important', 'mostly', 'ignored', 'tip', 'focus', 'best', 'practices', 'sections', 'azure', 'services', 'microsoft', 'learn', 'ive', 'read', 'lot', 'documentation', 'parts', 'often', 'helpful', 'exam', 'main', 'pages', 'examtopics', 'obviously', 'helpful', 'read', 'comments', 'theyre', 'essential', 'finally', 'think', 'shame', 'retiring', 'traditional', 'azure', 'environment', 'knowledge', 'seems', 'sort', 'industry', 'standard', 'companies', 'also', 'fabric', 'pricing', 'model', 'seems', 'quite', 'aggressive', 'juniors', 'would', 'really', 'good', 'still', 'able', 'background', 'knowledge', 'base', 'layer']",took azure last week course retiring literally tomorrow figured indeed broad certification give grounding scope azure also think still super early full fabric even job demand still really jobs still demand strong grounding azure services even wake fabric adoption pocing course retiring literally tomorrow unfortunately passed exam high score also worked internship directly fabric would say skills actually transfer quite nicely adf fdf notes resources future exams relied primarily tybulonazurehttpswwwyoutubecomtybulonazures excellent youtube channel playlist really great watch speed going back fabric seen pivoted fabriccentric content also great news also used official guide book version found surprisingly good way structuring learning hope equivalents fabric similar tbs labs microsoft learn honestly poorly designed offer tip tybul video labs use exams always focus conceptual understanding rote memorization another important mostly ignored tip focus best practices sections azure services microsoft learn ive read lot documentation parts often helpful exam main pages examtopics obviously helpful read comments theyre essential finally think shame retiring traditional azure environment knowledge seems sort industry standard companies also fabric pricing model seems quite aggressive juniors would really good still able background knowledge base layer,High,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
22,Should I stay in part-time role that uses Dagster or do internships in roles that use Airflow,"I am a part time data engineer/integrator who is in school at the moment. I work using Dagster, AWS, Snowflake, and Docker.

I was hoping Dagster would have roles where I lived but it seems everyone prefers Airflow.

Is it worth exploring data engineering internships that use Airflow at the expense of losing my current role? Do you guys see any growth in Dagster?",11,18,2025-03-30 01:25:17,0,False,False,False,False,2025-03-30 01:25:17,1,Sunday,64.0,365,66.94,5,96,8.8,0,0,NEGATIVE,-0.9822868704795837,"['part', 'time', 'data', 'engineerintegrator', 'school', 'moment', 'work', 'using', 'dagster', 'aws', 'snowflake', 'docker', 'hoping', 'dagster', 'would', 'roles', 'lived', 'seems', 'everyone', 'prefers', 'airflow', 'worth', 'exploring', 'data', 'engineering', 'internships', 'use', 'airflow', 'expense', 'losing', 'current', 'role', 'guys', 'see', 'growth', 'dagster']",part time data engineerintegrator school moment work using dagster aws snowflake docker hoping dagster would roles lived seems everyone prefers airflow worth exploring data engineering internships use airflow expense losing current role guys see growth dagster,High,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
23,What is expected of me as a Junior Data Engineer in 2025?,"Hello all, 

I've been interviewing for a proper Junior Data Engineer position and have been doing well in the rounds so far. I've done my recruiter call, HR call and coding assessment. Waiting on the 4th.

I want to be great. I am willing to learn from those of you who are more experienced than me.  
  
Can anyone share examples from their own careers on attitude, communication, soft skills, time management, charisma, willingness to learn and other soft skills that I should keep in mind. Or maybe what I should not do instead.

How should I approach the technical side? There are 1000's of technologies to learn. So I have been learning basics with soft skills and hoping everything works out.

3 years ago I had a labour job and did well in that too. So this grind has caused me to rewire my brain to work in tech and corporate work. I am aiming for 20 years more in this field.

Any insights are appreciated.

Thanks!",9,11,2025-03-30 18:19:54,0,False,False,False,False,2025-03-30 18:19:54,18,Sunday,167.0,925,76.32,14,234,9.8,0,0,POSITIVE,0.9991468191146851,"['hello', 'ive', 'interviewing', 'proper', 'junior', 'data', 'engineer', 'position', 'well', 'rounds', 'far', 'ive', 'done', 'recruiter', 'call', 'call', 'coding', 'assessment', 'waiting', 'want', 'great', 'willing', 'learn', 'experienced', 'anyone', 'share', 'examples', 'careers', 'attitude', 'communication', 'soft', 'skills', 'time', 'management', 'charisma', 'willingness', 'learn', 'soft', 'skills', 'keep', 'mind', 'maybe', 'instead', 'approach', 'technical', 'side', 'technologies', 'learn', 'learning', 'basics', 'soft', 'skills', 'hoping', 'everything', 'works', 'years', 'ago', 'labour', 'job', 'well', 'grind', 'caused', 'rewire', 'brain', 'work', 'tech', 'corporate', 'work', 'aiming', 'years', 'field', 'insights', 'appreciated', 'thanks']",hello ive interviewing proper junior data engineer position well rounds far ive done recruiter call call coding assessment waiting want great willing learn experienced anyone share examples careers attitude communication soft skills time management charisma willingness learn soft skills keep mind maybe instead approach technical side technologies learn learning basics soft skills hoping everything works years ago labour job well grind caused rewire brain work tech corporate work aiming years field insights appreciated thanks,High,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
24,Transitioning from DE to ML Engineer in 2025?,"I am a DE with 2 years of experience, but my background is mainly in statistics. I have been offered a position as an ML Engineer (de facto Data Scientist, but also working on deployment - it is a smaller IT department, so my scope of duties will be simply quite wide). 

The position is interesting, and there are multiple pros and cons to it (that I do not want to discuss in this post). However my question is a bit more general - in 2025, with all the LLMs performing quite well with code generation and fixing, which path would you say is more stable long-term - sticking to DE and becoming better and better at it, or moving more towards ML and doing data science projects?

Furthermore, I also wonder about growth in each field - in ML/DS, my fear is that I am not PhD nor excellent mathematician. In DE, on the other hand, my fear is lack of my solid CS/SWE foundations (as my background is more in statistics). 

Ultimately, it is just an honest question, as I am very curious of your perspective on the matter - does moving towards data science projects (XGBoost and other algorithms) in 2025 from DE (PySpark and Airflow) makes sense in 2025? Which path would you say is more reasonable, and what kind of growth I can expect for each position? Personally I am a bit reluctant to switch simply since I have already dedicated 2 years to growing as an DE, but on the other hand I also see how much more and more of my tasks can be automated. Thanks for tips and honest suggestions!",6,11,2025-03-30 17:47:04,0,False,False,False,False,2025-03-30 17:47:04,17,Sunday,278.0,1489,52.23,10,399,13.5,0,0,POSITIVE,0.9940673112869263,"['years', 'experience', 'background', 'mainly', 'statistics', 'offered', 'position', 'engineer', 'facto', 'data', 'scientist', 'also', 'working', 'deployment', 'smaller', 'department', 'scope', 'duties', 'simply', 'quite', 'wide', 'position', 'interesting', 'multiple', 'pros', 'cons', 'want', 'discuss', 'post', 'however', 'question', 'bit', 'general', 'llms', 'performing', 'quite', 'well', 'code', 'generation', 'fixing', 'path', 'would', 'say', 'stable', 'longterm', 'sticking', 'becoming', 'better', 'better', 'moving', 'towards', 'data', 'science', 'projects', 'furthermore', 'also', 'wonder', 'growth', 'field', 'mlds', 'fear', 'phd', 'excellent', 'mathematician', 'hand', 'fear', 'lack', 'solid', 'csswe', 'foundations', 'background', 'statistics', 'ultimately', 'honest', 'question', 'curious', 'perspective', 'matter', 'moving', 'towards', 'data', 'science', 'projects', 'xgboost', 'algorithms', 'pyspark', 'airflow', 'makes', 'sense', 'path', 'would', 'say', 'reasonable', 'kind', 'growth', 'expect', 'position', 'personally', 'bit', 'reluctant', 'switch', 'simply', 'since', 'already', 'dedicated', 'years', 'growing', 'hand', 'also', 'see', 'much', 'tasks', 'automated', 'thanks', 'tips', 'honest', 'suggestions']",years experience background mainly statistics offered position engineer facto data scientist also working deployment smaller department scope duties simply quite wide position interesting multiple pros cons want discuss post however question bit general llms performing quite well code generation fixing path would say stable longterm sticking becoming better better moving towards data science projects furthermore also wonder growth field mlds fear phd excellent mathematician hand fear lack solid csswe foundations background statistics ultimately honest question curious perspective matter moving towards data science projects xgboost algorithms pyspark airflow makes sense path would say reasonable kind growth expect position personally bit reluctant switch simply since already dedicated years growing hand also see much tasks automated thanks tips honest suggestions,High,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
25,"Struggling with Career Path – Stuck in Java, Want to Return to Data Engineering (6.5 YOE)","I've been working in IT for the past 6.5 years. I started as a Java Developer for a year before transitioning into Data Engineering, where I worked with Airflow, GCP, Python, and SQL (BigQuery).

In June 2022, I joined my second company as a Data Engineer, but after just six months, the project was shelved, and I was moved to a Java-based project (Spring Boot, Kafka, etc.). This happened during a market downturn and layoffs, so I was grateful to still have a job.

Now, after two years in this role, I feel stuck. I struggle with coding, don’t enjoy Java, and constantly feel like an imposter. I know for sure that I don’t want to continue in Java and Spring Boot. However, I’ve stayed in this role because it’s high-paying, and I have family responsibilities (supporting a family of five).

I want to transition back into Data Engineering, but now the job market expects a higher level of expertise given my experience and salary range. I’m unsure about the best way to upskill and make this switch without a major setback.

How can I strategically transition back into Data Engineering while balancing financial stability? Would love advice from those who have made similar career shifts.

Thanks in advance!",5,9,2025-03-30 17:08:46,0,False,False,False,False,2025-03-30 17:08:46,17,Sunday,209.0,1214,55.13,13,324,11.5,0,0,POSITIVE,0.7942933440208435,"['ive', 'working', 'past', 'years', 'started', 'java', 'developer', 'year', 'transitioning', 'data', 'engineering', 'worked', 'airflow', 'gcp', 'python', 'sql', 'bigquery', 'june', 'joined', 'second', 'company', 'data', 'engineer', 'six', 'months', 'project', 'shelved', 'moved', 'javabased', 'project', 'spring', 'boot', 'kafka', 'etc', 'happened', 'market', 'downturn', 'layoffs', 'grateful', 'still', 'job', 'two', 'years', 'role', 'feel', 'stuck', 'struggle', 'coding', 'dont', 'enjoy', 'java', 'constantly', 'feel', 'like', 'imposter', 'know', 'sure', 'dont', 'want', 'continue', 'java', 'spring', 'boot', 'however', 'ive', 'stayed', 'role', 'highpaying', 'family', 'responsibilities', 'supporting', 'family', 'five', 'want', 'transition', 'back', 'data', 'engineering', 'job', 'market', 'expects', 'higher', 'level', 'expertise', 'given', 'experience', 'salary', 'range', 'unsure', 'best', 'way', 'upskill', 'make', 'switch', 'without', 'major', 'setback', 'strategically', 'transition', 'back', 'data', 'engineering', 'balancing', 'financial', 'stability', 'would', 'love', 'advice', 'made', 'similar', 'career', 'shifts', 'thanks', 'advance']",ive working past years started java developer year transitioning data engineering worked airflow gcp python sql bigquery june joined second company data engineer six months project shelved moved javabased project spring boot kafka etc happened market downturn layoffs grateful still job two years role feel stuck struggle coding dont enjoy java constantly feel like imposter know sure dont want continue java spring boot however ive stayed role highpaying family responsibilities supporting family five want transition back data engineering job market expects higher level expertise given experience salary range unsure best way upskill make switch without major setback strategically transition back data engineering balancing financial stability would love advice made similar career shifts thanks advance,Mid,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
26,Introducing AnuDB: A Lightweight Embedded Document Database,"**AnuDB** \- a lightweight, embedded document database.

# Key Features

* **Embedded & Serverless**: Runs directly within your application - no separate server process required
* **JSON Document Storage**: Store and query complex JSON documents with ease
* **High Performance**: Built on RocksDB's LSM-tree architecture for optimized write performance
* **C++11 Compatible**: Works with most embedded device environments that adopt C++11
* **Cross-Platform**: Supports both Windows and Linux (including embedded Linux platforms)
* **Flexible Querying**: Rich query capabilities including equality, comparison, logical operators and sorting
* **Indexing**: Create indexes on frequently accessed fields to speed up queries
* **Compression**: Optional ZSTD compression support to reduce storage footprint
* **Transactional Properties**: Inherits atomic operations and configurable durability from RocksDB
* **Import/Export**: Easy JSON import and export for data migration or integration with other systems

Checkout README for more info: [https://github.com/hash-anu/AnuDB](https://github.com/hash-anu/AnuDB)",3,0,2025-03-30 04:26:19,0,False,False,False,False,2025-03-30 04:26:19,4,Sunday,139.0,1107,-42.73,2,272,0.0,1,1,POSITIVE,0.9677327275276184,"['anudb', 'lightweight', 'embedded', 'document', 'database', 'key', 'features', 'embedded', 'serverless', 'runs', 'directly', 'within', 'application', 'separate', 'server', 'process', 'required', 'json', 'document', 'storage', 'store', 'query', 'complex', 'json', 'documents', 'ease', 'high', 'performance', 'built', 'rocksdbs', 'lsmtree', 'architecture', 'optimized', 'write', 'performance', 'compatible', 'works', 'embedded', 'device', 'environments', 'adopt', 'crossplatform', 'supports', 'windows', 'linux', 'including', 'embedded', 'linux', 'platforms', 'flexible', 'querying', 'rich', 'query', 'capabilities', 'including', 'equality', 'comparison', 'logical', 'operators', 'sorting', 'indexing', 'create', 'indexes', 'frequently', 'accessed', 'fields', 'speed', 'queries', 'compression', 'optional', 'zstd', 'compression', 'support', 'reduce', 'storage', 'footprint', 'transactional', 'properties', 'inherits', 'atomic', 'operations', 'configurable', 'durability', 'rocksdb', 'importexport', 'easy', 'json', 'import', 'export', 'data', 'migration', 'integration', 'systems', 'checkout', 'readme', 'info', 'httpsgithubcomhashanuanudbhttpsgithubcomhashanuanudb']",anudb lightweight embedded document database key features embedded serverless runs directly within application separate server process required json document storage store query complex json documents ease high performance built rocksdbs lsmtree architecture optimized write performance compatible works embedded device environments adopt crossplatform supports windows linux including embedded linux platforms flexible querying rich query capabilities including equality comparison logical operators sorting indexing create indexes frequently accessed fields speed queries compression optional zstd compression support reduce storage footprint transactional properties inherits atomic operations configurable durability rocksdb importexport easy json import export data migration integration systems checkout readme info httpsgithubcomhashanuanudbhttpsgithubcomhashanuanudb,Mid,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
27,Serialisation and de-serialisation?,"I just got to know that even in today's OLAP era, but while communicating b/w the systems internally they convert it to row based storage even if the warehouses are columnar type...
This made me sickkk I never knew this at all!

So does this mean serialisation and de-serialisation?? 
I see these terms vary across many architecture ex: In spark they mention these terminologies when the data needs to searched at different instances.. they say data needs to be de-serialised which takes time...

But I am not clear how do I need to think when I hear these terminologies!!!

Source:
https://www.linkedin.com/posts/dipankar-mazumdar_dataengineering-softwareengineering-activity-7307566420828065793-LuVZ?utm_source=share&utm_medium=member_android&rcm=ACoAADeacu0BUNpPkSGeT5J-UjR35-nvjHNjhTM",4,2,2025-03-30 18:29:00,0,False,False,False,False,2025-03-30 18:29:00,18,Sunday,101.0,788,29.04,6,188,11.2,1,0,NEGATIVE,-0.997477114200592,"['got', 'know', 'even', 'todays', 'olap', 'era', 'communicating', 'systems', 'internally', 'convert', 'row', 'based', 'storage', 'even', 'warehouses', 'columnar', 'type', 'made', 'sickkk', 'never', 'knew', 'mean', 'serialisation', 'deserialisation', 'see', 'terms', 'vary', 'across', 'many', 'architecture', 'spark', 'mention', 'terminologies', 'data', 'needs', 'searched', 'different', 'instances', 'say', 'data', 'needs', 'deserialised', 'takes', 'time', 'clear', 'need', 'think', 'hear', 'terminologies', 'source', 'httpswwwlinkedincompostsdipankarmazumdardataengineeringsoftwareengineeringactivityluvzutmsourceshareutmmediummemberandroidrcmacoaadeacubunppksgetjujrnvjhnjhtm']",got know even todays olap era communicating systems internally convert row based storage even warehouses columnar type made sickkk never knew mean serialisation deserialisation see terms vary across many architecture spark mention terminologies data needs searched different instances say data needs deserialised takes time clear need think hear terminologies source httpswwwlinkedincompostsdipankarmazumdardataengineeringsoftwareengineeringactivityluvzutmsourceshareutmmediummemberandroidrcmacoaadeacubunppksgetjujrnvjhnjhtm,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
28,how to deal with azure vm nightmare?,"i am building data pipelines. i use azure vms for experimentation on sample data. when im not using them, i need to shut them off (working at bootstrapped startup).

when restarting my vm, it randomly fails. it says an allocation failure occurred due to capacity in the region (usually us-east). the only solution ive found is moving the resource to a new region, which takes 30–60 mins.

how do i prevent this issue in a cost-effective manner? can azure just allocate my vm to whatever region is available?

i’ve tried to troubleshoot this issue for weeks with azure support, but to no avail.

thanks all! :)
",3,12,2025-03-30 17:26:28,0,False,False,False,False,2025-03-30 17:26:28,17,Sunday,106.0,610,59.6,9,164,9.7,0,0,NEGATIVE,-0.999656081199646,"['building', 'data', 'pipelines', 'use', 'azure', 'vms', 'experimentation', 'sample', 'data', 'using', 'need', 'shut', 'working', 'bootstrapped', 'startup', 'restarting', 'randomly', 'fails', 'says', 'allocation', 'failure', 'occurred', 'due', 'capacity', 'region', 'usually', 'useast', 'solution', 'ive', 'found', 'moving', 'resource', 'new', 'region', 'takes', 'mins', 'prevent', 'issue', 'costeffective', 'manner', 'azure', 'allocate', 'whatever', 'region', 'available', 'ive', 'tried', 'troubleshoot', 'issue', 'weeks', 'azure', 'support', 'avail', 'thanks']",building data pipelines use azure vms experimentation sample data using need shut working bootstrapped startup restarting randomly fails says allocation failure occurred due capacity region usually useast solution ive found moving resource new region takes mins prevent issue costeffective manner azure allocate whatever region available ive tried troubleshoot issue weeks azure support avail thanks,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
29,3 years into Devops Engineering trying to move to Data Engineering,"I came to know that most of the skillset are matching 
in this 2 fields, apart from learning SQL, pyspark.

so would this be a better switching career ?",1,11,2025-03-30 07:42:11,0,False,False,False,False,2025-03-30 07:42:11,7,Sunday,29.0,152,82.65,2,37,0.0,0,0,NEGATIVE,-0.9942536950111389,"['came', 'know', 'skillset', 'matching', 'fields', 'apart', 'learning', 'sql', 'pyspark', 'would', 'better', 'switching', 'career']",came know skillset matching fields apart learning sql pyspark would better switching career,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
30,dezoomcamp project,"Real-world data engineering practice! 🏗️ Built an end-to-end **data pipeline** using GCP, BigQuery, dbt, and Airflow to analyze Airbnb trends. Learning + hands-on = the best combo! 💡  
\#dezoomcamp #dataengineering #learningbydoing",0,2,2025-03-31 00:57:43,0,False,False,False,False,2025-03-31 00:57:43,0,Monday,31.0,231,30.73,4,53,10.1,0,0,POSITIVE,0.9971866011619568,"['realworld', 'data', 'engineering', 'practice', 'built', 'endtoend', 'data', 'pipeline', 'using', 'gcp', 'bigquery', 'dbt', 'airflow', 'analyze', 'airbnb', 'trends', 'learning', 'handson', 'best', 'combo', 'dezoomcamp', 'dataengineering', 'learningbydoing']",realworld data engineering practice built endtoend data pipeline using gcp bigquery dbt airflow analyze airbnb trends learning handson best combo dezoomcamp dataengineering learningbydoing,Low,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
31,dezoomcamp project,"How I made my Airbnb analysis efficient:  
🔹 **Staging layer**: Standardized & cleaned raw data  
🔹 **Core layer**: Built fact & dimension tables  
🔹 **Analytics dataset**: Ready for insights!  
\#dezoomcamp #analyticsengineering",0,1,2025-03-31 00:57:17,0,False,False,False,False,2025-03-31 00:57:17,0,Monday,31.0,229,-5.68,1,56,0.0,0,0,NEGATIVE,-0.5790661573410034,"['made', 'airbnb', 'analysis', 'efficient', 'staging', 'layer', 'standardized', 'cleaned', 'raw', 'data', 'core', 'layer', 'built', 'fact', 'dimension', 'tables', 'analytics', 'dataset', 'ready', 'insights', 'dezoomcamp', 'analyticsengineering']",made airbnb analysis efficient staging layer standardized cleaned raw data core layer built fact dimension tables analytics dataset ready insights dezoomcamp analyticsengineering,Low,4,4,"files, api, tables, query, models, read, google, json, file, logic, write, case, csv, store, datasets, across, sources, metadata, created, schema, systems, large, platform, custom, script, changes, dataset, row, reporting, parquet, issues, ads, load, fields, analysis, cases, product, values, design, view, idea, layer, search, import, suggest, language, excel, app, blob, form, request, params, postgres, ingestion, raw, mysql, fact, processed, structures, either, separate, structured, connection, core, method, downstream, scripts, extract, maintain, send, hub, records, transactional, overhead, lineage, environment, object, external, pull, iot, order, typically, grid, hubs, validation, requires, mapping, batch, mechanism, location, updates, keeping, checkout, operations, runs, convert, optimized, avoid, matching, views, formats, matters, vpc, bring, reduce, migrate, consists, starts, reads, refresh, relationships, bronze, historical, tracking, developers, natural",8.0,7.1,169.4,-0.679,1041.7,44.5,9.5,267.7,10.7,24.1,17.9,49,102,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,7
32,dezoomcamp project,"Orchestrating **dbt runs with Airflow** ensures transformations only happen when new data arrives. No wasted compute, no stale dashboards—just efficient workflows! 🔥  
\#dezoomcamp #dbtcore #airflow",0,0,2025-03-31 00:56:49,0,False,False,False,False,2025-03-31 00:56:49,0,Monday,25.0,198,29.52,3,47,10.5,0,0,POSITIVE,0.9970724582672119,"['orchestrating', 'dbt', 'runs', 'airflow', 'ensures', 'transformations', 'happen', 'new', 'data', 'arrives', 'wasted', 'compute', 'stale', 'dashboardsjust', 'efficient', 'workflows', 'dezoomcamp', 'dbtcore', 'airflow']",orchestrating dbt runs airflow ensures transformations happen new data arrives wasted compute stale dashboardsjust efficient workflows dezoomcamp dbtcore airflow,Low,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
33,dezoomcamp project,"End-to-end pipeline **deployment steps**:  
1️⃣ Terraform: Set up GCS & BigQuery  
2️⃣ Python: Load data to GCS  
3️⃣ dbt: Transform data  
4️⃣ Airflow: Orchestrate  
5️⃣ Looker: Visualize 📊  
\#dezoomcamp",0,0,2025-03-31 00:56:16,0,False,False,False,False,2025-03-31 00:56:16,0,Monday,29.0,205,35.61,1,45,0.0,0,0,NEGATIVE,-0.9951328635215759,"['endtoend', 'pipeline', 'deployment', 'steps', 'terraform', 'set', 'gcs', 'bigquery', 'python', 'load', 'data', 'gcs', 'dbt', 'transform', 'data', 'airflow', 'orchestrate', 'looker', 'visualize', 'dezoomcamp']",endtoend pipeline deployment steps terraform set gcs bigquery python load data gcs dbt transform data airflow orchestrate looker visualize dezoomcamp,Low,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
34,dezoomcamp project,"Implemented **SCD Type 2** in dbt for tracking historical host changes. Now I can analyze how hosts evolve over time and their impact on reviews and pricing! 🔄  
\#dezoomcamp #dbt #datawarehousing",0,0,2025-03-31 00:55:39,0,False,False,False,False,2025-03-31 00:55:39,0,Monday,31.0,196,61.33,3,49,10.5,0,0,NEGATIVE,-0.5976836085319519,"['implemented', 'scd', 'type', 'dbt', 'tracking', 'historical', 'host', 'changes', 'analyze', 'hosts', 'evolve', 'time', 'impact', 'reviews', 'pricing', 'dezoomcamp', 'dbt', 'datawarehousing']",implemented scd type dbt tracking historical host changes analyze hosts evolve time impact reviews pricing dezoomcamp dbt datawarehousing,Low,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
35,First Major DE Project,"Hello everyone, I am working on this end-to-end process for processing Pitch-by-Pitch data with some inner workings for also enabling analytics to be done directly from the system with little set up. I began this project because I use different computers and it became an issue switching from device to device when it came to working on these projects, and I can use it as my school project to cut down on time spent. I have it posted on my GitHub here and would love for any feedback any of you could have on the overall direction of this project and ways I could improve this Thank you!

Github Link: [https://github.com/jwolfe972/mlb\_prediction\_app](https://github.com/jwolfe972/mlb_prediction_app)",1,1,2025-03-31 00:02:37,0,False,False,False,False,2025-03-31 00:02:37,0,Monday,111.0,703,43.26,4,175,12.6,1,0,NEGATIVE,-0.9955241680145264,"['hello', 'everyone', 'working', 'endtoend', 'process', 'processing', 'pitchbypitch', 'data', 'inner', 'workings', 'also', 'enabling', 'analytics', 'done', 'directly', 'system', 'little', 'set', 'began', 'project', 'use', 'different', 'computers', 'became', 'issue', 'switching', 'device', 'device', 'came', 'working', 'projects', 'use', 'school', 'project', 'cut', 'time', 'spent', 'posted', 'github', 'would', 'love', 'feedback', 'could', 'overall', 'direction', 'project', 'ways', 'could', 'improve', 'thank', 'github', 'link', 'httpsgithubcomjwolfemlbpredictionapphttpsgithubcomjwolfemlbpredictionapp']",hello everyone working endtoend process processing pitchbypitch data inner workings also enabling analytics done directly system little set began project use different computers became issue switching device device came working projects use school project cut time spent posted github would love feedback could overall direction project ways could improve thank github link httpsgithubcomjwolfemlbpredictionapphttpsgithubcomjwolfemlbpredictionapp,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
36,Example for complex data pipeline,"Hi community,

After working as a data analyst for several years, I've noticed a gap in tools for interactively exploring complex ETL pipeline dependencies. Many solutions handle smaller pipelines well, but struggle with 200+ tasks.

For larger pipelines, we need robust traversal features, like collapsing/expanding nodes to focus on specific sections during development or debugging. I've used `networkx` and `mermaid` for subgraph visualization, but an interactive UI would be more efficient.

I've developed a prototype and am seeking example cases to test it. I'm looking for pipelines with 60+ tasks and complex dependencies. I'm particularly interested in the challenges you face with these large pipelines. At my workplace, we have a 1500+ task pipeline, and I'm curious if this is a typical scale.

Specifically, I'd like to know:

* What challenges do you face when visualizing and managing large pipelines?
* Are pipelines with 1500+ tasks common?
* What features would you find most useful in a tool for this purpose?

If you can share sanitized examples or describe the complexity of your pipelines, it would be very helpful.

Thanks.",2,2,2025-03-30 20:29:36,0,False,False,False,False,2025-03-30 20:29:36,20,Sunday,180.0,1147,47.99,12,294,12.2,0,0,NEGATIVE,-0.9957324862480164,"['community', 'working', 'data', 'analyst', 'several', 'years', 'ive', 'noticed', 'gap', 'tools', 'interactively', 'exploring', 'complex', 'etl', 'pipeline', 'dependencies', 'many', 'solutions', 'handle', 'smaller', 'pipelines', 'well', 'struggle', 'tasks', 'larger', 'pipelines', 'need', 'robust', 'traversal', 'features', 'like', 'collapsingexpanding', 'nodes', 'focus', 'specific', 'sections', 'development', 'debugging', 'ive', 'used', 'networkx', 'mermaid', 'subgraph', 'visualization', 'interactive', 'would', 'efficient', 'ive', 'developed', 'prototype', 'seeking', 'example', 'cases', 'test', 'looking', 'pipelines', 'tasks', 'complex', 'dependencies', 'particularly', 'interested', 'challenges', 'face', 'large', 'pipelines', 'workplace', 'task', 'pipeline', 'curious', 'typical', 'scale', 'specifically', 'like', 'know', 'challenges', 'face', 'visualizing', 'managing', 'large', 'pipelines', 'pipelines', 'tasks', 'common', 'features', 'would', 'find', 'useful', 'tool', 'purpose', 'share', 'sanitized', 'examples', 'describe', 'complexity', 'pipelines', 'would', 'helpful', 'thanks']",community working data analyst several years ive noticed gap tools interactively exploring complex etl pipeline dependencies many solutions handle smaller pipelines well struggle tasks larger pipelines need robust traversal features like collapsingexpanding nodes focus specific sections development debugging ive used networkx mermaid subgraph visualization interactive would efficient ive developed prototype seeking example cases test looking pipelines tasks complex dependencies particularly interested challenges face large pipelines workplace task pipeline curious typical scale specifically like know challenges face visualizing managing large pipelines pipelines tasks common features would find useful tool purpose share sanitized examples describe complexity pipelines would helpful thanks,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
37,"As a data analytics/data science professional, how much data engineering am I supposed to know? Any advice is greatly appreciated","I am so confused. I am looking for roles in BI/analytics/data science and it seems data engineering has just taken over the entire thing or most of it, atleast. BI and DBA is just gone and everyone now wants cloud dev ops and data engineering stack as part of a BI/analytics role? Am I now supposed to become a software engineer and learn all this stack (airflow, airtable, dbt, hadoop, pyspark, cloud, devops etc?) - this seems so overwhelming to me! How am I supposed to know all this in addition to data science, strategy, stakeholder management, program management, team leadership....so damn exhausting! Any advice on how to navigate the job market and land BI/data analytics/data science roles and how much realistic data engineering am I supposed to learn?",0,6,2025-03-30 20:21:44,0,False,False,False,False,2025-03-30 20:21:44,20,Sunday,129.0,763,55.24,8,209,12.2,0,0,NEGATIVE,-0.9996577501296997,"['confused', 'looking', 'roles', 'bianalyticsdata', 'science', 'seems', 'data', 'engineering', 'taken', 'entire', 'thing', 'atleast', 'dba', 'gone', 'everyone', 'wants', 'cloud', 'dev', 'ops', 'data', 'engineering', 'stack', 'part', 'bianalytics', 'role', 'supposed', 'become', 'software', 'engineer', 'learn', 'stack', 'airflow', 'airtable', 'dbt', 'hadoop', 'pyspark', 'cloud', 'devops', 'etc', 'seems', 'overwhelming', 'supposed', 'know', 'addition', 'data', 'science', 'strategy', 'stakeholder', 'management', 'program', 'management', 'team', 'leadershipso', 'damn', 'exhausting', 'advice', 'navigate', 'job', 'market', 'land', 'bidata', 'analyticsdata', 'science', 'roles', 'much', 'realistic', 'data', 'engineering', 'supposed', 'learn']",confused looking roles bianalyticsdata science seems data engineering taken entire thing atleast dba gone everyone wants cloud dev ops data engineering stack part bianalytics role supposed become software engineer learn stack airflow airtable dbt hadoop pyspark cloud devops etc seems overwhelming supposed know addition data science strategy stakeholder management program management team leadershipso damn exhausting advice navigate job market land bidata analyticsdata science roles much realistic data engineering supposed learn,Low,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
38,Building a Cloud-Based Data Pipeline for Personal Use/Learning/Projects,"I am a data/functional analyst looking to get a better grasp on 'end-to-end data management and processing' (in quotations as that's the term my direct lead used when we aligned regarding my career objectives for the year, which definitely matches with my current interests).

Given this objective, my own interest in gaining a better grasp on general principles and functions of Data Engineering (and later on, Data Science), as well as a personal project I'm looking to finish for myself/my own portfolio by the end of the year, I'm looking for a cloud-based data pipeline solution for me to spend my role's training stipend on -- though I'm willing (and allowed) to use some of my personal funds on this as well since it's both for career progression and personal fun.

I started looking for small-scale subscriptions for some of the platforms we've previously used at work (AWS, Databricks), but the consensus I've gotten is that cost-wise, I'm potentially better off with multiple smaller platforms that I can just weave together, instead of going for platforms normally designed for enterprise use. And so I come here for help!

*(Though if this is not the right community for me to ask this, please let me know where instead)*

# Budget and Scope

* Base budget is a $200 stipend from our department, which refreshes at the end of the year
* I'm willing to add another $200-300/year, if necessary
* I'm from South-East Asia (converting our local currency, in case regional pricing matters)
* I prefer platforms/solutions that come with upfront costs instead of per-pull/use costs, so that's it's easier to manage the finances involved
* Languages I'm comfortable with are Python and SQL, since these are what I mainly use for work, but willing to learn basics of new languages where necessary 
* Since I will be using my personal computer, which is not that powerful currently, I am looking for a pipeline/platform that I can use completely via cloud (including running training models) 

# Learning Objectives 

*(aka things I want my platform/s to be able to do)*

**1. Data Gathering/Ingestion/Validation/Transformation/Storage**

* Obviously not too familiar with this step currently, so I ended up lumping a bunch of them together, but essentially ETL
* In terms of data I intend to use, I have three main sources I intend to work my way through, from simple to complex: **(1)** will be starting with ingesting CSV files I get from third-party free data providers like Kaggle, **(2)** then moving on to structured databases from sports stat sites such as BBREF, before  eventually work my way to **(3)** automating data collection from publicly available video games I normally play (Dota 2, Path of Exile -- if it matters)
* I want to be able to load these into SQL tables that I can regularly query form

**2. Data Querying and Extract Cloud Storage**

* I want to then be able to set it up so that I can easily query and then store-via-cloud what I need.
* To make my intention clear, most of my work experience is with AWS Athena and S3, so I hope to get platforms that function similarly.

**3. Cloud-Based Data Processing, Machine Learning, and Visualization**

* I currently do majority of my post-query data processing and modelling locally on my work laptop on Jupyter (via the Anaconda distribution), but one of my key objectives is learning to do all of these on cloud (especially since my peronal computer I'll be using would obviously not be as powerful as what I use for work)
* I definitely prefer Notebook-like environments, so perhaps something like AWS EMR
* My main experience at this level is mainly with Python (using specific packages such as Pandas, Numpy, Matplotlib, Numpy, Sklearn, etc.), but I'm looking to do more PySpark as well

**4. EXTRA/OPTIONAL: Dashboard Creation and Hosting**

* If I can get a platform/pipeline that will allow me to host interactive dashboards for me to just embed in my portfolio, that would be a plus, but I am very easily willing to drop this should it not fit my budget

#  Final Remarks

* Want to learn ETL, and cloud-based data processing on a personal data pipeline and processing platform/solution that also has SQL capabilities -- in line with my career and personal learning objectives for the year 
* Tried looking into personal subscriptions for 360 solutions like Databricks and AWS Sagemaker Unified Studio, but was told that for what I want I might be better off with patching together T2 solutions or something along those lines -- but I imagine this would be much more tricky to set up ",1,1,2025-03-30 14:20:16,0,False,False,False,False,2025-03-30 14:20:16,14,Sunday,778.0,4571,1.95,11,1197,22.0,0,1,NEGATIVE,-0.9714420437812805,"['datafunctional', 'analyst', 'looking', 'get', 'better', 'grasp', 'endtoend', 'data', 'management', 'processing', 'quotations', 'thats', 'term', 'direct', 'lead', 'used', 'aligned', 'regarding', 'career', 'objectives', 'year', 'definitely', 'matches', 'current', 'interests', 'given', 'objective', 'interest', 'gaining', 'better', 'grasp', 'general', 'principles', 'functions', 'data', 'engineering', 'later', 'data', 'science', 'well', 'personal', 'project', 'looking', 'finish', 'myselfmy', 'portfolio', 'end', 'year', 'looking', 'cloudbased', 'data', 'pipeline', 'solution', 'spend', 'roles', 'training', 'stipend', 'though', 'willing', 'allowed', 'use', 'personal', 'funds', 'well', 'since', 'career', 'progression', 'personal', 'fun', 'started', 'looking', 'smallscale', 'subscriptions', 'platforms', 'weve', 'previously', 'used', 'work', 'aws', 'databricks', 'consensus', 'ive', 'gotten', 'costwise', 'potentially', 'better', 'multiple', 'smaller', 'platforms', 'weave', 'together', 'instead', 'going', 'platforms', 'normally', 'designed', 'enterprise', 'use', 'come', 'help', 'though', 'right', 'community', 'ask', 'please', 'let', 'know', 'instead', 'budget', 'scope', 'base', 'budget', 'stipend', 'department', 'refreshes', 'end', 'year', 'willing', 'add', 'another', 'year', 'necessary', 'southeast', 'asia', 'converting', 'local', 'currency', 'case', 'regional', 'pricing', 'matters', 'prefer', 'platformssolutions', 'come', 'upfront', 'costs', 'instead', 'perpulluse', 'costs', 'thats', 'easier', 'manage', 'finances', 'involved', 'languages', 'comfortable', 'python', 'sql', 'since', 'mainly', 'use', 'work', 'willing', 'learn', 'basics', 'new', 'languages', 'necessary', 'since', 'using', 'personal', 'computer', 'powerful', 'currently', 'looking', 'pipelineplatform', 'use', 'completely', 'via', 'cloud', 'including', 'running', 'training', 'models', 'learning', 'objectives', 'aka', 'things', 'want', 'platforms', 'able', 'data', 'gatheringingestionvalidationtransformationstorage', 'obviously', 'familiar', 'step', 'currently', 'ended', 'lumping', 'bunch', 'together', 'essentially', 'etl', 'terms', 'data', 'intend', 'use', 'three', 'main', 'sources', 'intend', 'work', 'way', 'simple', 'complex', 'starting', 'ingesting', 'csv', 'files', 'get', 'thirdparty', 'free', 'data', 'providers', 'like', 'kaggle', 'moving', 'structured', 'databases', 'sports', 'stat', 'sites', 'bbref', 'eventually', 'work', 'way', 'automating', 'data', 'collection', 'publicly', 'available', 'video', 'games', 'normally', 'play', 'dota', 'path', 'exile', 'matters', 'want', 'able', 'load', 'sql', 'tables', 'regularly', 'query', 'form', 'data', 'querying', 'extract', 'cloud', 'storage', 'want', 'able', 'set', 'easily', 'query', 'storeviacloud', 'need', 'make', 'intention', 'clear', 'work', 'experience', 'aws', 'athena', 'hope', 'get', 'platforms', 'function', 'similarly', 'cloudbased', 'data', 'processing', 'machine', 'learning', 'visualization', 'currently', 'majority', 'postquery', 'data', 'processing', 'modelling', 'locally', 'work', 'laptop', 'jupyter', 'via', 'anaconda', 'distribution', 'one', 'key', 'objectives', 'learning', 'cloud', 'especially', 'since', 'peronal', 'computer', 'ill', 'using', 'would', 'obviously', 'powerful', 'use', 'work', 'definitely', 'prefer', 'notebooklike', 'environments', 'perhaps', 'something', 'like', 'aws', 'emr', 'main', 'experience', 'level', 'mainly', 'python', 'using', 'specific', 'packages', 'pandas', 'numpy', 'matplotlib', 'numpy', 'sklearn', 'etc', 'looking', 'pyspark', 'well', 'extraoptional', 'dashboard', 'creation', 'hosting', 'get', 'platformpipeline', 'allow', 'host', 'interactive', 'dashboards', 'embed', 'portfolio', 'would', 'plus', 'easily', 'willing', 'drop', 'fit', 'budget', 'final', 'remarks', 'want', 'learn', 'etl', 'cloudbased', 'data', 'processing', 'personal', 'data', 'pipeline', 'processing', 'platformsolution', 'also', 'sql', 'capabilities', 'line', 'career', 'personal', 'learning', 'objectives', 'year', 'tried', 'looking', 'personal', 'subscriptions', 'solutions', 'like', 'databricks', 'aws', 'sagemaker', 'unified', 'studio', 'told', 'want', 'might', 'better', 'patching', 'together', 'solutions', 'something', 'along', 'lines', 'imagine', 'would', 'much', 'tricky', 'set']",datafunctional analyst looking get better grasp endtoend data management processing quotations thats term direct lead used aligned regarding career objectives year definitely matches current interests given objective interest gaining better grasp general principles functions data engineering later data science well personal project looking finish myselfmy portfolio end year looking cloudbased data pipeline solution spend roles training stipend though willing allowed use personal funds well since career progression personal fun started looking smallscale subscriptions platforms weve previously used work aws databricks consensus ive gotten costwise potentially better multiple smaller platforms weave together instead going platforms normally designed enterprise use come help though right community ask please let know instead budget scope base budget stipend department refreshes end year willing add another year necessary southeast asia converting local currency case regional pricing matters prefer platformssolutions come upfront costs instead perpulluse costs thats easier manage finances involved languages comfortable python sql since mainly use work willing learn basics new languages necessary since using personal computer powerful currently looking pipelineplatform use completely via cloud including running training models learning objectives aka things want platforms able data gatheringingestionvalidationtransformationstorage obviously familiar step currently ended lumping bunch together essentially etl terms data intend use three main sources intend work way simple complex starting ingesting csv files get thirdparty free data providers like kaggle moving structured databases sports stat sites bbref eventually work way automating data collection publicly available video games normally play dota path exile matters want able load sql tables regularly query form data querying extract cloud storage want able set easily query storeviacloud need make intention clear work experience aws athena hope get platforms function similarly cloudbased data processing machine learning visualization currently majority postquery data processing modelling locally work laptop jupyter via anaconda distribution one key objectives learning cloud especially since peronal computer ill using would obviously powerful use work definitely prefer notebooklike environments perhaps something like aws emr main experience level mainly python using specific packages pandas numpy matplotlib numpy sklearn etc looking pyspark well extraoptional dashboard creation hosting get platformpipeline allow host interactive dashboards embed portfolio would plus easily willing drop fit budget final remarks want learn etl cloudbased data processing personal data pipeline processing platformsolution also sql capabilities line career personal learning objectives year tried looking personal subscriptions solutions like databricks aws sagemaker unified studio told want might better patching together solutions something along lines imagine would much tricky set,Mid,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
39,dezoomcamp project,"Ever wondered why some Airbnb listings are way more expensive? My project explores price trends, demand drivers, and traveler-friendly insights based on NYC Airbnb data! 🏠📈

\#dezoomcamp #dataanalysis #airbnb",0,2,2025-03-31 00:54:57,0,False,False,False,False,2025-03-31 00:54:57,0,Monday,29.0,208,53.58,3,48,9.7,0,0,POSITIVE,0.5334059000015259,"['ever', 'wondered', 'airbnb', 'listings', 'way', 'expensive', 'project', 'explores', 'price', 'trends', 'demand', 'drivers', 'travelerfriendly', 'insights', 'based', 'nyc', 'airbnb', 'data', 'dezoomcamp', 'dataanalysis', 'airbnb']",ever wondered airbnb listings way expensive project explores price trends demand drivers travelerfriendly insights based nyc airbnb data dezoomcamp dataanalysis airbnb,Low,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
40,DEZOOMCAMP Project,"Lessons learned from loading Airbnb data into GCP:  
✔️ Use **autodetect schema** in BigQuery for flexibility  
✔️ Handle CSV quirks with proper configs  
✔️ Optimize storage with partitioning  
\#dezoomcamp #gcp #dataengineering",0,0,2025-03-31 00:54:01,0,False,False,False,False,2025-03-31 00:54:01,0,Monday,31.0,229,9.22,1,55,0.0,0,0,NEGATIVE,-0.9926820397377014,"['lessons', 'learned', 'loading', 'airbnb', 'data', 'gcp', 'use', 'autodetect', 'schema', 'bigquery', 'flexibility', 'handle', 'csv', 'quirks', 'proper', 'configs', 'optimize', 'storage', 'partitioning', 'dezoomcamp', 'gcp', 'dataengineering']",lessons learned loading airbnb data gcp use autodetect schema bigquery flexibility handle csv quirks proper configs optimize storage partitioning dezoomcamp gcp dataengineering,Low,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
41,Data Camp Data engineering certification help,Hi I’ve been working through the data engineer in SQL track on DataCamp and decided to try the associate certification exam. There was quite a bit that didn’t seem to have been covered in the courses. Can anyone recommend any other resources to help me plug the gap please? Thanks,0,1,2025-03-31 00:51:17,0,False,False,False,False,2025-03-31 00:51:17,0,Monday,50.0,280,54.52,3,79,13.0,0,0,NEGATIVE,-0.995636522769928,"['ive', 'working', 'data', 'engineer', 'sql', 'track', 'datacamp', 'decided', 'try', 'associate', 'certification', 'exam', 'quite', 'bit', 'didnt', 'seem', 'covered', 'courses', 'anyone', 'recommend', 'resources', 'help', 'plug', 'gap', 'please', 'thanks']",ive working data engineer sql track datacamp decided try associate certification exam quite bit didnt seem covered courses anyone recommend resources help plug gap please thanks,Low,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
42,Unstructured to Structured,"Hi folks,
I know there have been some discussions on this topic; but given we had lot of development in technology and business space; would like to get your input on
1. How much is this still a problem?
2. Do agentic workflows open up some new challenges?
3. Is there any need to convert large excel files into  SQL tables?

",0,1,2025-03-30 23:48:51,0,False,False,False,False,2025-03-30 23:48:51,23,Sunday,61.0,326,72.87,4,86,9.5,0,0,NEGATIVE,-0.9942678213119507,"['folks', 'know', 'discussions', 'topic', 'given', 'lot', 'development', 'technology', 'business', 'space', 'would', 'like', 'get', 'input', 'much', 'still', 'problem', 'agentic', 'workflows', 'open', 'new', 'challenges', 'need', 'convert', 'large', 'excel', 'files', 'sql', 'tables']",folks know discussions topic given lot development technology business space would like get input much still problem agentic workflows open new challenges need convert large excel files sql tables,Low,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
43,Collect old news articles from mainstream media.,"What is the best way to collect like >10 years old news articles from the mainstream media and newspapers?
 ",0,1,2025-03-30 18:39:53,0,False,False,False,False,2025-03-30 18:39:53,18,Sunday,19.0,108,69.11,1,27,0.0,0,0,NEGATIVE,-0.9915058016777039,"['best', 'way', 'collect', 'like', 'years', 'old', 'news', 'articles', 'mainstream', 'media', 'newspapers']",best way collect like years old news articles mainstream media newspapers,Low,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
44,"I am learning data engineering from a course. I am a fresher with no job experience, a commerce background, and a two-year gap.",Will any company hire me? What certificate could I obtain that would help me?,0,9,2025-03-30 11:12:06,0,False,False,False,False,2025-03-30 11:12:06,11,Sunday,14.0,77,64.37,2,22,0.0,0,0,NEGATIVE,-0.9991484880447388,"['company', 'hire', 'certificate', 'could', 'obtain', 'would', 'help']",company hire certificate could obtain would help,Low,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
45,Data Stack,"What do you think about the progress into [agentic data stack](https://goagentdata.com/)?  
",0,1,2025-03-30 21:11:28,0,False,False,False,False,2025-03-30 21:11:28,21,Sunday,11.0,92,26.47,1,22,0.0,1,0,NEGATIVE,-0.9947469830513,"['think', 'progress', 'agentic', 'data', 'stackhttpsgoagentdatacom']",think progress agentic data stackhttpsgoagentdatacom,Low,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
46,Why is table extraction still not solved by modern multimodal models?,"There is a lot of hype around multimodal models, such as Qwen 2.5 VL or Omni, GOT, SmolDocling, etc. I would like to know if others made a similar experience in practice: While they can do impressive things, they still struggle with table extraction, in cases which are straight-forward for humans.

Attached is a simple example, all I need is a reconstruction of the table as a flat CSV, preserving empty all empty cells correctly. Which open source model is able to do that?

https://preview.redd.it/xg8f0624jvre1.png?width=1650&format=png&auto=webp&s=4c0a22d833cb308534abf4dc38b1b12581a6e227

",0,0,2025-03-30 18:55:50,0,False,False,False,False,2025-03-30 18:55:50,18,Sunday,85.0,596,45.76,5,143,12.7,1,0,NEGATIVE,-0.9969182014465332,"['lot', 'hype', 'around', 'multimodal', 'models', 'qwen', 'omni', 'got', 'smoldocling', 'etc', 'would', 'like', 'know', 'others', 'made', 'similar', 'experience', 'practice', 'impressive', 'things', 'still', 'struggle', 'table', 'extraction', 'cases', 'straightforward', 'humans', 'attached', 'simple', 'example', 'need', 'reconstruction', 'table', 'flat', 'csv', 'preserving', 'empty', 'empty', 'cells', 'correctly', 'open', 'source', 'model', 'able', 'httpspreviewredditxgfjvrepngwidthformatpngautowebpscadcbabfdcbbae']",lot hype around multimodal models qwen omni got smoldocling etc would like know others made similar experience practice impressive things still struggle table extraction cases straightforward humans attached simple example need reconstruction table flat csv preserving empty empty cells correctly open source model able httpspreviewredditxgfjvrepngwidthformatpngautowebpscadcbabfdcbbae,Low,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
47,Junior vs Senior role,"What is the difference between a junior and senior in this role? How much can you really know in data engineering; get the data, clean it, dump it somewhere with a cloud service. 

But what would take someone from a junior role to a senior role? Is it just the number years of experience? 

",0,18,2025-03-30 07:52:21,0,False,False,False,False,2025-03-30 07:52:21,7,Sunday,54.0,291,74.69,4,74,8.1,0,0,NEGATIVE,-0.9996652603149414,"['difference', 'junior', 'senior', 'role', 'much', 'really', 'know', 'data', 'engineering', 'get', 'data', 'clean', 'dump', 'somewhere', 'cloud', 'service', 'would', 'take', 'someone', 'junior', 'role', 'senior', 'role', 'number', 'years', 'experience']",difference junior senior role much really know data engineering get data clean dump somewhere cloud service would take someone junior role senior role number years experience,Low,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
48,Does your company use both Databricks & Snowflake? How does the architecture look like?,I'm just curious about this because these 2 companies have been very popular over the last few years.,78,38,2025-03-31 01:12:17,0,False,False,False,False,2025-03-31 01:12:17,1,Monday,18.0,101,53.21,1,28,0.0,0,0,POSITIVE,0.9994267225265503,"['curious', 'companies', 'popular', 'last', 'years']",curious companies popular last years,High,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
49,"Now, I know why am I struggling...","And why my coleagues were able to present outputs more eagerly than I do:

I am trying to deliver a 'perfect data set', which is too much to expect from a fully on-prem DW/DS filled with couple of thousands of tables with zero data documentation and governance in all 30 years of operation...

I am not even a perfectionist myself so IDK what lead me to this point. Probably I trusted myself way too much? Probably I am trying to prove I am ""one of the best data engineers they had""? (I am still on probation and this is my 4th month here)

The company is fine and has continued to prosper over the decades without much data engineering. They just looked at the big numbers and made decisions based of it intuitively. 

Then here I am, just spent hours today looking for the excess 0.4$ from a total revenue of 40Million$ from a report I broke down to a FactTable. Mathematically, this is just peanuts. I should have let it go and used my time effectively on other things.

I am letting go of this perfectionism. 

I want to get regularized in this company. I really, really want to.",42,11,2025-03-31 03:47:38,0,False,False,False,False,2025-03-31 03:47:38,3,Monday,200.0,1083,64.3,13,295,10.4,0,1,NEGATIVE,-0.9983017444610596,"['coleagues', 'able', 'present', 'outputs', 'eagerly', 'trying', 'deliver', 'perfect', 'data', 'set', 'much', 'expect', 'fully', 'onprem', 'dwds', 'filled', 'couple', 'thousands', 'tables', 'zero', 'data', 'documentation', 'governance', 'years', 'operation', 'even', 'perfectionist', 'idk', 'lead', 'point', 'probably', 'trusted', 'way', 'much', 'probably', 'trying', 'prove', 'one', 'best', 'data', 'engineers', 'still', 'probation', 'month', 'company', 'fine', 'continued', 'prosper', 'decades', 'without', 'much', 'data', 'engineering', 'looked', 'big', 'numbers', 'made', 'decisions', 'based', 'intuitively', 'spent', 'hours', 'today', 'looking', 'excess', 'total', 'revenue', 'million', 'report', 'broke', 'facttable', 'mathematically', 'peanuts', 'let', 'used', 'time', 'effectively', 'things', 'letting', 'perfectionism', 'want', 'get', 'regularized', 'company', 'really', 'really', 'want']",coleagues able present outputs eagerly trying deliver perfect data set much expect fully onprem dwds filled couple thousands tables zero data documentation governance years operation even perfectionist idk lead point probably trusted way much probably trying prove one best data engineers still probation month company fine continued prosper decades without much data engineering looked big numbers made decisions based intuitively spent hours today looking excess total revenue million report broke facttable mathematically peanuts let used time effectively things letting perfectionism want get regularized company really really want,High,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
50,Prefect - too expensive?,"Hey guys,
we’re currently using self-hosted Airflow for our internal ETL and data workflows. It gets the job done, but I never really liked it. Feels too far away from actual Python, gets overly complex at times, and local development and testing is honestly a nightmare.

I recently stumbled upon Prefect and gave the self-hosted version a try. Really liked what I saw. Super Pythonic, easy to set up locally, modern UI - just felt right from the start.

But the problem is: the open-source version doesn’t offer user management or logging, so we’d need the Cloud version. Pricing would be around 30k USD per year, which is way above what we pay for Airflow. Even with a discount, it would still be too much for us.

Is there any way to make the community version work for a small team? Usermanagement and Audit-Logs is definitely a must for us. Or is Prefect just not realistic without going Cloud?

Would be a shame, because I really liked their approach.

If not Prefect, any tips on making Airflow easier for local dev and testing?",41,47,2025-03-31 10:26:08,0,False,False,False,False,2025-03-31 10:26:08,10,Monday,183.0,1036,66.74,14,272,9.4,0,0,NEGATIVE,-0.9288020730018616,"['hey', 'guys', 'currently', 'using', 'selfhosted', 'airflow', 'internal', 'etl', 'data', 'workflows', 'gets', 'job', 'done', 'never', 'really', 'liked', 'feels', 'far', 'away', 'actual', 'python', 'gets', 'overly', 'complex', 'times', 'local', 'development', 'testing', 'honestly', 'nightmare', 'recently', 'stumbled', 'upon', 'prefect', 'gave', 'selfhosted', 'version', 'try', 'really', 'liked', 'saw', 'super', 'pythonic', 'easy', 'set', 'locally', 'modern', 'felt', 'right', 'start', 'problem', 'opensource', 'version', 'doesnt', 'offer', 'user', 'management', 'logging', 'wed', 'need', 'cloud', 'version', 'pricing', 'would', 'around', 'usd', 'per', 'year', 'way', 'pay', 'airflow', 'even', 'discount', 'would', 'still', 'much', 'way', 'make', 'community', 'version', 'work', 'small', 'team', 'usermanagement', 'auditlogs', 'definitely', 'must', 'prefect', 'realistic', 'without', 'going', 'cloud', 'would', 'shame', 'really', 'liked', 'approach', 'prefect', 'tips', 'making', 'airflow', 'easier', 'local', 'dev', 'testing']",hey guys currently using selfhosted airflow internal etl data workflows gets job done never really liked feels far away actual python gets overly complex times local development testing honestly nightmare recently stumbled upon prefect gave selfhosted version try really liked saw super pythonic easy set locally modern felt right start problem opensource version doesnt offer user management logging wed need cloud version pricing would around usd per year way pay airflow even discount would still much way make community version work small team usermanagement auditlogs definitely must prefect realistic without going cloud would shame really liked approach prefect tips making airflow easier local dev testing,High,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
51,what's your opinion?,"i’m designing functions to clean data for two separate pipelines: one has small string inputs, the other has medium-size pandas inputs. both pipelines require the same manipulations.

for example, which is a better design: clean\_v0 or clean\_v1?

that is, should i standardize object types inside or outside the cleaning function?

thanks all! this community has been a life saver :)

",26,36,2025-03-31 16:48:55,0,False,False,False,False,2025-03-31 16:48:55,16,Monday,60.0,386,59.5,5,94,10.4,0,1,NEGATIVE,-0.9804784655570984,"['designing', 'functions', 'clean', 'data', 'two', 'separate', 'pipelines', 'one', 'small', 'string', 'inputs', 'mediumsize', 'pandas', 'inputs', 'pipelines', 'require', 'manipulations', 'example', 'better', 'design', 'cleanv', 'cleanv', 'standardize', 'object', 'types', 'inside', 'outside', 'cleaning', 'function', 'thanks', 'community', 'life', 'saver']",designing functions clean data two separate pipelines one small string inputs mediumsize pandas inputs pipelines require manipulations example better design cleanv cleanv standardize object types inside outside cleaning function thanks community life saver,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
52,Is using Snowflake for near real time or hourly events an overkill ?,"I've been using Snowflake for a while for just data warehousing projects (analytics) where I update the data twice per day.

  
I have now a Use Case where I need to do some reads and writes to sql tables every hour (every 10 min would be even better but not necessary). **The purpose is not only analytics but also operational.**

  
I estimate every request **costs me 0.01$,** which is quite high.

I was thinking of using **Postgresql** instead of Snowflake but I will need to invest time and resources to build it and maintain it. 

  
I was wondering if you can give me **your opinion** about building **near real time or hourly projects** in Snowflake. Does it make sense ? or is it a clear no-go ?

  
Thanks!

  
",14,12,2025-03-31 14:57:29,0,False,False,False,False,2025-03-31 14:57:29,14,Monday,128.0,722,65.73,9,184,10.0,0,0,NEGATIVE,-0.9977191090583801,"['ive', 'using', 'snowflake', 'data', 'warehousing', 'projects', 'analytics', 'update', 'data', 'twice', 'per', 'day', 'use', 'case', 'need', 'reads', 'writes', 'sql', 'tables', 'every', 'hour', 'every', 'min', 'would', 'even', 'better', 'necessary', 'purpose', 'analytics', 'also', 'operational', 'estimate', 'every', 'request', 'costs', 'quite', 'high', 'thinking', 'using', 'postgresql', 'instead', 'snowflake', 'need', 'invest', 'time', 'resources', 'build', 'maintain', 'wondering', 'give', 'opinion', 'building', 'near', 'real', 'time', 'hourly', 'projects', 'snowflake', 'make', 'sense', 'clear', 'nogo', 'thanks']",ive using snowflake data warehousing projects analytics update data twice per day use case need reads writes sql tables every hour every min would even better necessary purpose analytics also operational estimate every request costs quite high thinking using postgresql instead snowflake need invest time resources build maintain wondering give opinion building near real time hourly projects snowflake make sense clear nogo thanks,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
53,AWS Data Engineering from Azure,"Hi Folks,

  
14+ years into data engineering with Onprem for 10 and 4 years into Azure DE with mainly expertise on python and Azure databricks.

Now trying to shift job but 4 out of 5 jobs i see are asking for  AWS (i am targeting only product companies or  GCC) . Is self learning AWS for DE possible.

Has anyone shifted from Azure stack DE to AWS ?

What services to focus .

any paid courses that you have taken like udemy etc

Thanks",11,9,2025-03-31 10:23:27,0,False,False,False,False,2025-03-31 10:23:27,10,Monday,84.0,439,66.23,6,119,9.7,0,0,NEGATIVE,-0.9940537810325623,"['folks', 'years', 'data', 'engineering', 'onprem', 'years', 'azure', 'mainly', 'expertise', 'python', 'azure', 'databricks', 'trying', 'shift', 'job', 'jobs', 'see', 'asking', 'aws', 'targeting', 'product', 'companies', 'gcc', 'self', 'learning', 'aws', 'possible', 'anyone', 'shifted', 'azure', 'stack', 'aws', 'services', 'focus', 'paid', 'courses', 'taken', 'like', 'udemy', 'etc', 'thanks']",folks years data engineering onprem years azure mainly expertise python azure databricks trying shift job jobs see asking aws targeting product companies gcc self learning aws possible anyone shifted azure stack aws services focus paid courses taken like udemy etc thanks,High,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
54,Asking for different tools for SQL Server + SSIS project.,"Hello guys. I work in a consultancy company and we recently got a job to set-up SQL Server as DWH and SSIS. Whole system is going to be build up from the scratch. The whole operation of the company was running on Excel spreadsheets with 20+ Excel Slave that copies and pastes some data from a source, CSV or email then presses the fancy refresh button. Company newly bought and they want to get rid of this stupid shit so SQL Server and SSIS combo is a huge improvement for them (lol).

But I want to integrate as much as fancy stuff in this project. Both of these tool will work on a Remote Desktop with no internet connection.  I want to integrate some DevOps tools into this project. I will be one of the 3 data engineers that is going to work on this project. So Git will be definitely on my list, as well as GitTea or a repo that works offline since there won't be a lot of people. But do you have any more free tools that I can use? Planning to integrate Jenkins in offline mode somehow, tsqlt for unit testing seems like a decent choice as well. dbt-core and airflow was on my list as well but my colleagues don't know any python so they are not gonna be on this list.

Do you have any other suggestions? Have you ever used a set-up like mine? I would love to hear your previous experiences as well. Thanks",7,9,2025-03-31 17:29:30,1,False,False,False,False,2025-03-31 17:29:30,17,Monday,253.0,1313,71.24,15,350,9.7,0,0,NEGATIVE,-0.9957214593887329,"['hello', 'guys', 'work', 'consultancy', 'company', 'recently', 'got', 'job', 'setup', 'sql', 'server', 'dwh', 'ssis', 'whole', 'system', 'going', 'build', 'scratch', 'whole', 'operation', 'company', 'running', 'excel', 'spreadsheets', 'excel', 'slave', 'copies', 'pastes', 'data', 'source', 'csv', 'email', 'presses', 'fancy', 'refresh', 'button', 'company', 'newly', 'bought', 'want', 'get', 'rid', 'stupid', 'shit', 'sql', 'server', 'ssis', 'combo', 'huge', 'improvement', 'lol', 'want', 'integrate', 'much', 'fancy', 'stuff', 'project', 'tool', 'work', 'remote', 'desktop', 'internet', 'connection', 'want', 'integrate', 'devops', 'tools', 'project', 'one', 'data', 'engineers', 'going', 'work', 'project', 'git', 'definitely', 'list', 'well', 'gittea', 'repo', 'works', 'offline', 'since', 'wont', 'lot', 'people', 'free', 'tools', 'use', 'planning', 'integrate', 'jenkins', 'offline', 'mode', 'somehow', 'tsqlt', 'unit', 'testing', 'seems', 'like', 'decent', 'choice', 'well', 'dbtcore', 'airflow', 'list', 'well', 'colleagues', 'dont', 'know', 'python', 'gonna', 'list', 'suggestions', 'ever', 'used', 'setup', 'like', 'mine', 'would', 'love', 'hear', 'previous', 'experiences', 'well', 'thanks']",hello guys work consultancy company recently got job setup sql server dwh ssis whole system going build scratch whole operation company running excel spreadsheets excel slave copies pastes data source csv email presses fancy refresh button company newly bought want get rid stupid shit sql server ssis combo huge improvement lol want integrate much fancy stuff project tool work remote desktop internet connection want integrate devops tools project one data engineers going work project git definitely list well gittea repo works offline since wont lot people free tools use planning integrate jenkins offline mode somehow tsqlt unit testing seems like decent choice well dbtcore airflow list well colleagues dont know python gonna list suggestions ever used setup like mine would love hear previous experiences well thanks,High,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
55,Date warehouse essentials guide,Check out my latest blog on data warehouses! Discover powerful insights and strategies that can transform your data management. Read it here: https://medium.com/@adityasharmah27/data-warehouse-essentials-guide-706d81eada07!,5,1,2025-03-31 13:13:37,0,False,False,False,False,2025-03-31 13:13:37,13,Monday,23.0,223,29.82,3,47,11.2,1,0,POSITIVE,0.9989581108093262,"['check', 'latest', 'blog', 'data', 'warehouses', 'discover', 'powerful', 'insights', 'strategies', 'transform', 'data', 'management', 'read', 'httpsmediumcomadityasharmahdatawarehouseessentialsguidedeada']",check latest blog data warehouses discover powerful insights strategies transform data management read httpsmediumcomadityasharmahdatawarehouseessentialsguidedeada,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
56,Operating systems and hardware available for employees in your company,"Hey guys,

I'm working as a DE in a German IT company that has about 500 employees. The company's policy regarding operating systems the employees are allowed to use is strange and unfair (IMO). All software engineers get access to Macbooks and thus, to MacOS while all other employees that have a differnt job title ""only"" get HP elite books (that are not elite at all) that run on Windows. WSL is allowed but a native Linux is not accepted because of security reasons (I don't know which security reasons).

As far as I know the company does not want other job positions to get Macbooks because the whole updating stuff for those Macbooks  is done by an external company which is quite expensive. The Windows laptops instead are maintained by an internal team.

A lot of people are very unhappy with this situation because many of them (including me) would prefer to use Linux or MacOS. Especially all DevOps are pissed because half a year ago they also got access to MacBooks but a change in the policy means that they will have to change back to Windows laptops once their MacBooks break or become too old.

My question(s): Can you choose the OS and/or hardware in your company? Do you have a clue why Linux may not be accepted? Is it really not that safe (which I cannot think of because the company has it's own data center where a lot of Linux servers run that are actually updated by an internal team)?",2,11,2025-03-31 17:48:33,0,False,False,False,False,2025-03-31 17:48:33,17,Monday,255.0,1410,56.39,11,382,12.7,0,0,NEGATIVE,-0.9994255304336548,"['hey', 'guys', 'working', 'german', 'company', 'employees', 'companys', 'policy', 'regarding', 'operating', 'systems', 'employees', 'allowed', 'use', 'strange', 'unfair', 'imo', 'software', 'engineers', 'get', 'access', 'macbooks', 'thus', 'macos', 'employees', 'differnt', 'job', 'title', 'get', 'elite', 'books', 'elite', 'run', 'windows', 'wsl', 'allowed', 'native', 'linux', 'accepted', 'security', 'reasons', 'dont', 'know', 'security', 'reasons', 'far', 'know', 'company', 'want', 'job', 'positions', 'get', 'macbooks', 'whole', 'updating', 'stuff', 'macbooks', 'done', 'external', 'company', 'quite', 'expensive', 'windows', 'laptops', 'instead', 'maintained', 'internal', 'team', 'lot', 'people', 'unhappy', 'situation', 'many', 'including', 'would', 'prefer', 'use', 'linux', 'macos', 'especially', 'devops', 'pissed', 'half', 'year', 'ago', 'also', 'got', 'access', 'macbooks', 'change', 'policy', 'means', 'change', 'back', 'windows', 'laptops', 'macbooks', 'break', 'become', 'old', 'questions', 'choose', 'andor', 'hardware', 'company', 'clue', 'linux', 'may', 'accepted', 'really', 'safe', 'cannot', 'think', 'company', 'data', 'center', 'lot', 'linux', 'servers', 'run', 'actually', 'updated', 'internal', 'team']",hey guys working german company employees companys policy regarding operating systems employees allowed use strange unfair imo software engineers get access macbooks thus macos employees differnt job title get elite books elite run windows wsl allowed native linux accepted security reasons dont know security reasons far know company want job positions get macbooks whole updating stuff macbooks done external company quite expensive windows laptops instead maintained internal team lot people unhappy situation many including would prefer use linux macos especially devops pissed half year ago also got access macbooks change policy means change back windows laptops macbooks break become old questions choose andor hardware company clue linux may accepted really safe cannot think company data center lot linux servers run actually updated internal team,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
57,How do I manage dev/test/prod when using Unity Catalog for Medallion Architecture with dbt?,"Hi everyone,

I'm in the process of setting up a dbt project on Databricks and planning to leverage Unity Catalog to implement a medallion architecture. I am not sure the correct approach.  I am considering a dev/test/prod catalog, with a bronze/silver/gold schema:

* dev.bronze
* test.bronze
* prod.bronze

However, this takes 2 of the namespaces so all of the other information has to live in a single namespace such as table type (dim/fact), department (hr/finance), and data source and table description.  It seems like a lot to cram in there.

I have used the medallion architecture as a guide, but never used it in the naming, but the current team I am on really wants it to be in the name.  Just wondering what approaches people have taken.

Thanks",5,2,2025-03-31 14:27:44,0,False,False,False,False,2025-03-31 14:27:44,14,Monday,129.0,756,61.67,7,195,12.8,0,0,NEGATIVE,-0.9990149736404419,"['everyone', 'process', 'setting', 'dbt', 'project', 'databricks', 'planning', 'leverage', 'unity', 'catalog', 'implement', 'medallion', 'architecture', 'sure', 'correct', 'approach', 'considering', 'devtestprod', 'catalog', 'bronzesilvergold', 'schema', 'devbronze', 'testbronze', 'prodbronze', 'however', 'takes', 'namespaces', 'information', 'live', 'single', 'namespace', 'table', 'type', 'dimfact', 'department', 'hrfinance', 'data', 'source', 'table', 'description', 'seems', 'like', 'lot', 'cram', 'used', 'medallion', 'architecture', 'guide', 'never', 'used', 'naming', 'current', 'team', 'really', 'wants', 'name', 'wondering', 'approaches', 'people', 'taken', 'thanks']",everyone process setting dbt project databricks planning leverage unity catalog implement medallion architecture sure correct approach considering devtestprod catalog bronzesilvergold schema devbronze testbronze prodbronze however takes namespaces information live single namespace table type dimfact department hrfinance data source table description seems like lot cram used medallion architecture guide never used naming current team really wants name wondering approaches people taken thanks,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
58,My company adopted a stateful REST API solution I built that's run locally on every machine. I would like to deploy it to the cloud but I am not sure that's smart. Thoughts?,"**Context:** I joined a finance consultancy a few years ago and noticed that most people in my department are frustrated with the current ""software"" our engineering team has built over decades (yes - not years, decades). The issue is that the software consists of a bundle of Python scripts that repeatedly read large CSV files whenever a user interacts with it. The master CSV file ranges in size from 20MB to 1GB.

For example, if a user wants to select an option from a dropdown menu, clicking the dropdown triggers the reading and aggregation of a 1GB file, after which the frontend is ""returned"" 20 strings (the dropdown options). By ""returned,"" I mean that a new CSV file is created somewhere on the user's local file system, and the ""frontend"" picks it up. I tested a similar functionality using a Flask REST API, and once the CSV file is loaded into virtual memory, the process takes only 100ms—compared to the current design, which takes a full minute (some of it is due to scripts needing to sort out dependencies, validation, etc.). However, our engineering team refuses to adopt web-based communication, arguing that it's not worth the effort. The idea of using a cloud-based relational database is essentially taboo; it has to be either CSV files or Python’s pickle dumps on each user's local system.

I have some experience in software engineering, so I made it my mission to redesign this legacy monster—with the blessing of a senior manager. So far, the transition has gone incredibly well. Last year, I did a soft launch of a small subset of features, and within days, every person in my department was using it.

**Question:** My current design requires users to set up a virtual environment and run an installation script that sorts out any environment variables, dependencies, etc. Each time they want to start the software, they must run a local Flask API, which interacts with a React TypeScript frontend. When the Flask API starts, it loads all necessary files into memory, does validation and other things (takes around a minute). After that, every subsequent request is easy and takes on average 100 to 200 ms. However, I dislike that each user needs a fully configured environment. Version control is also a headache since every user must manually run an update script.

I’d like to move my Flask API to the cloud so that either:

1. **A single server serves all colleagues**, or
2. **Each colleague gets a dedicated node/pod.**

The problem with a single server is that it would quickly run out of virtual memory if 100+ colleagues loaded large datasets simultaneously. The problem with one node per colleague is the complexity—it would require Kubernetes (K8s) or AWS Fargate, along with an orchestrator to manage node creation and termination, which is a significant engineering effort.

I then considered making my Flask API stateless: storing large datasets in S3, using DynamoDB for file mapping, and loading data into virtual memory on every request. I converted some sample datasets to Parquet, reducing their size significantly (down to \~10MB), but I worry about added latency. Repeatedly reading the same data (given that each user makes 1–10 requests per minute) seems highly inefficient.

Am I missing any alternatives? Based on this, a local Flask API still seems like the best option—unless I want to pay for an expensive 64GB vRAM EC2 instance or invest significant time in building a node-per-user architecture.

Thanks!",2,6,2025-03-31 21:17:37,1,False,False,False,False,2025-03-31 21:17:37,21,Monday,576.0,3463,41.4,27,961,14.1,0,1,NEGATIVE,-0.9993342757225037,"['context', 'joined', 'finance', 'consultancy', 'years', 'ago', 'noticed', 'people', 'department', 'frustrated', 'current', 'software', 'engineering', 'team', 'built', 'decades', 'yes', 'years', 'decades', 'issue', 'software', 'consists', 'bundle', 'python', 'scripts', 'repeatedly', 'read', 'large', 'csv', 'files', 'whenever', 'user', 'interacts', 'master', 'csv', 'file', 'ranges', 'size', 'example', 'user', 'wants', 'select', 'option', 'dropdown', 'menu', 'clicking', 'dropdown', 'triggers', 'reading', 'aggregation', 'file', 'frontend', 'returned', 'strings', 'dropdown', 'options', 'returned', 'mean', 'new', 'csv', 'file', 'created', 'somewhere', 'users', 'local', 'file', 'system', 'frontend', 'picks', 'tested', 'similar', 'functionality', 'using', 'flask', 'rest', 'api', 'csv', 'file', 'loaded', 'virtual', 'memory', 'process', 'takes', 'mscompared', 'current', 'design', 'takes', 'full', 'minute', 'due', 'scripts', 'needing', 'sort', 'dependencies', 'validation', 'etc', 'however', 'engineering', 'team', 'refuses', 'adopt', 'webbased', 'communication', 'arguing', 'worth', 'effort', 'idea', 'using', 'cloudbased', 'relational', 'database', 'essentially', 'taboo', 'either', 'csv', 'files', 'pythons', 'pickle', 'dumps', 'users', 'local', 'system', 'experience', 'software', 'engineering', 'made', 'mission', 'redesign', 'legacy', 'monsterwith', 'blessing', 'senior', 'manager', 'far', 'transition', 'gone', 'incredibly', 'well', 'last', 'year', 'soft', 'launch', 'small', 'subset', 'features', 'within', 'days', 'every', 'person', 'department', 'using', 'question', 'current', 'design', 'requires', 'users', 'set', 'virtual', 'environment', 'run', 'installation', 'script', 'sorts', 'environment', 'variables', 'dependencies', 'etc', 'time', 'want', 'start', 'software', 'must', 'run', 'local', 'flask', 'api', 'interacts', 'react', 'typescript', 'frontend', 'flask', 'api', 'starts', 'loads', 'necessary', 'files', 'memory', 'validation', 'things', 'takes', 'around', 'minute', 'every', 'subsequent', 'request', 'easy', 'takes', 'average', 'however', 'dislike', 'user', 'needs', 'fully', 'configured', 'environment', 'version', 'control', 'also', 'headache', 'since', 'every', 'user', 'must', 'manually', 'run', 'update', 'script', 'like', 'move', 'flask', 'api', 'cloud', 'either', 'single', 'server', 'serves', 'colleagues', 'colleague', 'gets', 'dedicated', 'nodepod', 'problem', 'single', 'server', 'would', 'quickly', 'run', 'virtual', 'memory', 'colleagues', 'loaded', 'large', 'datasets', 'simultaneously', 'problem', 'one', 'node', 'per', 'colleague', 'complexityit', 'would', 'require', 'kubernetes', 'aws', 'fargate', 'along', 'orchestrator', 'manage', 'node', 'creation', 'termination', 'significant', 'engineering', 'effort', 'considered', 'making', 'flask', 'api', 'stateless', 'storing', 'large', 'datasets', 'using', 'dynamodb', 'file', 'mapping', 'loading', 'data', 'virtual', 'memory', 'every', 'request', 'converted', 'sample', 'datasets', 'parquet', 'reducing', 'size', 'significantly', 'worry', 'added', 'latency', 'repeatedly', 'reading', 'data', 'given', 'user', 'makes', 'requests', 'per', 'minute', 'seems', 'highly', 'inefficient', 'missing', 'alternatives', 'based', 'local', 'flask', 'api', 'still', 'seems', 'like', 'best', 'optionunless', 'want', 'pay', 'expensive', 'vram', 'instance', 'invest', 'significant', 'time', 'building', 'nodeperuser', 'architecture', 'thanks']",context joined finance consultancy years ago noticed people department frustrated current software engineering team built decades yes years decades issue software consists bundle python scripts repeatedly read large csv files whenever user interacts master csv file ranges size example user wants select option dropdown menu clicking dropdown triggers reading aggregation file frontend returned strings dropdown options returned mean new csv file created somewhere users local file system frontend picks tested similar functionality using flask rest api csv file loaded virtual memory process takes mscompared current design takes full minute due scripts needing sort dependencies validation etc however engineering team refuses adopt webbased communication arguing worth effort idea using cloudbased relational database essentially taboo either csv files pythons pickle dumps users local system experience software engineering made mission redesign legacy monsterwith blessing senior manager far transition gone incredibly well last year soft launch small subset features within days every person department using question current design requires users set virtual environment run installation script sorts environment variables dependencies etc time want start software must run local flask api interacts react typescript frontend flask api starts loads necessary files memory validation things takes around minute every subsequent request easy takes average however dislike user needs fully configured environment version control also headache since every user must manually run update script like move flask api cloud either single server serves colleagues colleague gets dedicated nodepod problem single server would quickly run virtual memory colleagues loaded large datasets simultaneously problem one node per colleague complexityit would require kubernetes aws fargate along orchestrator manage node creation termination significant engineering effort considered making flask api stateless storing large datasets using dynamodb file mapping loading data virtual memory every request converted sample datasets parquet reducing size significantly worry added latency repeatedly reading data given user makes requests per minute seems highly inefficient missing alternatives based local flask api still seems like best optionunless want pay expensive vram instance invest significant time building nodeperuser architecture thanks,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
59,Need Feedback on data sharing module,"
Subject: Seeking Feedback: CrossLink - Faster Data Sharing Between Python/R/C++/Julia via Arrow & Shared Memory

Hey r/dataengineering


I've been working on a project called CrossLink aimed at tackling a common bottleneck: efficiently sharing large datasets (think multi-million row Arrow tables / Pandas DataFrames / R data.frames) between processes written in different languages (Python, R, C++, Julia) when they're running on the same machine/node.
Mainly given workflows where teams have different language expertise.

The Problem:
We often end up saving data to intermediate files (CSVs are slow, Parquet is better but still involves disk I/O and serialization/deserialization overhead) just to pass data from, say, a Python preprocessing script to an R analysis script, or a C++ simulation output to Python for plotting. This can dominate runtime for data-heavy pipelines.

CrossLink's Approach:
The idea is to create a high-performance IPC (Inter-Process Communication) layer specifically for this, leveraging:
Apache Arrow: As the common, efficient in-memory columnar format.
Shared Memory / Memory-Mapped Files: Using Arrow IPC format over these mechanisms for potential minimal-copy data transfer between processes on the same host.


DuckDB: To manage persistent metadata about the shared datasets (unique IDs, names, schemas, source language, location - shmem key or mmap path) and allow optional SQL queries across them.


Essentially, it tries to create a shared data pool where different language processes can push and pull Arrow tables with minimal overhead.


Performance:
Early benchmarks on a 100M row Python -> R pipeline are encouraging, showing CrossLink is:
Roughly 16x faster than passing data via CSV files.
Roughly 2x faster than passing data via disk-based Arrow/Parquet files.

It also now includes a streaming API with backpressure and disk-spilling capabilities for handling >RAM datasets.


Architecture:
It's built around a C++ core library (libcrosslink) handling the Arrow serialization, IPC (shmem/mmap via helper classes), and DuckDB metadata interactions. Language bindings (currently Python & R functional, Julia building) expose this functionality idiomatically.


Seeking Feedback:
I'd love to get your thoughts, especially on:
Architecture: Does using Arrow + DuckDB + (Shared Mem / MMap) seem like a reasonable approach for this problem? 

Any obvious pitfalls or complexities I might be underestimating (beyond the usual fun of shared memory management and cross-platform IPC)?


Usefulness: Is this data transfer bottleneck a significant pain point you actually encounter in your work? Would a library like CrossLink potentially fit into your workflows (e.g., local data science pipelines, multi-language services running on a single server, HPC node-local tasks)?


Alternatives: What are you currently using to handle this? (Just sticking with Parquet on shared disk? Using something like Ray's object store if you're in that ecosystem? Redis? Other IPC methods?)


Appreciate any constructive criticism or insights you might have! Happy to elaborate on any part of the design.

I built this to ease the pain of moving across different scripts and languages for a single file. Wanted to know if it useful for any of you here and would be a sensible open source project to maintain.

It is currently built only for local nodes, but looking to add support with arrow flight across nodes as well. ",2,5,2025-03-31 08:43:18,0,False,False,False,False,2025-03-31 08:43:18,8,Monday,512.0,3444,36.39,28,896,13.6,0,1,NEGATIVE,-0.9678376317024231,"['subject', 'seeking', 'feedback', 'crosslink', 'faster', 'data', 'sharing', 'pythonrcjulia', 'via', 'arrow', 'shared', 'memory', 'hey', 'rdataengineering', 'ive', 'working', 'project', 'called', 'crosslink', 'aimed', 'tackling', 'common', 'bottleneck', 'efficiently', 'sharing', 'large', 'datasets', 'think', 'multimillion', 'row', 'arrow', 'tables', 'pandas', 'dataframes', 'dataframes', 'processes', 'written', 'different', 'languages', 'python', 'julia', 'theyre', 'running', 'machinenode', 'mainly', 'given', 'workflows', 'teams', 'different', 'language', 'expertise', 'problem', 'often', 'end', 'saving', 'data', 'intermediate', 'files', 'csvs', 'slow', 'parquet', 'better', 'still', 'involves', 'disk', 'serializationdeserialization', 'overhead', 'pass', 'data', 'say', 'python', 'preprocessing', 'script', 'analysis', 'script', 'simulation', 'output', 'python', 'plotting', 'dominate', 'runtime', 'dataheavy', 'pipelines', 'crosslinks', 'approach', 'idea', 'create', 'highperformance', 'ipc', 'interprocess', 'communication', 'layer', 'specifically', 'leveraging', 'apache', 'arrow', 'common', 'efficient', 'inmemory', 'columnar', 'format', 'shared', 'memory', 'memorymapped', 'files', 'using', 'arrow', 'ipc', 'format', 'mechanisms', 'potential', 'minimalcopy', 'data', 'transfer', 'processes', 'host', 'duckdb', 'manage', 'persistent', 'metadata', 'shared', 'datasets', 'unique', 'ids', 'names', 'schemas', 'source', 'language', 'location', 'shmem', 'key', 'mmap', 'path', 'allow', 'optional', 'sql', 'queries', 'across', 'essentially', 'tries', 'create', 'shared', 'data', 'pool', 'different', 'language', 'processes', 'push', 'pull', 'arrow', 'tables', 'minimal', 'overhead', 'performance', 'early', 'benchmarks', 'row', 'python', 'pipeline', 'encouraging', 'showing', 'crosslink', 'roughly', 'faster', 'passing', 'data', 'via', 'csv', 'files', 'roughly', 'faster', 'passing', 'data', 'via', 'diskbased', 'arrowparquet', 'files', 'also', 'includes', 'streaming', 'api', 'backpressure', 'diskspilling', 'capabilities', 'handling', 'ram', 'datasets', 'architecture', 'built', 'around', 'core', 'library', 'libcrosslink', 'handling', 'arrow', 'serialization', 'ipc', 'shmemmmap', 'via', 'helper', 'classes', 'duckdb', 'metadata', 'interactions', 'language', 'bindings', 'currently', 'python', 'functional', 'julia', 'building', 'expose', 'functionality', 'idiomatically', 'seeking', 'feedback', 'love', 'get', 'thoughts', 'especially', 'architecture', 'using', 'arrow', 'duckdb', 'shared', 'mem', 'mmap', 'seem', 'like', 'reasonable', 'approach', 'problem', 'obvious', 'pitfalls', 'complexities', 'might', 'underestimating', 'beyond', 'usual', 'fun', 'shared', 'memory', 'management', 'crossplatform', 'ipc', 'usefulness', 'data', 'transfer', 'bottleneck', 'significant', 'pain', 'point', 'actually', 'encounter', 'work', 'would', 'library', 'like', 'crosslink', 'potentially', 'fit', 'workflows', 'local', 'data', 'science', 'pipelines', 'multilanguage', 'services', 'running', 'single', 'server', 'hpc', 'nodelocal', 'tasks', 'alternatives', 'currently', 'using', 'handle', 'sticking', 'parquet', 'shared', 'disk', 'using', 'something', 'like', 'rays', 'object', 'store', 'youre', 'ecosystem', 'redis', 'ipc', 'methods', 'appreciate', 'constructive', 'criticism', 'insights', 'might', 'happy', 'elaborate', 'part', 'design', 'built', 'ease', 'pain', 'moving', 'across', 'different', 'scripts', 'languages', 'single', 'file', 'wanted', 'know', 'useful', 'would', 'sensible', 'open', 'source', 'project', 'maintain', 'currently', 'built', 'local', 'nodes', 'looking', 'add', 'support', 'arrow', 'flight', 'across', 'nodes', 'well']",subject seeking feedback crosslink faster data sharing pythonrcjulia via arrow shared memory hey rdataengineering ive working project called crosslink aimed tackling common bottleneck efficiently sharing large datasets think multimillion row arrow tables pandas dataframes dataframes processes written different languages python julia theyre running machinenode mainly given workflows teams different language expertise problem often end saving data intermediate files csvs slow parquet better still involves disk serializationdeserialization overhead pass data say python preprocessing script analysis script simulation output python plotting dominate runtime dataheavy pipelines crosslinks approach idea create highperformance ipc interprocess communication layer specifically leveraging apache arrow common efficient inmemory columnar format shared memory memorymapped files using arrow ipc format mechanisms potential minimalcopy data transfer processes host duckdb manage persistent metadata shared datasets unique ids names schemas source language location shmem key mmap path allow optional sql queries across essentially tries create shared data pool different language processes push pull arrow tables minimal overhead performance early benchmarks row python pipeline encouraging showing crosslink roughly faster passing data via csv files roughly faster passing data via diskbased arrowparquet files also includes streaming api backpressure diskspilling capabilities handling ram datasets architecture built around core library libcrosslink handling arrow serialization ipc shmemmmap via helper classes duckdb metadata interactions language bindings currently python functional julia building expose functionality idiomatically seeking feedback love get thoughts especially architecture using arrow duckdb shared mem mmap seem like reasonable approach problem obvious pitfalls complexities might underestimating beyond usual fun shared memory management crossplatform ipc usefulness data transfer bottleneck significant pain point actually encounter work would library like crosslink potentially fit workflows local data science pipelines multilanguage services running single server hpc nodelocal tasks alternatives currently using handle sticking parquet shared disk using something like rays object store youre ecosystem redis ipc methods appreciate constructive criticism insights might happy elaborate part design built ease pain moving across different scripts languages single file wanted know useful would sensible open source project maintain currently built local nodes looking add support arrow flight across nodes well,Mid,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
60,Anyone try Semaphore?,I’ve been looking for something to unify our data and found Semaphore. Anyone have this in their company and how are they using it? Like it? Is there an alternative? Want to get some data before I engage the sales vultures ,1,0,2025-04-01 00:29:49,1,False,False,False,False,2025-04-01 00:29:49,0,Tuesday,41.0,223,69.48,4,61,9.5,0,0,NEGATIVE,-0.999652624130249,"['ive', 'looking', 'something', 'unify', 'data', 'found', 'semaphore', 'anyone', 'company', 'using', 'like', 'alternative', 'want', 'get', 'data', 'engage', 'sales', 'vultures']",ive looking something unify data found semaphore anyone company using like alternative want get data engage sales vultures,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
61,Question about preprocessing two time-series datasets from different measurement devices,"I have a question regarding the preprocessing step in a project I'm working on. I have two different measurement devices that both collect time-series data. My goal is to analyze the similarity between these two signals.

Although both devices measure the same phenomenon and I've converted the units to be consistent, I'm unsure whether this is sufficient for meaningful comparison, given that the devices themselves are different and may have distinct ranges or variances.

From the literature, I’ve found that z-score normalization is commonly used to address such issues. However, I’m concerned that applying z-score normalization to each dataset individually might make it impossible to compare across datasets, especially when I want to analyze multiple sessions or subjects later.

Is z-score normalization the right approach in this case? Or would it be better to normalize using a common reference (e.g., using statistics from a larger dataset)? Any guidance or references would be greatly appreciated.Thank you :)",1,1,2025-03-31 02:33:18,0,False,False,False,False,2025-03-31 02:33:18,2,Monday,156.0,1023,38.82,10,286,14.1,0,0,NEGATIVE,-0.9945716261863708,"['question', 'regarding', 'preprocessing', 'step', 'project', 'working', 'two', 'different', 'measurement', 'devices', 'collect', 'timeseries', 'data', 'goal', 'analyze', 'similarity', 'two', 'signals', 'although', 'devices', 'measure', 'phenomenon', 'ive', 'converted', 'units', 'consistent', 'unsure', 'whether', 'sufficient', 'meaningful', 'comparison', 'given', 'devices', 'different', 'may', 'distinct', 'ranges', 'variances', 'literature', 'ive', 'found', 'zscore', 'normalization', 'commonly', 'used', 'address', 'issues', 'however', 'concerned', 'applying', 'zscore', 'normalization', 'dataset', 'individually', 'might', 'make', 'impossible', 'compare', 'across', 'datasets', 'especially', 'want', 'analyze', 'multiple', 'sessions', 'subjects', 'later', 'zscore', 'normalization', 'right', 'approach', 'case', 'would', 'better', 'normalize', 'using', 'common', 'reference', 'using', 'statistics', 'larger', 'dataset', 'guidance', 'references', 'would', 'greatly', 'appreciatedthank']",question regarding preprocessing step project working two different measurement devices collect timeseries data goal analyze similarity two signals although devices measure phenomenon ive converted units consistent unsure whether sufficient meaningful comparison given devices different may distinct ranges variances literature ive found zscore normalization commonly used address issues however concerned applying zscore normalization dataset individually might make impossible compare across datasets especially want analyze multiple sessions subjects later zscore normalization right approach case would better normalize using common reference using statistics larger dataset guidance references would greatly appreciatedthank,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
62,Cloud Pandit Azure Data Engineering course feedback or can we take !!,Had anyone taken Cloud Pandit Azure Data Engg course. just wanted to know !!,0,3,2025-03-31 13:08:18,0,False,False,False,False,2025-03-31 13:08:18,13,Monday,14.0,76,73.34,2,20,0.0,0,0,NEGATIVE,-0.7058281302452087,"['anyone', 'taken', 'cloud', 'pandit', 'azure', 'data', 'engg', 'course', 'wanted', 'know']",anyone taken cloud pandit azure data engg course wanted know,Low,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
63,~33% faster Microsoft Fabric with e6data– Feedback Requested,"Hey folks,

I'm a data engineer at [e6data](https://www.e6data.com/), and we've been working on integrating our engine with Microsoft Fabric. We recently ran some benchmarks (TPC-DS) and observed around a **33% improvement in SQL query performance** while also significantly reducing costs compared to native Fabric compute engines.

Here's what our integration specifically enables:

* **33% faster SQL queries** directly on data stored in OneLake (TPC-DS benchmark results).
* **2-3x cost reduction** by optimizing compute efficiency.
* **Zero data movement**: direct querying of data from OneLake.
* Native vector search support for AI-driven workflows.
* Scalable to 1000+ QPS with sub-second latency and real-time autoscaling.
* Enterprise-level security measures.

We've documented our approach and benchmark results: [https://www.e6data.com/blog/e6data-fabric-increased-performance-optimized-capacity](https://www.e6data.com/blog/e6data-fabric-increased-performance-optimized-capacity)

We'd genuinely appreciate your thoughts, feedback, or questions about our approach or experiences with similar integrations.

https://preview.redd.it/vx65oc5zd2se1.jpg?width=6636&format=pjpg&auto=webp&s=57a2a75cd11a8cc616d04a042a12055bddfe5b4b

",0,0,2025-03-31 17:57:18,0,False,False,False,False,2025-03-31 17:57:18,17,Monday,132.0,1239,0.58,11,295,13.0,1,0,POSITIVE,0.972781240940094,"['hey', 'folks', 'data', 'engineer', 'edatahttpswwwedatacom', 'weve', 'working', 'integrating', 'engine', 'microsoft', 'fabric', 'recently', 'ran', 'benchmarks', 'tpcds', 'observed', 'around', 'improvement', 'sql', 'query', 'performance', 'also', 'significantly', 'reducing', 'costs', 'compared', 'native', 'fabric', 'compute', 'engines', 'heres', 'integration', 'specifically', 'enables', 'faster', 'sql', 'queries', 'directly', 'data', 'stored', 'onelake', 'tpcds', 'benchmark', 'results', 'cost', 'reduction', 'optimizing', 'compute', 'efficiency', 'zero', 'data', 'movement', 'direct', 'querying', 'data', 'onelake', 'native', 'vector', 'search', 'support', 'aidriven', 'workflows', 'scalable', 'qps', 'subsecond', 'latency', 'realtime', 'autoscaling', 'enterpriselevel', 'security', 'measures', 'weve', 'documented', 'approach', 'benchmark', 'results', 'httpswwwedatacomblogedatafabricincreasedperformanceoptimizedcapacityhttpswwwedatacomblogedatafabricincreasedperformanceoptimizedcapacity', 'wed', 'genuinely', 'appreciate', 'thoughts', 'feedback', 'questions', 'approach', 'experiences', 'similar', 'integrations', 'httpspreviewredditvxoczdsejpgwidthformatpjpgautowebpsaacdaccdaabddfebb']",hey folks data engineer edatahttpswwwedatacom weve working integrating engine microsoft fabric recently ran benchmarks tpcds observed around improvement sql query performance also significantly reducing costs compared native fabric compute engines heres integration specifically enables faster sql queries directly data stored onelake tpcds benchmark results cost reduction optimizing compute efficiency zero data movement direct querying data onelake native vector search support aidriven workflows scalable qps subsecond latency realtime autoscaling enterpriselevel security measures weve documented approach benchmark results httpswwwedatacomblogedatafabricincreasedperformanceoptimizedcapacityhttpswwwedatacomblogedatafabricincreasedperformanceoptimizedcapacity wed genuinely appreciate thoughts feedback questions approach experiences similar integrations httpspreviewredditvxoczdsejpgwidthformatpjpgautowebpsaacdaccdaabddfebb,Low,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
64,Ways to quickly get total rows?,"When i am testing things often i need to run some counts  in databricks.

What is the prefered way?

I am creating a pyspark.dataframe  using spark.sql statements and later
DF.count().

Further information can be provided.",0,1,2025-03-31 17:06:54,0,False,False,False,False,2025-03-31 17:06:54,17,Monday,35.0,222,74.05,6,53,7.8,0,0,NEGATIVE,-0.9980649352073669,"['testing', 'things', 'often', 'need', 'run', 'counts', 'databricks', 'prefered', 'way', 'creating', 'pysparkdataframe', 'using', 'sparksql', 'statements', 'later', 'dfcount', 'information', 'provided']",testing things often need run counts databricks prefered way creating pysparkdataframe using sparksql statements later dfcount information provided,Low,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
65,Seeking Advice from DE: Taking a Career Break to Work & Travel in Australia,"Hey DE,

I’d love to get your perspective on my situation.

# My Background

I’m a Brazilian Mechanical Engineer with 3 years of experience in the Data field—started as a Data Analyst for 1.5 years, then transitioned into Data Engineering. Next week, I’ll be starting as a Data Architect at a multinational with 100,000+ employees, mainly working with the Azure stack.

# The Plan

My girlfriend and I are planning to move to Australia for about a year to travel and build memories together before settling down (marriage, house, etc.). This new job came unexpectedly, but it offers a good salary (\~$2,000 USD/month).

The idea is to:

* Move to Australia
* Work hard & save around $1,000 USD/month
* Travel as much as possible for \~2 years
* Return and re-enter the data field

# The Challenge

The work visa limitation allows me to stay only 6 months with the same employer, making it tough to get good Data Engineering jobs. So, I plan to work in any job that pays well (fruit picking, hospitality, etc.), and my girlfriend will do the same.

# The Concern

When I return, how hard will it be to get back into the data field after a \~2-year break?

* I’ll have enough savings to stay unemployed for about a year if needed.
* This isn’t all my savings—I have the equivalent of 6 years of salary in reserve.
* I regularly get recruiter messages on LinkedIn.
* I speak Portuguese, English, and Spanish fluently.

Given your experience, how risky is this career break? is totally crazy ? Would you recommend a different approach? Any advice would be appreciated!",0,2,2025-03-31 14:34:32,0,False,False,False,False,2025-03-31 14:34:32,14,Monday,276.0,1564,56.66,18,421,12.1,0,0,POSITIVE,0.9331977367401123,"['hey', 'love', 'get', 'perspective', 'situation', 'background', 'brazilian', 'mechanical', 'engineer', 'years', 'experience', 'data', 'fieldstarted', 'data', 'analyst', 'years', 'transitioned', 'data', 'engineering', 'next', 'week', 'ill', 'starting', 'data', 'architect', 'multinational', 'employees', 'mainly', 'working', 'azure', 'stack', 'plan', 'girlfriend', 'planning', 'move', 'australia', 'year', 'travel', 'build', 'memories', 'together', 'settling', 'marriage', 'house', 'etc', 'new', 'job', 'came', 'unexpectedly', 'offers', 'good', 'salary', 'usdmonth', 'idea', 'move', 'australia', 'work', 'hard', 'save', 'around', 'usdmonth', 'travel', 'much', 'possible', 'years', 'return', 'reenter', 'data', 'field', 'challenge', 'work', 'visa', 'limitation', 'allows', 'stay', 'months', 'employer', 'making', 'tough', 'get', 'good', 'data', 'engineering', 'jobs', 'plan', 'work', 'job', 'pays', 'well', 'fruit', 'picking', 'hospitality', 'etc', 'girlfriend', 'concern', 'return', 'hard', 'get', 'back', 'data', 'field', 'year', 'break', 'ill', 'enough', 'savings', 'stay', 'unemployed', 'year', 'needed', 'isnt', 'savingsi', 'equivalent', 'years', 'salary', 'reserve', 'regularly', 'get', 'recruiter', 'messages', 'linkedin', 'speak', 'portuguese', 'english', 'spanish', 'fluently', 'given', 'experience', 'risky', 'career', 'break', 'totally', 'crazy', 'would', 'recommend', 'different', 'approach', 'advice', 'would', 'appreciated']",hey love get perspective situation background brazilian mechanical engineer years experience data fieldstarted data analyst years transitioned data engineering next week ill starting data architect multinational employees mainly working azure stack plan girlfriend planning move australia year travel build memories together settling marriage house etc new job came unexpectedly offers good salary usdmonth idea move australia work hard save around usdmonth travel much possible years return reenter data field challenge work visa limitation allows stay months employer making tough get good data engineering jobs plan work job pays well fruit picking hospitality etc girlfriend concern return hard get back data field year break ill enough savings stay unemployed year needed isnt savingsi equivalent years salary reserve regularly get recruiter messages linkedin speak portuguese english spanish fluently given experience risky career break totally crazy would recommend different approach advice would appreciated,Low,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
66,Anyone else feel like data engineering is way more stressful than expected?,"I used to work as a Tableau developer and honestly, life felt simpler. I still had deadlines, but the work was more visual, less complex, and didn’t bleed into my personal time as much.

Now that I'm in data engineering, I feel like I’m constantly thinking about pipelines, bugs, unexpected data issues, or some tool update I haven’t kept up with. Even on vacation, I catch myself checking Slack or thinking about the next sprint. I turned 30 recently and started wondering… is this normal career pressure, imposter syndrome, or am I chasing too much of management approval?

Is anyone else feeling this way? Is the stress worth it long term?",154,49,2025-04-01 05:25:58,0,False,False,False,False,2025-04-01 05:25:58,5,Tuesday,111.0,642,63.8,7,168,11.5,0,0,NEGATIVE,-0.987242579460144,"['used', 'work', 'tableau', 'developer', 'honestly', 'life', 'felt', 'simpler', 'still', 'deadlines', 'work', 'visual', 'less', 'complex', 'didnt', 'bleed', 'personal', 'time', 'much', 'data', 'engineering', 'feel', 'like', 'constantly', 'thinking', 'pipelines', 'bugs', 'unexpected', 'data', 'issues', 'tool', 'update', 'havent', 'kept', 'even', 'vacation', 'catch', 'checking', 'slack', 'thinking', 'next', 'sprint', 'turned', 'recently', 'started', 'wondering', 'normal', 'career', 'pressure', 'imposter', 'syndrome', 'chasing', 'much', 'management', 'approval', 'anyone', 'else', 'feeling', 'way', 'stress', 'worth', 'long', 'term']",used work tableau developer honestly life felt simpler still deadlines work visual less complex didnt bleed personal time much data engineering feel like constantly thinking pipelines bugs unexpected data issues tool update havent kept even vacation catch checking slack thinking next sprint turned recently started wondering normal career pressure imposter syndrome chasing much management approval anyone else feeling way stress worth long term,High,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
67,Found the perfect Data Dictionary tool!,"Just launched the [Urban Data Dictionary](https://www.urbandatadictionary.com/) and to celebrate what what we actually do in data engineering. Hope you find it fun and like it too. 

Check it out and add your own definitions. What terms would you contribute?

Happy April Fools!",131,11,2025-04-01 14:15:10,0,False,False,False,False,2025-04-01 14:15:10,14,Tuesday,42.0,278,64.37,6,68,8.8,1,0,POSITIVE,0.9995446801185608,"['launched', 'urban', 'data', 'dictionaryhttpswwwurbandatadictionarycom', 'celebrate', 'actually', 'data', 'engineering', 'hope', 'find', 'fun', 'like', 'check', 'add', 'definitions', 'terms', 'would', 'contribute', 'happy', 'april', 'fools']",launched urban data dictionaryhttpswwwurbandatadictionarycom celebrate actually data engineering hope find fun like check add definitions terms would contribute happy april fools,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
68,"What Python libraries, functions, methods, etc. do data engineers frequently use during the extraction and transformation steps of their ETL work?","I am currently learning and applying data engineering into my job. I am a data analyst with three years of experience. I am trying to learn ETL to construct automated data pipelines for my reports.

Using Python programming language, I am trying to extract data from Excel file and API data sources. I am then trying to manipulate that data. In essence, I am basically trying to use a more efficient and powerful form of Microsoft's Power Query.

What are the most common Python libraries, functions, methods, etc. that data engineers frequently use during the extraction and transformation steps of their ETL work?

P.S.

Please let me know if you recommend any books or YouTube channels so that I can further improve my skillset within the ETL portion of data engineering.

Thank you all for your help. I sincerely appreciate all your expertise. I am new to data engineering, so apologies if some of my terminology is wrong.



Edit:

Thank you all for the detailed responses. I highly appreciate all of this information.",89,60,2025-04-01 14:12:13,0,False,2025-04-01 15:26:11,False,False,2025-04-01 14:12:13,14,Tuesday,173.0,1023,50.43,14,288,11.4,0,1,NEGATIVE,-0.9917260408401489,"['currently', 'learning', 'applying', 'data', 'engineering', 'job', 'data', 'analyst', 'three', 'years', 'experience', 'trying', 'learn', 'etl', 'construct', 'automated', 'data', 'pipelines', 'reports', 'using', 'python', 'programming', 'language', 'trying', 'extract', 'data', 'excel', 'file', 'api', 'data', 'sources', 'trying', 'manipulate', 'data', 'essence', 'basically', 'trying', 'use', 'efficient', 'powerful', 'form', 'microsofts', 'power', 'query', 'common', 'python', 'libraries', 'functions', 'methods', 'etc', 'data', 'engineers', 'frequently', 'use', 'extraction', 'transformation', 'steps', 'etl', 'work', 'please', 'let', 'know', 'recommend', 'books', 'youtube', 'channels', 'improve', 'skillset', 'within', 'etl', 'portion', 'data', 'engineering', 'thank', 'help', 'sincerely', 'appreciate', 'expertise', 'new', 'data', 'engineering', 'apologies', 'terminology', 'wrong', 'edit', 'thank', 'detailed', 'responses', 'highly', 'appreciate', 'information']",currently learning applying data engineering job data analyst three years experience trying learn etl construct automated data pipelines reports using python programming language trying extract data excel file api data sources trying manipulate data essence basically trying use efficient powerful form microsofts power query common python libraries functions methods etc data engineers frequently use extraction transformation steps etl work please let know recommend books youtube channels improve skillset within etl portion data engineering thank help sincerely appreciate expertise new data engineering apologies terminology wrong edit thank detailed responses highly appreciate information,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
69,What is the best free BI dashboarding tool?,We have 5 developers and none of them are data scientists. We need to be able to create interactive dashboards for management.,19,25,2025-04-01 21:23:16,0,False,False,False,False,2025-04-01 21:23:16,21,Tuesday,22.0,126,60.31,2,36,0.0,0,0,NEGATIVE,-0.9990633130073547,"['developers', 'none', 'data', 'scientists', 'need', 'able', 'create', 'interactive', 'dashboards', 'management']",developers none data scientists need able create interactive dashboards management,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
70,Time-series analysis pipeline architecture,"Hi, I'm a bit outdated when it comes to all new cloud based solutions and request navigation on what architecture might be useful to start with (should be rather simple and not too much overhead to set up) while still be prepared for more data sources and more analysis requirements.

I'm using Azure

My use-case:
I have a time-series dataset coming from an API on which we perform a Python analysis. We would like to perform the Python analysis on a weekly basis, store the data and provide the output as a power bi dashboard. The dataset consists of like 500 000 rows each week, the analysis scripts processes a many to many calculation and I might be interested in adding more data sources as well as perform more KPI calculations pre-processed in data storage (i.e. not in power bi).",10,3,2025-04-01 08:03:41,1,False,False,False,False,2025-04-01 08:03:41,8,Tuesday,140.0,788,43.06,5,217,14.0,0,0,NEGATIVE,-0.9994192123413086,"['bit', 'outdated', 'comes', 'new', 'cloud', 'based', 'solutions', 'request', 'navigation', 'architecture', 'might', 'useful', 'start', 'rather', 'simple', 'much', 'overhead', 'set', 'still', 'prepared', 'data', 'sources', 'analysis', 'requirements', 'using', 'azure', 'usecase', 'timeseries', 'dataset', 'coming', 'api', 'perform', 'python', 'analysis', 'would', 'like', 'perform', 'python', 'analysis', 'weekly', 'basis', 'store', 'data', 'provide', 'output', 'power', 'dashboard', 'dataset', 'consists', 'like', 'rows', 'week', 'analysis', 'scripts', 'processes', 'many', 'many', 'calculation', 'might', 'interested', 'adding', 'data', 'sources', 'well', 'perform', 'kpi', 'calculations', 'preprocessed', 'data', 'storage', 'power']",bit outdated comes new cloud based solutions request navigation architecture might useful start rather simple much overhead set still prepared data sources analysis requirements using azure usecase timeseries dataset coming api perform python analysis would like perform python analysis weekly basis store data provide output power dashboard dataset consists like rows week analysis scripts processes many many calculation might interested adding data sources well perform kpi calculations preprocessed data storage power,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
71,Cloud platform for dbt,"I recently started learning dbt and was using Snowflake as my database. However, my 30-day trial has ended. Are there any free cloud databases I can use to continue learning dbt and later work on projects that I can showcase on GitHub?

Which cloud database would you recommend? Most options seem quite expensive for a learning setup.

Additionally, do you have any recommendations for dbt projects that would be valuable for hands-on practice and portfolio building?

Looking forward to your suggestions!",5,13,2025-04-01 16:02:14,0,False,False,False,False,2025-04-01 16:02:14,16,Tuesday,81.0,505,51.24,7,136,10.9,0,0,NEGATIVE,-0.9948680400848389,"['recently', 'started', 'learning', 'dbt', 'using', 'snowflake', 'database', 'however', 'day', 'trial', 'ended', 'free', 'cloud', 'databases', 'use', 'continue', 'learning', 'dbt', 'later', 'work', 'projects', 'showcase', 'github', 'cloud', 'database', 'would', 'recommend', 'options', 'seem', 'quite', 'expensive', 'learning', 'setup', 'additionally', 'recommendations', 'dbt', 'projects', 'would', 'valuable', 'handson', 'practice', 'portfolio', 'building', 'looking', 'forward', 'suggestions']",recently started learning dbt using snowflake database however day trial ended free cloud databases use continue learning dbt later work projects showcase github cloud database would recommend options seem quite expensive learning setup additionally recommendations dbt projects would valuable handson practice portfolio building looking forward suggestions,Mid,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
72,Monthly General Discussion - Apr 2025,"This thread is a place where you can share things that might not warrant their own thread. It is automatically posted each month and you can find previous threads in the collection.

Examples:

* What are you working on this month?
* What was something you accomplished?
* What was something you learned recently?
* What is something frustrating you currently?

As always, sub rules apply. Please be respectful and stay curious.

**Community Links:**

* [Monthly newsletter](https://dataengineeringcommunity.substack.com/)
* [Data Engineering Events](https://dataengineering.wiki/Community/Events)
* [Data Engineering Meetups](https://dataengineering.wiki/Community/Meetups)
* [Get involved in the community](https://dataengineering.wiki/Community/Get+Involved)",5,0,2025-04-01 16:00:57,0,False,False,False,True,2025-04-01 16:00:57,16,Tuesday,91.0,761,22.17,12,173,9.9,1,0,NEGATIVE,-0.9886267781257629,"['thread', 'place', 'share', 'things', 'might', 'warrant', 'thread', 'automatically', 'posted', 'month', 'find', 'previous', 'threads', 'collection', 'examples', 'working', 'month', 'something', 'accomplished', 'something', 'learned', 'recently', 'something', 'frustrating', 'currently', 'always', 'sub', 'rules', 'apply', 'please', 'respectful', 'stay', 'curious', 'community', 'links', 'monthly', 'newsletterhttpsdataengineeringcommunitysubstackcom', 'data', 'engineering', 'eventshttpsdataengineeringwikicommunityevents', 'data', 'engineering', 'meetupshttpsdataengineeringwikicommunitymeetups', 'get', 'involved', 'communityhttpsdataengineeringwikicommunitygetinvolved']",thread place share things might warrant thread automatically posted month find previous threads collection examples working month something accomplished something learned recently something frustrating currently always sub rules apply please respectful stay curious community links monthly newsletterhttpsdataengineeringcommunitysubstackcom data engineering eventshttpsdataengineeringwikicommunityevents data engineering meetupshttpsdataengineeringwikicommunitymeetups get involved communityhttpsdataengineeringwikicommunitygetinvolved,Mid,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
73,Opinions on Vertex AI,"From a more technical perspective what's your opinion about Vertex AI.  
I am trying to deploy a machine learning pipeline and my data science colleges are real data scientists and I do not trust them to bring everything into production.  
What's your experience with vertex ai?",6,2,2025-04-01 15:49:03,0,False,False,False,False,2025-04-01 15:49:03,15,Tuesday,46.0,278,55.95,3,75,12.5,0,0,NEGATIVE,-0.9926102757453918,"['technical', 'perspective', 'whats', 'opinion', 'vertex', 'trying', 'deploy', 'machine', 'learning', 'pipeline', 'data', 'science', 'colleges', 'real', 'data', 'scientists', 'trust', 'bring', 'everything', 'production', 'whats', 'experience', 'vertex']",technical perspective whats opinion vertex trying deploy machine learning pipeline data science colleges real data scientists trust bring everything production whats experience vertex,High,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
74,any alternatives to alteryx?,"most of our data is on prem sql server. we also have some data sources in snowflake as well (10-15% of the data). we also connect to some api's as well using the python tool. our reporting db is sql server on prem. currently we are using alteryx, and we are researching what our options are before we have to renew our contract. any suggestions that we can explore or if someone has been through a similar scenario, what did you end up with and why? please let me know if I can add more information to the context.

also,I forgot to mention that not all of my team members are familiar with python. Looking for GUI options.

Edit: thank you all. I’ll look into the mentioned options.",1,10,2025-04-01 15:08:23,0,False,2025-04-02 00:14:33,False,False,2025-04-01 15:08:23,15,Tuesday,128.0,683,68.16,11,187,8.6,0,1,NEGATIVE,-0.997328519821167,"['data', 'prem', 'sql', 'server', 'also', 'data', 'sources', 'snowflake', 'well', 'data', 'also', 'connect', 'apis', 'well', 'using', 'python', 'tool', 'reporting', 'sql', 'server', 'prem', 'currently', 'using', 'alteryx', 'researching', 'options', 'renew', 'contract', 'suggestions', 'explore', 'someone', 'similar', 'scenario', 'end', 'please', 'let', 'know', 'add', 'information', 'context', 'alsoi', 'forgot', 'mention', 'team', 'members', 'familiar', 'python', 'looking', 'gui', 'options', 'edit', 'thank', 'ill', 'look', 'mentioned', 'options']",data prem sql server also data sources snowflake well data also connect apis well using python tool reporting sql server prem currently using alteryx researching options renew contract suggestions explore someone similar scenario end please let know add information context alsoi forgot mention team members familiar python looking gui options edit thank ill look mentioned options,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
75,Getting data from SAP HANA to snowflake,"So i have this project that will need to ingest data from SAP HANA into snowflake, it can be considered as any on-premise DB using JBDC, the big issue is, I cannot use any external ETL services as per project requirements. What is the best path to follow?  


I need to fetch the data in bulk for some tables with truncate / copy into, and some tables need to be incremental with little (10 min) delay. The tables do not contain any watermark, modified time or anything...  


There isnt much data, 20M rows tops.

If you guys can give me a hand, i'm new to snowflake and strugling to find any sources on this.",2,6,2025-04-01 11:15:13,0,False,False,False,False,2025-04-01 11:15:13,11,Tuesday,114.0,610,69.31,6,161,9.7,0,0,NEGATIVE,-0.9991737008094788,"['project', 'need', 'ingest', 'data', 'sap', 'hana', 'snowflake', 'considered', 'onpremise', 'using', 'jbdc', 'big', 'issue', 'cannot', 'use', 'external', 'etl', 'services', 'per', 'project', 'requirements', 'best', 'path', 'follow', 'need', 'fetch', 'data', 'bulk', 'tables', 'truncate', 'copy', 'tables', 'need', 'incremental', 'little', 'min', 'delay', 'tables', 'contain', 'watermark', 'modified', 'time', 'anything', 'isnt', 'much', 'data', 'rows', 'tops', 'guys', 'give', 'hand', 'new', 'snowflake', 'strugling', 'find', 'sources']",project need ingest data sap hana snowflake considered onpremise using jbdc big issue cannot use external etl services per project requirements best path follow need fetch data bulk tables truncate copy tables need incremental little min delay tables contain watermark modified time anything isnt much data rows tops guys give hand new snowflake strugling find sources,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
76,"Career improves, but projects don't? [discussion]","I started 6 years ago and my career has been on a growing trajectory since.

While this is very nice for me, I can’t say the same about the projects I encounter. What I mean is that I was expecting the engineering soundness of the projects I encounter to grow alongside my seniority in this field.

Instead, I’ve found that regardless of where I end up (the last two companies were data consulting shops), the projects I am assigned to tend to have questionable engineering decisions (often involving an unnecessary use of Spark to move 7 rows of data).

The latest one involves ETL out of MSSQL and into object storage, using a combination of Azure synapse spark notebooks, drag and drop GUI pipelines, absolutely no tests or CICD whatsoever, and debatable modeling once data lands in the lake.

This whole thing scares me quite a lot due to the lack of guardrails, while testing and deployments are done manually. While I'd love to rewrite everything from scratch, my eng lead said since that part it's complete and there isn't a plan to change it in the future, that it's not a priority at all, and I agree with this.

What's your experience in situations like this? How do you juggle the competing priorities (client wanting new things vs. optimizing old stuff etc...)?
",1,18,2025-04-01 10:51:01,0,False,False,False,False,2025-04-01 10:51:01,10,Tuesday,223.0,1274,57.3,10,339,13.0,0,0,NEGATIVE,-0.9926280379295349,"['started', 'years', 'ago', 'career', 'growing', 'trajectory', 'since', 'nice', 'cant', 'say', 'projects', 'encounter', 'mean', 'expecting', 'engineering', 'soundness', 'projects', 'encounter', 'grow', 'alongside', 'seniority', 'field', 'instead', 'ive', 'found', 'regardless', 'end', 'last', 'two', 'companies', 'data', 'consulting', 'shops', 'projects', 'assigned', 'tend', 'questionable', 'engineering', 'decisions', 'often', 'involving', 'unnecessary', 'use', 'spark', 'move', 'rows', 'data', 'latest', 'one', 'involves', 'etl', 'mssql', 'object', 'storage', 'using', 'combination', 'azure', 'synapse', 'spark', 'notebooks', 'drag', 'drop', 'gui', 'pipelines', 'absolutely', 'tests', 'cicd', 'whatsoever', 'debatable', 'modeling', 'data', 'lands', 'lake', 'whole', 'thing', 'scares', 'quite', 'lot', 'due', 'lack', 'guardrails', 'testing', 'deployments', 'done', 'manually', 'love', 'rewrite', 'everything', 'scratch', 'eng', 'lead', 'said', 'since', 'part', 'complete', 'isnt', 'plan', 'change', 'future', 'priority', 'agree', 'whats', 'experience', 'situations', 'like', 'juggle', 'competing', 'priorities', 'client', 'wanting', 'new', 'things', 'optimizing', 'old', 'stuff', 'etc']",started years ago career growing trajectory since nice cant say projects encounter mean expecting engineering soundness projects encounter grow alongside seniority field instead ive found regardless end last two companies data consulting shops projects assigned tend questionable engineering decisions often involving unnecessary use spark move rows data latest one involves etl mssql object storage using combination azure synapse spark notebooks drag drop gui pipelines absolutely tests cicd whatsoever debatable modeling data lands lake whole thing scares quite lot due lack guardrails testing deployments done manually love rewrite everything scratch eng lead said since part complete isnt plan change future priority agree whats experience situations like juggle competing priorities client wanting new things optimizing old stuff etc,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
77,What is the best approach for a Bronze layer?,"Hello,

We are starting a new Big Data project in my company with Cloudera, Hive, Hadoop HDFS, and a medallion architecture, but I have some questions about ""Bronze"" layer.

Our source is a FTP and in this FTP are allocated the daily/monthly files (.txt, .csv, .xlsx...).  
We bring those files to our HDFS in separated in folders by date (E.G: xxxx/2025/4)

Here start my doubts:  
\- Our bronze layer are those files in the HDFS?  
\- For build our bronze layer, we need to load those files incrementally into a ""bronze table"" partitioned by date

Reading on internet I saw that we have to do the second option, but I saw that option like a rubbish table

Which will be the best approach?

  
For the other layers, I don't have any doubts.",2,11,2025-04-01 07:20:04,0,False,False,False,False,2025-04-01 07:20:04,7,Tuesday,132.0,741,57.91,6,189,11.2,0,0,NEGATIVE,-0.9959003329277039,"['hello', 'starting', 'new', 'big', 'data', 'project', 'company', 'cloudera', 'hive', 'hadoop', 'hdfs', 'medallion', 'architecture', 'questions', 'bronze', 'layer', 'source', 'ftp', 'ftp', 'allocated', 'dailymonthly', 'files', 'txt', 'csv', 'xlsx', 'bring', 'files', 'hdfs', 'separated', 'folders', 'date', 'xxxx', 'start', 'doubts', 'bronze', 'layer', 'files', 'hdfs', 'build', 'bronze', 'layer', 'need', 'load', 'files', 'incrementally', 'bronze', 'table', 'partitioned', 'date', 'reading', 'internet', 'saw', 'second', 'option', 'saw', 'option', 'like', 'rubbish', 'table', 'best', 'approach', 'layers', 'dont', 'doubts']",hello starting new big data project company cloudera hive hadoop hdfs medallion architecture questions bronze layer source ftp ftp allocated dailymonthly files txt csv xlsx bring files hdfs separated folders date xxxx start doubts bronze layer files hdfs build bronze layer need load files incrementally bronze table partitioned date reading internet saw second option saw option like rubbish table best approach layers dont doubts,Mid,4,4,"files, api, tables, query, models, read, google, json, file, logic, write, case, csv, store, datasets, across, sources, metadata, created, schema, systems, large, platform, custom, script, changes, dataset, row, reporting, parquet, issues, ads, load, fields, analysis, cases, product, values, design, view, idea, layer, search, import, suggest, language, excel, app, blob, form, request, params, postgres, ingestion, raw, mysql, fact, processed, structures, either, separate, structured, connection, core, method, downstream, scripts, extract, maintain, send, hub, records, transactional, overhead, lineage, environment, object, external, pull, iot, order, typically, grid, hubs, validation, requires, mapping, batch, mechanism, location, updates, keeping, checkout, operations, runs, convert, optimized, avoid, matching, views, formats, matters, vpc, bring, reduce, migrate, consists, starts, reads, refresh, relationships, bronze, historical, tracking, developers, natural",8.0,7.1,169.4,-0.679,1041.7,44.5,9.5,267.7,10.7,24.1,17.9,49,102,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,7
78,Knime on Anaconda Nacigator,Is it possible to install Knime on Anaconda Navigator? ,1,0,2025-04-02 02:25:22,0,False,False,False,False,2025-04-02 02:25:22,2,Wednesday,9.0,55,20.04,1,19,0.0,0,0,NEGATIVE,-0.9977378845214844,"['possible', 'install', 'knime', 'anaconda', 'navigator']",possible install knime anaconda navigator,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
79,Facebook Marketing API - Anyone have a successful ETL experience?,"We have a python integration set up where we pull data from Google Ads and Facebook Marketing into our data warehouse. We're pulling data about all 3 hierarchy tiers and some daily metrics:

1. Campaigns (id, name, start time, stop time)
2. Ad Groups/Ad Sets (id, name)
3. Ads (id, name, URL)
4. Metrics (clicks, impressions, spend) for the previous day

For the Google Ads API, you basically send a SQL query and the return time is like a tenth of a second.

For Facebook, we see returns times in the minutes, especially on the Ads piece. Was hoping to get an idea of how others might have successfully set up a process to get this data from Facebook in a more timely fashion, and possibly without hitting the rate limiting threshold.

Not the exact code we're using - I can get it off my work system tomorrow - but the gist:

    from facebook_business.adobjects.adaccount import AdAccount
    from facebook_business.adobjects.campaign import Campaign
    from facebook_business.adobjects.ad import AdSet
    from facebook_business.adobjects.ad import Ad
    from facebook_business.adobjects.adcreative import AdCreative
    campaigns = AdAccount('act_123456789').get_campaigns(
        params={},
        fields=[Campaign.Field.id,Campaign.Field.name,Campaign.Field.start_time,Campaign.Field.stop_time]
    )
    adsets= AdAccount('act_123456789').get_ad_sets(
        params={},
        fields=[AdSet.Field.id,AdSet.Field.name]
    )
    ads = AdAccount('act_123456789').get_ads(
        params={},
        fields=[Ad.Field.id,Ad.Field.name,Ad.Field.creative]
    )
    object_urls = AdAccount('act_123456789').get_ad_creatives(
        params={},
        fields=[AdCreative.Field.object_story_spec]
    )
    asset_urls = AdAccount('act_123456789').get_ad_creatives(
        params={},
        fields=[AdCreative.Field.asset_feed_spec]
    )

We then have to do some joining between ads/object\_urls/asset\_urls to match the Ad with the destination URL if the ad is clicked on.

The performance is so slow, that I hope we are doing it wrong. I was never able to get the batch call to work and I'm not sure how to improve things.

Sincerely a data analyst who crosses over into data engineering because our data engineers don't know python.",1,3,2025-04-02 02:17:27,0,False,False,False,False,2025-04-02 02:17:27,2,Wednesday,273.0,2244,44.71,27,482,10.1,0,1,NEGATIVE,-0.9983181953430176,"['python', 'integration', 'set', 'pull', 'data', 'google', 'ads', 'facebook', 'marketing', 'data', 'warehouse', 'pulling', 'data', 'hierarchy', 'tiers', 'daily', 'metrics', 'campaigns', 'name', 'start', 'time', 'stop', 'time', 'groupsad', 'sets', 'name', 'ads', 'name', 'url', 'metrics', 'clicks', 'impressions', 'spend', 'previous', 'day', 'google', 'ads', 'api', 'basically', 'send', 'sql', 'query', 'return', 'time', 'like', 'tenth', 'second', 'facebook', 'see', 'returns', 'times', 'minutes', 'especially', 'ads', 'piece', 'hoping', 'get', 'idea', 'others', 'might', 'successfully', 'set', 'process', 'get', 'data', 'facebook', 'timely', 'fashion', 'possibly', 'without', 'hitting', 'rate', 'limiting', 'threshold', 'exact', 'code', 'using', 'get', 'work', 'system', 'tomorrow', 'gist', 'facebookbusinessadobjectsadaccount', 'import', 'adaccount', 'facebookbusinessadobjectscampaign', 'import', 'campaign', 'facebookbusinessadobjectsad', 'import', 'adset', 'facebookbusinessadobjectsad', 'import', 'facebookbusinessadobjectsadcreative', 'import', 'adcreative', 'campaigns', 'adaccountactgetcampaigns', 'params', 'fieldscampaignfieldidcampaignfieldnamecampaignfieldstarttimecampaignfieldstoptime', 'adsets', 'adaccountactgetadsets', 'params', 'fieldsadsetfieldidadsetfieldname', 'ads', 'adaccountactgetads', 'params', 'fieldsadfieldidadfieldnameadfieldcreative', 'objecturls', 'adaccountactgetadcreatives', 'params', 'fieldsadcreativefieldobjectstoryspec', 'asseturls', 'adaccountactgetadcreatives', 'params', 'fieldsadcreativefieldassetfeedspec', 'joining', 'adsobjecturlsasseturls', 'match', 'destination', 'url', 'clicked', 'performance', 'slow', 'hope', 'wrong', 'never', 'able', 'get', 'batch', 'call', 'work', 'sure', 'improve', 'things', 'sincerely', 'data', 'analyst', 'crosses', 'data', 'engineering', 'data', 'engineers', 'dont', 'know', 'python']",python integration set pull data google ads facebook marketing data warehouse pulling data hierarchy tiers daily metrics campaigns name start time stop time groupsad sets name ads name url metrics clicks impressions spend previous day google ads api basically send sql query return time like tenth second facebook see returns times minutes especially ads piece hoping get idea others might successfully set process get data facebook timely fashion possibly without hitting rate limiting threshold exact code using get work system tomorrow gist facebookbusinessadobjectsadaccount import adaccount facebookbusinessadobjectscampaign import campaign facebookbusinessadobjectsad import adset facebookbusinessadobjectsad import facebookbusinessadobjectsadcreative import adcreative campaigns adaccountactgetcampaigns params fieldscampaignfieldidcampaignfieldnamecampaignfieldstarttimecampaignfieldstoptime adsets adaccountactgetadsets params fieldsadsetfieldidadsetfieldname ads adaccountactgetads params fieldsadfieldidadfieldnameadfieldcreative objecturls adaccountactgetadcreatives params fieldsadcreativefieldobjectstoryspec asseturls adaccountactgetadcreatives params fieldsadcreativefieldassetfeedspec joining adsobjecturlsasseturls match destination url clicked performance slow hope wrong never able get batch call work sure improve things sincerely data analyst crosses data engineering data engineers dont know python,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
80,Resources for learning AbInitio Tool,"I tried to search the entire internet to find AbInito related tutorials/tranings. Hard luck finding anything. I came to know it's a closed source tool and everything is behind a login wall only for partner companies. 

Can anyone share me stuff they found useful?

Thanks in advance.",1,1,2025-04-02 02:17:07,0,False,False,False,False,2025-04-02 02:17:07,2,Wednesday,47.0,283,61.93,5,74,10.4,0,0,NEGATIVE,-0.9983043670654297,"['tried', 'search', 'entire', 'internet', 'find', 'abinito', 'related', 'tutorialstranings', 'hard', 'luck', 'finding', 'anything', 'came', 'know', 'closed', 'source', 'tool', 'everything', 'behind', 'login', 'wall', 'partner', 'companies', 'anyone', 'share', 'stuff', 'found', 'useful', 'thanks', 'advance']",tried search entire internet find abinito related tutorialstranings hard luck finding anything came know closed source tool everything behind login wall partner companies anyone share stuff found useful thanks advance,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
81,Not in the field and I need help understanding how data migrations work and how they're done,"I'm an engineer in an unrelated field and want to understand how data migrations work for work (I might be put in charge of it at my job even though we're not data engineers).  Any good sources, preferably a video that would a mock walkthrough of one (maybe using an ETL too)?",1,2,2025-04-01 17:07:19,0,False,False,False,False,2025-04-01 17:07:19,17,Tuesday,52.0,276,53.55,2,76,0.0,0,0,NEGATIVE,-0.9970316886901855,"['engineer', 'unrelated', 'field', 'want', 'understand', 'data', 'migrations', 'work', 'work', 'might', 'put', 'charge', 'job', 'even', 'though', 'data', 'engineers', 'good', 'sources', 'preferably', 'video', 'would', 'mock', 'walkthrough', 'one', 'maybe', 'using', 'etl']",engineer unrelated field want understand data migrations work work might put charge job even though data engineers good sources preferably video would mock walkthrough one maybe using etl,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
82,ELI5 - High-Level Diagram of a Data Strategy,"Hello everyone! 

I am not a data engineer, but I am trying to help other people within my organization (as well as myself) get a better understanding of what an overall data strategy looks like.  So, I figured I would ask the experts.    

**Do you have a go-to high-level diagram you use that simplifies the complexities of an overall data solution and helps you communicate what that should look like to non-technical people like myself?** 

I’m a very visual learner so seeing something that shows what the journey of data should look like from beginning to end would be extremely helpful.  I’ve searched online but almost everything I see is created by a vendor trying to show why their product is better.  I’d much rather see an unbiased explanation of what the overall process should be and then layer in vendor choices later.

I apologize if the question is phrased incorrectly or too vague.  If clarifying questions/answers are needed, please let me know and I’ll do my best to answer them.  Thanks in advance for your help.",1,0,2025-04-01 16:58:49,0,False,False,False,False,2025-04-01 16:58:49,16,Tuesday,177.0,1033,51.48,9,278,12.8,0,1,POSITIVE,0.9941459894180298,"['hello', 'everyone', 'data', 'engineer', 'trying', 'help', 'people', 'within', 'organization', 'well', 'get', 'better', 'understanding', 'overall', 'data', 'strategy', 'looks', 'like', 'figured', 'would', 'ask', 'experts', 'goto', 'highlevel', 'diagram', 'use', 'simplifies', 'complexities', 'overall', 'data', 'solution', 'helps', 'communicate', 'look', 'like', 'nontechnical', 'people', 'like', 'visual', 'learner', 'seeing', 'something', 'shows', 'journey', 'data', 'look', 'like', 'beginning', 'end', 'would', 'extremely', 'helpful', 'ive', 'searched', 'online', 'almost', 'everything', 'see', 'created', 'vendor', 'trying', 'show', 'product', 'better', 'much', 'rather', 'see', 'unbiased', 'explanation', 'overall', 'process', 'layer', 'vendor', 'choices', 'later', 'apologize', 'question', 'phrased', 'incorrectly', 'vague', 'clarifying', 'questionsanswers', 'needed', 'please', 'let', 'know', 'ill', 'best', 'answer', 'thanks', 'advance', 'help']",hello everyone data engineer trying help people within organization well get better understanding overall data strategy looks like figured would ask experts goto highlevel diagram use simplifies complexities overall data solution helps communicate look like nontechnical people like visual learner seeing something shows journey data look like beginning end would extremely helpful ive searched online almost everything see created vendor trying show product better much rather see unbiased explanation overall process layer vendor choices later apologize question phrased incorrectly vague clarifying questionsanswers needed please let know ill best answer thanks advance help,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
83,Dimensional modelling -> Datetime column,"Hi All,

Im learning Dimensional modelling. Im working on the NYC taxi dataset ( [here is the data dictionary](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf) ).

Im struggling to model Datetime columns: tpep\_pickup\_datetime, tpep\_dropoff\_datetime.  
Does these columns should be in Dimensions table or in Fact table? 

What I understand from the Kimball datawarehouse toolkit book is to have a DateDim table populated with dates from start\_date to end\_date with details like month, year, quarter, day of week etc. but what about timestamp?

Lets say if I want to see the data for certain time of the day like nights? In this case, do I need to split the columns: tpep\_pickup\_datetime, tpep\_dropoff\_datetime into date, hour, mins in fact table and join to a dim table with the timestamp details like hour, mins etc? ( so two dim tables - date and timestamp )

It would be great someone can help me here?",1,2,2025-04-01 15:21:53,0,False,False,False,False,2025-04-01 15:21:53,15,Tuesday,145.0,951,55.64,9,230,10.5,1,0,NEGATIVE,-0.9994295239448547,"['learning', 'dimensional', 'modelling', 'working', 'nyc', 'taxi', 'dataset', 'data', 'dictionaryhttpswwwnycgovassetstlcdownloadspdfdatadictionarytriprecordsyellowpdf', 'struggling', 'model', 'datetime', 'columns', 'tpeppickupdatetime', 'tpepdropoffdatetime', 'columns', 'dimensions', 'table', 'fact', 'table', 'understand', 'kimball', 'datawarehouse', 'toolkit', 'book', 'datedim', 'table', 'populated', 'dates', 'startdate', 'enddate', 'details', 'like', 'month', 'year', 'quarter', 'day', 'week', 'etc', 'timestamp', 'lets', 'say', 'want', 'see', 'data', 'certain', 'time', 'day', 'like', 'nights', 'case', 'need', 'split', 'columns', 'tpeppickupdatetime', 'tpepdropoffdatetime', 'date', 'hour', 'mins', 'fact', 'table', 'join', 'dim', 'table', 'timestamp', 'details', 'like', 'hour', 'mins', 'etc', 'two', 'dim', 'tables', 'date', 'timestamp', 'would', 'great', 'someone', 'help']",learning dimensional modelling working nyc taxi dataset data dictionaryhttpswwwnycgovassetstlcdownloadspdfdatadictionarytriprecordsyellowpdf struggling model datetime columns tpeppickupdatetime tpepdropoffdatetime columns dimensions table fact table understand kimball datawarehouse toolkit book datedim table populated dates startdate enddate details like month year quarter day week etc timestamp lets say want see data certain time day like nights case need split columns tpeppickupdatetime tpepdropoffdatetime date hour mins fact table join dim table timestamp details like hour mins etc two dim tables date timestamp would great someone help,Mid,0,0,"need, table, event, source, without, instead, every, replay, day, example, single, change, per, business, daily, based, keys, manually, model, keep, needs, events, doesnt, name, list, option, user, must, takes, history, often, truth, full, column, hour, address, requirements, date, reading, track, whenever, metrics, result, update, feature, primary, columns, meaning, pricing, structure, details, rows, simply, available, dedicated, country, means, facebook, error, missing, evolve, involves, names, size, domain, place, addresses, timestamp, parse, mean, slow, schemas, heres, return, dimensions, delete, state, times, arent, ground, errors, receive, min, added, geocoding, shape, sourcing, wasnt, insert, minutes, value, flow, save, normal, little, region, usually, bad, becomes, failure, replication, entirely, products, infrastructure, minute, person, average, servers, internet, looked, probably, campaigns, workloads, looks",8.2,7.4,175.5,-0.663,1069.0,47.9,9.8,275.5,11.4,22.2,15.5,52,108,34,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",4,4
84,SQL Templating (without DBT?),"I’d like to implement jinja templated SQL for a project. But I don’t want or need DBT’s extra bells and whistles. I just need/want to write macros, templated .sql files, then on execution (from python application), render the SQL at runtime.

What’s the solution here? Pure jinja? (What’re some resources for that?) Are there OSS libraries I can use? Or, do I just use DBT, but only use it from a python driver?",0,3,2025-04-01 16:26:46,0,False,False,False,False,2025-04-01 16:26:46,16,Tuesday,73.0,411,70.7,8,110,9.8,0,0,NEGATIVE,-0.998688280582428,"['like', 'implement', 'jinja', 'templated', 'sql', 'project', 'dont', 'want', 'need', 'dbts', 'extra', 'bells', 'whistles', 'needwant', 'write', 'macros', 'templated', 'sql', 'files', 'execution', 'python', 'application', 'render', 'sql', 'runtime', 'whats', 'solution', 'pure', 'jinja', 'whatre', 'resources', 'oss', 'libraries', 'use', 'use', 'dbt', 'use', 'python', 'driver']",like implement jinja templated sql project dont want need dbts extra bells whistles needwant write macros templated sql files execution python application render sql runtime whats solution pure jinja whatre resources oss libraries use use dbt use python driver,Low,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
85,Lessons from operating big ClickHouse clusters for several years,"My coworker Javi Santana wrote a lengthy post about what it takes to operate large ClickHouse clusters based on his experience starting Tinybird. If you're managing any kind of OSS CH cluster, you might find this interesting.

[https://www.tinybird.co/blog-posts/what-i-learned-operating-clickhouse](https://www.tinybird.co/blog-posts/what-i-learned-operating-clickhouse)",0,1,2025-04-01 14:02:37,0,False,False,False,False,2025-04-01 14:02:37,14,Tuesday,38.0,371,9.89,2,80,0.0,1,0,POSITIVE,0.9667989611625671,"['coworker', 'javi', 'santana', 'wrote', 'lengthy', 'post', 'takes', 'operate', 'large', 'clickhouse', 'clusters', 'based', 'experience', 'starting', 'tinybird', 'youre', 'managing', 'kind', 'oss', 'cluster', 'might', 'find', 'interesting', 'httpswwwtinybirdcoblogpostswhatilearnedoperatingclickhousehttpswwwtinybirdcoblogpostswhatilearnedoperatingclickhouse']",coworker javi santana wrote lengthy post takes operate large clickhouse clusters based experience starting tinybird youre managing kind oss cluster might find interesting httpswwwtinybirdcoblogpostswhatilearnedoperatingclickhousehttpswwwtinybirdcoblogpostswhatilearnedoperatingclickhouse,Low,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
86,Newbie to DE needs help with the approach to the architecture of a project,"So I was hired as a data analyst a few months ago and I have a background in software development. A few months ago I was moved to a smallish project with the objective of streamlining some administrative tasks that were all calculated ""manually"" with Excel.  By the time, all I had worked with were very basic, low code tools from the Microsoft enviroment: PBI for dashboards, Power Automate, Power Apps for data entry, Sharepoint lists, etc, so that's what I used to set it up. 

The cost for the client is basically nonexistent right now, apart from a couple of PBI licenses. The closest I've done to ETL work has been with power query, if you can even call it that. 

  
Now I'm at a point where it feels like that's not gonna cut it anymore. I'm going to be working with larger volumes of data, with more complex relationships between tables and transformations that need to be done earlier in the process. I could technically keep going with what I have but I want to actually build something durable and move towards actual data engineering, but I don't know where to start with a solution that's cost efficient and well structured. For example, I wanted to move the data from Sharepoint lists to a proper database but then we'd have to pay for multiple premium licenses to be able to connect to them in powerapps. Where do I even start?

I know the very basics of data engineering and I've done a couple of tutorial projects with Snowflake and Databricks as my team seems to want to focus on cloud based solutions. So I'm not starting from absolute scratch, but I feel pretty lost as I'm sure you can tell. I'd appreciate any kind of advice or input as to where to head from here, as I'm on my own right now.",0,4,2025-04-01 11:00:15,0,False,False,False,False,2025-04-01 11:00:15,11,Tuesday,314.0,1716,55.37,13,460,12.6,0,0,NEGATIVE,-0.996629536151886,"['hired', 'data', 'analyst', 'months', 'ago', 'background', 'software', 'development', 'months', 'ago', 'moved', 'smallish', 'project', 'objective', 'streamlining', 'administrative', 'tasks', 'calculated', 'manually', 'excel', 'time', 'worked', 'basic', 'low', 'code', 'tools', 'microsoft', 'enviroment', 'pbi', 'dashboards', 'power', 'automate', 'power', 'apps', 'data', 'entry', 'sharepoint', 'lists', 'etc', 'thats', 'used', 'set', 'cost', 'client', 'basically', 'nonexistent', 'right', 'apart', 'couple', 'pbi', 'licenses', 'closest', 'ive', 'done', 'etl', 'work', 'power', 'query', 'even', 'call', 'point', 'feels', 'like', 'thats', 'gonna', 'cut', 'anymore', 'going', 'working', 'larger', 'volumes', 'data', 'complex', 'relationships', 'tables', 'transformations', 'need', 'done', 'earlier', 'process', 'could', 'technically', 'keep', 'going', 'want', 'actually', 'build', 'something', 'durable', 'move', 'towards', 'actual', 'data', 'engineering', 'dont', 'know', 'start', 'solution', 'thats', 'cost', 'efficient', 'well', 'structured', 'example', 'wanted', 'move', 'data', 'sharepoint', 'lists', 'proper', 'database', 'wed', 'pay', 'multiple', 'premium', 'licenses', 'able', 'connect', 'powerapps', 'even', 'start', 'know', 'basics', 'data', 'engineering', 'ive', 'done', 'couple', 'tutorial', 'projects', 'snowflake', 'databricks', 'team', 'seems', 'want', 'focus', 'cloud', 'based', 'solutions', 'starting', 'absolute', 'scratch', 'feel', 'pretty', 'lost', 'sure', 'tell', 'appreciate', 'kind', 'advice', 'input', 'head', 'right']",hired data analyst months ago background software development months ago moved smallish project objective streamlining administrative tasks calculated manually excel time worked basic low code tools microsoft enviroment pbi dashboards power automate power apps data entry sharepoint lists etc thats used set cost client basically nonexistent right apart couple pbi licenses closest ive done etl work power query even call point feels like thats gonna cut anymore going working larger volumes data complex relationships tables transformations need done earlier process could technically keep going want actually build something durable move towards actual data engineering dont know start solution thats cost efficient well structured example wanted move data sharepoint lists proper database wed pay multiple premium licenses able connect powerapps even start know basics data engineering ive done couple tutorial projects snowflake databricks team seems want focus cloud based solutions starting absolute scratch feel pretty lost sure tell appreciate kind advice input head right,Low,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
87,How do you build tests for processing data with variations,"How do you test a data pipeline which parses data having a lot of variation

I'm working on a project to parse pdfs (earnings calls), they have a common general structure, but sometimes I'll get variations in the data (very common, half of docs have some kind of variation). It's a pain to debug when things go wrong, I have to run tests on a lot of files which takes up time.

I want to build good tests, and learn to do this better in the future, then refactor the code (it's garbage right now)",0,7,2025-04-01 09:44:04,0,False,False,False,False,2025-04-01 09:44:04,9,Tuesday,95.0,496,64.68,3,125,9.7,0,0,NEGATIVE,-0.9992403984069824,"['test', 'data', 'pipeline', 'parses', 'data', 'lot', 'variation', 'working', 'project', 'parse', 'pdfs', 'earnings', 'calls', 'common', 'general', 'structure', 'sometimes', 'ill', 'get', 'variations', 'data', 'common', 'half', 'docs', 'kind', 'variation', 'pain', 'debug', 'things', 'wrong', 'run', 'tests', 'lot', 'files', 'takes', 'time', 'want', 'build', 'good', 'tests', 'learn', 'better', 'future', 'refactor', 'code', 'garbage', 'right']",test data pipeline parses data lot variation working project parse pdfs earnings calls common general structure sometimes ill get variations data common half docs kind variation pain debug things wrong run tests lot files takes time want build good tests learn better future refactor code garbage right,Low,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
88,Built a visual tool on top of Pandas that runs Python transformations row-by-row - What do you guys think?,"Hey data engineers,

For client implementations I thought it was a pain to write python scripts over and over, so I built a tool on top of Pandas to solve my own frustration and as a personal hobby. The goal was to make it so I didn't have to start from the ground up and rewrite and keep track of each script for each data source I had.

**What I Built:**  
A visual transformation tool with some features I thought might interest this community:

1. **Python execution on a row-by-row basis** \- Write Python once per field, save the mapping, and process. It applies each field's mapping logic to each row and returns the result without loops
2. **Visual logic builder** that generates Python from the drag and drop interface. It can re-parse the python so you can go back and edit form the UI again
3. **AI Co-Pilot** that can write Python logic based on your requirements
4. **No environment setup** \- just upload your data and start transforming
5. **Handles nested JSON** with a simple dot notation for complex structures

Here's a screenshot of the logic builder in action:

https://preview.redd.it/znh4fom8y9se1.png?width=2690&format=png&auto=webp&s=2daf229aab2f5de272c4f5668a782d8011ff3207

I'd love some feedback from people who deal with data transformations regularly. If anyone wants to give it a try feel free to shoot me a message or comment, and I can give you lifetime access if the app is of use. Not trying to sell here, just looking for some feedback and thoughts since I just built it.

**Technical Details:**

* Supports CSV, Excel, and JSON inputs/outputs, concatenating files, header & delimiter selection
* Transformations are saved as editable mapping files
* Handles large datasets by processing chunks in parallel
* Built on Pandas. Supports Pandas and re libraries

[DataFlowMapper.com](http://DataFlowMapper.com)",0,7,2025-04-01 19:28:00,0,False,2025-04-01 19:56:02,False,False,2025-04-01 19:28:00,19,Tuesday,293.0,1843,60.55,15,443,11.6,1,0,NEGATIVE,-0.9895069003105164,"['hey', 'data', 'engineers', 'client', 'implementations', 'thought', 'pain', 'write', 'python', 'scripts', 'built', 'tool', 'top', 'pandas', 'solve', 'frustration', 'personal', 'hobby', 'goal', 'make', 'didnt', 'start', 'ground', 'rewrite', 'keep', 'track', 'script', 'data', 'source', 'built', 'visual', 'transformation', 'tool', 'features', 'thought', 'might', 'interest', 'community', 'python', 'execution', 'rowbyrow', 'basis', 'write', 'python', 'per', 'field', 'save', 'mapping', 'process', 'applies', 'fields', 'mapping', 'logic', 'row', 'returns', 'result', 'without', 'loops', 'visual', 'logic', 'builder', 'generates', 'python', 'drag', 'drop', 'interface', 'reparse', 'python', 'back', 'edit', 'form', 'copilot', 'write', 'python', 'logic', 'based', 'requirements', 'environment', 'setup', 'upload', 'data', 'start', 'transforming', 'handles', 'nested', 'json', 'simple', 'dot', 'notation', 'complex', 'structures', 'heres', 'screenshot', 'logic', 'builder', 'action', 'httpspreviewredditznhfomysepngwidthformatpngautowebpsdafaabfdecfadff', 'love', 'feedback', 'people', 'deal', 'data', 'transformations', 'regularly', 'anyone', 'wants', 'give', 'try', 'feel', 'free', 'shoot', 'message', 'comment', 'give', 'lifetime', 'access', 'app', 'use', 'trying', 'sell', 'looking', 'feedback', 'thoughts', 'since', 'built', 'technical', 'details', 'supports', 'csv', 'excel', 'json', 'inputsoutputs', 'concatenating', 'files', 'header', 'delimiter', 'selection', 'transformations', 'saved', 'editable', 'mapping', 'files', 'handles', 'large', 'datasets', 'processing', 'chunks', 'parallel', 'built', 'pandas', 'supports', 'pandas', 'libraries', 'dataflowmappercomhttpdataflowmappercom']",hey data engineers client implementations thought pain write python scripts built tool top pandas solve frustration personal hobby goal make didnt start ground rewrite keep track script data source built visual transformation tool features thought might interest community python execution rowbyrow basis write python per field save mapping process applies fields mapping logic row returns result without loops visual logic builder generates python drag drop interface reparse python back edit form copilot write python logic based requirements environment setup upload data start transforming handles nested json simple dot notation complex structures heres screenshot logic builder action httpspreviewredditznhfomysepngwidthformatpngautowebpsdafaabfdecfadff love feedback people deal data transformations regularly anyone wants give try feel free shoot message comment give lifetime access app use trying sell looking feedback thoughts since built technical details supports csv excel json inputsoutputs concatenating files header delimiter selection transformations saved editable mapping files handles large datasets processing chunks parallel built pandas supports pandas libraries dataflowmappercomhttpdataflowmappercom,Low,4,4,"files, api, tables, query, models, read, google, json, file, logic, write, case, csv, store, datasets, across, sources, metadata, created, schema, systems, large, platform, custom, script, changes, dataset, row, reporting, parquet, issues, ads, load, fields, analysis, cases, product, values, design, view, idea, layer, search, import, suggest, language, excel, app, blob, form, request, params, postgres, ingestion, raw, mysql, fact, processed, structures, either, separate, structured, connection, core, method, downstream, scripts, extract, maintain, send, hub, records, transactional, overhead, lineage, environment, object, external, pull, iot, order, typically, grid, hubs, validation, requires, mapping, batch, mechanism, location, updates, keeping, checkout, operations, runs, convert, optimized, avoid, matching, views, formats, matters, vpc, bring, reduce, migrate, consists, starts, reads, refresh, relationships, bronze, historical, tracking, developers, natural",8.0,7.1,169.4,-0.679,1041.7,44.5,9.5,267.7,10.7,24.1,17.9,49,102,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,7
89,Data Developer vs Data Engineer,"I know it varies by company blah blah blah, but also aside from a Google search, what have you guys in the field noticed to be core differences between these positions? ",0,3,2025-04-01 23:53:39,0,False,False,False,False,2025-04-01 23:53:39,23,Tuesday,31.0,169,56.93,1,44,0.0,0,0,NEGATIVE,-0.975967526435852,"['know', 'varies', 'company', 'blah', 'blah', 'blah', 'also', 'aside', 'google', 'search', 'guys', 'field', 'noticed', 'core', 'differences', 'positions']",know varies company blah blah blah also aside google search guys field noticed core differences positions,Low,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
90,"Introducing the Knowledge Graph: things, not strings","""Fully Managed Graph Database Service | Neo4j AuraDB"" https://neo4j.com/product/auradb/?utm_source=GSearch&utm_medium=PaidSearch&utm_campaign=Evergreen&utm_content=AMS-Search-SEMCE-DSA-None-SEM-SEM-NonABM&utm_term=&utm_adgroup=DSA&gad_source=1&gclid=Cj0KCQjwna6_BhCbARIsALId2Z27LAb-nD-42tRRF5viybJfBVull8EeBvj46w_V7OCs1RdtbR7hqBQaAuObEALw_wcB",0,0,2025-04-01 17:03:16,0,False,False,False,False,2025-04-01 17:03:16,17,Tuesday,9.0,342,-503.47,1,66,0.0,1,0,NEGATIVE,-0.9922335147857666,"['fully', 'managed', 'graph', 'database', 'service', 'neoj', 'auradb', 'httpsneojcomproductauradbutmsourcegsearchutmmediumpaidsearchutmcampaignevergreenutmcontentamssearchsemcedsanonesemsemnonabmutmtermutmadgroupdsagadsourcegclidcjkcqjwnabhcbarisalidzlabndtrrfviybjfbvulleebvjwvocsrdtbrhqbqaauobealwwcb']",fully managed graph database service neoj auradb httpsneojcomproductauradbutmsourcegsearchutmmediumpaidsearchutmcampaignevergreenutmcontentamssearchsemcedsanonesemsemnonabmutmtermutmadgroupdsagadsourcegclidcjkcqjwnabhcbarisalidzlabndtrrfviybjfbvulleebvjwvocsrdtbrhqbqaauobealwwcb,Low,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
91,We cut Databricks costs without sacrificing performance—here’s how,"About 6 months ago, I led a Databricks cost optimization project where we cut down costs, improved workload speed, and made life easier for engineers. I finally had time to write it all up a few days ago—cluster family selection, autoscaling, serverless, EBS tweaks, and more. I also included a real example with numbers. If you’re using Databricks, this might help: [https://medium.com/datadarvish/databricks-cost-optimization-practical-tips-for-performance-and-savings-7665be665f52](https://medium.com/datadarvish/databricks-cost-optimization-practical-tips-for-performance-and-savings-7665be665f52)",0,3,2025-04-01 16:45:48,0,False,False,False,False,2025-04-01 16:45:48,16,Tuesday,62.0,601,21.9,4,127,12.6,1,0,NEGATIVE,-0.9968274235725403,"['months', 'ago', 'led', 'databricks', 'cost', 'optimization', 'project', 'cut', 'costs', 'improved', 'workload', 'speed', 'made', 'life', 'easier', 'engineers', 'finally', 'time', 'write', 'days', 'agocluster', 'family', 'selection', 'autoscaling', 'serverless', 'ebs', 'tweaks', 'also', 'included', 'real', 'example', 'numbers', 'youre', 'using', 'databricks', 'might', 'help', 'httpsmediumcomdatadarvishdatabrickscostoptimizationpracticaltipsforperformanceandsavingsbefhttpsmediumcomdatadarvishdatabrickscostoptimizationpracticaltipsforperformanceandsavingsbef']",months ago led databricks cost optimization project cut costs improved workload speed made life easier engineers finally time write days agocluster family selection autoscaling serverless ebs tweaks also included real example numbers youre using databricks might help httpsmediumcomdatadarvishdatabrickscostoptimizationpracticaltipsforperformanceandsavingsbefhttpsmediumcomdatadarvishdatabrickscostoptimizationpracticaltipsforperformanceandsavingsbef,Low,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
92,Is Databricks Becoming a Requirement for Data Engineers?,"Hey everyone,

I’m a Data Engineer with 5 years of experience, mostly working with traditional data pipelines, cloud data warehouses(AWS and Azure) and tools like Airflow, Kafka, and Spark. However, I’ve never used Databricks in a professional setting.

Lately, I see Databricks appearing more and more in job postings, and it seems like it's becoming a key player in the data world. For those of you working with Databricks, do you think it's a necessity for Data Engineers now? I see that it is mandatory requirement in job offerings but I don't have opportunity to get first experience in it.

What is your opinion, what should I do?",84,41,2025-04-02 09:41:41,0,False,False,False,False,2025-04-02 09:41:41,9,Wednesday,108.0,636,53.21,6,168,12.7,0,0,POSITIVE,0.8220954537391663,"['hey', 'everyone', 'data', 'engineer', 'years', 'experience', 'mostly', 'working', 'traditional', 'data', 'pipelines', 'cloud', 'data', 'warehousesaws', 'azure', 'tools', 'like', 'airflow', 'kafka', 'spark', 'however', 'ive', 'never', 'used', 'databricks', 'professional', 'setting', 'lately', 'see', 'databricks', 'appearing', 'job', 'postings', 'seems', 'like', 'becoming', 'key', 'player', 'data', 'world', 'working', 'databricks', 'think', 'necessity', 'data', 'engineers', 'see', 'mandatory', 'requirement', 'job', 'offerings', 'dont', 'opportunity', 'get', 'first', 'experience', 'opinion']",hey everyone data engineer years experience mostly working traditional data pipelines cloud data warehousesaws azure tools like airflow kafka spark however ive never used databricks professional setting lately see databricks appearing job postings seems like becoming key player data world working databricks think necessity data engineers see mandatory requirement job offerings dont opportunity get first experience opinion,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
93,Does anyone feel the DE tools are chaging too fast to track,"TL;DR: a guy feeling stuck in the job and cannot figure out what skills are needed to move to a better position 

I am data engineer at a big 4 firm (may be just a etl developer) in india.

I work with Informatica Power Center, Oracle, Unix on the daily basis. Now, when I tried to switch companies for career boost, I realised nobody uses these tech anymore. 

Everyone uses pyspark for etl.
I though fair enough and started leaning pyspark dataframe api. I am so good with sql, pl/sql and python, so it was easy for me.

Then I came to know learning pyspark is not enough, you need to know databricks, snowflake, dbt kind of tools.

Even before making my mind to decide what to learn, things changed and now airflow/dagster, redshift, delta lake, duckdb. I don't what else is in trend now.

Honestly, It feels a lot, like the world is moving in the fastest pace possible and I cannot even decide what to do.

Every job has different tools, and to do the ""fake it till you make it"", I am afraid they would ask any niche question about the tool to which you can only answer if you have the experience.

My profile  is not even getting picked and I feel stuck in the job I am doing.

I am great at what I do, that is one reason the project is not letting me leave even after all the senior folks has left for better projects. The guy with 3 years of experience is the senior most developer and lead now.

But honestly, I dont think I can make it anymore.

If I was just stuck with something like SAP ABAP, frontend or core python, things might have been good. Recruiters will at least look at your profile even though you are not a perfect match as you can learn the rest to do the job. (I might be wrong in this thought)

But for DE roles, the job descriptions are becoming too specific to a tool and people are expecting complete data architect level of skills at 3 years.

I was so ambitious to get a job in a different country with big 4 experience, but now I can't even get a job in india.",42,32,2025-04-02 09:26:11,0,False,2025-04-02 09:30:54,False,False,2025-04-02 09:26:11,9,Wednesday,382.0,1993,67.99,19,524,10.7,0,0,NEGATIVE,-0.6717806458473206,"['tldr', 'guy', 'feeling', 'stuck', 'job', 'cannot', 'figure', 'skills', 'needed', 'move', 'better', 'position', 'data', 'engineer', 'big', 'firm', 'may', 'etl', 'developer', 'india', 'work', 'informatica', 'power', 'center', 'oracle', 'unix', 'daily', 'basis', 'tried', 'switch', 'companies', 'career', 'boost', 'realised', 'nobody', 'uses', 'tech', 'anymore', 'everyone', 'uses', 'pyspark', 'etl', 'though', 'fair', 'enough', 'started', 'leaning', 'pyspark', 'dataframe', 'api', 'good', 'sql', 'plsql', 'python', 'easy', 'came', 'know', 'learning', 'pyspark', 'enough', 'need', 'know', 'databricks', 'snowflake', 'dbt', 'kind', 'tools', 'even', 'making', 'mind', 'decide', 'learn', 'things', 'changed', 'airflowdagster', 'redshift', 'delta', 'lake', 'duckdb', 'dont', 'else', 'trend', 'honestly', 'feels', 'lot', 'like', 'world', 'moving', 'fastest', 'pace', 'possible', 'cannot', 'even', 'decide', 'every', 'job', 'different', 'tools', 'fake', 'till', 'make', 'afraid', 'would', 'ask', 'niche', 'question', 'tool', 'answer', 'experience', 'profile', 'even', 'getting', 'picked', 'feel', 'stuck', 'job', 'great', 'one', 'reason', 'project', 'letting', 'leave', 'even', 'senior', 'folks', 'left', 'better', 'projects', 'guy', 'years', 'experience', 'senior', 'developer', 'lead', 'honestly', 'dont', 'think', 'make', 'anymore', 'stuck', 'something', 'like', 'sap', 'abap', 'frontend', 'core', 'python', 'things', 'might', 'good', 'recruiters', 'least', 'look', 'profile', 'even', 'though', 'perfect', 'match', 'learn', 'rest', 'job', 'might', 'wrong', 'thought', 'roles', 'job', 'descriptions', 'becoming', 'specific', 'tool', 'people', 'expecting', 'complete', 'data', 'architect', 'level', 'skills', 'years', 'ambitious', 'get', 'job', 'different', 'country', 'big', 'experience', 'cant', 'even', 'get', 'job', 'india']",tldr guy feeling stuck job cannot figure skills needed move better position data engineer big firm may etl developer india work informatica power center oracle unix daily basis tried switch companies career boost realised nobody uses tech anymore everyone uses pyspark etl though fair enough started leaning pyspark dataframe api good sql plsql python easy came know learning pyspark enough need know databricks snowflake dbt kind tools even making mind decide learn things changed airflowdagster redshift delta lake duckdb dont else trend honestly feels lot like world moving fastest pace possible cannot even decide every job different tools fake till make afraid would ask niche question tool answer experience profile even getting picked feel stuck job great one reason project letting leave even senior folks left better projects guy years experience senior developer lead honestly dont think make anymore stuck something like sap abap frontend core python things might good recruiters least look profile even though perfect match learn rest job might wrong thought roles job descriptions becoming specific tool people expecting complete data architect level skills years ambitious get job different country big experience cant even get job india,High,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
94,Am i doomed moving forward,"I am scared my job is a lightning strike that doesnt exist elsewhere. Im classified as a “data engineer” but only work in snowflake building datasets for tableau. Basically im a middle man between IT who ingests the data and then analysts who visualize in tableau. I live in fear (lol) that if i were to lose this job i would qualify for nothing else because i havent touched python or any ingesting tools or tableau and any visualizing tools in years. 
Am as as out of the norm as i feel?",18,10,2025-04-02 13:33:31,0,False,False,False,False,2025-04-02 13:33:31,13,Wednesday,92.0,489,69.72,5,131,10.8,0,0,NEGATIVE,-0.9985540509223938,"['scared', 'job', 'lightning', 'strike', 'doesnt', 'exist', 'elsewhere', 'classified', 'data', 'engineer', 'work', 'snowflake', 'building', 'datasets', 'tableau', 'basically', 'middle', 'man', 'ingests', 'data', 'analysts', 'visualize', 'tableau', 'live', 'fear', 'lol', 'lose', 'job', 'would', 'qualify', 'nothing', 'else', 'havent', 'touched', 'python', 'ingesting', 'tools', 'tableau', 'visualizing', 'tools', 'years', 'norm', 'feel']",scared job lightning strike doesnt exist elsewhere classified data engineer work snowflake building datasets tableau basically middle man ingests data analysts visualize tableau live fear lol lose job would qualify nothing else havent touched python ingesting tools tableau visualizing tools years norm feel,High,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
95,"Lucked into a junior data engineer role, where do I go from here?","
About a month ago I was hired at a very small startup (3 employees including me) to be their ""data engineer and analyst"", replacing the previous data engineer who moved on to a grad scheme.

I recently graduated in a non-CS discipline, so my Python and SQL skills aren't exactly amazing but I'm a fast learner. It helps that the other employees are non-technical and the previous data engineer was extremely helpful while training me.

The job has been going well so far. I can see myself getting my skills up to a good standard, and it's a great role to learn the ropes BUT I can't see myself in this role for longer than a year or two. So what should I prepare for next? A more demanding data engineer job? Further education?

I'd like to have a technical job in the financial sector within the next 5-6 years e.g. data engineer for a quant firm.
",13,5,2025-04-02 09:23:29,0,False,False,False,False,2025-04-02 09:23:29,9,Wednesday,157.0,850,62.27,9,234,12.5,0,0,POSITIVE,0.9960736036300659,"['month', 'ago', 'hired', 'small', 'startup', 'employees', 'including', 'data', 'engineer', 'analyst', 'replacing', 'previous', 'data', 'engineer', 'moved', 'grad', 'scheme', 'recently', 'graduated', 'noncs', 'discipline', 'python', 'sql', 'skills', 'arent', 'exactly', 'amazing', 'fast', 'learner', 'helps', 'employees', 'nontechnical', 'previous', 'data', 'engineer', 'extremely', 'helpful', 'training', 'job', 'going', 'well', 'far', 'see', 'getting', 'skills', 'good', 'standard', 'great', 'role', 'learn', 'ropes', 'cant', 'see', 'role', 'longer', 'year', 'two', 'prepare', 'next', 'demanding', 'data', 'engineer', 'job', 'education', 'like', 'technical', 'job', 'financial', 'sector', 'within', 'next', 'years', 'data', 'engineer', 'quant', 'firm']",month ago hired small startup employees including data engineer analyst replacing previous data engineer moved grad scheme recently graduated noncs discipline python sql skills arent exactly amazing fast learner helps employees nontechnical previous data engineer extremely helpful training job going well far see getting skills good standard great role learn ropes cant see role longer year two prepare next demanding data engineer job education like technical job financial sector within next years data engineer quant firm,High,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
96,Skills to Stay Relevant in Data Engineering Over the Next 5-10 Years,"Hey r/dataengineering,

I've been in data engineering for about **3 years now**, and while I love what I do, I can't help but wonder: **what’s next?** With tech evolving so fast, I'm a bit concerned about what could make our current skills obsolete.

That said, Spark didn’t exactly kill the demand for Hadoop, Impala, etc.—so maybe the fear is overblown. But still, I want to make sure I'm **learning the right things** to stay ahead and not be caught off guard by layoffs or major shifts in the industry.

My current stack: **Python, SQL, Spark, AWS (Glue, Redshift, EMR), Airflow.**

What skills/tech would you bet on for the next **5-10 years**? Is it **real-time data processing? DataOps? AI/ML integration?** Would love to hear from those who’ve been in the game longer!",14,12,2025-04-02 20:07:35,0,False,False,False,False,2025-04-02 20:07:35,20,Wednesday,131.0,776,73.58,9,180,9.7,0,0,NEGATIVE,-0.942424476146698,"['hey', 'rdataengineering', 'ive', 'data', 'engineering', 'years', 'love', 'cant', 'help', 'wonder', 'whats', 'next', 'tech', 'evolving', 'fast', 'bit', 'concerned', 'could', 'make', 'current', 'skills', 'obsolete', 'said', 'spark', 'didnt', 'exactly', 'kill', 'demand', 'hadoop', 'impala', 'etcso', 'maybe', 'fear', 'overblown', 'still', 'want', 'make', 'sure', 'learning', 'right', 'things', 'stay', 'ahead', 'caught', 'guard', 'layoffs', 'major', 'shifts', 'industry', 'current', 'stack', 'python', 'sql', 'spark', 'aws', 'glue', 'redshift', 'emr', 'airflow', 'skillstech', 'would', 'bet', 'next', 'years', 'realtime', 'data', 'processing', 'dataops', 'aiml', 'integration', 'would', 'love', 'hear', 'whove', 'game', 'longer']",hey rdataengineering ive data engineering years love cant help wonder whats next tech evolving fast bit concerned could make current skills obsolete said spark didnt exactly kill demand hadoop impala etcso maybe fear overblown still want make sure learning right things stay ahead caught guard layoffs major shifts industry current stack python sql spark aws glue redshift emr airflow skillstech would bet next years realtime data processing dataops aiml integration would love hear whove game longer,High,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
97,DBT and Snowflake,"Hello all, I am trying to implement dbt and snowflake on a personal project, most of my experience comes from databricks so I would like to know if the best approach for this would be to:
1- a server dedicated to dbt that will connect to snowflake and execute transformations.
2- snowflake of course deployed in azure .
3- azure data factory for raw ingestion and to schedule the transformation pipeline and future dbt dataquality pipelines.

What you guys think about this? ",9,12,2025-04-02 15:54:12,0,False,False,False,False,2025-04-02 15:54:12,15,Wednesday,82.0,475,59.33,4,123,12.2,0,0,NEGATIVE,-0.9965283274650574,"['hello', 'trying', 'implement', 'dbt', 'snowflake', 'personal', 'project', 'experience', 'comes', 'databricks', 'would', 'like', 'know', 'best', 'approach', 'would', 'server', 'dedicated', 'dbt', 'connect', 'snowflake', 'execute', 'transformations', 'snowflake', 'course', 'deployed', 'azure', 'azure', 'data', 'factory', 'raw', 'ingestion', 'schedule', 'transformation', 'pipeline', 'future', 'dbt', 'dataquality', 'pipelines', 'guys', 'think']",hello trying implement dbt snowflake personal project experience comes databricks would like know best approach would server dedicated dbt connect snowflake execute transformations snowflake course deployed azure azure data factory raw ingestion schedule transformation pipeline future dbt dataquality pipelines guys think,High,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
98,Iceberg catalog in gcp,"Which is your preferred way to host your data catalog inside of gcp? I know that inside of aws, glue is the preferred way?  
I know that it can make sense to use dataproc Metastore and/or big data lake Metastore.

I know that there are also a lot open source tools that you can use?

what do you prefer? what's your experience?",9,3,2025-04-02 11:51:11,1,False,False,False,False,2025-04-02 11:51:11,11,Wednesday,62.0,327,86.4,6,82,7.8,0,0,NEGATIVE,-0.9954168796539307,"['preferred', 'way', 'host', 'data', 'catalog', 'inside', 'gcp', 'know', 'inside', 'aws', 'glue', 'preferred', 'way', 'know', 'make', 'sense', 'use', 'dataproc', 'metastore', 'andor', 'big', 'data', 'lake', 'metastore', 'know', 'also', 'lot', 'open', 'source', 'tools', 'use', 'prefer', 'whats', 'experience']",preferred way host data catalog inside gcp know inside aws glue preferred way know make sense use dataproc metastore andor big data lake metastore know also lot open source tools use prefer whats experience,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
99,Latest Thoughtworks TechRadar - data blips,"Thoughtworks have published their latest Technology Radar: https://www.thoughtworks.com/radar

FWIW, here are a few of the 'blips' (as they call them) of note in the data space:

🟢 Adopt: [Data product thinking](https://www.thoughtworks.com/radar/techniques/data-product-thinking)

🟢 Adopt: [Trino](https://www.thoughtworks.com/radar/platforms/trino)

👍 Trial: [Databricks Delta Live Tables](https://www.thoughtworks.com/radar/tools/databricks-delta-live-tables)

👍 Trial: [Metabase](https://www.thoughtworks.com/radar/tools/metabase)

✋ Hold: [Reverse ETL](https://www.thoughtworks.com/radar/techniques/reverse-etl)

On Reverse ETL they say: 

> we're seeing a growing trend where product vendors use Reverse ETL as an excuse to move increasing amounts of business logic into a centralized platform — their product. This approach exacerbates many of the issues caused by centralized data architectures, and we suggest exercising extreme caution when introducing data flows from a sprawling, central data platform to transaction processing systems.",7,2,2025-04-02 10:17:58,0,False,False,False,False,2025-04-02 10:17:58,10,Wednesday,113.0,1048,24.14,8,212,11.2,1,0,NEGATIVE,-0.9821859002113342,"['thoughtworks', 'published', 'latest', 'technology', 'radar', 'httpswwwthoughtworkscomradar', 'fwiw', 'blips', 'call', 'note', 'data', 'space', 'adopt', 'data', 'product', 'thinkinghttpswwwthoughtworkscomradartechniquesdataproductthinking', 'adopt', 'trinohttpswwwthoughtworkscomradarplatformstrino', 'trial', 'databricks', 'delta', 'live', 'tableshttpswwwthoughtworkscomradartoolsdatabricksdeltalivetables', 'trial', 'metabasehttpswwwthoughtworkscomradartoolsmetabase', 'hold', 'reverse', 'etlhttpswwwthoughtworkscomradartechniquesreverseetl', 'reverse', 'etl', 'say', 'seeing', 'growing', 'trend', 'product', 'vendors', 'use', 'reverse', 'etl', 'excuse', 'move', 'increasing', 'amounts', 'business', 'logic', 'centralized', 'platform', 'product', 'approach', 'exacerbates', 'many', 'issues', 'caused', 'centralized', 'data', 'architectures', 'suggest', 'exercising', 'extreme', 'caution', 'introducing', 'data', 'flows', 'sprawling', 'central', 'data', 'platform', 'transaction', 'processing', 'systems']",thoughtworks published latest technology radar httpswwwthoughtworkscomradar fwiw blips call note data space adopt data product thinkinghttpswwwthoughtworkscomradartechniquesdataproductthinking adopt trinohttpswwwthoughtworkscomradarplatformstrino trial databricks delta live tableshttpswwwthoughtworkscomradartoolsdatabricksdeltalivetables trial metabasehttpswwwthoughtworkscomradartoolsmetabase hold reverse etlhttpswwwthoughtworkscomradartechniquesreverseetl reverse etl say seeing growing trend product vendors use reverse etl excuse move increasing amounts business logic centralized platform product approach exacerbates many issues caused centralized data architectures suggest exercising extreme caution introducing data flows sprawling central data platform transaction processing systems,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
100,"New to Data Engineering — Feeling a Bit Overwhelmed, Looking for Advice","Hey everyone, I could really use some advice from fellow engineers. I'm pretty new to the data world — I messed up uni, then did an online analytics course, and after about a year and a half of grinding, I finally landed my first role. Along the way, I found a real passion for Python and SQL.

My first job involved a ton of patchy reporting because of messy infra and data. I started automating painful tasks using basic ETL pipelines I built myself. I showed an interest in APIs and, out of nowhere, 6 months in, I was offered a data engineering role.

Fast forward to now — I’ve been in the new role for a month, and I’m the company’s only data engineer. I’m doing a data engineering apprenticeship at the same time, which helps, but the imposter syndrome is real. The company’s been limping along with a 25-year-old piece of software that populates our SQL Server DB, and we’re now migrating to something new. I’ve been asked to learn MuleSoft for ETL and replace some existing pipelines that were built in Python.

I love the subject — I’m genuinely passionate about programming and networking — and I’m keen to take on new tech, improve the infra, and build up strong skills. But I’m not sure if I’m going too deep too fast. For example, today I was learning Docker to deploy Python scripts, just to avoid issues with hundreds of brittle batch files that break if we update Python.

My boss seems to think MuleSoft will fully replace Python, but I see it more as a tool that complements certain workflows rather than a full replacement. What worries me more is that I don’t really have any technical peers. Most people in my team only know basic SQL, and it’s hard to communicate strategy or get proper feedback.

My current priorities are getting comfortable with MuleSoft, Git, and Docker. I’m constantly learning, but sometimes I leave work feeling overwhelmed. There’s so much broken or duct-taped together, I don’t even know where to start. I keep telling myself I don’t need to “save the world,” but I really want to do a good job and come away with solid experience.

Long term, they want to deploy this new software, rebuild the database, and eventually use AI to help employees query the business. There’s a shit ton to do, and I’m still figuring out basics — like setting up a VM just so I can run Docker.

Am I jumping the gun with how I’m feeling, or is this as wild a situation as it seems? Any advice for a new engineer navigating bad infra, limited support, and a mountain of work would be seriously appreciated.",8,3,2025-04-02 19:27:27,0,False,False,False,False,2025-04-02 19:27:27,19,Wednesday,457.0,2534,60.85,24,666,11.0,0,0,NEGATIVE,-0.9753324389457703,"['hey', 'everyone', 'could', 'really', 'use', 'advice', 'fellow', 'engineers', 'pretty', 'new', 'data', 'world', 'messed', 'uni', 'online', 'analytics', 'course', 'year', 'half', 'grinding', 'finally', 'landed', 'first', 'role', 'along', 'way', 'found', 'real', 'passion', 'python', 'sql', 'first', 'job', 'involved', 'ton', 'patchy', 'reporting', 'messy', 'infra', 'data', 'started', 'automating', 'painful', 'tasks', 'using', 'basic', 'etl', 'pipelines', 'built', 'showed', 'interest', 'apis', 'nowhere', 'months', 'offered', 'data', 'engineering', 'role', 'fast', 'forward', 'ive', 'new', 'role', 'month', 'companys', 'data', 'engineer', 'data', 'engineering', 'apprenticeship', 'time', 'helps', 'imposter', 'syndrome', 'real', 'companys', 'limping', 'along', 'yearold', 'piece', 'software', 'populates', 'sql', 'server', 'migrating', 'something', 'new', 'ive', 'asked', 'learn', 'mulesoft', 'etl', 'replace', 'existing', 'pipelines', 'built', 'python', 'love', 'subject', 'genuinely', 'passionate', 'programming', 'networking', 'keen', 'take', 'new', 'tech', 'improve', 'infra', 'build', 'strong', 'skills', 'sure', 'going', 'deep', 'fast', 'example', 'today', 'learning', 'docker', 'deploy', 'python', 'scripts', 'avoid', 'issues', 'hundreds', 'brittle', 'batch', 'files', 'break', 'update', 'python', 'boss', 'seems', 'think', 'mulesoft', 'fully', 'replace', 'python', 'see', 'tool', 'complements', 'certain', 'workflows', 'rather', 'full', 'replacement', 'worries', 'dont', 'really', 'technical', 'peers', 'people', 'team', 'know', 'basic', 'sql', 'hard', 'communicate', 'strategy', 'get', 'proper', 'feedback', 'current', 'priorities', 'getting', 'comfortable', 'mulesoft', 'git', 'docker', 'constantly', 'learning', 'sometimes', 'leave', 'work', 'feeling', 'overwhelmed', 'theres', 'much', 'broken', 'ducttaped', 'together', 'dont', 'even', 'know', 'start', 'keep', 'telling', 'dont', 'need', 'save', 'world', 'really', 'want', 'good', 'job', 'come', 'away', 'solid', 'experience', 'long', 'term', 'want', 'deploy', 'new', 'software', 'rebuild', 'database', 'eventually', 'use', 'help', 'employees', 'query', 'business', 'theres', 'shit', 'ton', 'still', 'figuring', 'basics', 'like', 'setting', 'run', 'docker', 'jumping', 'gun', 'feeling', 'wild', 'situation', 'seems', 'advice', 'new', 'engineer', 'navigating', 'bad', 'infra', 'limited', 'support', 'mountain', 'work', 'would', 'seriously', 'appreciated']",hey everyone could really use advice fellow engineers pretty new data world messed uni online analytics course year half grinding finally landed first role along way found real passion python sql first job involved ton patchy reporting messy infra data started automating painful tasks using basic etl pipelines built showed interest apis nowhere months offered data engineering role fast forward ive new role month companys data engineer data engineering apprenticeship time helps imposter syndrome real companys limping along yearold piece software populates sql server migrating something new ive asked learn mulesoft etl replace existing pipelines built python love subject genuinely passionate programming networking keen take new tech improve infra build strong skills sure going deep fast example today learning docker deploy python scripts avoid issues hundreds brittle batch files break update python boss seems think mulesoft fully replace python see tool complements certain workflows rather full replacement worries dont really technical peers people team know basic sql hard communicate strategy get proper feedback current priorities getting comfortable mulesoft git docker constantly learning sometimes leave work feeling overwhelmed theres much broken ducttaped together dont even know start keep telling dont need save world really want good job come away solid experience long term want deploy new software rebuild database eventually use help employees query business theres shit ton still figuring basics like setting run docker jumping gun feeling wild situation seems advice new engineer navigating bad infra limited support mountain work would seriously appreciated,High,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
101,Creating a Beginner Data Engineering Group,"Hey everyone! I’m starting a beginner-friendly Data Engineering group to learn, share resources, and stay motivated together.

If you’re just starting out and want support, accountability, and useful learning materials, drop a comment or DM me! Let’s grow together.

Here's the whatsapp link to join:
https://chat.whatsapp.com/GfAh5OQimLE7uKoo1y5JrH",9,13,2025-04-02 05:28:15,0,False,2025-04-02 05:40:31,False,False,2025-04-02 05:28:15,5,Wednesday,46.0,349,34.42,4,87,12.2,1,0,POSITIVE,0.9891039133071899,"['hey', 'everyone', 'starting', 'beginnerfriendly', 'data', 'engineering', 'group', 'learn', 'share', 'resources', 'stay', 'motivated', 'together', 'youre', 'starting', 'want', 'support', 'accountability', 'useful', 'learning', 'materials', 'drop', 'comment', 'lets', 'grow', 'together', 'heres', 'whatsapp', 'link', 'join', 'httpschatwhatsappcomgfahoqimleukooyjrh']",hey everyone starting beginnerfriendly data engineering group learn share resources stay motivated together youre starting want support accountability useful learning materials drop comment lets grow together heres whatsapp link join httpschatwhatsappcomgfahoqimleukooyjrh,High,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
102,"For those who work in data governance but in a data engineering capacity, what are you developing?","Recruiter reached out about a role on a data governance team but the job itself is data engineering. Recruiter was sharing what was in the job post but it didn't clarify much

I'm not formally experienced with data governance but have implemented data quality tests, written documentation, etc. Is that all considered data governance? What would be data engineering responsibilities and day to day work be like on a governance team? 

Would be interested to hear especially if anyone worked in and implemented data governance from scratch, and not used 3rd party software, as this team seems to be trying to do that.",8,5,2025-04-02 14:25:44,0,False,False,False,False,2025-04-02 14:25:44,14,Wednesday,103.0,616,42.11,5,173,15.1,0,0,NEGATIVE,-0.9818763732910156,"['recruiter', 'reached', 'role', 'data', 'governance', 'team', 'job', 'data', 'engineering', 'recruiter', 'sharing', 'job', 'post', 'didnt', 'clarify', 'much', 'formally', 'experienced', 'data', 'governance', 'implemented', 'data', 'quality', 'tests', 'written', 'documentation', 'etc', 'considered', 'data', 'governance', 'would', 'data', 'engineering', 'responsibilities', 'day', 'day', 'work', 'like', 'governance', 'team', 'would', 'interested', 'hear', 'especially', 'anyone', 'worked', 'implemented', 'data', 'governance', 'scratch', 'used', 'party', 'software', 'team', 'seems', 'trying']",recruiter reached role data governance team job data engineering recruiter sharing job post didnt clarify much formally experienced data governance implemented data quality tests written documentation etc considered data governance would data engineering responsibilities day day work like governance team would interested hear especially anyone worked implemented data governance scratch used party software team seems trying,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
103,"Hi, what does a data engineer do on a day-to-day basis in a company?","Right now I work as a data scientist, but I find it very, very repetitive.

That's why I'm studying Data Engineering concepts.  Right now, I'm able to create pipelines to automate ETL loads into Amazon Redshift databases (sort of) using Airflow with Dicker and Kubernetes.

I'm specialized in Python, so I'm also looking at Kafka and Apache PySpark.

Anyway, I'm just starting out in this field, so I feel overwhelmed and not sure what a company expects of me.

Help me understand your role better, thank you!",5,4,2025-04-02 22:20:21,1,False,False,False,False,2025-04-02 22:20:21,22,Wednesday,87.0,509,56.76,6,139,11.9,0,0,NEGATIVE,-0.9655258059501648,"['right', 'work', 'data', 'scientist', 'find', 'repetitive', 'thats', 'studying', 'data', 'engineering', 'concepts', 'right', 'able', 'create', 'pipelines', 'automate', 'etl', 'loads', 'amazon', 'redshift', 'databases', 'sort', 'using', 'airflow', 'dicker', 'kubernetes', 'specialized', 'python', 'also', 'looking', 'kafka', 'apache', 'pyspark', 'anyway', 'starting', 'field', 'feel', 'overwhelmed', 'sure', 'company', 'expects', 'help', 'understand', 'role', 'better', 'thank']",right work data scientist find repetitive thats studying data engineering concepts right able create pipelines automate etl loads amazon redshift databases sort using airflow dicker kubernetes specialized python also looking kafka apache pyspark anyway starting field feel overwhelmed sure company expects help understand role better thank,Mid,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
104,Transition from on-prem to cloud,"Hi everyone,

I’ve been working in data for almost three years, mainly with on-prem technologies like SQL, SSIS, and Power BI, plus some experience with SSRS, datastage, Microstrategy and pl/SQL.

Lately, I’ve been looking for new opportunities, but most roles require Spark, Python, Databricks, Snowflake, and cloud experience, which I don’t have. My company won’t move me to a cloud-related project, but they do pay for some certifications (mainly related to Azure/Microsoft)—I’ve done Azure Data Fundamentals and I'm currently taking a Databricks course and plan to take the certification after.

What’s the best way to gain hands-on experience with cloud and these technologies? How did you make the transition?

Would love to hear your advice!",6,6,2025-04-02 16:44:10,0,False,False,False,False,2025-04-02 16:44:10,16,Wednesday,115.0,748,51.99,6,185,13.0,0,0,NEGATIVE,-0.9116083383560181,"['everyone', 'ive', 'working', 'data', 'almost', 'three', 'years', 'mainly', 'onprem', 'technologies', 'like', 'sql', 'ssis', 'power', 'plus', 'experience', 'ssrs', 'datastage', 'microstrategy', 'plsql', 'lately', 'ive', 'looking', 'new', 'opportunities', 'roles', 'require', 'spark', 'python', 'databricks', 'snowflake', 'cloud', 'experience', 'dont', 'company', 'wont', 'move', 'cloudrelated', 'project', 'pay', 'certifications', 'mainly', 'related', 'azuremicrosoftive', 'done', 'azure', 'data', 'fundamentals', 'currently', 'taking', 'databricks', 'course', 'plan', 'take', 'certification', 'whats', 'best', 'way', 'gain', 'handson', 'experience', 'cloud', 'technologies', 'make', 'transition', 'would', 'love', 'hear', 'advice']",everyone ive working data almost three years mainly onprem technologies like sql ssis power plus experience ssrs datastage microstrategy plsql lately ive looking new opportunities roles require spark python databricks snowflake cloud experience dont company wont move cloudrelated project pay certifications mainly related azuremicrosoftive done azure data fundamentals currently taking databricks course plan take certification whats best way gain handson experience cloud technologies make transition would love hear advice,High,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
105,Which is easier? AWS or Azure,Data engineering on azure cloud easier or aws? which one would you say? im currently learning azure :p,4,10,2025-04-02 20:18:15,0,False,False,False,False,2025-04-02 20:18:15,20,Wednesday,18.0,102,65.39,3,29,8.8,0,0,NEGATIVE,-0.984093964099884,"['data', 'engineering', 'azure', 'cloud', 'easier', 'aws', 'one', 'would', 'say', 'currently', 'learning', 'azure']",data engineering azure cloud easier aws one would say currently learning azure,Mid,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
106,Managing 1000's of small file writes from AWS Lambda,"Hi everyone,

I have a microservices architecture where I have a lambda function that takes an ID, sends it to an API for enrichment, and then resultant response is recorded in an S3 Bucket. My issue is that over \~200 concurrent lambdas and in effort to keep memory usage low, I am getting 1000's of small 30 - 200kb compressed ndjson files that make downstream computation a little challenging.

I tried to use Firehose but quickly get throttled and getting ""Slow Down."" error. Is there a tool or architecture decision I should consider besides just a downstream process that might consolidate these files perhaps in Glue?",2,4,2025-04-02 21:21:25,1,False,False,False,False,2025-04-02 21:21:25,21,Wednesday,106.0,624,44.78,4,163,14.6,0,1,NEGATIVE,-0.9995373487472534,"['everyone', 'microservices', 'architecture', 'lambda', 'function', 'takes', 'sends', 'api', 'enrichment', 'resultant', 'response', 'recorded', 'bucket', 'issue', 'concurrent', 'lambdas', 'effort', 'keep', 'memory', 'usage', 'low', 'getting', 'small', 'compressed', 'ndjson', 'files', 'make', 'downstream', 'computation', 'little', 'challenging', 'tried', 'use', 'firehose', 'quickly', 'get', 'throttled', 'getting', 'slow', 'error', 'tool', 'architecture', 'decision', 'consider', 'besides', 'downstream', 'process', 'might', 'consolidate', 'files', 'perhaps', 'glue']",everyone microservices architecture lambda function takes sends api enrichment resultant response recorded bucket issue concurrent lambdas effort keep memory usage low getting small compressed ndjson files make downstream computation little challenging tried use firehose quickly get throttled getting slow error tool architecture decision consider besides downstream process might consolidate files perhaps glue,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
107,DBA to Data Engineer,"Hi Everyone,
I have been working as an Oracle DBA for a while now, but I am not enjoying what am I doing. A year ago, I got interested in data engineering and tried to self-learn while juggling a full-time job, GRE prep(planning to go for masters as it’s always been my dream), and everything else… safe to say, it wasn’t easy. Since my job didn’t really involve coding and I ended up with mostly theoretical knowledge. I do know Python, Azure(again theoretical knowledge) and SQL (thanks to work), but I still have a long way to go in data engineering. Now that I’m finally taking this step, I am thinking to quit my current job and put all my efforts solely on switching from DBA to data engineering. I’d really appreciate any advice on how to go about this what tech stacks I should focus on and whether transitioning within six months is realistic.",2,1,2025-04-02 18:51:20,0,False,False,False,False,2025-04-02 18:51:20,18,Wednesday,155.0,852,53.75,6,235,13.0,0,0,NEGATIVE,-0.9710626006126404,"['everyone', 'working', 'oracle', 'dba', 'enjoying', 'year', 'ago', 'got', 'interested', 'data', 'engineering', 'tried', 'selflearn', 'juggling', 'fulltime', 'job', 'gre', 'prepplanning', 'masters', 'always', 'dream', 'everything', 'else', 'safe', 'say', 'wasnt', 'easy', 'since', 'job', 'didnt', 'really', 'involve', 'coding', 'ended', 'mostly', 'theoretical', 'knowledge', 'know', 'python', 'azureagain', 'theoretical', 'knowledge', 'sql', 'thanks', 'work', 'still', 'long', 'way', 'data', 'engineering', 'finally', 'taking', 'step', 'thinking', 'quit', 'current', 'job', 'put', 'efforts', 'solely', 'switching', 'dba', 'data', 'engineering', 'really', 'appreciate', 'advice', 'tech', 'stacks', 'focus', 'whether', 'transitioning', 'within', 'six', 'months', 'realistic']",everyone working oracle dba enjoying year ago got interested data engineering tried selflearn juggling fulltime job gre prepplanning masters always dream everything else safe say wasnt easy since job didnt really involve coding ended mostly theoretical knowledge know python azureagain theoretical knowledge sql thanks work still long way data engineering finally taking step thinking quit current job put efforts solely switching dba data engineering really appreciate advice tech stacks focus whether transitioning within six months realistic,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
108,Suggestions for workflow automation,"Hey there :)

  
I hope I find myself in the right subreddit for this as I am trying to **engineer** my computer to push around some **data** ;) 

I'm currently working on a project to fully automate the processing of test results for a scientific study with students. 

The workflow consists of several stages:

1. **Data Extraction:** The test data is extracted from a local SQL database.
2. **SPSS Processing:** The extracted data is then processed using SPSS with a custom-built syntax (legacy). This step generates multiple files from the data. I have been looking into how I can transition this syntax to a python script, so this step might be cut later.
3. **Python Automation:** A Python script takes over the further processing. It reads the files, splits the data per class, inserts it into pre-designed Excel reporting templates.
4. **File Upload:** The files are then automatically uploaded to a self-hosted Nextcloud instance.
5. **Notification:** Once the workflow is complete, a notification  

I have been thinking about different ways to implement this. Right now the inputs and outputs for the different steps are still done manually. 

At work I have been using Jenkins lately and I think it feels natural  to do it in Jenkins and just describe the whole workflow in a pipeline with different stages to run. Besides that I have some experience with AWS Lambda and n8n but I am not sure if they would be helpful with this task.

I´m not that experienced setting up such workflows as my work background is more in Infosec, so please forgive my uneducated guesses about how I best go about this :D Just trying not to take decisions that will be problematic later.



Greetings from Germany",2,2,2025-04-02 11:06:05,0,False,False,False,False,2025-04-02 11:06:05,11,Wednesday,288.0,1705,60.55,15,443,12.2,0,0,NEGATIVE,-0.9954885840415955,"['hey', 'hope', 'find', 'right', 'subreddit', 'trying', 'engineer', 'computer', 'push', 'around', 'data', 'currently', 'working', 'project', 'fully', 'automate', 'processing', 'test', 'results', 'scientific', 'study', 'students', 'workflow', 'consists', 'several', 'stages', 'data', 'extraction', 'test', 'data', 'extracted', 'local', 'sql', 'database', 'spss', 'processing', 'extracted', 'data', 'processed', 'using', 'spss', 'custombuilt', 'syntax', 'legacy', 'step', 'generates', 'multiple', 'files', 'data', 'looking', 'transition', 'syntax', 'python', 'script', 'step', 'might', 'cut', 'later', 'python', 'automation', 'python', 'script', 'takes', 'processing', 'reads', 'files', 'splits', 'data', 'per', 'class', 'inserts', 'predesigned', 'excel', 'reporting', 'templates', 'file', 'upload', 'files', 'automatically', 'uploaded', 'selfhosted', 'nextcloud', 'instance', 'notification', 'workflow', 'complete', 'notification', 'thinking', 'different', 'ways', 'implement', 'right', 'inputs', 'outputs', 'different', 'steps', 'still', 'done', 'manually', 'work', 'using', 'jenkins', 'lately', 'think', 'feels', 'natural', 'jenkins', 'describe', 'whole', 'workflow', 'pipeline', 'different', 'stages', 'run', 'besides', 'experience', 'aws', 'lambda', 'sure', 'would', 'helpful', 'task', 'experienced', 'setting', 'workflows', 'work', 'background', 'infosec', 'please', 'forgive', 'uneducated', 'guesses', 'best', 'trying', 'take', 'decisions', 'problematic', 'later', 'greetings', 'germany']",hey hope find right subreddit trying engineer computer push around data currently working project fully automate processing test results scientific study students workflow consists several stages data extraction test data extracted local sql database spss processing extracted data processed using spss custombuilt syntax legacy step generates multiple files data looking transition syntax python script step might cut later python automation python script takes processing reads files splits data per class inserts predesigned excel reporting templates file upload files automatically uploaded selfhosted nextcloud instance notification workflow complete notification thinking different ways implement right inputs outputs different steps still done manually work using jenkins lately think feels natural jenkins describe whole workflow pipeline different stages run besides experience aws lambda sure would helpful task experienced setting workflows work background infosec please forgive uneducated guesses best trying take decisions problematic later greetings germany,Mid,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
109,How are you working with your DWH ?,"I would like to understand how you manage your DWW in day-to-day basis, solution, tools, architecture, workflows, ETL, serving...",2,1,2025-04-02 07:17:28,0,False,False,False,False,2025-04-02 07:17:28,7,Wednesday,19.0,129,43.73,1,32,0.0,0,0,POSITIVE,0.9990832805633545,"['would', 'like', 'understand', 'manage', 'dww', 'daytoday', 'basis', 'solution', 'tools', 'architecture', 'workflows', 'etl', 'serving']",would like understand manage dww daytoday basis solution tools architecture workflows etl serving,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
110,Facebook Marketing API - Anyone have a successful ETL experience?,"We have a python integration set up where we pull data from Google Ads and Facebook Marketing into our data warehouse. We're pulling data about all 3 hierarchy tiers and some daily metrics:

1. Campaigns (id, name, start time, stop time)
2. Ad Groups/Ad Sets (id, name)
3. Ads (id, name, URL)
4. Metrics (clicks, impressions, spend) for the previous day

For the Google Ads API, you basically send a SQL query and the return time is like a tenth of a second.

For Facebook, we see returns times in the minutes, especially on the Ads piece. Was hoping to get an idea of how others might have successfully set up a process to get this data from Facebook in a more timely fashion, and possibly without hitting the rate limiting threshold.

Not the exact code we're using - I can get it off my work system tomorrow - but the gist:

    from facebook_business.adobjects.adaccount import AdAccount
    from facebook_business.adobjects.campaign import Campaign
    from facebook_business.adobjects.ad import AdSet
    from facebook_business.adobjects.ad import Ad
    from facebook_business.adobjects.adcreative import AdCreative
    campaigns = AdAccount('act_123456789').get_campaigns(
        params={},
        fields=[Campaign.Field.id,Campaign.Field.name,Campaign.Field.start_time,Campaign.Field.stop_time]
    )
    adsets= AdAccount('act_123456789').get_ad_sets(
        params={},
        fields=[AdSet.Field.id,AdSet.Field.name]
    )
    ads = AdAccount('act_123456789').get_ads(
        params={},
        fields=[Ad.Field.id,Ad.Field.name,Ad.Field.creative]
    )
    object_urls = AdAccount('act_123456789').get_ad_creatives(
        params={},
        fields=[AdCreative.Field.object_story_spec]
    )
    asset_urls = AdAccount('act_123456789').get_ad_creatives(
        params={},
        fields=[AdCreative.Field.asset_feed_spec]
    )

We then have to do some joining between ads/object\_urls/asset\_urls to match the Ad with the destination URL if the ad is clicked on.

The performance is so slow, that I hope we are doing it wrong. I was never able to get the batch call to work and I'm not sure how to improve things.

Sincerely a data analyst who crosses over into data engineering because our data engineers don't know python.",2,6,2025-04-02 02:17:27,0,False,False,False,False,2025-04-02 02:17:27,2,Wednesday,273.0,2244,44.71,27,482,10.1,0,1,NEGATIVE,-0.9983181953430176,"['python', 'integration', 'set', 'pull', 'data', 'google', 'ads', 'facebook', 'marketing', 'data', 'warehouse', 'pulling', 'data', 'hierarchy', 'tiers', 'daily', 'metrics', 'campaigns', 'name', 'start', 'time', 'stop', 'time', 'groupsad', 'sets', 'name', 'ads', 'name', 'url', 'metrics', 'clicks', 'impressions', 'spend', 'previous', 'day', 'google', 'ads', 'api', 'basically', 'send', 'sql', 'query', 'return', 'time', 'like', 'tenth', 'second', 'facebook', 'see', 'returns', 'times', 'minutes', 'especially', 'ads', 'piece', 'hoping', 'get', 'idea', 'others', 'might', 'successfully', 'set', 'process', 'get', 'data', 'facebook', 'timely', 'fashion', 'possibly', 'without', 'hitting', 'rate', 'limiting', 'threshold', 'exact', 'code', 'using', 'get', 'work', 'system', 'tomorrow', 'gist', 'facebookbusinessadobjectsadaccount', 'import', 'adaccount', 'facebookbusinessadobjectscampaign', 'import', 'campaign', 'facebookbusinessadobjectsad', 'import', 'adset', 'facebookbusinessadobjectsad', 'import', 'facebookbusinessadobjectsadcreative', 'import', 'adcreative', 'campaigns', 'adaccountactgetcampaigns', 'params', 'fieldscampaignfieldidcampaignfieldnamecampaignfieldstarttimecampaignfieldstoptime', 'adsets', 'adaccountactgetadsets', 'params', 'fieldsadsetfieldidadsetfieldname', 'ads', 'adaccountactgetads', 'params', 'fieldsadfieldidadfieldnameadfieldcreative', 'objecturls', 'adaccountactgetadcreatives', 'params', 'fieldsadcreativefieldobjectstoryspec', 'asseturls', 'adaccountactgetadcreatives', 'params', 'fieldsadcreativefieldassetfeedspec', 'joining', 'adsobjecturlsasseturls', 'match', 'destination', 'url', 'clicked', 'performance', 'slow', 'hope', 'wrong', 'never', 'able', 'get', 'batch', 'call', 'work', 'sure', 'improve', 'things', 'sincerely', 'data', 'analyst', 'crosses', 'data', 'engineering', 'data', 'engineers', 'dont', 'know', 'python']",python integration set pull data google ads facebook marketing data warehouse pulling data hierarchy tiers daily metrics campaigns name start time stop time groupsad sets name ads name url metrics clicks impressions spend previous day google ads api basically send sql query return time like tenth second facebook see returns times minutes especially ads piece hoping get idea others might successfully set process get data facebook timely fashion possibly without hitting rate limiting threshold exact code using get work system tomorrow gist facebookbusinessadobjectsadaccount import adaccount facebookbusinessadobjectscampaign import campaign facebookbusinessadobjectsad import adset facebookbusinessadobjectsad import facebookbusinessadobjectsadcreative import adcreative campaigns adaccountactgetcampaigns params fieldscampaignfieldidcampaignfieldnamecampaignfieldstarttimecampaignfieldstoptime adsets adaccountactgetadsets params fieldsadsetfieldidadsetfieldname ads adaccountactgetads params fieldsadfieldidadfieldnameadfieldcreative objecturls adaccountactgetadcreatives params fieldsadcreativefieldobjectstoryspec asseturls adaccountactgetadcreatives params fieldsadcreativefieldassetfeedspec joining adsobjecturlsasseturls match destination url clicked performance slow hope wrong never able get batch call work sure improve things sincerely data analyst crosses data engineering data engineers dont know python,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
111,Resources for learning AbInitio Tool,"I tried to search the entire internet to find AbInito related tutorials/tranings. Hard luck finding anything. I came to know it's a closed source tool and everything is behind a login wall only for partner companies. 

Can anyone share me stuff they found useful?

Thanks in advance.",2,2,2025-04-02 02:17:07,0,False,False,False,False,2025-04-02 02:17:07,2,Wednesday,47.0,283,61.93,5,74,10.4,0,0,NEGATIVE,-0.9983043670654297,"['tried', 'search', 'entire', 'internet', 'find', 'abinito', 'related', 'tutorialstranings', 'hard', 'luck', 'finding', 'anything', 'came', 'know', 'closed', 'source', 'tool', 'everything', 'behind', 'login', 'wall', 'partner', 'companies', 'anyone', 'share', 'stuff', 'found', 'useful', 'thanks', 'advance']",tried search entire internet find abinito related tutorialstranings hard luck finding anything came know closed source tool everything behind login wall partner companies anyone share stuff found useful thanks advance,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
112,Feeling stuck. How to move ahead,"I have been working for a consulting firm for the past 5 years. The kind of work they assign me to is fairly basic - developing pipelines using Informatica and writing SQL queries for it. That's been majority of my experience. For the past # months, I've been assigned to a PowerBI developer role, but I just tweak the data/queries to do what the client asks. When I try to apply for data engineering/etl roles, I get asked what I think are pretty advanced questions - for example I got asked about what gaps I have noticed in Microsoft Fabric and what are best practices for data modeling etc. I tend to give general answera based on my research and theoretical answers, but I can never relate it to my actual experience because day to day I don't do anything high level. I get asked about how I optimzied queries or pipelines, the truth is I worked with small enough datasets that I never really had to do anything. Again, I give answers based on my research - like indexing or partitioning but I feel the people asking questions are always looking for more. 

I cannot leave or take a break, I'm on a visa, but how do I actually get further then. Is anyone else feeling the same? ",3,0,2025-04-02 22:56:33,1,False,False,False,False,2025-04-02 22:56:33,22,Wednesday,219.0,1183,58.11,10,324,12.3,0,0,NEGATIVE,-0.994392454624176,"['working', 'consulting', 'firm', 'past', 'years', 'kind', 'work', 'assign', 'fairly', 'basic', 'developing', 'pipelines', 'using', 'informatica', 'writing', 'sql', 'queries', 'thats', 'majority', 'experience', 'past', 'months', 'ive', 'assigned', 'powerbi', 'developer', 'role', 'tweak', 'dataqueries', 'client', 'asks', 'try', 'apply', 'data', 'engineeringetl', 'roles', 'get', 'asked', 'think', 'pretty', 'advanced', 'questions', 'example', 'got', 'asked', 'gaps', 'noticed', 'microsoft', 'fabric', 'best', 'practices', 'data', 'modeling', 'etc', 'tend', 'give', 'general', 'answera', 'based', 'research', 'theoretical', 'answers', 'never', 'relate', 'actual', 'experience', 'day', 'day', 'dont', 'anything', 'high', 'level', 'get', 'asked', 'optimzied', 'queries', 'pipelines', 'truth', 'worked', 'small', 'enough', 'datasets', 'never', 'really', 'anything', 'give', 'answers', 'based', 'research', 'like', 'indexing', 'partitioning', 'feel', 'people', 'asking', 'questions', 'always', 'looking', 'cannot', 'leave', 'take', 'break', 'visa', 'actually', 'get', 'anyone', 'else', 'feeling']",working consulting firm past years kind work assign fairly basic developing pipelines using informatica writing sql queries thats majority experience past months ive assigned powerbi developer role tweak dataqueries client asks try apply data engineeringetl roles get asked think pretty advanced questions example got asked gaps noticed microsoft fabric best practices data modeling etc tend give general answera based research theoretical answers never relate actual experience day day dont anything high level get asked optimzied queries pipelines truth worked small enough datasets never really anything give answers based research like indexing partitioning feel people asking questions always looking cannot leave take break visa actually get anyone else feeling,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
113,Resources to learn developing production-ready APIs?,"Books, articles, courses... what resources have been useful to you for learning how to develop production-ready APIs? Production-ready meaning robust, secure, performant, modular etc

Thanks!",1,1,2025-04-02 22:43:35,1,False,False,False,False,2025-04-02 22:43:35,22,Wednesday,25.0,191,29.21,3,50,12.5,0,0,POSITIVE,0.9983838796615601,"['books', 'articles', 'courses', 'resources', 'useful', 'learning', 'develop', 'productionready', 'apis', 'productionready', 'meaning', 'robust', 'secure', 'performant', 'modular', 'etc', 'thanks']",books articles courses resources useful learning develop productionready apis productionready meaning robust secure performant modular etc thanks,Mid,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
114,Where next with my DE journey?,"I have completed Microsoft Azure Data Engineering (DP 203) certification which has given me a solid foundation of data engineering on Azure. 

Next, I followed along and did this project by Ansh Lamba: [https://www.youtube.com/watch?v=uc-u\_juRg-w&t=16941s&ab\_channel=AnshLamba](https://www.youtube.com/watch?v=uc-u_juRg-w&t=16941s&ab_channel=AnshLamba) 

  
What should be my next step to enhance my skills? Any recommendation? 4 weeks ago I didn't know anything about data engineering :p",2,0,2025-04-02 20:21:01,0,False,False,False,False,2025-04-02 20:21:01,20,Wednesday,57.0,490,23.12,4,114,12.2,1,0,NEGATIVE,-0.8584842681884766,"['completed', 'microsoft', 'azure', 'data', 'engineering', 'certification', 'given', 'solid', 'foundation', 'data', 'engineering', 'azure', 'next', 'followed', 'along', 'project', 'ansh', 'lamba', 'httpswwwyoutubecomwatchvucujurgwtsabchannelanshlambahttpswwwyoutubecomwatchvucujurgwtsabchannelanshlamba', 'next', 'step', 'enhance', 'skills', 'recommendation', 'weeks', 'ago', 'didnt', 'know', 'anything', 'data', 'engineering']",completed microsoft azure data engineering certification given solid foundation data engineering azure next followed along project ansh lamba httpswwwyoutubecomwatchvucujurgwtsabchannelanshlambahttpswwwyoutubecomwatchvucujurgwtsabchannelanshlamba next step enhance skills recommendation weeks ago didnt know anything data engineering,Mid,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
115,Yet another iceberg catalog choice question,"We are an AWS and Databricks shop. We want to explore open source engines for cost savings and reduce vendor lock. 

We want to introduce iceberg. This interoperability with Flink, Snowflake, Trino. 

We are considering Glue,  Snowflake-version-of-Polaris or another catalog.

I appreciate any recommendations and experices from this group.

  
Databricks unity-uniform enables reading the data as a iceberg table but we cannot write a table using Flink. We use Trino and Snowflake for reads.

",1,3,2025-04-02 19:59:54,0,False,False,False,False,2025-04-02 19:59:54,19,Wednesday,74.0,494,53.58,8,125,9.5,0,0,NEGATIVE,-0.9332886338233948,"['aws', 'databricks', 'shop', 'want', 'explore', 'open', 'source', 'engines', 'cost', 'savings', 'reduce', 'vendor', 'lock', 'want', 'introduce', 'iceberg', 'interoperability', 'flink', 'snowflake', 'trino', 'considering', 'glue', 'snowflakeversionofpolaris', 'another', 'catalog', 'appreciate', 'recommendations', 'experices', 'group', 'databricks', 'unityuniform', 'enables', 'reading', 'data', 'iceberg', 'table', 'cannot', 'write', 'table', 'using', 'flink', 'use', 'trino', 'snowflake', 'reads']",aws databricks shop want explore open source engines cost savings reduce vendor lock want introduce iceberg interoperability flink snowflake trino considering glue snowflakeversionofpolaris another catalog appreciate recommendations experices group databricks unityuniform enables reading data iceberg table cannot write table using flink use trino snowflake reads,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
116,What is a research and BI analyst?,"Hey, before this gets taken down \*I have read the wiki and it did not answer my question\*

I've just signed the contract for a Data Engineering role, but it lists me as a Research and BI Analyst without any mention of data engineering. I should note I'm gonna be an intern and I have zero corporate experience so job titles are new territory for me, sorry if it's really obvious and I'm being clueless.

Is this is a type of data engineer? Have they made a mistake on the contract? Does BI stand for Business Intelligence? What do I even do???

The Analyst bit makes me quite happy because that's what I ultimately want to do in the future but I'm kind of confused as to how this is data engineering as all my other research leading up to this contract tells me Data Analysts and Data Engineers are different lol any help appreciated, thank you!",1,2,2025-04-02 17:36:47,0,False,False,False,False,2025-04-02 17:36:47,17,Wednesday,157.0,847,57.2,7,235,11.8,0,0,NEGATIVE,-0.9993926286697388,"['hey', 'gets', 'taken', 'read', 'wiki', 'answer', 'question', 'ive', 'signed', 'contract', 'data', 'engineering', 'role', 'lists', 'research', 'analyst', 'without', 'mention', 'data', 'engineering', 'note', 'gonna', 'intern', 'zero', 'corporate', 'experience', 'job', 'titles', 'new', 'territory', 'sorry', 'really', 'obvious', 'clueless', 'type', 'data', 'engineer', 'made', 'mistake', 'contract', 'stand', 'business', 'intelligence', 'even', 'analyst', 'bit', 'makes', 'quite', 'happy', 'thats', 'ultimately', 'want', 'future', 'kind', 'confused', 'data', 'engineering', 'research', 'leading', 'contract', 'tells', 'data', 'analysts', 'data', 'engineers', 'different', 'lol', 'help', 'appreciated', 'thank']",hey gets taken read wiki answer question ive signed contract data engineering role lists research analyst without mention data engineering note gonna intern zero corporate experience job titles new territory sorry really obvious clueless type data engineer made mistake contract stand business intelligence even analyst bit makes quite happy thats ultimately want future kind confused data engineering research leading contract tells data analysts data engineers different lol help appreciated thank,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
117,Roast my simple project. STAR schema database containing London weather data,"Hey all,

I've just created my second mini-project. Again, just to practice the skill I have learnt through DataCamp's courses.

  
I imported London's weather data via OpenWeather's API, cleaned it and created a database from it (STAR Schema)

  
If I had to do it again I will probably write functions instead of doing transformations manually. I really don't know why I didn't start of using function

  
I think my next project will include multiple different data sources and will also include some form of orchestration.

Here is the link: [https://www.datacamp.com/datalab/w/6aa0a025-9fe8-4291-bafd-67e1fc0d0005/edit](https://www.datacamp.com/datalab/w/6aa0a025-9fe8-4291-bafd-67e1fc0d0005/edit)

Any and all feedback is welcome.

Thanks!",0,6,2025-04-02 17:19:33,0,False,False,False,False,2025-04-02 17:19:33,17,Wednesday,97.0,745,38.11,6,174,11.9,1,1,NEGATIVE,-0.9977104663848877,"['hey', 'ive', 'created', 'second', 'miniproject', 'practice', 'skill', 'learnt', 'datacamps', 'courses', 'imported', 'londons', 'weather', 'data', 'via', 'openweathers', 'api', 'cleaned', 'created', 'database', 'star', 'schema', 'probably', 'write', 'functions', 'instead', 'transformations', 'manually', 'really', 'dont', 'know', 'didnt', 'start', 'using', 'function', 'think', 'next', 'project', 'include', 'multiple', 'different', 'data', 'sources', 'also', 'include', 'form', 'orchestration', 'link', 'httpswwwdatacampcomdatalabwaaafebafdefcdedithttpswwwdatacampcomdatalabwaaafebafdefcdedit', 'feedback', 'welcome', 'thanks']",hey ive created second miniproject practice skill learnt datacamps courses imported londons weather data via openweathers api cleaned created database star schema probably write functions instead transformations manually really dont know didnt start using function think next project include multiple different data sources also include form orchestration link httpswwwdatacampcomdatalabwaaafebafdefcdedithttpswwwdatacampcomdatalabwaaafebafdefcdedit feedback welcome thanks,Low,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
118,Help a noob understand whether this is feasible,"Hey all,
I’m working on a project that involves building a comprehensive overview of all therapist-related businesses in my country. I’ve found a public online source that lists approximately 16,000 such businesses, spread across many paginated result pages.

Each entry links to a detail page with information such as:

Business name
Business owner (person or legal entity)
Registration number (similar to a company ID)
Location (optional)
No consistent link to a website, but it's often listed in the details

What I need help with:

(1) Scrape all business data into a structured list (CSV, JSON or database).
This involves crawling through all paginated pages and collecting each business profile’s content.

(2) Automatically search for a homepage/website for each business.
The source doesn't always list websites, so for those missing, I'd like to auto-search Google (or use a business API if necessary) to find the most likely company homepage.
(3) If a homepage is found: scrape relevant data from the website itself.

Goal:
To build a clean, filterable dataset that can be used for matching clients with therapists (via a separate platform I'm developing).

Questions I’d like help with:

Is this technically feasible using open tools or affordable APIs? What/who exactly would I be looking for? I have tried navigating Fiverr, but I am simply not sure what I need to be frank...

Thanks in advance!",1,4,2025-04-02 12:12:24,0,False,False,False,False,2025-04-02 12:12:24,12,Wednesday,225.0,1409,52.39,12,368,12.9,0,0,NEGATIVE,-0.9856077432632446,"['hey', 'working', 'project', 'involves', 'building', 'comprehensive', 'overview', 'therapistrelated', 'businesses', 'country', 'ive', 'found', 'public', 'online', 'source', 'lists', 'approximately', 'businesses', 'spread', 'across', 'many', 'paginated', 'result', 'pages', 'entry', 'links', 'detail', 'page', 'information', 'business', 'name', 'business', 'owner', 'person', 'legal', 'entity', 'registration', 'number', 'similar', 'company', 'location', 'optional', 'consistent', 'link', 'website', 'often', 'listed', 'details', 'need', 'help', 'scrape', 'business', 'data', 'structured', 'list', 'csv', 'json', 'database', 'involves', 'crawling', 'paginated', 'pages', 'collecting', 'business', 'profiles', 'content', 'automatically', 'search', 'homepagewebsite', 'business', 'source', 'doesnt', 'always', 'list', 'websites', 'missing', 'like', 'autosearch', 'google', 'use', 'business', 'api', 'necessary', 'find', 'likely', 'company', 'homepage', 'homepage', 'found', 'scrape', 'relevant', 'data', 'website', 'goal', 'build', 'clean', 'filterable', 'dataset', 'used', 'matching', 'clients', 'therapists', 'via', 'separate', 'platform', 'developing', 'questions', 'like', 'help', 'technically', 'feasible', 'using', 'open', 'tools', 'affordable', 'apis', 'whatwho', 'exactly', 'would', 'looking', 'tried', 'navigating', 'fiverr', 'simply', 'sure', 'need', 'frank', 'thanks', 'advance']",hey working project involves building comprehensive overview therapistrelated businesses country ive found public online source lists approximately businesses spread across many paginated result pages entry links detail page information business name business owner person legal entity registration number similar company location optional consistent link website often listed details need help scrape business data structured list csv json database involves crawling paginated pages collecting business profiles content automatically search homepagewebsite business source doesnt always list websites missing like autosearch google use business api necessary find likely company homepage homepage found scrape relevant data website goal build clean filterable dataset used matching clients therapists via separate platform developing questions like help technically feasible using open tools affordable apis whatwho exactly would looking tried navigating fiverr simply sure need frank thanks advance,Mid,0,0,"need, table, event, source, without, instead, every, replay, day, example, single, change, per, business, daily, based, keys, manually, model, keep, needs, events, doesnt, name, list, option, user, must, takes, history, often, truth, full, column, hour, address, requirements, date, reading, track, whenever, metrics, result, update, feature, primary, columns, meaning, pricing, structure, details, rows, simply, available, dedicated, country, means, facebook, error, missing, evolve, involves, names, size, domain, place, addresses, timestamp, parse, mean, slow, schemas, heres, return, dimensions, delete, state, times, arent, ground, errors, receive, min, added, geocoding, shape, sourcing, wasnt, insert, minutes, value, flow, save, normal, little, region, usually, bad, becomes, failure, replication, entirely, products, infrastructure, minute, person, average, servers, internet, looked, probably, campaigns, workloads, looks",8.2,7.4,175.5,-0.663,1069.0,47.9,9.8,275.5,11.4,22.2,15.5,52,108,34,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",4,4
119,Feedback on Terraform Data Stack Starter,"Hi, everyone!

I'm a solo data consultant and over the past few years, I’ve been helping companies in Europe build their data stacks.

I noticed I was repeatedly performing the same tasks across my projects: setting up dbt, configuring Snowflake, and, more recently, migrating to Iceberg data lakes.

So I've been working on a solution for the past few months called [**Boring Data**](http://boringdata.io).

It's a set of Terraform templates ready to be deployed in AWS and/or Snowflake with pre-built integrations for ELT tools and orchestrators.

I think these templates are a great fit for many projects:

* Pay once, own it forever
* Get started fast
* Full control

I'd love to get feedback on this approach, which isn't very common (from what I've seen) in the data industry.

Is Terraform commonly used on your teams, or is that a barrier to using templates like these?

Is there a starter template that you'd wished you had for an implementation in the past?",1,3,2025-04-02 11:40:59,0,False,False,False,False,2025-04-02 11:40:59,11,Wednesday,162.0,967,56.89,7,238,12.0,1,0,NEGATIVE,-0.9990179538726807,"['everyone', 'solo', 'data', 'consultant', 'past', 'years', 'ive', 'helping', 'companies', 'europe', 'build', 'data', 'stacks', 'noticed', 'repeatedly', 'performing', 'tasks', 'across', 'projects', 'setting', 'dbt', 'configuring', 'snowflake', 'recently', 'migrating', 'iceberg', 'data', 'lakes', 'ive', 'working', 'solution', 'past', 'months', 'called', 'boring', 'datahttpboringdataio', 'set', 'terraform', 'templates', 'ready', 'deployed', 'aws', 'andor', 'snowflake', 'prebuilt', 'integrations', 'elt', 'tools', 'orchestrators', 'think', 'templates', 'great', 'fit', 'many', 'projects', 'pay', 'forever', 'get', 'started', 'fast', 'full', 'control', 'love', 'get', 'feedback', 'approach', 'isnt', 'common', 'ive', 'seen', 'data', 'industry', 'terraform', 'commonly', 'used', 'teams', 'barrier', 'using', 'templates', 'like', 'starter', 'template', 'youd', 'wished', 'implementation', 'past']",everyone solo data consultant past years ive helping companies europe build data stacks noticed repeatedly performing tasks across projects setting dbt configuring snowflake recently migrating iceberg data lakes ive working solution past months called boring datahttpboringdataio set terraform templates ready deployed aws andor snowflake prebuilt integrations elt tools orchestrators think templates great fit many projects pay forever get started fast full control love get feedback approach isnt common ive seen data industry terraform commonly used teams barrier using templates like starter template youd wished implementation past,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
120,Help with a data engineering project,"Hello guys, me and teammates want to do a project from a-z to practice what we learned in an internship we are in and we wanted to the project to be about a telecom company’s data and we have searched a lot for a dataset that mimics the datasets of real telecom companies but we never found what we are looking for so we thought about creating the data we want using AI but for some reason it’s also not working out for us so i would love to hear some suggestions about what we should do and about telecom data warehouses and databases because i feel maybe we just don’t still quite understand how telecom companies generally operate and perhaps that’s why we are not successful in generating the data.

I hope this post makes sense because i’m just very confused and don’t know what to do for this project. 

Thank you for anyone who will respond in advance!",1,0,2025-04-02 11:13:35,0,False,False,False,False,2025-04-02 11:13:35,11,Wednesday,160.0,859,25.84,3,233,17.9,0,0,NEGATIVE,-0.9769105911254883,"['hello', 'guys', 'teammates', 'want', 'project', 'practice', 'learned', 'internship', 'wanted', 'project', 'telecom', 'companys', 'data', 'searched', 'lot', 'dataset', 'mimics', 'datasets', 'real', 'telecom', 'companies', 'never', 'found', 'looking', 'thought', 'creating', 'data', 'want', 'using', 'reason', 'also', 'working', 'would', 'love', 'hear', 'suggestions', 'telecom', 'data', 'warehouses', 'databases', 'feel', 'maybe', 'dont', 'still', 'quite', 'understand', 'telecom', 'companies', 'generally', 'operate', 'perhaps', 'thats', 'successful', 'generating', 'data', 'hope', 'post', 'makes', 'sense', 'confused', 'dont', 'know', 'project', 'thank', 'anyone', 'respond', 'advance']",hello guys teammates want project practice learned internship wanted project telecom companys data searched lot dataset mimics datasets real telecom companies never found looking thought creating data want using reason also working would love hear suggestions telecom data warehouses databases feel maybe dont still quite understand telecom companies generally operate perhaps thats successful generating data hope post makes sense confused dont know project thank anyone respond advance,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
121,[BIGQUERY] How long does it take for a backfill and for the buffer resulting from that to clear?,"Hey all, 

1. I have two tables which are about 20-30 gbs and I created a backfill for them as I noticed that two days data was missing, now after an hour the backfill completed, now I am seeing some items in the streaming buffer, I need to update my seniors when the data is ready for analysis, so when can I safely say the data is present?

2. Also, one more question, if I insert a row manually into Bigquery and then create a backfill for it to fetch the data again from transactional database, will the entry I added manually (which doesn't exist in transactional database) be erased?

3. Is there a way to track the ingestion of data into BigQuery?

",1,2,2025-04-02 08:57:23,0,False,False,False,False,2025-04-02 08:57:23,8,Wednesday,123.0,656,48.67,4,185,12.2,0,0,NEGATIVE,-0.9991772770881653,"['hey', 'two', 'tables', 'gbs', 'created', 'backfill', 'noticed', 'two', 'days', 'data', 'missing', 'hour', 'backfill', 'completed', 'seeing', 'items', 'streaming', 'buffer', 'need', 'update', 'seniors', 'data', 'ready', 'analysis', 'safely', 'say', 'data', 'present', 'also', 'one', 'question', 'insert', 'row', 'manually', 'bigquery', 'create', 'backfill', 'fetch', 'data', 'transactional', 'database', 'entry', 'added', 'manually', 'doesnt', 'exist', 'transactional', 'database', 'erased', 'way', 'track', 'ingestion', 'data', 'bigquery']",hey two tables gbs created backfill noticed two days data missing hour backfill completed seeing items streaming buffer need update seniors data ready analysis safely say data present also one question insert row manually bigquery create backfill fetch data transactional database entry added manually doesnt exist transactional database erased way track ingestion data bigquery,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
122,Beginner using API (AWS),"Hi. I work for the state and some of the tools we have are limited. Each week I go to AWS QuickSight to download a CSV file back to our NAS drive where it feeds my Power BI dashboard. I have a gateway setup for cloud to talk to my on-premise NAS drive so auto refresh works. 

Now, my next task: I want to automate the AWS data directly from Power BI so I don’t have to log into their website each week but how do I accomplish this without a programming background? (I majored in Asian History so I don’t know much about data engineering/setting up pipelines)

I read some articles and it seems to indicate that using API can accomplish this but I don’t know Python/SDKs nor do I use CLI (I did some Powershell) and even if I do what services should I use to run CLI for me behind the scenes? Can Power BI make API calls and handle JSON? 

Thanks 🙏 ",1,0,2025-04-02 08:02:21,0,False,False,False,False,2025-04-02 08:02:21,8,Wednesday,166.0,849,60.48,6,225,12.5,0,0,NEGATIVE,-0.9974368214607239,"['work', 'state', 'tools', 'limited', 'week', 'aws', 'quicksight', 'download', 'csv', 'file', 'back', 'nas', 'drive', 'feeds', 'power', 'dashboard', 'gateway', 'setup', 'cloud', 'talk', 'onpremise', 'nas', 'drive', 'auto', 'refresh', 'works', 'next', 'task', 'want', 'automate', 'aws', 'data', 'directly', 'power', 'dont', 'log', 'website', 'week', 'accomplish', 'without', 'programming', 'background', 'majored', 'asian', 'history', 'dont', 'know', 'much', 'data', 'engineeringsetting', 'pipelines', 'read', 'articles', 'seems', 'indicate', 'using', 'api', 'accomplish', 'dont', 'know', 'pythonsdks', 'use', 'cli', 'powershell', 'even', 'services', 'use', 'run', 'cli', 'behind', 'scenes', 'power', 'make', 'api', 'calls', 'handle', 'json', 'thanks']",work state tools limited week aws quicksight download csv file back nas drive feeds power dashboard gateway setup cloud talk onpremise nas drive auto refresh works next task want automate aws data directly power dont log website week accomplish without programming background majored asian history dont know much data engineeringsetting pipelines read articles seems indicate using api accomplish dont know pythonsdks use cli powershell even services use run cli behind scenes power make api calls handle json thanks,Mid,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
123,Is my career choice taking me away from Data engineering jobs ?,"Hello everyone,

First of all English is not my first language so I apologize if there are mistakes or if everything is not clear.

I've been working for 6 years and my career path is not very consistent.  
I started in non-technical positions for 3 years and then moved on to a more technical one. 

For 3 years I had a very diversified job with software development (Php, Python), database management, Linux system administration, a bit of Cloud and a big part of “Data” with ETL flows (Talend) and a lot of SQL. The project was quite large and the team very small, so I was working on several tasks at once.

I really enjoyed the Data part and I got it into my head that I wanted to be a 'real' Data Engineer and not just drag and drop on Talend.

I was just starting my research when a friend of mine contacted me because a software engineer position was opening up in his company. I went through the recruitment process and accepted their proposal.

  
As in my previous position, I'll be working on a lot of things (mobile development, backend, a bit of frontend, cloud, devops) and the salary offered was 20% higher than what I had in my previous job. (I'm now at 48k€ and I don't live in a big city).  
The offer was really attractive and as the market is a bit complicated at the moment, I accepted.

But I'm wondering if this choice will take me even further away from the Data Engineer job i wanted.

Do you find my career path coherent?  
Could I switch back to Data in a few years' time?

Thank you for reading me !",0,16,2025-04-02 07:34:18,0,False,False,False,False,2025-04-02 07:34:18,7,Wednesday,286.0,1528,69.11,15,412,11.7,0,0,NEGATIVE,-0.9992550015449524,"['hello', 'everyone', 'first', 'english', 'first', 'language', 'apologize', 'mistakes', 'everything', 'clear', 'ive', 'working', 'years', 'career', 'path', 'consistent', 'started', 'nontechnical', 'positions', 'years', 'moved', 'technical', 'one', 'years', 'diversified', 'job', 'software', 'development', 'php', 'python', 'database', 'management', 'linux', 'system', 'administration', 'bit', 'cloud', 'big', 'part', 'data', 'etl', 'flows', 'talend', 'lot', 'sql', 'project', 'quite', 'large', 'team', 'small', 'working', 'several', 'tasks', 'really', 'enjoyed', 'data', 'part', 'got', 'head', 'wanted', 'real', 'data', 'engineer', 'drag', 'drop', 'talend', 'starting', 'research', 'friend', 'mine', 'contacted', 'software', 'engineer', 'position', 'opening', 'company', 'went', 'recruitment', 'process', 'accepted', 'proposal', 'previous', 'position', 'ill', 'working', 'lot', 'things', 'mobile', 'development', 'backend', 'bit', 'frontend', 'cloud', 'devops', 'salary', 'offered', 'higher', 'previous', 'job', 'dont', 'live', 'big', 'city', 'offer', 'really', 'attractive', 'market', 'bit', 'complicated', 'moment', 'accepted', 'wondering', 'choice', 'take', 'even', 'away', 'data', 'engineer', 'job', 'wanted', 'find', 'career', 'path', 'coherent', 'could', 'switch', 'back', 'data', 'years', 'time', 'thank', 'reading']",hello everyone first english first language apologize mistakes everything clear ive working years career path consistent started nontechnical positions years moved technical one years diversified job software development php python database management linux system administration bit cloud big part data etl flows talend lot sql project quite large team small working several tasks really enjoyed data part got head wanted real data engineer drag drop talend starting research friend mine contacted software engineer position opening company went recruitment process accepted proposal previous position ill working lot things mobile development backend bit frontend cloud devops salary offered higher previous job dont live big city offer really attractive market bit complicated moment accepted wondering choice take even away data engineer job wanted find career path coherent could switch back data years time thank reading,Low,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
124,Unable to copy data from mysql to azure on Mac,I am trying to load/copy data from a local mysql database in my mac into azure using Data factory. Most of the material i found online suggest to created an integration runtime which requires an installation of an app aimed at windows Os. Is there a way where i could load/copy data from my mysql on mac into azure ?,1,0,2025-04-02 07:13:52,0,False,False,False,False,2025-04-02 07:13:52,7,Wednesday,60.0,316,51.48,3,92,11.9,0,0,NEGATIVE,-0.9993128776550293,"['trying', 'loadcopy', 'data', 'local', 'mysql', 'database', 'mac', 'azure', 'using', 'data', 'factory', 'material', 'found', 'online', 'suggest', 'created', 'integration', 'runtime', 'requires', 'installation', 'app', 'aimed', 'windows', 'way', 'could', 'loadcopy', 'data', 'mysql', 'mac', 'azure']",trying loadcopy data local mysql database mac azure using data factory material found online suggest created integration runtime requires installation app aimed windows way could loadcopy data mysql mac azure,Mid,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
125,Knime on Anaconda Nacigator,Is it possible to install Knime on Anaconda Navigator? ,1,0,2025-04-02 02:25:22,0,False,False,False,False,2025-04-02 02:25:22,2,Wednesday,9.0,55,20.04,1,19,0.0,0,0,NEGATIVE,-0.9977378845214844,"['possible', 'install', 'knime', 'anaconda', 'navigator']",possible install knime anaconda navigator,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
126,I Want To Improve an Internal Process At My Company,"Hey r/dataengineering,

I'm currently transitioning from a software engineering role to data engineering, and I've identified a potential project at my company that I think would be a great learning experience and a chance to introduce some data engineering best practices.

Project Overview:

We have a dashboard that displays employee utilization data, sourced from two main systems: Harvest (time tracking) and Forecast (projected utilization).

Current Process:

* Harvest Data: Currently, we're using cron jobs running on an EC2 instance to periodically pull data from Harvest.
* Forecast Data: Due to the lack of an API, we're relying on Playwright (web scraping) to extract data from their web reports, which are then saved to S3.
* Data Processing: Another cron job on EC2 processes the S3 reports and loads the data into a PostgreSQL database.
* Dashboard: A custom frontend application (using Azure OAuth) queries the PostgreSQL database to display the utilization data.

Proposed Solution:

I'm proposing a serverless architecture on AWS, using the following components:

* API Gateway + Lambda: To create a robust API for our frontend application.
* Lambda for ETL: To automate data extraction, transformation, and loading from Harvest and Forecast.
* AWS Step Functions: To orchestrate the data pipeline and manage dependencies.
* Amazon RDS PostgreSQL: To serve as our data warehouse for analytical queries.
* API Gateway Authorizer: To integrate Azure OAuth authentication.
* CI/CD with CodePipeline and CodeBuild: To automate testing and deployment.
* Docker and SAM CLI: For local development and testing.

My Goals:

* Gain hands-on experience with AWS serverless technologies.
* Implement data engineering best practices for ETL and data warehousing.
* Improve the reliability and scalability of our data pipeline.
* Potentially expand this architecture to serve as a central data warehouse for other company analytical data.

My Questions:

1. For those with experience in similar projects, what are some key considerations or potential challenges I should be aware of?
2. Any advice on best practices for designing and implementing a serverless data pipeline on AWS?
3. Are there any specific AWS services or tools that you would recommend for this project?
4. How would you recommend getting started on a project like this, what would you focus on first?
5. What would be some good ways to test this type of system?

I'm eager to learn and contribute, and I appreciate any insights or advice you can offer.

Thanks!",0,3,2025-04-02 14:57:52,0,False,False,False,False,2025-04-02 14:57:52,14,Wednesday,400.0,2537,38.32,24,688,13.3,0,1,POSITIVE,0.9837194085121155,"['hey', 'rdataengineering', 'currently', 'transitioning', 'software', 'engineering', 'role', 'data', 'engineering', 'ive', 'identified', 'potential', 'project', 'company', 'think', 'would', 'great', 'learning', 'experience', 'chance', 'introduce', 'data', 'engineering', 'best', 'practices', 'project', 'overview', 'dashboard', 'displays', 'employee', 'utilization', 'data', 'sourced', 'two', 'main', 'systems', 'harvest', 'time', 'tracking', 'forecast', 'projected', 'utilization', 'current', 'process', 'harvest', 'data', 'currently', 'using', 'cron', 'jobs', 'running', 'instance', 'periodically', 'pull', 'data', 'harvest', 'forecast', 'data', 'due', 'lack', 'api', 'relying', 'playwright', 'web', 'scraping', 'extract', 'data', 'web', 'reports', 'saved', 'data', 'processing', 'another', 'cron', 'job', 'processes', 'reports', 'loads', 'data', 'postgresql', 'database', 'dashboard', 'custom', 'frontend', 'application', 'using', 'azure', 'oauth', 'queries', 'postgresql', 'database', 'display', 'utilization', 'data', 'proposed', 'solution', 'proposing', 'serverless', 'architecture', 'aws', 'using', 'following', 'components', 'api', 'gateway', 'lambda', 'create', 'robust', 'api', 'frontend', 'application', 'lambda', 'etl', 'automate', 'data', 'extraction', 'transformation', 'loading', 'harvest', 'forecast', 'aws', 'step', 'functions', 'orchestrate', 'data', 'pipeline', 'manage', 'dependencies', 'amazon', 'rds', 'postgresql', 'serve', 'data', 'warehouse', 'analytical', 'queries', 'api', 'gateway', 'authorizer', 'integrate', 'azure', 'oauth', 'authentication', 'cicd', 'codepipeline', 'codebuild', 'automate', 'testing', 'deployment', 'docker', 'sam', 'cli', 'local', 'development', 'testing', 'goals', 'gain', 'handson', 'experience', 'aws', 'serverless', 'technologies', 'implement', 'data', 'engineering', 'best', 'practices', 'etl', 'data', 'warehousing', 'improve', 'reliability', 'scalability', 'data', 'pipeline', 'potentially', 'expand', 'architecture', 'serve', 'central', 'data', 'warehouse', 'company', 'analytical', 'data', 'questions', 'experience', 'similar', 'projects', 'key', 'considerations', 'potential', 'challenges', 'aware', 'advice', 'best', 'practices', 'designing', 'implementing', 'serverless', 'data', 'pipeline', 'aws', 'specific', 'aws', 'services', 'tools', 'would', 'recommend', 'project', 'would', 'recommend', 'getting', 'started', 'project', 'like', 'would', 'focus', 'first', 'would', 'good', 'ways', 'test', 'type', 'system', 'eager', 'learn', 'contribute', 'appreciate', 'insights', 'advice', 'offer', 'thanks']",hey rdataengineering currently transitioning software engineering role data engineering ive identified potential project company think would great learning experience chance introduce data engineering best practices project overview dashboard displays employee utilization data sourced two main systems harvest time tracking forecast projected utilization current process harvest data currently using cron jobs running instance periodically pull data harvest forecast data due lack api relying playwright web scraping extract data web reports saved data processing another cron job processes reports loads data postgresql database dashboard custom frontend application using azure oauth queries postgresql database display utilization data proposed solution proposing serverless architecture aws using following components api gateway lambda create robust api frontend application lambda etl automate data extraction transformation loading harvest forecast aws step functions orchestrate data pipeline manage dependencies amazon rds postgresql serve data warehouse analytical queries api gateway authorizer integrate azure oauth authentication cicd codepipeline codebuild automate testing deployment docker sam cli local development testing goals gain handson experience aws serverless technologies implement data engineering best practices etl data warehousing improve reliability scalability data pipeline potentially expand architecture serve central data warehouse company analytical data questions experience similar projects key considerations potential challenges aware advice best practices designing implementing serverless data pipeline aws specific aws services tools would recommend project would recommend getting started project like would focus first would good ways test type system eager learn contribute appreciate insights advice offer thanks,Low,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
127,"How would you solve a low-tech, distributed attendance tracking and service impact problem for a nonprofit with no digital infrastructure?","I’m working with a nonprofit, supporting 17 veteran communities. The communities aren’t brick-and-mortar — they meet at churches and community spaces, and track attendance manually. There’s very little technology — no computers, mostly just phones and Facebook.

They want to understand:
	•	What services are being offered at the community level
	•	Who’s attending (recurring vs new)
	•	No-show rates
	•	Cost per veteran for services

The challenge: no digital systems or staff capacity for manual data entry.

What tech-light solutions or data collection flows would you recommend to gather this info and make it analyzable? Bonus if it can integrate later with HubSpot or a simple PostgreSQL DB.",0,2,2025-04-02 13:01:43,0,False,False,False,False,2025-04-02 13:01:43,13,Wednesday,109.0,697,37.1,6,189,15.0,0,1,NEGATIVE,-0.9838479161262512,"['working', 'nonprofit', 'supporting', 'veteran', 'communities', 'communities', 'arent', 'brickandmortar', 'meet', 'churches', 'community', 'spaces', 'track', 'attendance', 'manually', 'theres', 'little', 'technology', 'computers', 'mostly', 'phones', 'facebook', 'want', 'understand', 'services', 'offered', 'community', 'level', 'whos', 'attending', 'recurring', 'new', 'noshow', 'rates', 'cost', 'per', 'veteran', 'services', 'challenge', 'digital', 'systems', 'staff', 'capacity', 'manual', 'data', 'entry', 'techlight', 'solutions', 'data', 'collection', 'flows', 'would', 'recommend', 'gather', 'info', 'make', 'analyzable', 'bonus', 'integrate', 'later', 'hubspot', 'simple', 'postgresql']",working nonprofit supporting veteran communities communities arent brickandmortar meet churches community spaces track attendance manually theres little technology computers mostly phones facebook want understand services offered community level whos attending recurring new noshow rates cost per veteran services challenge digital systems staff capacity manual data entry techlight solutions data collection flows would recommend gather info make analyzable bonus integrate later hubspot simple postgresql,Low,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
128,Data Developer vs Data Engineer,"I know it varies by company blah blah blah, but also aside from a Google search, what have you guys in the field noticed to be core differences between these positions? ",0,3,2025-04-01 23:53:39,0,False,False,False,False,2025-04-01 23:53:39,23,Tuesday,31.0,169,56.93,1,44,0.0,0,0,NEGATIVE,-0.975967526435852,"['know', 'varies', 'company', 'blah', 'blah', 'blah', 'also', 'aside', 'google', 'search', 'guys', 'field', 'noticed', 'core', 'differences', 'positions']",know varies company blah blah blah also aside google search guys field noticed core differences positions,Low,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
129,How AI will dramatically change DE,"After some struggle with a pipeline today, Gemini 2.5 one-shotted the solution. It's superior in most software problems compared to humans (check coders eval) and we're just two and a half years in.

The capabilities are mind-bending. Data engineering as we know it will change drastically with new AI tooling and self-adjusting infrastructure.

We know this profession will evolve drastically. What do you think where things are heading and how to hedge against AI? Become more social / human I guess 😂

A few hypotheses:
- pipelines and infra manages itself with much higher accuracy and less misconfigurations
- the data engineer profile will shift, they become subject matter experts, they must understand the business and do product management
- technical skills do not matter since the gap from idiot to genius is much smaller than from genius to agi/asi",0,8,2025-04-02 20:34:45,0,False,False,False,False,2025-04-02 20:34:45,20,Wednesday,140.0,860,54.32,8,219,12.2,0,0,POSITIVE,0.9916075468063354,"['struggle', 'pipeline', 'today', 'gemini', 'oneshotted', 'solution', 'superior', 'software', 'problems', 'compared', 'humans', 'check', 'coders', 'eval', 'two', 'half', 'years', 'capabilities', 'mindbending', 'data', 'engineering', 'know', 'change', 'drastically', 'new', 'tooling', 'selfadjusting', 'infrastructure', 'know', 'profession', 'evolve', 'drastically', 'think', 'things', 'heading', 'hedge', 'become', 'social', 'human', 'guess', 'hypotheses', 'pipelines', 'infra', 'manages', 'much', 'higher', 'accuracy', 'less', 'misconfigurations', 'data', 'engineer', 'profile', 'shift', 'become', 'subject', 'matter', 'experts', 'must', 'understand', 'business', 'product', 'management', 'technical', 'skills', 'matter', 'since', 'gap', 'idiot', 'genius', 'much', 'smaller', 'genius', 'agiasi']",struggle pipeline today gemini oneshotted solution superior software problems compared humans check coders eval two half years capabilities mindbending data engineering know change drastically new tooling selfadjusting infrastructure know profession evolve drastically think things heading hedge become social human guess hypotheses pipelines infra manages much higher accuracy less misconfigurations data engineer profile shift become subject matter experts must understand business product management technical skills matter since gap idiot genius much smaller genius agiasi,Low,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
130,Want to know Data engineering hiring trend at present in India,"Until about a month ago hiring seemed to be freezed - lot of fake job postings, people posting google form links collecting resumes, reposting old job roles on linkedin...  Then since about three weeks ago, it seemed like hring is restarted. But now I am having my doubts again - ghosted by recruiters after first screening even told me my CV fits the role well. And not getting other shortlists too. Another thing is huge range of experience 3 yrs - 7 yrs , 2 yrs to 9 yrs experience being posted for majority of the JDs. Obviously if a 7 yrs candidate and if a 3 yrs candidate applies to the same role, they would prefer the 7 yrs exp candidate. What's going on these days? Are they not hiring anyone below 6/7 yrs work exp at all?",0,4,2025-04-02 08:21:32,0,False,False,False,False,2025-04-02 08:21:32,8,Wednesday,139.0,733,71.24,8,194,10.4,0,0,NEGATIVE,-0.9919129610061646,"['month', 'ago', 'hiring', 'seemed', 'freezed', 'lot', 'fake', 'job', 'postings', 'people', 'posting', 'google', 'form', 'links', 'collecting', 'resumes', 'reposting', 'old', 'job', 'roles', 'linkedin', 'since', 'three', 'weeks', 'ago', 'seemed', 'like', 'hring', 'restarted', 'doubts', 'ghosted', 'recruiters', 'first', 'screening', 'even', 'told', 'fits', 'role', 'well', 'getting', 'shortlists', 'another', 'thing', 'huge', 'range', 'experience', 'yrs', 'yrs', 'yrs', 'yrs', 'experience', 'posted', 'majority', 'jds', 'obviously', 'yrs', 'candidate', 'yrs', 'candidate', 'applies', 'role', 'would', 'prefer', 'yrs', 'exp', 'candidate', 'whats', 'going', 'days', 'hiring', 'anyone', 'yrs', 'work', 'exp']",month ago hiring seemed freezed lot fake job postings people posting google form links collecting resumes reposting old job roles linkedin since three weeks ago seemed like hring restarted doubts ghosted recruiters first screening even told fits role well getting shortlists another thing huge range experience yrs yrs yrs yrs experience posted majority jds obviously yrs candidate yrs candidate applies role would prefer yrs exp candidate whats going days hiring anyone yrs work exp,Low,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
131,What's the non-technical biggest barrier you face at work?,"What’s currently challenging for me is getting access to things.

I design a data pipeline, present it to the team that will benefit from it, and everyone gets super excited.

Then I reach out to the internal department or an external party to either grant me admin access to the platform I need, or to help me obtain an API.

A week goes by—nothing. I follow up via email. Eventually, someone replies and says it's not possible to give me admin credentials. Fine. So I ask, “Can you help me get the API instead? It’s very straightforward.”

Another week goes by—still nothing. I send another follow-up…

Now the other person is kind of frustrated (because I’m asking them to do something slightly different, even though I’m offering guidance).

What follows is just a back-and-forth with long, frustrating waiting periods in between. Meanwhile, the team I presented the pipeline or project to starts getting frustrated with me and probably thinks I’m full of crap.

Once I finally get the damn API or whatever access I needed, I complete the project in 1–2 days but delayed by weeks or even months.

Aaaaaaah!",48,17,2025-04-04 02:23:50,0,False,False,False,False,2025-04-04 02:23:50,2,Friday,190.0,1110,65.12,13,293,11.5,0,0,NEGATIVE,-0.9957934617996216,"['whats', 'currently', 'challenging', 'getting', 'access', 'things', 'design', 'data', 'pipeline', 'present', 'team', 'benefit', 'everyone', 'gets', 'super', 'excited', 'reach', 'internal', 'department', 'external', 'party', 'either', 'grant', 'admin', 'access', 'platform', 'need', 'help', 'obtain', 'api', 'week', 'goes', 'bynothing', 'follow', 'via', 'email', 'eventually', 'someone', 'replies', 'says', 'possible', 'give', 'admin', 'credentials', 'fine', 'ask', 'help', 'get', 'api', 'instead', 'straightforward', 'another', 'week', 'goes', 'bystill', 'nothing', 'send', 'another', 'followup', 'person', 'kind', 'frustrated', 'asking', 'something', 'slightly', 'different', 'even', 'though', 'offering', 'guidance', 'follows', 'backandforth', 'long', 'frustrating', 'waiting', 'periods', 'meanwhile', 'team', 'presented', 'pipeline', 'project', 'starts', 'getting', 'frustrated', 'probably', 'thinks', 'full', 'crap', 'finally', 'get', 'damn', 'api', 'whatever', 'access', 'needed', 'complete', 'project', 'days', 'delayed', 'weeks', 'even', 'months', 'aaaaaaah']",whats currently challenging getting access things design data pipeline present team benefit everyone gets super excited reach internal department external party either grant admin access platform need help obtain api week goes bynothing follow via email eventually someone replies says possible give admin credentials fine ask help get api instead straightforward another week goes bystill nothing send another followup person kind frustrated asking something slightly different even though offering guidance follows backandforth long frustrating waiting periods meanwhile team presented pipeline project starts getting frustrated probably thinks full crap finally get damn api whatever access needed complete project days delayed weeks even months aaaaaaah,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
132,Are Hyperscalers becoming more expensive in Europe due to the tariffs?,"Hi,

With the recent tariffs in mind, are cloud providers like AWS, Azure, and Google Cloud becoming more expensive for European companies? And what about other techs like Snowflake or Databricks – are they affected too?

Would it be wise for European businesses to consider open-source alternatives, both for cost and strategic independence?

And from a personal perspective: should we, as employees, expand our skill sets toward open-source tech stacks to stay future-proof?",30,26,2025-04-04 07:31:32,0,False,False,False,False,2025-04-04 07:31:32,7,Friday,73.0,476,44.75,4,120,14.6,0,0,NEGATIVE,-0.9742586016654968,"['recent', 'tariffs', 'mind', 'cloud', 'providers', 'like', 'aws', 'azure', 'google', 'cloud', 'becoming', 'expensive', 'european', 'companies', 'techs', 'like', 'snowflake', 'databricks', 'affected', 'would', 'wise', 'european', 'businesses', 'consider', 'opensource', 'alternatives', 'cost', 'strategic', 'independence', 'personal', 'perspective', 'employees', 'expand', 'skill', 'sets', 'toward', 'opensource', 'tech', 'stacks', 'stay', 'futureproof']",recent tariffs mind cloud providers like aws azure google cloud becoming expensive european companies techs like snowflake databricks affected would wise european businesses consider opensource alternatives cost strategic independence personal perspective employees expand skill sets toward opensource tech stacks stay futureproof,High,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
133,Which tool do you use to move data from the cloud to Snowflake?,"Hey, r/dataengineering 

I’m working on a project where I need to move data from our cloud-hosted databases into Snowflake, and I’m trying to figure out the best tool for the job. Ideally, I’d like something that’s cost-effective and scales well. 

If you’ve done this before, what did you use?
Would love to hear about your experience—how reliable it is, how much it roughly costs, and any pros/cons you’ve noticed. Appreciate any insights!

[View Poll](https://www.reddit.com/poll/1jr70yg)",7,12,2025-04-04 08:30:40,0,False,False,False,False,2025-04-04 08:30:40,8,Friday,74.0,491,56.45,5,120,10.8,1,0,POSITIVE,0.9951165914535522,"['hey', 'rdataengineering', 'working', 'project', 'need', 'move', 'data', 'cloudhosted', 'databases', 'snowflake', 'trying', 'figure', 'best', 'tool', 'job', 'ideally', 'like', 'something', 'thats', 'costeffective', 'scales', 'well', 'youve', 'done', 'use', 'would', 'love', 'hear', 'experiencehow', 'reliable', 'much', 'roughly', 'costs', 'proscons', 'youve', 'noticed', 'appreciate', 'insights', 'view', 'pollhttpswwwredditcompolljryg']",hey rdataengineering working project need move data cloudhosted databases snowflake trying figure best tool job ideally like something thats costeffective scales well youve done use would love hear experiencehow reliable much roughly costs proscons youve noticed appreciate insights view pollhttpswwwredditcompolljryg,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
134,Data Engineer Consulting Rate?,"I currently work as a mid-level DE (3y) and I’ve recently been offered an opportunity in Consulting. I’m clueless what rate I should ask for. Should it be 25% more than what I currently earn? 50% more? Double!? 

I know that leaping into consulting means compromising job stability and higher expectations for deliveries, so I want to ask for a much higher rate without high or low balling a ridiculous offer. Does someone have experience going from DE to consultant DE? Thanks!",5,16,2025-04-04 16:11:28,0,False,False,False,False,2025-04-04 16:11:28,16,Friday,82.0,478,54.83,5,130,12.7,0,0,NEGATIVE,-0.9897195100784302,"['currently', 'work', 'midlevel', 'ive', 'recently', 'offered', 'opportunity', 'consulting', 'clueless', 'rate', 'ask', 'currently', 'earn', 'double', 'know', 'leaping', 'consulting', 'means', 'compromising', 'job', 'stability', 'higher', 'expectations', 'deliveries', 'want', 'ask', 'much', 'higher', 'rate', 'without', 'high', 'low', 'balling', 'ridiculous', 'offer', 'someone', 'experience', 'going', 'consultant', 'thanks']",currently work midlevel ive recently offered opportunity consulting clueless rate ask currently earn double know leaping consulting means compromising job stability higher expectations deliveries want ask much higher rate without high low balling ridiculous offer someone experience going consultant thanks,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
135,Logging in Spark applications.,"Hi guys, i am moving to on-prem managed Spark applications with Kuberenetes. I am wondering what do u use for logging? I am talking about Python and PySpark. Do u setup log4j? Or just use Python's logging library for application? What is the standard here? I have not seen much about log4j within PySpark.",5,2,2025-04-04 15:01:43,0,False,False,False,False,2025-04-04 15:01:43,15,Friday,54.0,305,72.12,7,79,7.4,0,0,NEGATIVE,-0.9993705153465271,"['guys', 'moving', 'onprem', 'managed', 'spark', 'applications', 'kuberenetes', 'wondering', 'use', 'logging', 'talking', 'python', 'pyspark', 'setup', 'logj', 'use', 'pythons', 'logging', 'library', 'application', 'standard', 'seen', 'much', 'logj', 'within', 'pyspark']",guys moving onprem managed spark applications kuberenetes wondering use logging talking python pyspark setup logj use pythons logging library application standard seen much logj within pyspark,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
136,Anyone know of any vscode linter for sql that can accommodate pyspark sql?,"In pyspark 3.4 you can write sql as 

spark.sql(SELECT * FROM {df_input}, df_input = df_input) 

The popular sql linters I tried SQL Formatter and and Prettier SQL Vscode currently does not accommodate{}. Does anyone know of any linters that does? Thank you",5,0,2025-04-04 14:54:19,1,False,False,False,False,2025-04-04 14:54:19,14,Friday,42.0,257,44.41,4,72,12.6,0,1,NEGATIVE,-0.9986238479614258,"['pyspark', 'write', 'sql', 'sparksqlselect', 'dfinput', 'dfinput', 'dfinput', 'popular', 'sql', 'linters', 'tried', 'sql', 'formatter', 'prettier', 'sql', 'vscode', 'currently', 'accommodate', 'anyone', 'know', 'linters', 'thank']",pyspark write sql sparksqlselect dfinput dfinput dfinput popular sql linters tried sql formatter prettier sql vscode currently accommodate anyone know linters thank,Mid,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
137,Faster way to view + debug data,"Hi r/dataengineering!

  
I wanted to share a project that I have been working on. It's an intuitive data editor where you can interact with local and remote data (e.g. Athena & BigQuery). For several important tasks, it can speed you up by 10x or more. (see website for more)

  
For data engineering specifically, this would be really useful in debugging pipelines, cleaning local or remote data, and being able to easy create new tables within data warehouses etc.

I know this could be a lot faster than having to type everything out, especially if you're just poking around. I personally find myself using this before trying any manual work.

Also, for those doing complex queries, you can split them up and work with the frame visually and add queries when needed. Super useful for when you want to iteratively build an analysis or new frame ***without writing a super long query.***

  
As for data size, it can handle local data up to around 1B rows, and remote data is only limited by your data warehouse.

  
You don't have to migrate *anything* either.

  
If you're interested, you can check it out here: [https://www.cocoalemana.com](https://www.cocoalemana.com)

  
I'd love to hear about your workflow, and see what we can change to make it cover more data engineering use cases.

  
Cheers!

[Coco Alemana](https://preview.redd.it/02wogjj72rse1.jpg?width=3820&format=pjpg&auto=webp&s=0905bd40927b4dd7e80521568982ebe82994a5fe)

",4,3,2025-04-04 05:00:10,0,False,False,False,False,2025-04-04 05:00:10,5,Friday,217.0,1443,44.75,12,361,11.2,1,0,POSITIVE,0.9925779700279236,"['rdataengineering', 'wanted', 'share', 'project', 'working', 'intuitive', 'data', 'editor', 'interact', 'local', 'remote', 'data', 'athena', 'bigquery', 'several', 'important', 'tasks', 'speed', 'see', 'website', 'data', 'engineering', 'specifically', 'would', 'really', 'useful', 'debugging', 'pipelines', 'cleaning', 'local', 'remote', 'data', 'able', 'easy', 'create', 'new', 'tables', 'within', 'data', 'warehouses', 'etc', 'know', 'could', 'lot', 'faster', 'type', 'everything', 'especially', 'youre', 'poking', 'around', 'personally', 'find', 'using', 'trying', 'manual', 'work', 'also', 'complex', 'queries', 'split', 'work', 'frame', 'visually', 'add', 'queries', 'needed', 'super', 'useful', 'want', 'iteratively', 'build', 'analysis', 'new', 'frame', 'without', 'writing', 'super', 'long', 'query', 'data', 'size', 'handle', 'local', 'data', 'around', 'rows', 'remote', 'data', 'limited', 'data', 'warehouse', 'dont', 'migrate', 'anything', 'either', 'youre', 'interested', 'check', 'httpswwwcocoalemanacomhttpswwwcocoalemanacom', 'love', 'hear', 'workflow', 'see', 'change', 'make', 'cover', 'data', 'engineering', 'use', 'cases', 'cheers', 'coco', 'alemanahttpspreviewredditwogjjrsejpgwidthformatpjpgautowebpsbdbddeebeafe']",rdataengineering wanted share project working intuitive data editor interact local remote data athena bigquery several important tasks speed see website data engineering specifically would really useful debugging pipelines cleaning local remote data able easy create new tables within data warehouses etc know could lot faster type everything especially youre poking around personally find using trying manual work also complex queries split work frame visually add queries needed super useful want iteratively build analysis new frame without writing super long query data size handle local data around rows remote data limited data warehouse dont migrate anything either youre interested check httpswwwcocoalemanacomhttpswwwcocoalemanacom love hear workflow see change make cover data engineering use cases cheers coco alemanahttpspreviewredditwogjjrsejpgwidthformatpjpgautowebpsbdbddeebeafe,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
138,How to stream results of a complex SQL query,"Hello,

I'm writing you because I have a problem with a side project and maybe here somebody can help me. I have to run a complex query with a potentially high number of results and it takes a lot of time. However, for my project I don't need all the results to be showed together, perhaps after some hours/days. It would be much more useful to get a stream of the partial results in real time. How can I achieve this? I would prefer to use free software, however please suggest me any solution you have in mind.

Thank you in advance!",3,12,2025-04-04 07:35:07,0,False,False,False,False,2025-04-04 07:35:07,7,Friday,102.0,535,73.58,7,139,8.4,0,0,NEGATIVE,-0.9974018335342407,"['hello', 'writing', 'problem', 'side', 'project', 'maybe', 'somebody', 'help', 'run', 'complex', 'query', 'potentially', 'high', 'number', 'results', 'takes', 'lot', 'time', 'however', 'project', 'dont', 'need', 'results', 'showed', 'together', 'perhaps', 'hoursdays', 'would', 'much', 'useful', 'get', 'stream', 'partial', 'results', 'real', 'time', 'achieve', 'would', 'prefer', 'use', 'free', 'software', 'however', 'please', 'suggest', 'solution', 'mind', 'thank', 'advance']",hello writing problem side project maybe somebody help run complex query potentially high number results takes lot time however project dont need results showed together perhaps hoursdays would much useful get stream partial results real time achieve would prefer use free software however please suggest solution mind thank advance,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
139,How do I get out of this rut,"I’m currently about the finish an early career rotational program with a top 10 bank. The rotation I am currently on and where the company is placing me post program (I tried to get placed somewhere else) is as a data engineer on a data delivery team. When I was advertised this rotation and the team I was told pretty specifically we would be using all the relevant technologies and I would be very hands on keyboard building pipelines with python , configuring cloud services and snowflake, being a part of data modeling. Mind you I’m not completely new I have experience with all this in personal projects and previous work experience as a SWE and researcher in college. 

Turns out all of that was a lie. I later learned there is an army of contractors that do the actual work. I was stuck with analyzing .egp files and other SAS files documenting it and handing off to consultants to rebuild in Talend to ingest into snowflake. The only tech that I use is Visio and Word.

I coped with that by saying after I’m out of the program I’ll get to do the actual work. But I had a conversation with my manager today about what my role will be post program. He basically said there are a lot more of these SAS procedures they are porting over to talend and snowflake and I’ll be documenting them and handing over to contractors so they can implement the new process. Honestly that is all really quick and easy to do because there isn’t that much complicated business logic for the LOBs we support just joins and the occasional aggregation so most days I’m not doing anything.

When I told him I would really like to be involved in the technical work or the data modeling , he said that is not my job anymore and that is what we pay the contractors to do so I can’t do it. Almost made it seem like I should be grateful and he is doing me a favor somehow.

It just feels like I was misled or even outright lied to about the position. We don’t use any of the technologies that were advertised (Drag and drop/low code tools seem like fake engineering), I don’t get to be hands on keyboard at all. Just seems like there really I no growth or opportunity in this role. I would leave but I took relocation and a signing bonus for this and if I leave too early I owe it back. I also can’t internally transfer anywhere for a year after starting my new role.

I guess my rant is just to ask what should I be doing in this situation? I work on personal projects and open source and I have gotten a few certs in the downtime at work but I don’t know if it’s enough to make sure my skills don’t atrophy while I wait out my repayment period. I consider myself a somewhat technical guy but I have been boxed into a non technical role.

",3,6,2025-04-04 03:13:53,0,False,2025-04-04 09:08:09,False,False,2025-04-04 03:13:53,3,Friday,514.0,2718,65.76,23,736,12.3,0,0,NEGATIVE,-0.9935377240180969,"['currently', 'finish', 'early', 'career', 'rotational', 'program', 'top', 'bank', 'rotation', 'currently', 'company', 'placing', 'post', 'program', 'tried', 'get', 'placed', 'somewhere', 'else', 'data', 'engineer', 'data', 'delivery', 'team', 'advertised', 'rotation', 'team', 'told', 'pretty', 'specifically', 'would', 'using', 'relevant', 'technologies', 'would', 'hands', 'keyboard', 'building', 'pipelines', 'python', 'configuring', 'cloud', 'services', 'snowflake', 'part', 'data', 'modeling', 'mind', 'completely', 'new', 'experience', 'personal', 'projects', 'previous', 'work', 'experience', 'swe', 'researcher', 'college', 'turns', 'lie', 'later', 'learned', 'army', 'contractors', 'actual', 'work', 'stuck', 'analyzing', 'egp', 'files', 'sas', 'files', 'documenting', 'handing', 'consultants', 'rebuild', 'talend', 'ingest', 'snowflake', 'tech', 'use', 'visio', 'word', 'coped', 'saying', 'program', 'ill', 'get', 'actual', 'work', 'conversation', 'manager', 'today', 'role', 'post', 'program', 'basically', 'said', 'lot', 'sas', 'procedures', 'porting', 'talend', 'snowflake', 'ill', 'documenting', 'handing', 'contractors', 'implement', 'new', 'process', 'honestly', 'really', 'quick', 'easy', 'isnt', 'much', 'complicated', 'business', 'logic', 'lobs', 'support', 'joins', 'occasional', 'aggregation', 'days', 'anything', 'told', 'would', 'really', 'like', 'involved', 'technical', 'work', 'data', 'modeling', 'said', 'job', 'anymore', 'pay', 'contractors', 'cant', 'almost', 'made', 'seem', 'like', 'grateful', 'favor', 'somehow', 'feels', 'like', 'misled', 'even', 'outright', 'lied', 'position', 'dont', 'use', 'technologies', 'advertised', 'drag', 'droplow', 'code', 'tools', 'seem', 'like', 'fake', 'engineering', 'dont', 'get', 'hands', 'keyboard', 'seems', 'like', 'really', 'growth', 'opportunity', 'role', 'would', 'leave', 'took', 'relocation', 'signing', 'bonus', 'leave', 'early', 'owe', 'back', 'also', 'cant', 'internally', 'transfer', 'anywhere', 'year', 'starting', 'new', 'role', 'guess', 'rant', 'ask', 'situation', 'work', 'personal', 'projects', 'open', 'source', 'gotten', 'certs', 'downtime', 'work', 'dont', 'know', 'enough', 'make', 'sure', 'skills', 'dont', 'atrophy', 'wait', 'repayment', 'period', 'consider', 'somewhat', 'technical', 'guy', 'boxed', 'non', 'technical', 'role']",currently finish early career rotational program top bank rotation currently company placing post program tried get placed somewhere else data engineer data delivery team advertised rotation team told pretty specifically would using relevant technologies would hands keyboard building pipelines python configuring cloud services snowflake part data modeling mind completely new experience personal projects previous work experience swe researcher college turns lie later learned army contractors actual work stuck analyzing egp files sas files documenting handing consultants rebuild talend ingest snowflake tech use visio word coped saying program ill get actual work conversation manager today role post program basically said lot sas procedures porting talend snowflake ill documenting handing contractors implement new process honestly really quick easy isnt much complicated business logic lobs support joins occasional aggregation days anything told would really like involved technical work data modeling said job anymore pay contractors cant almost made seem like grateful favor somehow feels like misled even outright lied position dont use technologies advertised drag droplow code tools seem like fake engineering dont get hands keyboard seems like really growth opportunity role would leave took relocation signing bonus leave early owe back also cant internally transfer anywhere year starting new role guess rant ask situation work personal projects open source gotten certs downtime work dont know enough make sure skills dont atrophy wait repayment period consider somewhat technical guy boxed non technical role,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
140,"Built a real-time e-commerce data pipeline with Kinesis, Spark, Redshift & QuickSight — looking for feedback","I recently completed a real-time ETL pipeline project as part of my data engineering portfolio, and I’d love to share it here and get some feedback from the community.

# What it does:

* Streams transactional data using **Amazon Kinesis**
* Backs up raw data in **S3** (Parquet format)
* Processes and transforms data with **Apache Spark**
* Loads the transformed data into **Redshift Serverless**
* Orchestrates the pipeline with **Apache Airflow (Docker)**
* Visualizes insights through a **QuickSight dashboard**

# Key Metrics Visualized:

* Total Revenue
* Orders Over Time
* Average Order Value
* Top Products
* Revenue by Category (donut chart)

I built this to practice real-time ingestion, transformation, and visualization in a scalable, production-like setup using AWS-native services.

# GitHub Repo:

[https://github.com/amanuel496/real-time-ecommerce-etl-pipeline](https://github.com/amanuel496/real-time-ecommerce-etl-pipeline)

If you have any thoughts on how to improve the architecture, scale it better, or handle ops/monitoring more effectively, I’d love to hear your input.

Thanks!",5,6,2025-04-04 02:55:36,0,False,False,False,False,2025-04-04 02:55:36,2,Friday,152.0,1103,11.08,4,265,18.8,1,0,NEGATIVE,-0.5143194198608398,"['recently', 'completed', 'realtime', 'etl', 'pipeline', 'project', 'part', 'data', 'engineering', 'portfolio', 'love', 'share', 'get', 'feedback', 'community', 'streams', 'transactional', 'data', 'using', 'amazon', 'kinesis', 'backs', 'raw', 'data', 'parquet', 'format', 'processes', 'transforms', 'data', 'apache', 'spark', 'loads', 'transformed', 'data', 'redshift', 'serverless', 'orchestrates', 'pipeline', 'apache', 'airflow', 'docker', 'visualizes', 'insights', 'quicksight', 'dashboard', 'key', 'metrics', 'visualized', 'total', 'revenue', 'orders', 'time', 'average', 'order', 'value', 'top', 'products', 'revenue', 'category', 'donut', 'chart', 'built', 'practice', 'realtime', 'ingestion', 'transformation', 'visualization', 'scalable', 'productionlike', 'setup', 'using', 'awsnative', 'services', 'github', 'repo', 'httpsgithubcomamanuelrealtimeecommerceetlpipelinehttpsgithubcomamanuelrealtimeecommerceetlpipeline', 'thoughts', 'improve', 'architecture', 'scale', 'better', 'handle', 'opsmonitoring', 'effectively', 'love', 'hear', 'input', 'thanks']",recently completed realtime etl pipeline project part data engineering portfolio love share get feedback community streams transactional data using amazon kinesis backs raw data parquet format processes transforms data apache spark loads transformed data redshift serverless orchestrates pipeline apache airflow docker visualizes insights quicksight dashboard key metrics visualized total revenue orders time average order value top products revenue category donut chart built practice realtime ingestion transformation visualization scalable productionlike setup using awsnative services github repo httpsgithubcomamanuelrealtimeecommerceetlpipelinehttpsgithubcomamanuelrealtimeecommerceetlpipeline thoughts improve architecture scale better handle opsmonitoring effectively love hear input thanks,Mid,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
141,"Airbyte Connector Builder now supports GraphQL, Async Requests and Custom Components","Hello, Marcos from the Airbyte Team.

For those who may not be familiar, Airbyte is an open-source data integration (EL) platform with over 500 connectors for APIs, databases, and file storage.

In our last release we added several new features to our no-code Connector Builder:

* [GraphQL Support](https://docs.airbyte.com/connector-development/config-based/understanding-the-yaml-file/request-options#graphql-request-injection): In addition to REST, you can now make requests to GraphQL APIs (and properly handle pagination!)
* [Async Data Requests](https://docs.airbyte.com/connector-development/connector-builder-ui/async-streams): There are some reporting APIs that do not return responses immediately. For instance, with Google Ads.  You can now request a custom report from these sources and wait for the report to be processed and downloaded.
* [Custom Python Code Components](https://docs.airbyte.com/connector-development/connector-builder-ui/custom-components): We recognize that some APIs behave uniquely—for example, by returning records as key-value pairs instead of arrays or by not ordering data correctly. To address these cases, our open-source platform now supports custom Python components that extend the capabilities of the no-code framework without blocking you from building your connector.

We believe these updates will make connector development faster and more accessible, helping you get the most out of your data integration projects.

We understand there are discussions about the trade-offs between no-code and low-code solutions. At Airbyte, transitioning from fully coded connectors to a low-code approach allowed us to maintain a large connector catalog using standard components.  We were also able to create a better build and test process directly in the UI. Users frequently give us the feedback that the no-code connector Builder enables less technical users to create and ship connectors. This reduces the workload on senior data engineers allowing them to focus on critical data pipelines.

Something else that has been top of mind is speed and performance. With a robust and stable connector framework, the engineering team has been dedicating significant resources to introduce concurrency to enhance sync speed. You can read this[ blog post](https://airbyte.com/blog/improving-connector-sync-speed-up-to-10x-faster) about how the team implemented concurrency in the Klaviyo connector, resulting in a speed increase of about 10x for syncs.

I hope you like the news! Let me know if you want to discuss any missing features or provide feedback about Airbyte.",3,2,2025-04-04 12:26:58,0,False,False,False,False,2025-04-04 12:26:58,12,Friday,351.0,2602,30.77,23,654,12.4,1,1,NEGATIVE,-0.9846518039703369,"['hello', 'marcos', 'airbyte', 'team', 'may', 'familiar', 'airbyte', 'opensource', 'data', 'integration', 'platform', 'connectors', 'apis', 'databases', 'file', 'storage', 'last', 'release', 'added', 'several', 'new', 'features', 'nocode', 'connector', 'builder', 'graphql', 'supporthttpsdocsairbytecomconnectordevelopmentconfigbasedunderstandingtheyamlfilerequestoptionsgraphqlrequestinjection', 'addition', 'rest', 'make', 'requests', 'graphql', 'apis', 'properly', 'handle', 'pagination', 'async', 'data', 'requestshttpsdocsairbytecomconnectordevelopmentconnectorbuilderuiasyncstreams', 'reporting', 'apis', 'return', 'responses', 'immediately', 'instance', 'google', 'ads', 'request', 'custom', 'report', 'sources', 'wait', 'report', 'processed', 'downloaded', 'custom', 'python', 'code', 'componentshttpsdocsairbytecomconnectordevelopmentconnectorbuilderuicustomcomponents', 'recognize', 'apis', 'behave', 'uniquelyfor', 'example', 'returning', 'records', 'keyvalue', 'pairs', 'instead', 'arrays', 'ordering', 'data', 'correctly', 'address', 'cases', 'opensource', 'platform', 'supports', 'custom', 'python', 'components', 'extend', 'capabilities', 'nocode', 'framework', 'without', 'blocking', 'building', 'connector', 'believe', 'updates', 'make', 'connector', 'development', 'faster', 'accessible', 'helping', 'get', 'data', 'integration', 'projects', 'understand', 'discussions', 'tradeoffs', 'nocode', 'lowcode', 'solutions', 'airbyte', 'transitioning', 'fully', 'coded', 'connectors', 'lowcode', 'approach', 'allowed', 'maintain', 'large', 'connector', 'catalog', 'using', 'standard', 'components', 'also', 'able', 'create', 'better', 'build', 'test', 'process', 'directly', 'users', 'frequently', 'give', 'feedback', 'nocode', 'connector', 'builder', 'enables', 'less', 'technical', 'users', 'create', 'ship', 'connectors', 'reduces', 'workload', 'senior', 'data', 'engineers', 'allowing', 'focus', 'critical', 'data', 'pipelines', 'something', 'else', 'top', 'mind', 'speed', 'performance', 'robust', 'stable', 'connector', 'framework', 'engineering', 'team', 'dedicating', 'significant', 'resources', 'introduce', 'concurrency', 'enhance', 'sync', 'speed', 'read', 'blog', 'posthttpsairbytecomblogimprovingconnectorsyncspeeduptoxfaster', 'team', 'implemented', 'concurrency', 'klaviyo', 'connector', 'resulting', 'speed', 'increase', 'syncs', 'hope', 'like', 'news', 'let', 'know', 'want', 'discuss', 'missing', 'features', 'provide', 'feedback', 'airbyte']",hello marcos airbyte team may familiar airbyte opensource data integration platform connectors apis databases file storage last release added several new features nocode connector builder graphql supporthttpsdocsairbytecomconnectordevelopmentconfigbasedunderstandingtheyamlfilerequestoptionsgraphqlrequestinjection addition rest make requests graphql apis properly handle pagination async data requestshttpsdocsairbytecomconnectordevelopmentconnectorbuilderuiasyncstreams reporting apis return responses immediately instance google ads request custom report sources wait report processed downloaded custom python code componentshttpsdocsairbytecomconnectordevelopmentconnectorbuilderuicustomcomponents recognize apis behave uniquelyfor example returning records keyvalue pairs instead arrays ordering data correctly address cases opensource platform supports custom python components extend capabilities nocode framework without blocking building connector believe updates make connector development faster accessible helping get data integration projects understand discussions tradeoffs nocode lowcode solutions airbyte transitioning fully coded connectors lowcode approach allowed maintain large connector catalog using standard components also able create better build test process directly users frequently give feedback nocode connector builder enables less technical users create ship connectors reduces workload senior data engineers allowing focus critical data pipelines something else top mind speed performance robust stable connector framework engineering team dedicating significant resources introduce concurrency enhance sync speed read blog posthttpsairbytecomblogimprovingconnectorsyncspeeduptoxfaster team implemented concurrency klaviyo connector resulting speed increase syncs hope like news let know want discuss missing features provide feedback airbyte,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
142,Marketing Report & Fivetran,"Fishing for advice as I'm sure many have been here before. I came from DE at a SaaS company where I was more focused on the infra but now I'm in a role much close to the business and currently working with marketing. I'm sure this could make the Top-5 all time repeated DE tasks. A daily marketing report showing metrics like Spend, cost-per-click, engagement rate, cost-add-to-cart, cost-per-traffic... etc. These are per campaign based on various data sources like GA4, Google Ads, Facebook Ads, TikTok etc. Data updates once a day.

It should be obvious I'm not writing API connectors for a dozen different services. I'm just one person doing this and have many other things to do. I have Fivetran up and running getting the data I need but MY GOD is it ever expensive for something that seems like it should be simple, infrequent & low volume. It comes with a ton of build in reports that I don't even need sucking rows and bloating the bill. I can't seem to get what I need without pulling millions of event rows which costs a fortune to do.

Are there other similar but (way) cheaper solutions are out there? I know of others but any recommendations for this specific purpose?",2,5,2025-04-04 20:45:27,0,False,False,False,False,2025-04-04 20:45:27,20,Friday,211.0,1182,63.49,13,308,10.6,0,0,NEGATIVE,-0.9397996068000793,"['fishing', 'advice', 'sure', 'many', 'came', 'saas', 'company', 'focused', 'infra', 'role', 'much', 'close', 'business', 'currently', 'working', 'marketing', 'sure', 'could', 'make', 'top', 'time', 'repeated', 'tasks', 'daily', 'marketing', 'report', 'showing', 'metrics', 'like', 'spend', 'costperclick', 'engagement', 'rate', 'costaddtocart', 'costpertraffic', 'etc', 'per', 'campaign', 'based', 'various', 'data', 'sources', 'like', 'google', 'ads', 'facebook', 'ads', 'tiktok', 'etc', 'data', 'updates', 'day', 'obvious', 'writing', 'api', 'connectors', 'dozen', 'different', 'services', 'one', 'person', 'many', 'things', 'fivetran', 'running', 'getting', 'data', 'need', 'god', 'ever', 'expensive', 'something', 'seems', 'like', 'simple', 'infrequent', 'low', 'volume', 'comes', 'ton', 'build', 'reports', 'dont', 'even', 'need', 'sucking', 'rows', 'bloating', 'bill', 'cant', 'seem', 'get', 'need', 'without', 'pulling', 'millions', 'event', 'rows', 'costs', 'fortune', 'similar', 'way', 'cheaper', 'solutions', 'know', 'others', 'recommendations', 'specific', 'purpose']",fishing advice sure many came saas company focused infra role much close business currently working marketing sure could make top time repeated tasks daily marketing report showing metrics like spend costperclick engagement rate costaddtocart costpertraffic etc per campaign based various data sources like google ads facebook ads tiktok etc data updates day obvious writing api connectors dozen different services one person many things fivetran running getting data need god ever expensive something seems like simple infrequent low volume comes ton build reports dont even need sucking rows bloating bill cant seem get need without pulling millions event rows costs fortune similar way cheaper solutions know others recommendations specific purpose,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
143,PII Obfuscation in Databricks,"Hi Data Champs,

I have been recently given chance to explore PII obfuscation technique in databricks.

I proposed using sql aes_encryption or python fernet for PII column level encryption before landing to bronze.

And use column masking on delta tables which has built in logic for group membership check and decryption so to avoid the overhead of a new view per table.

My HDE was more interested in sql approach than the fernet but fernet offers built in key rotation out of the box.

Has anyone used aes_encryption 
Is it secure, easy to work with and relatively more robust.

From my experience for data type other than binary like long, int, double it needs to be first converted to binary (don’t like it)

Apart from that usual error here and there for padding and generic error when decrypting sometimes.

So given the choice what will be your architecture 

What you will prefer, what you don’t and why

I am open to DM if you wanna 💬 ",2,1,2025-04-04 14:28:31,0,False,2025-04-04 14:33:56,False,False,2025-04-04 14:28:31,14,Friday,166.0,945,55.98,7,251,13.3,0,0,NEGATIVE,-0.9411407709121704,"['data', 'champs', 'recently', 'given', 'chance', 'explore', 'pii', 'obfuscation', 'technique', 'databricks', 'proposed', 'using', 'sql', 'aesencryption', 'python', 'fernet', 'pii', 'column', 'level', 'encryption', 'landing', 'bronze', 'use', 'column', 'masking', 'delta', 'tables', 'built', 'logic', 'group', 'membership', 'check', 'decryption', 'avoid', 'overhead', 'new', 'view', 'per', 'table', 'hde', 'interested', 'sql', 'approach', 'fernet', 'fernet', 'offers', 'built', 'key', 'rotation', 'box', 'anyone', 'used', 'aesencryption', 'secure', 'easy', 'work', 'relatively', 'robust', 'experience', 'data', 'type', 'binary', 'like', 'long', 'int', 'double', 'needs', 'first', 'converted', 'binary', 'dont', 'like', 'apart', 'usual', 'error', 'padding', 'generic', 'error', 'decrypting', 'sometimes', 'given', 'choice', 'architecture', 'prefer', 'dont', 'open', 'wanna']",data champs recently given chance explore pii obfuscation technique databricks proposed using sql aesencryption python fernet pii column level encryption landing bronze use column masking delta tables built logic group membership check decryption avoid overhead new view per table hde interested sql approach fernet fernet offers built key rotation box anyone used aesencryption secure easy work relatively robust experience data type binary like long int double needs first converted binary dont like apart usual error padding generic error decrypting sometimes given choice architecture prefer dont open wanna,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
144,General question about data consulting,"Let's say there's a data consulting company working within a certain industry (e.g., utilities or energy). How do they gain access to their clients' databases if they want to perform ETL or other services? How about working with their data in a cloud setting (e.g., AWS)? What is the usual process for that? Is the consulting company responsible for setting and managing AWS costs, etc.?",2,9,2025-04-04 01:23:44,0,False,False,False,False,2025-04-04 01:23:44,1,Friday,65.0,387,52.05,6,109,11.5,0,0,NEGATIVE,-0.9991369843482971,"['lets', 'say', 'theres', 'data', 'consulting', 'company', 'working', 'within', 'certain', 'industry', 'utilities', 'energy', 'gain', 'access', 'clients', 'databases', 'want', 'perform', 'etl', 'services', 'working', 'data', 'cloud', 'setting', 'aws', 'usual', 'process', 'consulting', 'company', 'responsible', 'setting', 'managing', 'aws', 'costs', 'etc']",lets say theres data consulting company working within certain industry utilities energy gain access clients databases want perform etl services working data cloud setting aws usual process consulting company responsible setting managing aws costs etc,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
145,Question about file sync,"Pardon the noob question. I'm building a simple ETL process using Airflow on a remote Linux server and need a way for users to upload input files and download processed files.

I would prefer a method that is easy to use for users like a shared drive (like Google Drive).

I've considered Syncthing, and in the worst case, SFTP access. What solutions do you typically use or recommend for this? Thanks!",3,1,2025-04-04 22:25:11,1,False,False,False,False,2025-04-04 22:25:11,22,Friday,71.0,402,73.98,5,101,8.2,0,0,NEGATIVE,-0.9951960444450378,"['pardon', 'noob', 'question', 'building', 'simple', 'etl', 'process', 'using', 'airflow', 'remote', 'linux', 'server', 'need', 'way', 'users', 'upload', 'input', 'files', 'download', 'processed', 'files', 'would', 'prefer', 'method', 'easy', 'use', 'users', 'like', 'shared', 'drive', 'like', 'google', 'drive', 'ive', 'considered', 'syncthing', 'worst', 'case', 'sftp', 'access', 'solutions', 'typically', 'use', 'recommend', 'thanks']",pardon noob question building simple etl process using airflow remote linux server need way users upload input files download processed files would prefer method easy use users like shared drive like google drive ive considered syncthing worst case sftp access solutions typically use recommend thanks,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
146,Great Expectations Implementation,"Our company is implementing data quality testing and we are interested in borrowing from the Great Expectations suite of open source tests. I've read mostly negative reviews of the initial implementation of Great Expectations, but am curious if anyone else set up a much more lightweight configuration?

Ultimately, we plan to use the GX python code to run tests on data in Snowflake and then make the results available in Snowflake. Has anyone done something similar to this?",1,2,2025-04-04 14:18:53,0,False,False,False,False,2025-04-04 14:18:53,14,Friday,78.0,476,43.22,4,134,14.9,0,0,NEGATIVE,-0.9983974099159241,"['company', 'implementing', 'data', 'quality', 'testing', 'interested', 'borrowing', 'great', 'expectations', 'suite', 'open', 'source', 'tests', 'ive', 'read', 'mostly', 'negative', 'reviews', 'initial', 'implementation', 'great', 'expectations', 'curious', 'anyone', 'else', 'set', 'much', 'lightweight', 'configuration', 'ultimately', 'plan', 'use', 'python', 'code', 'run', 'tests', 'data', 'snowflake', 'make', 'results', 'available', 'snowflake', 'anyone', 'done', 'something', 'similar']",company implementing data quality testing interested borrowing great expectations suite open source tests ive read mostly negative reviews initial implementation great expectations curious anyone else set much lightweight configuration ultimately plan use python code run tests data snowflake make results available snowflake anyone done something similar,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
147,Can you call an aimless star schema a data mart?,"So,

  
as always that's for the insight from other people, I find a lot of these discussions around points very entertaining and very helpful!

I'm having an argument with someone who is several levels above me. This might sound petty so I apologise in advance. It centres around the definition of a Mart. Our Mart is a single Fact with around 20 dimensions. The Fact is extremely wide and deep. Indeed we usually put it into a de normalised table for reporting. To me this isn't a MART as it isn't based on requirements but rather a star schema that supposedly servers multiple purposed or potential purposes. When engaged on requirements the person leans on there experience in the domain and says a user probable wants to do X, Y and Z. I've never seen anything written down. Constantly that report also defers to Kimball methodology and how this follows them closely. My take on the book is that these things need to be based of requirement, business requirements. 

My questions is, is it fair to say that a data mart needs to have requirements and ideally a business domain in mind or else its just a star schema?

Yes this  is  very theoretical... yes I probable need a hobby but look there hasn't been a decent RTS game in years and its friday!!!

Have a good weekend everyone",1,3,2025-04-04 13:41:31,0,False,False,False,False,2025-04-04 13:41:31,13,Friday,229.0,1285,65.42,16,347,10.6,0,0,POSITIVE,0.9540523290634155,"['always', 'thats', 'insight', 'people', 'find', 'lot', 'discussions', 'around', 'points', 'entertaining', 'helpful', 'argument', 'someone', 'several', 'levels', 'might', 'sound', 'petty', 'apologise', 'advance', 'centres', 'around', 'definition', 'mart', 'mart', 'single', 'fact', 'around', 'dimensions', 'fact', 'extremely', 'wide', 'deep', 'indeed', 'usually', 'put', 'normalised', 'table', 'reporting', 'isnt', 'mart', 'isnt', 'based', 'requirements', 'rather', 'star', 'schema', 'supposedly', 'servers', 'multiple', 'purposed', 'potential', 'purposes', 'engaged', 'requirements', 'person', 'leans', 'experience', 'domain', 'says', 'user', 'probable', 'wants', 'ive', 'never', 'seen', 'anything', 'written', 'constantly', 'report', 'also', 'defers', 'kimball', 'methodology', 'follows', 'closely', 'take', 'book', 'things', 'need', 'based', 'requirement', 'business', 'requirements', 'questions', 'fair', 'say', 'data', 'mart', 'needs', 'requirements', 'ideally', 'business', 'domain', 'mind', 'else', 'star', 'schema', 'yes', 'theoretical', 'yes', 'probable', 'need', 'hobby', 'look', 'hasnt', 'decent', 'rts', 'game', 'years', 'friday', 'good', 'weekend', 'everyone']",always thats insight people find lot discussions around points entertaining helpful argument someone several levels might sound petty apologise advance centres around definition mart mart single fact around dimensions fact extremely wide deep indeed usually put normalised table reporting isnt mart isnt based requirements rather star schema supposedly servers multiple purposed potential purposes engaged requirements person leans experience domain says user probable wants ive never seen anything written constantly report also defers kimball methodology follows closely take book things need based requirement business requirements questions fair say data mart needs requirements ideally business domain mind else star schema yes theoretical yes probable need hobby look hasnt decent rts game years friday good weekend everyone,Mid,0,0,"need, table, event, source, without, instead, every, replay, day, example, single, change, per, business, daily, based, keys, manually, model, keep, needs, events, doesnt, name, list, option, user, must, takes, history, often, truth, full, column, hour, address, requirements, date, reading, track, whenever, metrics, result, update, feature, primary, columns, meaning, pricing, structure, details, rows, simply, available, dedicated, country, means, facebook, error, missing, evolve, involves, names, size, domain, place, addresses, timestamp, parse, mean, slow, schemas, heres, return, dimensions, delete, state, times, arent, ground, errors, receive, min, added, geocoding, shape, sourcing, wasnt, insert, minutes, value, flow, save, normal, little, region, usually, bad, becomes, failure, replication, entirely, products, infrastructure, minute, person, average, servers, internet, looked, probably, campaigns, workloads, looks",8.2,7.4,175.5,-0.663,1069.0,47.9,9.8,275.5,11.4,22.2,15.5,52,108,34,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",4,4
148,Data Engineering Performance -  Authors,I having worked in BI and transitioned to DE have followed best practices reading books by authors like Ralph Kimball in BI. Is there someone in DE with a similar level of reputation. I am not looking for specific technologies but rather want to pick up DE fundamentals especially in the performance and optimization space.,1,1,2025-04-04 13:08:48,0,False,False,False,False,2025-04-04 13:08:48,13,Friday,55.0,323,52.9,3,89,13.6,0,0,NEGATIVE,-0.8508272767066956,"['worked', 'transitioned', 'followed', 'best', 'practices', 'reading', 'books', 'authors', 'like', 'ralph', 'kimball', 'someone', 'similar', 'level', 'reputation', 'looking', 'specific', 'technologies', 'rather', 'want', 'pick', 'fundamentals', 'especially', 'performance', 'optimization', 'space']",worked transitioned followed best practices reading books authors like ralph kimball someone similar level reputation looking specific technologies rather want pick fundamentals especially performance optimization space,Mid,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
149,Unstructured Data,"I see this has been asked prior but I didn't see a clear answer. We have a smallish database (glorified spreadsheet) where one field contains text. It houses details regarding customers, etc calling in for various issues. For various reasons (in-house) they want to keep using the simple app (it's a SharePoint List). I can easily download the data to a CSV file, for example, but is there a fairly simple method (AI?) to make sense of this data and correlate it? Maybe a creative prompt? Or is there a tool for this? (I'm not a software engineer). Thanks!",1,5,2025-04-04 12:39:42,0,False,False,False,False,2025-04-04 12:39:42,12,Friday,99.0,556,68.77,9,149,10.0,0,0,NEGATIVE,-0.9991177916526794,"['see', 'asked', 'prior', 'didnt', 'see', 'clear', 'answer', 'smallish', 'database', 'glorified', 'spreadsheet', 'one', 'field', 'contains', 'text', 'houses', 'details', 'regarding', 'customers', 'etc', 'calling', 'various', 'issues', 'various', 'reasons', 'inhouse', 'want', 'keep', 'using', 'simple', 'app', 'sharepoint', 'list', 'easily', 'download', 'data', 'csv', 'file', 'example', 'fairly', 'simple', 'method', 'make', 'sense', 'data', 'correlate', 'maybe', 'creative', 'prompt', 'tool', 'software', 'engineer', 'thanks']",see asked prior didnt see clear answer smallish database glorified spreadsheet one field contains text houses details regarding customers etc calling various issues various reasons inhouse want keep using simple app sharepoint list easily download data csv file example fairly simple method make sense data correlate maybe creative prompt tool software engineer thanks,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
150,Do you need statistics to land a DE job?,"As the title suggests. Even if stats are not used on the job, will having stats qualifications give me an edge in the hiring process?",1,20,2025-04-04 08:03:59,0,False,False,False,False,2025-04-04 08:03:59,8,Friday,25.0,133,75.71,2,35,0.0,0,0,NEGATIVE,-0.9890056848526001,"['title', 'suggests', 'even', 'stats', 'used', 'job', 'stats', 'qualifications', 'give', 'edge', 'hiring', 'process']",title suggests even stats used job stats qualifications give edge hiring process,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
151,[Seeking Guidance] Aspiring GCP Data Engineer – Will Work Pro Bono for Hands-On Experience!,"Hey r/dataengineering community,  

I’m deep into prepping for the Google Cloud Professional Data Engineer cert and want to transition from theory to real-world projects. To ace the exam and build job-ready skills, I’m looking for:  

- Hands-on opportunities (pro bono!) to work with GCP tools like BigQuery, Dataflow, Pub/Sub, Cloud Composer, etc.  
- Mentorship or collaboration on data pipelines, workflow optimization, or cloud architecture projects.  
- Open-source/community projects needing an extra pair of hands.  

Why me? I’m motivated, detail-oriented, and eager to learn. I’ll treat your project like my own!  

If you’re working on anything data-related in GCP - or know someone who is - I’d hugely appreciate a chance to contribute (or even just advice on where to start). Comment/DM me, and thanks for being an awesome community!  

P.S. Upvotes for visibility help a ton! 🙏",1,3,2025-04-04 01:33:00,0,False,False,False,False,2025-04-04 01:33:00,1,Friday,137.0,891,49.72,10,227,11.8,0,0,NEGATIVE,-0.9431983232498169,"['hey', 'rdataengineering', 'community', 'deep', 'prepping', 'google', 'cloud', 'professional', 'data', 'engineer', 'cert', 'want', 'transition', 'theory', 'realworld', 'projects', 'ace', 'exam', 'build', 'jobready', 'skills', 'looking', 'handson', 'opportunities', 'pro', 'bono', 'work', 'gcp', 'tools', 'like', 'bigquery', 'dataflow', 'pubsub', 'cloud', 'composer', 'etc', 'mentorship', 'collaboration', 'data', 'pipelines', 'workflow', 'optimization', 'cloud', 'architecture', 'projects', 'opensourcecommunity', 'projects', 'needing', 'extra', 'pair', 'hands', 'motivated', 'detailoriented', 'eager', 'learn', 'ill', 'treat', 'project', 'like', 'youre', 'working', 'anything', 'datarelated', 'gcp', 'know', 'someone', 'hugely', 'appreciate', 'chance', 'contribute', 'even', 'advice', 'start', 'commentdm', 'thanks', 'awesome', 'community', 'upvotes', 'visibility', 'help', 'ton']",hey rdataengineering community deep prepping google cloud professional data engineer cert want transition theory realworld projects ace exam build jobready skills looking handson opportunities pro bono work gcp tools like bigquery dataflow pubsub cloud composer etc mentorship collaboration data pipelines workflow optimization cloud architecture projects opensourcecommunity projects needing extra pair hands motivated detailoriented eager learn ill treat project like youre working anything datarelated gcp know someone hugely appreciate chance contribute even advice start commentdm thanks awesome community upvotes visibility help ton,Mid,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
152,Just wanted to share a recent win that made our whole team feel pretty good.,"We worked with this e-commerce client last month (kitchen products company, can't name names) who was dealing with data chaos.

When they came to us, their situation was rough. Dashboards taking forever to load, some poor analyst manually combining data from 5 different sources, and their CEO breathing down everyone's neck for daily conversion reports. Classic spreadsheet hell that we've all seen before.

We spent about two weeks redesigning their entire data architecture. Built them a proper [**data warehouse solution** ](https://datafortune.com/services/enterprise-data-management/data-warehouse/)with automated ETL pipelines that consolidated everything into one central location. Created some logical data models and connected it all to their existing BI tools.

The transformation was honestly pretty incredible to watch. Reports that used to take hours now run in seconds. Their analyst actually took a vacation for the first time in a year. And we got this really nice email from their CTO saying we'd ""changed how they make decisions"" which gave us all the warm fuzzies.

It's projects like these that remind us why we got into this field in the first place. There's something so satisfying about taking a messy data situation and turning it into something clean and efficient that actually helps people do their jobs better.",0,9,2025-04-04 14:20:44,0,False,False,False,False,2025-04-04 14:20:44,14,Friday,203.0,1339,48.3,14,343,12.0,1,0,NEGATIVE,-0.9992449283599854,"['worked', 'ecommerce', 'client', 'last', 'month', 'kitchen', 'products', 'company', 'cant', 'name', 'names', 'dealing', 'data', 'chaos', 'came', 'situation', 'rough', 'dashboards', 'taking', 'forever', 'load', 'poor', 'analyst', 'manually', 'combining', 'data', 'different', 'sources', 'ceo', 'breathing', 'everyones', 'neck', 'daily', 'conversion', 'reports', 'classic', 'spreadsheet', 'hell', 'weve', 'seen', 'spent', 'two', 'weeks', 'redesigning', 'entire', 'data', 'architecture', 'built', 'proper', 'data', 'warehouse', 'solution', 'httpsdatafortunecomservicesenterprisedatamanagementdatawarehousewith', 'automated', 'etl', 'pipelines', 'consolidated', 'everything', 'one', 'central', 'location', 'created', 'logical', 'data', 'models', 'connected', 'existing', 'tools', 'transformation', 'honestly', 'pretty', 'incredible', 'watch', 'reports', 'used', 'take', 'hours', 'run', 'seconds', 'analyst', 'actually', 'took', 'vacation', 'first', 'time', 'year', 'got', 'really', 'nice', 'email', 'cto', 'saying', 'wed', 'changed', 'make', 'decisions', 'gave', 'warm', 'fuzzies', 'projects', 'like', 'remind', 'got', 'field', 'first', 'place', 'theres', 'something', 'satisfying', 'taking', 'messy', 'data', 'situation', 'turning', 'something', 'clean', 'efficient', 'actually', 'helps', 'people', 'jobs', 'better']",worked ecommerce client last month kitchen products company cant name names dealing data chaos came situation rough dashboards taking forever load poor analyst manually combining data different sources ceo breathing everyones neck daily conversion reports classic spreadsheet hell weve seen spent two weeks redesigning entire data architecture built proper data warehouse solution httpsdatafortunecomservicesenterprisedatamanagementdatawarehousewith automated etl pipelines consolidated everything one central location created logical data models connected existing tools transformation honestly pretty incredible watch reports used take hours run seconds analyst actually took vacation first time year got really nice email cto saying wed changed make decisions gave warm fuzzies projects like remind got field first place theres something satisfying taking messy data situation turning something clean efficient actually helps people jobs better,Low,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
153,Jira: Is it still helping teams... or just slowing them down?,"I’ve been part of (and led) a teams over the last decade — in enterprises

And one tool keeps showing up everywhere: **Jira**.

It’s the ""default"" for a lot of engineering orgs. Everyone knows it. Everyone uses it.  
But **I don’t seen anyone who actually likes it.**

Not in the *""ugh it's corporate but fine""* way — I mean people who are actively frustrated by it but still use it daily.

Here are some of the most common friction points I’ve either experienced or heard from other devs/product folks:

1. **Custom workflows spiral out of control** — What starts as ""just a few tweaks"" becomes an unmanageable mess.
2. **Slow performance** — Large projects? Boards crawling? Yup.
3. **Search that requires sorcery** — Good luck finding an old ticket without a detailed Jira PhD.
4. **New team members struggle to onboard** — It’s not exactly intuitive.
5. **The “tool tax”** — Teams spend hours updating Jira instead of moving work forward.

And yet... most teams stick with it. Because switching is painful. Because “at least everyone knows Jira.” Because the alternative is more uncertainty.  
What's your take on this?",64,47,2025-04-08 07:40:50,0,False,False,False,False,2025-04-08 07:40:50,7,Tuesday,189.0,1123,69.07,17,275,9.8,0,0,POSITIVE,0.9553472399711609,"['ive', 'part', 'led', 'teams', 'last', 'decade', 'enterprises', 'one', 'tool', 'keeps', 'showing', 'everywhere', 'jira', 'default', 'lot', 'engineering', 'orgs', 'everyone', 'knows', 'everyone', 'uses', 'dont', 'seen', 'anyone', 'actually', 'likes', 'ugh', 'corporate', 'fine', 'way', 'mean', 'people', 'actively', 'frustrated', 'still', 'use', 'daily', 'common', 'friction', 'points', 'ive', 'either', 'experienced', 'heard', 'devsproduct', 'folks', 'custom', 'workflows', 'spiral', 'control', 'starts', 'tweaks', 'becomes', 'unmanageable', 'mess', 'slow', 'performance', 'large', 'projects', 'boards', 'crawling', 'yup', 'search', 'requires', 'sorcery', 'good', 'luck', 'finding', 'old', 'ticket', 'without', 'detailed', 'jira', 'phd', 'new', 'team', 'members', 'struggle', 'onboard', 'exactly', 'intuitive', 'tool', 'tax', 'teams', 'spend', 'hours', 'updating', 'jira', 'instead', 'moving', 'work', 'forward', 'yet', 'teams', 'stick', 'switching', 'painful', 'least', 'everyone', 'knows', 'jira', 'alternative', 'uncertainty', 'whats', 'take']",ive part led teams last decade enterprises one tool keeps showing everywhere jira default lot engineering orgs everyone knows everyone uses dont seen anyone actually likes ugh corporate fine way mean people actively frustrated still use daily common friction points ive either experienced heard devsproduct folks custom workflows spiral control starts tweaks becomes unmanageable mess slow performance large projects boards crawling yup search requires sorcery good luck finding old ticket without detailed jira phd new team members struggle onboard exactly intuitive tool tax teams spend hours updating jira instead moving work forward yet teams stick switching painful least everyone knows jira alternative uncertainty whats take,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
154,Why do you dislike MS Fabric?,"Title.  I've only tested it. It seems like not a good solution for us (at least currently) for various reasons, but beyond that...

It seems people generally don't feel it's production ready - how specifically?  What issues have you found?",40,41,2025-04-08 18:33:12,0,False,2025-04-08 23:46:51,False,False,2025-04-08 18:33:12,18,Tuesday,40.0,239,61.53,4,61,10.1,0,0,NEGATIVE,-0.999496340751648,"['title', 'ive', 'tested', 'seems', 'like', 'good', 'solution', 'least', 'currently', 'various', 'reasons', 'beyond', 'seems', 'people', 'generally', 'dont', 'feel', 'production', 'ready', 'specifically', 'issues', 'found']",title ive tested seems like good solution least currently various reasons beyond seems people generally dont feel production ready specifically issues found,High,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
155,How did you start your data engineering journey?,"I am getting into this role, I wondered how other people became data engineers? Most didn't start as a junior data engineer; some came from an analyst(business or data), software engineers, or database administrators. 

What helped you become one or motivated you to become one?",16,39,2025-04-08 09:39:06,0,False,False,False,False,2025-04-08 09:39:06,9,Tuesday,45.0,278,47.79,3,78,11.9,0,0,POSITIVE,0.9054082036018372,"['getting', 'role', 'wondered', 'people', 'became', 'data', 'engineers', 'didnt', 'start', 'junior', 'data', 'engineer', 'came', 'analystbusiness', 'data', 'software', 'engineers', 'database', 'administrators', 'helped', 'become', 'one', 'motivated', 'become', 'one']",getting role wondered people became data engineers didnt start junior data engineer came analystbusiness data software engineers database administrators helped become one motivated become one,High,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
156,Ingesting a billion small .csv files from blob?,"Currently, we're ""streaming"" data by having an Azure Function write event grid messages to csv in blob storage, and then by having snowpipe ingest them. There's about a million csv's generated daily. The blob is not partitioned at all.

What's the best way to ingest/delete everything? Snowpipe has a configuration error, and a portion of the data hasn't been loaded, ever. ADF was pretty slow when I tested it out.

This was all done by consultants before I was in house btw.


edit: I was a bit unclear in my message. I mean, that we've had snowpipe ingesting these files. However, now we need to re-ingest the billion or so small .csv's that are in the blob, to compare the data to the already ingested data.

What further complicates this is:

- some files have two additional columns
- we also need to parse the filename to a column
- there is absolutely no partitioning at all",16,4,2025-04-08 06:14:25,0,False,2025-04-08 21:44:57,False,False,2025-04-08 06:14:25,6,Tuesday,157.0,882,66.94,12,230,9.9,0,1,NEGATIVE,-0.9993509650230408,"['currently', 'streaming', 'data', 'azure', 'function', 'write', 'event', 'grid', 'messages', 'csv', 'blob', 'storage', 'snowpipe', 'ingest', 'theres', 'million', 'csvs', 'generated', 'daily', 'blob', 'partitioned', 'whats', 'best', 'way', 'ingestdelete', 'everything', 'snowpipe', 'configuration', 'error', 'portion', 'data', 'hasnt', 'loaded', 'ever', 'adf', 'pretty', 'slow', 'tested', 'done', 'consultants', 'house', 'btw', 'edit', 'bit', 'unclear', 'message', 'mean', 'weve', 'snowpipe', 'ingesting', 'files', 'however', 'need', 'reingest', 'billion', 'small', 'csvs', 'blob', 'compare', 'data', 'already', 'ingested', 'data', 'complicates', 'files', 'two', 'additional', 'columns', 'also', 'need', 'parse', 'filename', 'column', 'absolutely', 'partitioning']",currently streaming data azure function write event grid messages csv blob storage snowpipe ingest theres million csvs generated daily blob partitioned whats best way ingestdelete everything snowpipe configuration error portion data hasnt loaded ever adf pretty slow tested done consultants house btw edit bit unclear message mean weve snowpipe ingesting files however need reingest billion small csvs blob compare data already ingested data complicates files two additional columns also need parse filename column absolutely partitioning,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
157,Hung DBT jobs,"According to the DBT Cloud [api](https://docs.getdbt.com/dbt-cloud/api-v2#/operations/Retrieve%20Run%20Failure%20Details), I can only tell that a job has failed and retrieve the failure details. 

There's no way for me to know when a job is hung.

Yesterday, an issue with our Fivetran replication and several of our DBT jobs hung for several hours.

Any idea how to monitor for hung DBT jobs?",13,3,2025-04-08 19:45:39,0,False,False,False,False,2025-04-08 19:45:39,19,Tuesday,58.0,393,51.24,5,96,9.4,1,0,NEGATIVE,-0.999728262424469,"['according', 'dbt', 'cloud', 'apihttpsdocsgetdbtcomdbtcloudapivoperationsretrieverunfailuredetails', 'tell', 'job', 'failed', 'retrieve', 'failure', 'details', 'theres', 'way', 'know', 'job', 'hung', 'yesterday', 'issue', 'fivetran', 'replication', 'several', 'dbt', 'jobs', 'hung', 'several', 'hours', 'idea', 'monitor', 'hung', 'dbt', 'jobs']",according dbt cloud apihttpsdocsgetdbtcomdbtcloudapivoperationsretrieverunfailuredetails tell job failed retrieve failure details theres way know job hung yesterday issue fivetran replication several dbt jobs hung several hours idea monitor hung dbt jobs,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
158,What are the Python Data Engineering approaches every data scientist should know?,"Is it building data pipelines to connect to a DB?
Is it automatically downloading data from a DB and creating reports or is it something else? 
I am a data scientist who would like to polish his Data Engineering skills with Python because my company is beginning to incorporate more and more Python and I think I can be helpful. ",13,5,2025-04-08 15:25:54,0,False,False,False,False,2025-04-08 15:25:54,15,Tuesday,60.0,329,51.18,3,94,12.5,0,0,POSITIVE,0.910548746585846,"['building', 'data', 'pipelines', 'connect', 'automatically', 'downloading', 'data', 'creating', 'reports', 'something', 'else', 'data', 'scientist', 'would', 'like', 'polish', 'data', 'engineering', 'skills', 'python', 'company', 'beginning', 'incorporate', 'python', 'think', 'helpful']",building data pipelines connect automatically downloading data creating reports something else data scientist would like polish data engineering skills python company beginning incorporate python think helpful,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
159,Clean architecture for Data Engineering,"Hi Guys,

Do anyone use or tried to use clean architecture for data engineering projects? If yes, May I know, how did it go and any comments on it or any references on github if you have?

Please don't give negative comments/responses without reasons.

Best regards",9,5,2025-04-08 18:13:04,0,False,False,False,False,2025-04-08 18:13:04,18,Tuesday,46.0,265,64.41,3,71,11.2,0,0,NEGATIVE,-0.9136924743652344,"['guys', 'anyone', 'use', 'tried', 'use', 'clean', 'architecture', 'data', 'engineering', 'projects', 'yes', 'may', 'know', 'comments', 'references', 'github', 'please', 'dont', 'give', 'negative', 'commentsresponses', 'without', 'reasons', 'best', 'regards']",guys anyone use tried use clean architecture data engineering projects yes may know comments references github please dont give negative commentsresponses without reasons best regards,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
160,"reflect-cpp - a C++20 library for fast serialization, deserialization and validation using reflection, like Python's Pydantic or Rust's serde.","[https://github.com/getml/reflect-cpp](https://github.com/getml/reflect-cpp)

I am a data engineer, ML engineer and software developer with strong background in functional programming. As such, I am a strong proponent of the ""Parse, Don't Validate"" principle (https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/).  
  
Unfortunately, C++ does not yet support reflection, which is necessary to do something apply these principles. However, after some discussions on the topic over on r/cpp, we figured out a way to do this anyway. This library emerged out of these discussions.

I have personally used this library in real-world projects and it has been very useful. I hope other people in data engineering can benefit from it as well.

And before you ask: Yes, I use C++ for data engineering. It is quite common in finance and energy or other fields where you really care about speed. ",6,0,2025-04-08 06:26:36,0,False,False,False,False,2025-04-08 06:26:36,6,Tuesday,129.0,901,40.04,9,226,12.8,1,1,NEGATIVE,-0.9446321129798889,"['httpsgithubcomgetmlreflectcpphttpsgithubcomgetmlreflectcpp', 'data', 'engineer', 'engineer', 'software', 'developer', 'strong', 'background', 'functional', 'programming', 'strong', 'proponent', 'parse', 'dont', 'validate', 'principle', 'httpslexilambdagithubioblogparsedontvalidate', 'unfortunately', 'yet', 'support', 'reflection', 'necessary', 'something', 'apply', 'principles', 'however', 'discussions', 'topic', 'rcpp', 'figured', 'way', 'anyway', 'library', 'emerged', 'discussions', 'personally', 'used', 'library', 'realworld', 'projects', 'useful', 'hope', 'people', 'data', 'engineering', 'benefit', 'well', 'ask', 'yes', 'use', 'data', 'engineering', 'quite', 'common', 'finance', 'energy', 'fields', 'really', 'care', 'speed']",httpsgithubcomgetmlreflectcpphttpsgithubcomgetmlreflectcpp data engineer engineer software developer strong background functional programming strong proponent parse dont validate principle httpslexilambdagithubioblogparsedontvalidate unfortunately yet support reflection necessary something apply principles however discussions topic rcpp figured way anyway library emerged discussions personally used library realworld projects useful hope people data engineering benefit well ask yes use data engineering quite common finance energy fields really care speed,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
161,Best way to handle loading JSON API data into database in pipelines,"Greetings, this is my first post here. I've been working in DE for the last 5 years now doing various things with Airflow and Dagster.  I have a question regarding design of data flow from APIs to our database.

I am using Dagster/Python to perform the API pulls and loads into Snowflake.

My team lead insists that we load JSON data into our Snowflake RAW\_DATA in the following way:

ID (should be a surrogate/non-native PK)  
PAYLOAD (raw JSON payload, either as a VARCHAR or VARIANT type)  
CREATED\_DATE (timestamp this row was created in Snowflake)  
UPDATE\_DATE (timestamp this row was updated in Snowflake)

Flattening of the payload then happens in SQL as a plain View, which we currently autogenerate using Python and manually edit and add to Snowflake.

He does not want us (DE team) to use DBT to do any transforming of RAW\_DATA. DBT is only for the Data Analyst team to use for creating models.

The main advantage I see to this approach is flexibility if the JSON schema changes. You can freely append/drop/insert/reorder/rename columns. whereas a normal table you can only drop, append, and rename.

On the downside, it is slow and clunky to parse with SQL and access the data as a view. It just seems inefficient to have to recompute the view and parse all those JSON payloads whenever you want to access the table.  
  
I'd much rather do the flattening in Python, either manually or using dlt. Some JSON payloads I 'pre-flatten' in Python to make them easier to parse in SQL.

Is there a better way, or is this how you all handle this as well?",6,1,2025-04-08 23:56:12,0,False,2025-04-09 00:01:00,False,False,2025-04-08 23:56:12,23,Tuesday,272.0,1563,61.56,15,416,11.7,0,0,NEGATIVE,-0.9981860518455505,"['greetings', 'first', 'post', 'ive', 'working', 'last', 'years', 'various', 'things', 'airflow', 'dagster', 'question', 'regarding', 'design', 'data', 'flow', 'apis', 'database', 'using', 'dagsterpython', 'perform', 'api', 'pulls', 'loads', 'snowflake', 'team', 'lead', 'insists', 'load', 'json', 'data', 'snowflake', 'rawdata', 'following', 'way', 'surrogatenonnative', 'payload', 'raw', 'json', 'payload', 'either', 'varchar', 'variant', 'type', 'createddate', 'timestamp', 'row', 'created', 'snowflake', 'updatedate', 'timestamp', 'row', 'updated', 'snowflake', 'flattening', 'payload', 'happens', 'sql', 'plain', 'view', 'currently', 'autogenerate', 'using', 'python', 'manually', 'edit', 'add', 'snowflake', 'want', 'team', 'use', 'dbt', 'transforming', 'rawdata', 'dbt', 'data', 'analyst', 'team', 'use', 'creating', 'models', 'main', 'advantage', 'see', 'approach', 'flexibility', 'json', 'schema', 'changes', 'freely', 'appenddropinsertreorderrename', 'columns', 'whereas', 'normal', 'table', 'drop', 'append', 'rename', 'downside', 'slow', 'clunky', 'parse', 'sql', 'access', 'data', 'view', 'seems', 'inefficient', 'recompute', 'view', 'parse', 'json', 'payloads', 'whenever', 'want', 'access', 'table', 'much', 'rather', 'flattening', 'python', 'either', 'manually', 'using', 'dlt', 'json', 'payloads', 'preflatten', 'python', 'make', 'easier', 'parse', 'sql', 'better', 'way', 'handle', 'well']",greetings first post ive working last years various things airflow dagster question regarding design data flow apis database using dagsterpython perform api pulls loads snowflake team lead insists load json data snowflake rawdata following way surrogatenonnative payload raw json payload either varchar variant type createddate timestamp row created snowflake updatedate timestamp row updated snowflake flattening payload happens sql plain view currently autogenerate using python manually edit add snowflake want team use dbt transforming rawdata dbt data analyst team use creating models main advantage see approach flexibility json schema changes freely appenddropinsertreorderrename columns whereas normal table drop append rename downside slow clunky parse sql access data view seems inefficient recompute view parse json payloads whenever want access table much rather flattening python either manually using dlt json payloads preflatten python make easier parse sql better way handle well,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
162,Azure Course for Beginners | Learn Azure & Data Bricks in 1 Hour,"# FREE Azure Course for Beginners | Learn Azure & Data Bricks in 1 Hour

[https://www.youtube.com/watch?v=8XH2vTyzL7c](https://www.youtube.com/watch?v=8XH2vTyzL7c)",4,1,2025-04-09 02:36:56,0,False,False,False,False,2025-04-09 02:36:56,2,Wednesday,16.0,163,7.52,1,29,0.0,1,0,NEGATIVE,-0.988972008228302,"['free', 'azure', 'course', 'beginners', 'learn', 'azure', 'data', 'bricks', 'hour', 'httpswwwyoutubecomwatchvxhvtyzlchttpswwwyoutubecomwatchvxhvtyzlc']",free azure course beginners learn azure data bricks hour httpswwwyoutubecomwatchvxhvtyzlchttpswwwyoutubecomwatchvxhvtyzlc,Mid,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
163,Azure vs Microsoft Fabric?,"As a data engineer, I really like the control and customization that Azure offers.
At the same time, I can see how Fabric is more business-friendly and leans toward a low/no-code experience.

But with all the content and comparisons floating around the internet, why is no one talking about how insanely expensive Fabric is?! Seriously—am I missing something here?


-
",4,7,2025-04-09 01:20:51,0,False,False,False,False,2025-04-09 01:20:51,1,Wednesday,60.0,369,56.45,4,95,11.7,0,0,NEGATIVE,-0.9886941313743591,"['data', 'engineer', 'really', 'like', 'control', 'customization', 'azure', 'offers', 'time', 'see', 'fabric', 'businessfriendly', 'leans', 'toward', 'lownocode', 'experience', 'content', 'comparisons', 'floating', 'around', 'internet', 'one', 'talking', 'insanely', 'expensive', 'fabric', 'seriouslyam', 'missing', 'something']",data engineer really like control customization azure offers time see fabric businessfriendly leans toward lownocode experience content comparisons floating around internet one talking insanely expensive fabric seriouslyam missing something,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
164,Lessons from optimizing dashboard performance on Looker Studio with BigQuery data,"We’ve been using Looker Studio (formerly Data Studio) to build reporting dashboards for digital marketing and SEO data. At first, things worked fine—but as datasets grew, dashboard performance dropped significantly.



The biggest bottlenecks were:

• Overuse of blended data sources

• Direct querying of large GA4 datasets

• Too many calculated fields applied in the visualization layer



To fix this, we adjusted our approach on the data engineering side:

• Moved most calculations (e.g., conversion rates, ROAS) to the query layer in BigQuery

• Created materialized views for campaign-level summaries

• Used scheduled queries to pre-aggregate weekly and monthly data

• Limited Looker Studio to one direct connector per dashboard and cached data where possible



Result: dashboards now load in \~3 seconds instead of 15–20, and we can scale them across accounts with minimal changes.



Just sharing this in case others are using BI tools on top of large datasets—interested to hear how others here are managing dashboard performance from a data pipeline perspective.",3,1,2025-04-08 17:43:22,0,False,False,False,False,2025-04-08 17:43:22,17,Tuesday,164.0,1077,22.68,5,278,17.6,0,0,NEGATIVE,-0.9984896183013916,"['weve', 'using', 'looker', 'studio', 'formerly', 'data', 'studio', 'build', 'reporting', 'dashboards', 'digital', 'marketing', 'seo', 'data', 'first', 'things', 'worked', 'finebut', 'datasets', 'grew', 'dashboard', 'performance', 'dropped', 'significantly', 'biggest', 'bottlenecks', 'overuse', 'blended', 'data', 'sources', 'direct', 'querying', 'large', 'datasets', 'many', 'calculated', 'fields', 'applied', 'visualization', 'layer', 'fix', 'adjusted', 'approach', 'data', 'engineering', 'side', 'moved', 'calculations', 'conversion', 'rates', 'roas', 'query', 'layer', 'bigquery', 'created', 'materialized', 'views', 'campaignlevel', 'summaries', 'used', 'scheduled', 'queries', 'preaggregate', 'weekly', 'monthly', 'data', 'limited', 'looker', 'studio', 'one', 'direct', 'connector', 'per', 'dashboard', 'cached', 'data', 'possible', 'result', 'dashboards', 'load', 'seconds', 'instead', 'scale', 'across', 'accounts', 'minimal', 'changes', 'sharing', 'case', 'others', 'using', 'tools', 'top', 'large', 'datasetsinterested', 'hear', 'others', 'managing', 'dashboard', 'performance', 'data', 'pipeline', 'perspective']",weve using looker studio formerly data studio build reporting dashboards digital marketing seo data first things worked finebut datasets grew dashboard performance dropped significantly biggest bottlenecks overuse blended data sources direct querying large datasets many calculated fields applied visualization layer fix adjusted approach data engineering side moved calculations conversion rates roas query layer bigquery created materialized views campaignlevel summaries used scheduled queries preaggregate weekly monthly data limited looker studio one direct connector per dashboard cached data possible result dashboards load seconds instead scale across accounts minimal changes sharing case others using tools top large datasetsinterested hear others managing dashboard performance data pipeline perspective,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
165,Help: Looking to set up a decent data architecture (data lake and/or warehouse),"Hi, I need help. I need a proper architecture for a department, and I am trying to get a data lake/warehouse.

Why: We have a lot of data sources from SaaS to manually created documents. We use a lot of SaaS products, but we have no centralised repository to store and stage the data, so we end up with a lot of workaround such as using SharePoint and csv stored in folders for reporting. We also change SaaS products quite frequently, so sources can change often. It is difficult to do advanced analytics. 

I prefer a lake & warehouse approach because (1) for SaaS users, they can can just drop the data to the lake and (2) transformation and processing can be done for reporting, and we could combine the datasets even when we change the SaaS software. 

My huge considerations are that (1) the data is to be accessible within the department only and (2) it has to be decent cost. Currently considered Azure Data Lake Storage Gen2 & DataBricks, or Snowflake (to have both the lake and warehouse). My previous experience was only with Data Lake Storage Gen2.

I'm willing to work my way up for my technical limitations, but at this stage I am exploring the software solutions to get the buy in to kickstart this project. 

Any sharing is much appreciated, and if you worked with such an environment, I appreciate your guidance and learnings as well. Thank you in advance.",3,1,2025-04-08 16:27:08,0,False,False,False,False,2025-04-08 16:27:08,16,Tuesday,245.0,1373,60.95,13,366,12.0,0,0,NEGATIVE,-0.9981398582458496,"['need', 'help', 'need', 'proper', 'architecture', 'department', 'trying', 'get', 'data', 'lakewarehouse', 'lot', 'data', 'sources', 'saas', 'manually', 'created', 'documents', 'use', 'lot', 'saas', 'products', 'centralised', 'repository', 'store', 'stage', 'data', 'end', 'lot', 'workaround', 'using', 'sharepoint', 'csv', 'stored', 'folders', 'reporting', 'also', 'change', 'saas', 'products', 'quite', 'frequently', 'sources', 'change', 'often', 'difficult', 'advanced', 'analytics', 'prefer', 'lake', 'warehouse', 'approach', 'saas', 'users', 'drop', 'data', 'lake', 'transformation', 'processing', 'done', 'reporting', 'could', 'combine', 'datasets', 'even', 'change', 'saas', 'software', 'huge', 'considerations', 'data', 'accessible', 'within', 'department', 'decent', 'cost', 'currently', 'considered', 'azure', 'data', 'lake', 'storage', 'gen', 'databricks', 'snowflake', 'lake', 'warehouse', 'previous', 'experience', 'data', 'lake', 'storage', 'gen', 'willing', 'work', 'way', 'technical', 'limitations', 'stage', 'exploring', 'software', 'solutions', 'get', 'buy', 'kickstart', 'project', 'sharing', 'much', 'appreciated', 'worked', 'environment', 'appreciate', 'guidance', 'learnings', 'well', 'thank', 'advance']",need help need proper architecture department trying get data lakewarehouse lot data sources saas manually created documents use lot saas products centralised repository store stage data end lot workaround using sharepoint csv stored folders reporting also change saas products quite frequently sources change often difficult advanced analytics prefer lake warehouse approach saas users drop data lake transformation processing done reporting could combine datasets even change saas software huge considerations data accessible within department decent cost currently considered azure data lake storage gen databricks snowflake lake warehouse previous experience data lake storage gen willing work way technical limitations stage exploring software solutions get buy kickstart project sharing much appreciated worked environment appreciate guidance learnings well thank advance,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
166,How are entry level data engineering roles at Amazon?,"If anyone on this sub has worked for Amazon as a Data engineer, preferably entry level or early careers, how has your experience been working at amazon at Amazon? 

I’ve heard their work culture is very startup like, and their is an abundance of poor managers. The company just cars about share holder value, instead of caring for their customers and employees.

I wanted to hear on this sub, how has your experience been? How was the hiring process like? What all skills I should develop to work for Amazon?",2,2,2025-04-08 20:43:31,0,False,False,False,False,2025-04-08 20:43:31,20,Tuesday,90.0,508,56.25,6,140,12.2,0,0,NEGATIVE,-0.9991601705551147,"['anyone', 'sub', 'worked', 'amazon', 'data', 'engineer', 'preferably', 'entry', 'level', 'early', 'careers', 'experience', 'working', 'amazon', 'amazon', 'ive', 'heard', 'work', 'culture', 'startup', 'like', 'abundance', 'poor', 'managers', 'company', 'cars', 'share', 'holder', 'value', 'instead', 'caring', 'customers', 'employees', 'wanted', 'hear', 'sub', 'experience', 'hiring', 'process', 'like', 'skills', 'develop', 'work', 'amazon']",anyone sub worked amazon data engineer preferably entry level early careers experience working amazon amazon ive heard work culture startup like abundance poor managers company cars share holder value instead caring customers employees wanted hear sub experience hiring process like skills develop work amazon,Mid,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
167,Question around migrating to dbt,"We're considering moving from a dated ETL system to dbt with data being ingested via AWS Glue.

We have a data warehouse which uses a Kimball dimensional model, and I am wondering how we would migrate the dimension load processes.

We don't have access to all historic data, so it's not a case of being able to look across all files and then pull out the dimensions. Would it make sense fur the dimension table to be bothered a source and a dimension?

I'm still trying to pivot my way of thinking away from the traditional ETL approach so might be missing something obvious.",2,2,2025-04-08 15:09:02,0,False,False,False,False,2025-04-08 15:09:02,15,Tuesday,104.0,575,58.82,5,157,12.0,0,0,NEGATIVE,-0.9992192983627319,"['considering', 'moving', 'dated', 'etl', 'system', 'dbt', 'data', 'ingested', 'via', 'aws', 'glue', 'data', 'warehouse', 'uses', 'kimball', 'dimensional', 'model', 'wondering', 'would', 'migrate', 'dimension', 'load', 'processes', 'dont', 'access', 'historic', 'data', 'case', 'able', 'look', 'across', 'files', 'pull', 'dimensions', 'would', 'make', 'sense', 'fur', 'dimension', 'table', 'bothered', 'source', 'dimension', 'still', 'trying', 'pivot', 'way', 'thinking', 'away', 'traditional', 'etl', 'approach', 'might', 'missing', 'something', 'obvious']",considering moving dated etl system dbt data ingested via aws glue data warehouse uses kimball dimensional model wondering would migrate dimension load processes dont access historic data case able look across files pull dimensions would make sense fur dimension table bothered source dimension still trying pivot way thinking away traditional etl approach might missing something obvious,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
168,Beginner Predictive Model Feedback/Guidance,"My predictive modeling folks, beginner here could use some feedback guidance. Go easy on me, this is my first machine learning/predictive model project and I had very basic python experience before this.


I’ve been working on a personal project building a model that predicts NFL player performance using full career, game-by-game data for any offensive player who logged a snap between 2017–2024. 

I trained the model using data through 2023 with XGBoost Regressor, and then used actual 2024 matchups — including player demographics (age, team, position, depth chart) and opponent defensive stats (Pass YPG, Rush YPG, Points Allowed, etc.) — as inputs to predict game-level performance in 2024.

The model performs really well for some stats (e.g., R² > 0.875 for Completions, Pass Attempts, CMP%, Pass Yards, and Passer Rating), but others — like Touchdowns, Fumbles, or Yards per Target — aren’t as strong.

Here’s where I need input:

-What’s a solid baseline R², RMSE, and MAE to aim for — and does that benchmark shift depending on the industry?

-Could trying other models/a combination of models improve the weaker stats? Should I use different models for different stat categories (e.g., XGBoost for high-R² ones, something else for low-R²)?

-How do you typically decide which model is the best fit? Trial and error? Is there a structured way to choose based on the stat being predicted?

-I used XGBRegressor based on common recommendations — are there variants of XGBoost or alternatives you'd suggest trying? Any others you like better?

-Are these considered “good” model results for sports data?

-Are sports models generally harder to predict than industries like retail, finance, or real estate?

-What should my next step be if I want to make this model more complete and reliable (more accurate) across all stat types?

-How do people generally feel about manually adding in more intangible stats to tweak data and model performance? Example: Adding an injury index/strength multiplier for a Defense that has a lot of injuries, or more player’s coming back from injury, etc.? Is this a generally accepted method or not really utilized?

Any advice, criticism, resources, or just general direction is welcomed.",1,0,2025-04-09 02:27:57,0,False,False,False,False,2025-04-09 02:27:57,2,Wednesday,357.0,2230,56.05,23,575,11.9,0,0,NEGATIVE,-0.9959864020347595,"['predictive', 'modeling', 'folks', 'beginner', 'could', 'use', 'feedback', 'guidance', 'easy', 'first', 'machine', 'learningpredictive', 'model', 'project', 'basic', 'python', 'experience', 'ive', 'working', 'personal', 'project', 'building', 'model', 'predicts', 'nfl', 'player', 'performance', 'using', 'full', 'career', 'gamebygame', 'data', 'offensive', 'player', 'logged', 'snap', 'trained', 'model', 'using', 'data', 'xgboost', 'regressor', 'used', 'actual', 'matchups', 'including', 'player', 'demographics', 'age', 'team', 'position', 'depth', 'chart', 'opponent', 'defensive', 'stats', 'pass', 'ypg', 'rush', 'ypg', 'points', 'allowed', 'etc', 'inputs', 'predict', 'gamelevel', 'performance', 'model', 'performs', 'really', 'well', 'stats', 'completions', 'pass', 'attempts', 'cmp', 'pass', 'yards', 'passer', 'rating', 'others', 'like', 'touchdowns', 'fumbles', 'yards', 'per', 'target', 'arent', 'strong', 'heres', 'need', 'input', 'whats', 'solid', 'baseline', 'rmse', 'mae', 'aim', 'benchmark', 'shift', 'depending', 'industry', 'could', 'trying', 'modelsa', 'combination', 'models', 'improve', 'weaker', 'stats', 'use', 'different', 'models', 'different', 'stat', 'categories', 'xgboost', 'highr', 'ones', 'something', 'else', 'lowr', 'typically', 'decide', 'model', 'best', 'fit', 'trial', 'error', 'structured', 'way', 'choose', 'based', 'stat', 'predicted', 'used', 'xgbregressor', 'based', 'common', 'recommendations', 'variants', 'xgboost', 'alternatives', 'youd', 'suggest', 'trying', 'others', 'like', 'better', 'considered', 'good', 'model', 'results', 'sports', 'data', 'sports', 'models', 'generally', 'harder', 'predict', 'industries', 'like', 'retail', 'finance', 'real', 'estate', 'next', 'step', 'want', 'make', 'model', 'complete', 'reliable', 'accurate', 'across', 'stat', 'types', 'people', 'generally', 'feel', 'manually', 'adding', 'intangible', 'stats', 'tweak', 'data', 'model', 'performance', 'example', 'adding', 'injury', 'indexstrength', 'multiplier', 'defense', 'lot', 'injuries', 'players', 'coming', 'back', 'injury', 'etc', 'generally', 'accepted', 'method', 'really', 'utilized', 'advice', 'criticism', 'resources', 'general', 'direction', 'welcomed']",predictive modeling folks beginner could use feedback guidance easy first machine learningpredictive model project basic python experience ive working personal project building model predicts nfl player performance using full career gamebygame data offensive player logged snap trained model using data xgboost regressor used actual matchups including player demographics age team position depth chart opponent defensive stats pass ypg rush ypg points allowed etc inputs predict gamelevel performance model performs really well stats completions pass attempts cmp pass yards passer rating others like touchdowns fumbles yards per target arent strong heres need input whats solid baseline rmse mae aim benchmark shift depending industry could trying modelsa combination models improve weaker stats use different models different stat categories xgboost highr ones something else lowr typically decide model best fit trial error structured way choose based stat predicted used xgbregressor based common recommendations variants xgboost alternatives youd suggest trying others like better considered good model results sports data sports models generally harder predict industries like retail finance real estate next step want make model complete reliable accurate across stat types people generally feel manually adding intangible stats tweak data model performance example adding injury indexstrength multiplier defense lot injuries players coming back injury etc generally accepted method really utilized advice criticism resources general direction welcomed,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
169,Cornerstone data,"Hi all,

Has anybody pulled cornerstone training data using their APIs or used anyother method to pull the data?",1,0,2025-04-08 15:54:25,0,False,False,False,False,2025-04-08 15:54:25,15,Tuesday,19.0,112,60.65,1,29,0.0,0,0,NEGATIVE,-0.9992966651916504,"['anybody', 'pulled', 'cornerstone', 'training', 'data', 'using', 'apis', 'used', 'anyother', 'method', 'pull', 'data']",anybody pulled cornerstone training data using apis used anyother method pull data,Mid,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
170,GizmoSQL: Power your Enterprise analytics with Arrow Flight SQL and DuckDB,"Hi! This is Phil - Founder of [GizmoData](https://gizmodata.com). We have a new commercial database engine product called: [GizmoSQL](https://gizmodata.com/gizmosql) \- built with Apache Arrow Flight SQL (for remote connectivity) and DuckDB (or optionally: SQLite) as a back-end execution engine.

This product allows you to run DuckDB or SQLite as a server (remotely) - harnessing the power of computers in the cloud - which typically have more CPUs, more memory, and faster storage (NVMe) than your laptop. In fact, running GizmoSQL on a modern arm64-based VM in Azure, GCP, or AWS allows you to run at terabyte scale - with equivalent (or better) performance - for a fraction of the cost of other popular platforms such as Snowflake, BigQuery, or Databricks SQL.

**GizmoSQL** is self-hosted (for now) - with a possible SaaS offering in the near future. It has these features to differentiate it from ""base"" DuckDB:

* Run DuckDB or SQLite as a server (remote connectivity)
* Concurrency - allows multiple users to work simultaneously - with independent, ACID-compliant sessions
* Security
   * Authentication
   * TLS for encryption of traffic to/from the database
* Static executable with Arrow Flight SQL, DuckDB, SQLite, and JWT-CPP built-in. There are no dependencies to install - just a single executable file to run
* Free for use in development, evaluation, and testing
* Easily containerized for running in the Cloud - especially in Kubernetes
* Easy to talk to - with ADBC, JDBC, and ODBC drivers, and now a Websocket proxy server (created by GizmoData) - so it is easy to use with javascript frameworks
   * Use it with Tableau, PowerBI, Apache Superset dashboards, and more
* Easy to work with in Python - use ADBC, or the new experimental Ibis back-end - details here: [https://github.com/gizmodata/ibis-gizmosql](https://github.com/gizmodata/ibis-gizmosql)

Because it is powered by DuckDB - GizmoSQL can work with the popular open-source data formats - such as Iceberg, Delta Lake, Parquet, and more.

GizmoSQL performs very well (when running DuckDB as its back-end execution engine) - check out our graph comparing popular SQL engines for TPC-H at scale-factor 1 Terabyte - on the homepage at: [https://gizmodata.com/gizmosql](https://gizmodata.com/gizmosql) \- there you will find it also costs far less than other options.

We would love to get your feedback on the software - it is easy to get started for free in two different ways:

* For a limited time - try GizmoSQL online on our dime - with the SQL Query Navigator - it just requires a quick registration and sign-in to get going - at: [https://app.gizmodata.com](https://app.gizmodata.com) \- where we have a read-only 1TB TPC-H database mounted for you to query in real-time. It is running on an Azure Cobalt 100 VM - with local NVMe SSD's - so it should be quite zippy.
* Download and self-host GizmoSQL - using our Docker image or executables for Linux and macOS for both x86-64 and arm64 architectures. See our README at: [https://github.com/gizmodata/gizmosql-public](https://github.com/gizmodata/gizmosql-public) for details on how to easily and quickly get started that way

Thank you for taking a look at GizmoSQL. We are excited and are glad to answer any questions you may have!

* **Public facing repo (README):** [https://github.com/gizmodata/gizmosql-public?tab=readme-ov-file](https://github.com/gizmodata/gizmosql-public?tab=readme-ov-file)
* **HomePage**: [https://gizmodata.com/gizmosql](https://gizmodata.com/gizmosql)
* **ProductHunt:** [https://www.producthunt.com/posts/gizmosql?embed=true&utm\_source=badge-featured&utm\_medium=badge&utm\_souce=badge-gizmosql](https://www.producthunt.com/posts/gizmosql?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-gizmosql)
* **Try GizmoSQL online:** [https://app.gizmodata.com](https://app.gizmodata.com)
* **GizmoSQL in action video:** [https://youtu.be/QSlE6FWlAaM](https://youtu.be/QSlE6FWlAaM)",1,1,2025-04-08 13:30:54,0,False,False,False,False,2025-04-08 13:30:54,13,Tuesday,538.0,3960,16.02,23,972,13.8,1,0,NEGATIVE,-0.9957109689712524,"['phil', 'founder', 'gizmodatahttpsgizmodatacom', 'new', 'commercial', 'database', 'engine', 'product', 'called', 'gizmosqlhttpsgizmodatacomgizmosql', 'built', 'apache', 'arrow', 'flight', 'sql', 'remote', 'connectivity', 'duckdb', 'optionally', 'sqlite', 'backend', 'execution', 'engine', 'product', 'allows', 'run', 'duckdb', 'sqlite', 'server', 'remotely', 'harnessing', 'power', 'computers', 'cloud', 'typically', 'cpus', 'memory', 'faster', 'storage', 'nvme', 'laptop', 'fact', 'running', 'gizmosql', 'modern', 'armbased', 'azure', 'gcp', 'aws', 'allows', 'run', 'terabyte', 'scale', 'equivalent', 'better', 'performance', 'fraction', 'cost', 'popular', 'platforms', 'snowflake', 'bigquery', 'databricks', 'sql', 'gizmosql', 'selfhosted', 'possible', 'saas', 'offering', 'near', 'future', 'features', 'differentiate', 'base', 'duckdb', 'run', 'duckdb', 'sqlite', 'server', 'remote', 'connectivity', 'concurrency', 'allows', 'multiple', 'users', 'work', 'simultaneously', 'independent', 'acidcompliant', 'sessions', 'security', 'authentication', 'tls', 'encryption', 'traffic', 'tofrom', 'database', 'static', 'executable', 'arrow', 'flight', 'sql', 'duckdb', 'sqlite', 'jwtcpp', 'builtin', 'dependencies', 'install', 'single', 'executable', 'file', 'run', 'free', 'use', 'development', 'evaluation', 'testing', 'easily', 'containerized', 'running', 'cloud', 'especially', 'kubernetes', 'easy', 'talk', 'adbc', 'jdbc', 'odbc', 'drivers', 'websocket', 'proxy', 'server', 'created', 'gizmodata', 'easy', 'use', 'javascript', 'frameworks', 'use', 'tableau', 'powerbi', 'apache', 'superset', 'dashboards', 'easy', 'work', 'python', 'use', 'adbc', 'new', 'experimental', 'ibis', 'backend', 'details', 'httpsgithubcomgizmodataibisgizmosqlhttpsgithubcomgizmodataibisgizmosql', 'powered', 'duckdb', 'gizmosql', 'work', 'popular', 'opensource', 'data', 'formats', 'iceberg', 'delta', 'lake', 'parquet', 'gizmosql', 'performs', 'well', 'running', 'duckdb', 'backend', 'execution', 'engine', 'check', 'graph', 'comparing', 'popular', 'sql', 'engines', 'tpch', 'scalefactor', 'terabyte', 'homepage', 'httpsgizmodatacomgizmosqlhttpsgizmodatacomgizmosql', 'find', 'also', 'costs', 'far', 'less', 'options', 'would', 'love', 'get', 'feedback', 'software', 'easy', 'get', 'started', 'free', 'two', 'different', 'ways', 'limited', 'time', 'try', 'gizmosql', 'online', 'dime', 'sql', 'query', 'navigator', 'requires', 'quick', 'registration', 'signin', 'get', 'going', 'httpsappgizmodatacomhttpsappgizmodatacom', 'readonly', 'tpch', 'database', 'mounted', 'query', 'realtime', 'running', 'azure', 'cobalt', 'local', 'nvme', 'ssds', 'quite', 'zippy', 'download', 'selfhost', 'gizmosql', 'using', 'docker', 'image', 'executables', 'linux', 'macos', 'arm', 'architectures', 'see', 'readme', 'httpsgithubcomgizmodatagizmosqlpublichttpsgithubcomgizmodatagizmosqlpublic', 'details', 'easily', 'quickly', 'get', 'started', 'way', 'thank', 'taking', 'look', 'gizmosql', 'excited', 'glad', 'answer', 'questions', 'may', 'public', 'facing', 'repo', 'readme', 'httpsgithubcomgizmodatagizmosqlpublictabreadmeovfilehttpsgithubcomgizmodatagizmosqlpublictabreadmeovfile', 'homepage', 'httpsgizmodatacomgizmosqlhttpsgizmodatacomgizmosql', 'producthunt', 'httpswwwproducthuntcompostsgizmosqlembedtrueutmsourcebadgefeaturedutmmediumbadgeutmsoucebadgegizmosqlhttpswwwproducthuntcompostsgizmosqlembedtrueutmsourcebadgefeaturedutmmediumbadgeutmsoucebadgegizmosql', 'try', 'gizmosql', 'online', 'httpsappgizmodatacomhttpsappgizmodatacom', 'gizmosql', 'action', 'video', 'httpsyoutubeqslefwlaamhttpsyoutubeqslefwlaam']",phil founder gizmodatahttpsgizmodatacom new commercial database engine product called gizmosqlhttpsgizmodatacomgizmosql built apache arrow flight sql remote connectivity duckdb optionally sqlite backend execution engine product allows run duckdb sqlite server remotely harnessing power computers cloud typically cpus memory faster storage nvme laptop fact running gizmosql modern armbased azure gcp aws allows run terabyte scale equivalent better performance fraction cost popular platforms snowflake bigquery databricks sql gizmosql selfhosted possible saas offering near future features differentiate base duckdb run duckdb sqlite server remote connectivity concurrency allows multiple users work simultaneously independent acidcompliant sessions security authentication tls encryption traffic tofrom database static executable arrow flight sql duckdb sqlite jwtcpp builtin dependencies install single executable file run free use development evaluation testing easily containerized running cloud especially kubernetes easy talk adbc jdbc odbc drivers websocket proxy server created gizmodata easy use javascript frameworks use tableau powerbi apache superset dashboards easy work python use adbc new experimental ibis backend details httpsgithubcomgizmodataibisgizmosqlhttpsgithubcomgizmodataibisgizmosql powered duckdb gizmosql work popular opensource data formats iceberg delta lake parquet gizmosql performs well running duckdb backend execution engine check graph comparing popular sql engines tpch scalefactor terabyte homepage httpsgizmodatacomgizmosqlhttpsgizmodatacomgizmosql find also costs far less options would love get feedback software easy get started free two different ways limited time try gizmosql online dime sql query navigator requires quick registration signin get going httpsappgizmodatacomhttpsappgizmodatacom readonly tpch database mounted query realtime running azure cobalt local nvme ssds quite zippy download selfhost gizmosql using docker image executables linux macos arm architectures see readme httpsgithubcomgizmodatagizmosqlpublichttpsgithubcomgizmodatagizmosqlpublic details easily quickly get started way thank taking look gizmosql excited glad answer questions may public facing repo readme httpsgithubcomgizmodatagizmosqlpublictabreadmeovfilehttpsgithubcomgizmodatagizmosqlpublictabreadmeovfile homepage httpsgizmodatacomgizmosqlhttpsgizmodatacomgizmosql producthunt httpswwwproducthuntcompostsgizmosqlembedtrueutmsourcebadgefeaturedutmmediumbadgeutmsoucebadgegizmosqlhttpswwwproducthuntcompostsgizmosqlembedtrueutmsourcebadgefeaturedutmmediumbadgeutmsoucebadgegizmosql try gizmosql online httpsappgizmodatacomhttpsappgizmodatacom gizmosql action video httpsyoutubeqslefwlaamhttpsyoutubeqslefwlaam,Mid,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
171,Is there any tool you use to keep track on the dates you need to reset API keys?,"I currently use teams events where I set a day on my calendar to update keys, but there has to be a better way. How do you guys do it?

Edit: The idea is to renew keys before they expire and there are no errors in the pipelines",1,7,2025-04-08 12:26:25,0,False,2025-04-08 19:04:12,False,False,2025-04-08 12:26:25,12,Tuesday,48.0,227,80.62,3,63,8.8,0,0,NEGATIVE,-0.91176438331604,"['currently', 'use', 'teams', 'events', 'set', 'day', 'calendar', 'update', 'keys', 'better', 'way', 'guys', 'edit', 'idea', 'renew', 'keys', 'expire', 'errors', 'pipelines']",currently use teams events set day calendar update keys better way guys edit idea renew keys expire errors pipelines,Mid,0,0,"need, table, event, source, without, instead, every, replay, day, example, single, change, per, business, daily, based, keys, manually, model, keep, needs, events, doesnt, name, list, option, user, must, takes, history, often, truth, full, column, hour, address, requirements, date, reading, track, whenever, metrics, result, update, feature, primary, columns, meaning, pricing, structure, details, rows, simply, available, dedicated, country, means, facebook, error, missing, evolve, involves, names, size, domain, place, addresses, timestamp, parse, mean, slow, schemas, heres, return, dimensions, delete, state, times, arent, ground, errors, receive, min, added, geocoding, shape, sourcing, wasnt, insert, minutes, value, flow, save, normal, little, region, usually, bad, becomes, failure, replication, entirely, products, infrastructure, minute, person, average, servers, internet, looked, probably, campaigns, workloads, looks",8.2,7.4,175.5,-0.663,1069.0,47.9,9.8,275.5,11.4,22.2,15.5,52,108,34,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",4,4
172,How do you group your tables into pipelines?,"I was wondering how do data engineers in different company group their pipelines together ?

Usually tables need to be refreshed at some specific refresh rates. This means that some table upstream might require 1h refresh while downstream table might require daily.

I can see people grouping things by domain and running domain one after each other sequentially, but then this break the concept of having different refresh rate per table or domain. I can see table configure with multiple corn but then I see issues with needing to schedule offset in cron jobs. 

Like most of the domain are very close to each other so when creating them I might be mixing a lot of stuff together which would impact downstream.

What’s your experience in structuring pipeline? Or any good reference I can read ?
",1,9,2025-04-08 11:13:44,0,False,False,False,False,2025-04-08 11:13:44,11,Tuesday,136.0,797,54.42,8,209,11.7,0,0,NEGATIVE,-0.9974367022514343,"['wondering', 'data', 'engineers', 'different', 'company', 'group', 'pipelines', 'together', 'usually', 'tables', 'need', 'refreshed', 'specific', 'refresh', 'rates', 'means', 'table', 'upstream', 'might', 'require', 'refresh', 'downstream', 'table', 'might', 'require', 'daily', 'see', 'people', 'grouping', 'things', 'domain', 'running', 'domain', 'one', 'sequentially', 'break', 'concept', 'different', 'refresh', 'rate', 'per', 'table', 'domain', 'see', 'table', 'configure', 'multiple', 'corn', 'see', 'issues', 'needing', 'schedule', 'offset', 'cron', 'jobs', 'like', 'domain', 'close', 'creating', 'might', 'mixing', 'lot', 'stuff', 'together', 'would', 'impact', 'downstream', 'whats', 'experience', 'structuring', 'pipeline', 'good', 'reference', 'read']",wondering data engineers different company group pipelines together usually tables need refreshed specific refresh rates means table upstream might require refresh downstream table might require daily see people grouping things domain running domain one sequentially break concept different refresh rate per table domain see table configure multiple corn see issues needing schedule offset cron jobs like domain close creating might mixing lot stuff together would impact downstream whats experience structuring pipeline good reference read,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
173,What is the best way to reflect data in clickhouse from MySQL other than the MySQL engine?,"Hi everyone, I am working on a project currently where we have a MySQL database. We are using clickhouse as our warehouse. 

What we need to achieve is to reflect the data from MySQL to clickhouse for certain tables. For this, I found a few ways and am looking to get some insights on which method has the most potential and if there are other methods as welp:

1. Use the MySQL engine in clickhouse. 

Pros: No need to store data in clickhouse as it can just proxy it directly from MySQL.

Cons: This however puts extra reads on MySQL and doesn't help us if MySQL ever goes down. 

2. Use signals to send the data to clickhouse whenever there is a change in MySQL.

Pros: We don't have a lot of tables currently so it's the quickest to setup. 

Cons: Extremely inefficient and not scalable. 

3. Use some sort of third party sink to achieve this. I have found this https://github.com/Altinity/clickhouse-sink-connector which seems to do the job but it has way too many open issues and not sure if it is reliable enough. Plus, it complicates our tech stack which we are looking not to do. 

I'm open to any other ideas. We would ideally not want to duplicate this data in clickhouse but if that's the last resort we would go for it. 

Thanks in advance. 

P.S, I am a beginner in data engineering so feel free to correct me if I've used some wrong jargons or if I am seriously deviating from the right path. ",1,9,2025-04-08 06:49:07,0,False,False,False,False,2025-04-08 06:49:07,6,Tuesday,258.0,1408,73.88,18,363,9.3,1,0,NEGATIVE,-0.994644284248352,"['everyone', 'working', 'project', 'currently', 'mysql', 'database', 'using', 'clickhouse', 'warehouse', 'need', 'achieve', 'reflect', 'data', 'mysql', 'clickhouse', 'certain', 'tables', 'found', 'ways', 'looking', 'get', 'insights', 'method', 'potential', 'methods', 'welp', 'use', 'mysql', 'engine', 'clickhouse', 'pros', 'need', 'store', 'data', 'clickhouse', 'proxy', 'directly', 'mysql', 'cons', 'however', 'puts', 'extra', 'reads', 'mysql', 'doesnt', 'help', 'mysql', 'ever', 'goes', 'use', 'signals', 'send', 'data', 'clickhouse', 'whenever', 'change', 'mysql', 'pros', 'dont', 'lot', 'tables', 'currently', 'quickest', 'setup', 'cons', 'extremely', 'inefficient', 'scalable', 'use', 'sort', 'third', 'party', 'sink', 'achieve', 'found', 'httpsgithubcomaltinityclickhousesinkconnector', 'seems', 'job', 'way', 'many', 'open', 'issues', 'sure', 'reliable', 'enough', 'plus', 'complicates', 'tech', 'stack', 'looking', 'open', 'ideas', 'would', 'ideally', 'want', 'duplicate', 'data', 'clickhouse', 'thats', 'last', 'resort', 'would', 'thanks', 'advance', 'beginner', 'data', 'engineering', 'feel', 'free', 'correct', 'ive', 'used', 'wrong', 'jargons', 'seriously', 'deviating', 'right', 'path']",everyone working project currently mysql database using clickhouse warehouse need achieve reflect data mysql clickhouse certain tables found ways looking get insights method potential methods welp use mysql engine clickhouse pros need store data clickhouse proxy directly mysql cons however puts extra reads mysql doesnt help mysql ever goes use signals send data clickhouse whenever change mysql pros dont lot tables currently quickest setup cons extremely inefficient scalable use sort third party sink achieve found httpsgithubcomaltinityclickhousesinkconnector seems job way many open issues sure reliable enough plus complicates tech stack looking open ideas would ideally want duplicate data clickhouse thats last resort would thanks advance beginner data engineering feel free correct ive used wrong jargons seriously deviating right path,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
174,Designing a database ERP from scratch.,"My goal is to re create something like Oracle's Net-suite, are there any help full resources on how i can go about it. i have previously worked on simple Finance management systems but this one is more complicated. i need sample ERD's books or anything helpfull atp

",0,2,2025-04-08 20:09:25,0,False,False,False,False,2025-04-08 20:09:25,20,Tuesday,47.0,267,55.54,3,73,11.9,0,0,NEGATIVE,-0.9986122846603394,"['goal', 'create', 'something', 'like', 'oracles', 'netsuite', 'help', 'full', 'resources', 'previously', 'worked', 'simple', 'finance', 'management', 'systems', 'one', 'complicated', 'need', 'sample', 'erds', 'books', 'anything', 'helpfull', 'atp']",goal create something like oracles netsuite help full resources previously worked simple finance management systems one complicated need sample erds books anything helpfull atp,Low,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
175,Beginning Data Scientist in Azure needing some help (iot),"Hi all,

I currently am working on a new structure to save sensor data coming from Azure Iot Hub in Azure to store it into Azure Blob Storage for historical data, and Clickhouse for hot data with TTL (around half year). The sensor data is coming from different entities (e.g building1, boat1, boat2) and should be partioned by entity. The data we’re processing daily is around 300-2 million records per day.

I know Azure Iot Hub is essentially a built-in Azure Hub. I had a few questions since I’ve tried multiple solutions. 

1. Normal message routing to Azure Blob
Issue: no custom partitioning on file structure (e.g entityid/timestamp_sensor/) it requires you to use the enqueued time. And there is no dead letter queue for fallback

2. IoT hub -> Azure Functions -> Blob Storage & Clickhouse
Issue: this should work correctly but I have not that much experience in Azure Functions, I tried creating a function with the IoT Hub template but it seems I need to also have an Event Hubs namespace which is not what I want. HTTP trigger is also not what I want. I don’t find any good documentation on it aswell. I know I can maybe use Event Hubs trigger and use the Iot Hub connection string but I didn’t manage to do this yet.

3. IoT hub -> Event Grid 
Someone suggested using Event Grid, however to my knowledge Event Grid is not used for telemetry data despite there being an option for. Is this beneficial? I don’t really know what the flow would be since you can’t use Event Grid to send data to Clickhouse. You would still need an Azure Functions.

4. IoT Hub -> Event Grid -> Event Hubs -> Azure Functions -> Azure Blob & Clickhouse
This one seemed the most appealing to me but I don’t know if it’s the smartest, it can get expensive (maybe).
But the idea here is that we use Event Grid for batching the data and to have a dead letter queue.
Arrived in Event Hubs we use an Azure Function to send the data to blob storage and clickhouse.

The only problem is I might need some delay to sending to Clickhouse & Blob Storage (around maybe every 15 minutes) to reduce the risks of memory usage in Clickhouse and to reduce costs.

Can someone help me out? Am I forgetting something crucial? I am a graduated data scientist, however I have no in depth experience with Azure.


",0,8,2025-04-08 17:18:34,0,False,2025-04-08 18:22:26,False,False,2025-04-08 17:18:34,17,Tuesday,415.0,2281,62.78,24,595,9.7,1,1,NEGATIVE,-0.996820330619812,"['currently', 'working', 'new', 'structure', 'save', 'sensor', 'data', 'coming', 'azure', 'iot', 'hub', 'azure', 'store', 'azure', 'blob', 'storage', 'historical', 'data', 'clickhouse', 'hot', 'data', 'ttl', 'around', 'half', 'year', 'sensor', 'data', 'coming', 'different', 'entities', 'building', 'boat', 'boat', 'partioned', 'entity', 'data', 'processing', 'daily', 'around', 'million', 'records', 'per', 'day', 'know', 'azure', 'iot', 'hub', 'essentially', 'builtin', 'azure', 'hub', 'questions', 'since', 'ive', 'tried', 'multiple', 'solutions', 'normal', 'message', 'routing', 'azure', 'blob', 'issue', 'custom', 'partitioning', 'file', 'structure', 'entityidtimestampsensor', 'requires', 'use', 'enqueued', 'time', 'dead', 'letter', 'queue', 'fallback', 'iot', 'hub', 'azure', 'functions', 'blob', 'storage', 'clickhouse', 'issue', 'work', 'correctly', 'much', 'experience', 'azure', 'functions', 'tried', 'creating', 'function', 'iot', 'hub', 'template', 'seems', 'need', 'also', 'event', 'hubs', 'namespace', 'want', 'http', 'trigger', 'also', 'want', 'dont', 'find', 'good', 'documentation', 'aswell', 'know', 'maybe', 'use', 'event', 'hubs', 'trigger', 'use', 'iot', 'hub', 'connection', 'string', 'didnt', 'manage', 'yet', 'iot', 'hub', 'event', 'grid', 'someone', 'suggested', 'using', 'event', 'grid', 'however', 'knowledge', 'event', 'grid', 'used', 'telemetry', 'data', 'despite', 'option', 'beneficial', 'dont', 'really', 'know', 'flow', 'would', 'since', 'cant', 'use', 'event', 'grid', 'send', 'data', 'clickhouse', 'would', 'still', 'need', 'azure', 'functions', 'iot', 'hub', 'event', 'grid', 'event', 'hubs', 'azure', 'functions', 'azure', 'blob', 'clickhouse', 'one', 'seemed', 'appealing', 'dont', 'know', 'smartest', 'get', 'expensive', 'maybe', 'idea', 'use', 'event', 'grid', 'batching', 'data', 'dead', 'letter', 'queue', 'arrived', 'event', 'hubs', 'use', 'azure', 'function', 'send', 'data', 'blob', 'storage', 'clickhouse', 'problem', 'might', 'need', 'delay', 'sending', 'clickhouse', 'blob', 'storage', 'around', 'maybe', 'every', 'minutes', 'reduce', 'risks', 'memory', 'usage', 'clickhouse', 'reduce', 'costs', 'someone', 'help', 'forgetting', 'something', 'crucial', 'graduated', 'data', 'scientist', 'however', 'depth', 'experience', 'azure']",currently working new structure save sensor data coming azure iot hub azure store azure blob storage historical data clickhouse hot data ttl around half year sensor data coming different entities building boat boat partioned entity data processing daily around million records per day know azure iot hub essentially builtin azure hub questions since ive tried multiple solutions normal message routing azure blob issue custom partitioning file structure entityidtimestampsensor requires use enqueued time dead letter queue fallback iot hub azure functions blob storage clickhouse issue work correctly much experience azure functions tried creating function iot hub template seems need also event hubs namespace want http trigger also want dont find good documentation aswell know maybe use event hubs trigger use iot hub connection string didnt manage yet iot hub event grid someone suggested using event grid however knowledge event grid used telemetry data despite option beneficial dont really know flow would since cant use event grid send data clickhouse would still need azure functions iot hub event grid event hubs azure functions azure blob clickhouse one seemed appealing dont know smartest get expensive maybe idea use event grid batching data dead letter queue arrived event hubs use azure function send data blob storage clickhouse problem might need delay sending clickhouse blob storage around maybe every minutes reduce risks memory usage clickhouse reduce costs someone help forgetting something crucial graduated data scientist however depth experience azure,Low,4,4,"files, api, tables, query, models, read, google, json, file, logic, write, case, csv, store, datasets, across, sources, metadata, created, schema, systems, large, platform, custom, script, changes, dataset, row, reporting, parquet, issues, ads, load, fields, analysis, cases, product, values, design, view, idea, layer, search, import, suggest, language, excel, app, blob, form, request, params, postgres, ingestion, raw, mysql, fact, processed, structures, either, separate, structured, connection, core, method, downstream, scripts, extract, maintain, send, hub, records, transactional, overhead, lineage, environment, object, external, pull, iot, order, typically, grid, hubs, validation, requires, mapping, batch, mechanism, location, updates, keeping, checkout, operations, runs, convert, optimized, avoid, matching, views, formats, matters, vpc, bring, reduce, migrate, consists, starts, reads, refresh, relationships, bronze, historical, tracking, developers, natural",8.0,7.1,169.4,-0.679,1041.7,44.5,9.5,267.7,10.7,24.1,17.9,49,102,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,7
176,Experienced data engineer looking to expand to devops,"Hey everyone, I've been a working a few years as a data engineer, I'd say I'm very comfortable in python (databricks), sql and git and have mostly worked in Azure. I would like to get comfortable with devops, setting up proper ci/cd, iac etc.

What resources would you recommend?

Where I work we 2 repos set up, an infratsructure repo that I am totally clueless about that is mostly terraform and another repo where we make changes to notebooks and pipelines etc whose structure makes more sense to me.

The whole thing was initially set up by consultants. My goal is really to understand how it was set up, why 2 different repos, how to change the ci/cd pipeline to add testing etc.

Thanks!",0,5,2025-04-08 05:33:41,0,False,False,False,False,2025-04-08 05:33:41,5,Tuesday,124.0,693,58.92,6,191,12.7,0,0,NEGATIVE,-0.9000080823898315,"['hey', 'everyone', 'ive', 'working', 'years', 'data', 'engineer', 'say', 'comfortable', 'python', 'databricks', 'sql', 'git', 'mostly', 'worked', 'azure', 'would', 'like', 'get', 'comfortable', 'devops', 'setting', 'proper', 'cicd', 'iac', 'etc', 'resources', 'would', 'recommend', 'work', 'repos', 'set', 'infratsructure', 'repo', 'totally', 'clueless', 'mostly', 'terraform', 'another', 'repo', 'make', 'changes', 'notebooks', 'pipelines', 'etc', 'whose', 'structure', 'makes', 'sense', 'whole', 'thing', 'initially', 'set', 'consultants', 'goal', 'really', 'understand', 'set', 'different', 'repos', 'change', 'cicd', 'pipeline', 'add', 'testing', 'etc', 'thanks']",hey everyone ive working years data engineer say comfortable python databricks sql git mostly worked azure would like get comfortable devops setting proper cicd iac etc resources would recommend work repos set infratsructure repo totally clueless mostly terraform another repo make changes notebooks pipelines etc whose structure makes sense whole thing initially set consultants goal really understand set different repos change cicd pipeline add testing etc thanks,Low,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
177,Mirror snowflake to PG,"Hi everyone,
Once per day, my team needs to mirror a lot of tables from snowflake to postgres. 
Currently, we are copying data with script written with GO.
do you familiar with tools, or any idea what is the best way to mirror the tables?",0,6,2025-04-08 16:38:21,0,False,2025-04-08 17:13:11,False,False,2025-04-08 16:38:21,16,Tuesday,45.0,238,73.17,3,64,10.5,0,0,NEGATIVE,-0.9989258646965027,"['everyone', 'per', 'day', 'team', 'needs', 'mirror', 'lot', 'tables', 'snowflake', 'postgres', 'currently', 'copying', 'data', 'script', 'written', 'familiar', 'tools', 'idea', 'best', 'way', 'mirror', 'tables']",everyone per day team needs mirror lot tables snowflake postgres currently copying data script written familiar tools idea best way mirror tables,Low,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
178,Hot Take: You shouldn't be a data engineer if you've never been a data analyst,"You're better able to understand the needs and goals of what you're actually working towards when you being as an analyst. Not to mention the other skills that you develop whist being an analyst. Understanding downstream requirements helps build DE pipelines carefully keeping in mind the end goals.

What are you thoughts on this?",0,6,2025-04-08 18:11:04,0,False,False,False,False,2025-04-08 18:11:04,18,Tuesday,54.0,331,66.23,4,83,11.2,0,0,POSITIVE,0.9828042984008789,"['youre', 'better', 'able', 'understand', 'needs', 'goals', 'youre', 'actually', 'working', 'towards', 'analyst', 'mention', 'skills', 'develop', 'whist', 'analyst', 'understanding', 'downstream', 'requirements', 'helps', 'build', 'pipelines', 'carefully', 'keeping', 'mind', 'end', 'goals', 'thoughts']",youre better able understand needs goals youre actually working towards analyst mention skills develop whist analyst understanding downstream requirements helps build pipelines carefully keeping mind end goals thoughts,Low,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
179,Understand basics of Snowflake ❄️❄️,"Exciting news, a new blog post about Snowflake architecture. Dive in and explore all the amazing features! 

https://medium.com/@adityasharmah27/understanding-snowflake-architecture-a-beginners-guide-to-cloud-data-warehousing-22a6f4e3a6be?sk=40c0128a3f07d30ba0cd92ab710112ae",21,0,2025-04-12 06:38:55,0,False,False,False,False,2025-04-12 06:38:55,6,Saturday,18.0,274,-22.27,2,46,0.0,1,0,POSITIVE,0.9985686540603638,"['exciting', 'news', 'new', 'blog', 'post', 'snowflake', 'architecture', 'dive', 'explore', 'amazing', 'features', 'httpsmediumcomadityasharmahunderstandingsnowflakearchitectureabeginnersguidetoclouddatawarehousingafeabeskcafdbacdabae']",exciting news new blog post snowflake architecture dive explore amazing features httpsmediumcomadityasharmahunderstandingsnowflakearchitectureabeginnersguidetoclouddatawarehousingafeabeskcafdbacdabae,High,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
180,How do my fellow on-prem DEs keep their sanity...,"...the joys of memory and compute resources seems to be a neverending suck 😭

We're building ETL pipelines, using Airflow in one K8s namespace and Spark in another (the latter having dedicated hardware). Most data workloads aren't really Spark-worthy as files are typically <20GB, and we keep hitting pain points where processes struggle in Airflow's memory (workers are 6Gi and 6 CPU, with a limit of 10GI; no KEDA or HPA). We are looking into more efficient data structures like DuckDB, Polars, etc or running ""mid-tier"" processes as separate K8s jobs but then we hit constraints like tools/libraries relying on Pandas use so we seem stuck with eager processes.

Case in point, I just learned that our teams are having to split files into smaller files of 125k records so Pydantic schema validation won't fail on memory. I looked into GX Core and see the main source options there again appear to be Pandas or Spark dataframes (yes, I'm going to try DuckDB through SQLAlchemy). I could bite the bullet and just say to go with Spark, but then our pipelines will be using Spark for QA and not for ETL which will be fun to keep clarifying. 

Sisyphus is the patron saint of Data Engineering... just sayin'

[Make it stoooooooooop!](https://preview.redd.it/qwikfhcpihue1.png?width=503&format=png&auto=webp&s=6565d874d8d2213835c172a8ed449b14cff8214a)

(there may be some internal sobbing/laughing whenever I see posts asking ""should I get into DE..."")",15,6,2025-04-12 23:08:19,0,False,False,False,False,2025-04-12 23:08:19,23,Saturday,227.0,1448,54.46,9,348,12.8,1,1,NEGATIVE,-0.9991188645362854,"['joys', 'memory', 'compute', 'resources', 'seems', 'neverending', 'suck', 'building', 'etl', 'pipelines', 'using', 'airflow', 'one', 'namespace', 'spark', 'another', 'latter', 'dedicated', 'hardware', 'data', 'workloads', 'arent', 'really', 'sparkworthy', 'files', 'typically', 'keep', 'hitting', 'pain', 'points', 'processes', 'struggle', 'airflows', 'memory', 'workers', 'cpu', 'limit', 'keda', 'hpa', 'looking', 'efficient', 'data', 'structures', 'like', 'duckdb', 'polars', 'etc', 'running', 'midtier', 'processes', 'separate', 'jobs', 'hit', 'constraints', 'like', 'toolslibraries', 'relying', 'pandas', 'use', 'seem', 'stuck', 'eager', 'processes', 'case', 'point', 'learned', 'teams', 'split', 'files', 'smaller', 'files', 'records', 'pydantic', 'schema', 'validation', 'wont', 'fail', 'memory', 'looked', 'core', 'see', 'main', 'source', 'options', 'appear', 'pandas', 'spark', 'dataframes', 'yes', 'going', 'try', 'duckdb', 'sqlalchemy', 'could', 'bite', 'bullet', 'say', 'spark', 'pipelines', 'using', 'spark', 'etl', 'fun', 'keep', 'clarifying', 'sisyphus', 'patron', 'saint', 'data', 'engineering', 'sayin', 'make', 'stoooooooooophttpspreviewredditqwikfhcpihuepngwidthformatpngautowebpsdddcaedbcffa', 'may', 'internal', 'sobbinglaughing', 'whenever', 'see', 'posts', 'asking', 'get']",joys memory compute resources seems neverending suck building etl pipelines using airflow one namespace spark another latter dedicated hardware data workloads arent really sparkworthy files typically keep hitting pain points processes struggle airflows memory workers cpu limit keda hpa looking efficient data structures like duckdb polars etc running midtier processes separate jobs hit constraints like toolslibraries relying pandas use seem stuck eager processes case point learned teams split files smaller files records pydantic schema validation wont fail memory looked core see main source options appear pandas spark dataframes yes going try duckdb sqlalchemy could bite bullet say spark pipelines using spark etl fun keep clarifying sisyphus patron saint data engineering sayin make stoooooooooophttpspreviewredditqwikfhcpihuepngwidthformatpngautowebpsdddcaedbcffa may internal sobbinglaughing whenever see posts asking get,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
181,Need course advice on building ETL Piplines in Databricks using Python.,Please suggest Courses/YT Channels on building ETL Pipelines in Databricks using Python. I have good knowledge on Pandas and NumPy and also used Databricks for my personal projects but never build ETL Piplines.,13,5,2025-04-12 06:15:15,0,False,False,False,False,2025-04-12 06:15:15,6,Saturday,33.0,210,63.19,2,48,0.0,0,0,NEGATIVE,-0.9793593883514404,"['please', 'suggest', 'coursesyt', 'channels', 'building', 'etl', 'pipelines', 'databricks', 'using', 'python', 'good', 'knowledge', 'pandas', 'numpy', 'also', 'used', 'databricks', 'personal', 'projects', 'never', 'build', 'etl', 'piplines']",please suggest coursesyt channels building etl pipelines databricks using python good knowledge pandas numpy also used databricks personal projects never build etl piplines,High,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
182,Data Inserts best practices with Iceberg,"I receive various files at different intervals which are not defined. Can be every seconds, hour, daily, etc.

I don’t have any indication also of when something is finished. For example, it’s highly possible to have 100 files that would end up being 100% of my daily table, but I receive them scattered over 15min-30 when the data become available and my ingestion process ingest it. Can be 1 to 12 hours after the day is over.

Not that’s it’s also possible to have 10000 very small files per day.

I’m wondering how is this solves with Iceberg tables. Very newbie Iceberg guy here. Like I don’t see throughput write benchmark anywhere but I figure that rewriting the metadata files must be a big overhead if there’s a very large amount of files so inserting every times there’s a new one must not be the ideal solution.

I’ve read some medium post saying that there was a snapshot feature which track new files so you don’t have to do some fancy things to load them incrementally. But again if every insert is a query that change the metadata files it must be bad at some point.

Do you wait and usually build a process to store a list of files before inserting them or is this a feature build somewhere already in a doc I can’t find ?

Any help would be appreciated.

",9,0,2025-04-12 23:44:32,1,False,False,False,False,2025-04-12 23:44:32,23,Saturday,233.0,1272,61.87,13,342,11.4,0,0,NEGATIVE,-0.9944283962249756,"['receive', 'various', 'files', 'different', 'intervals', 'defined', 'every', 'seconds', 'hour', 'daily', 'etc', 'dont', 'indication', 'also', 'something', 'finished', 'example', 'highly', 'possible', 'files', 'would', 'end', 'daily', 'table', 'receive', 'scattered', 'min', 'data', 'become', 'available', 'ingestion', 'process', 'ingest', 'hours', 'day', 'thats', 'also', 'possible', 'small', 'files', 'per', 'day', 'wondering', 'solves', 'iceberg', 'tables', 'newbie', 'iceberg', 'guy', 'like', 'dont', 'see', 'throughput', 'write', 'benchmark', 'anywhere', 'figure', 'rewriting', 'metadata', 'files', 'must', 'big', 'overhead', 'theres', 'large', 'amount', 'files', 'inserting', 'every', 'times', 'theres', 'new', 'one', 'must', 'ideal', 'solution', 'ive', 'read', 'medium', 'post', 'saying', 'snapshot', 'feature', 'track', 'new', 'files', 'dont', 'fancy', 'things', 'load', 'incrementally', 'every', 'insert', 'query', 'change', 'metadata', 'files', 'must', 'bad', 'point', 'wait', 'usually', 'build', 'process', 'store', 'list', 'files', 'inserting', 'feature', 'build', 'somewhere', 'already', 'doc', 'cant', 'find', 'help', 'would', 'appreciated']",receive various files different intervals defined every seconds hour daily etc dont indication also something finished example highly possible files would end daily table receive scattered min data become available ingestion process ingest hours day thats also possible small files per day wondering solves iceberg tables newbie iceberg guy like dont see throughput write benchmark anywhere figure rewriting metadata files must big overhead theres large amount files inserting every times theres new one must ideal solution ive read medium post saying snapshot feature track new files dont fancy things load incrementally every insert query change metadata files must bad point wait usually build process store list files inserting feature build somewhere already doc cant find help would appreciated,High,0,0,"need, table, event, source, without, instead, every, replay, day, example, single, change, per, business, daily, based, keys, manually, model, keep, needs, events, doesnt, name, list, option, user, must, takes, history, often, truth, full, column, hour, address, requirements, date, reading, track, whenever, metrics, result, update, feature, primary, columns, meaning, pricing, structure, details, rows, simply, available, dedicated, country, means, facebook, error, missing, evolve, involves, names, size, domain, place, addresses, timestamp, parse, mean, slow, schemas, heres, return, dimensions, delete, state, times, arent, ground, errors, receive, min, added, geocoding, shape, sourcing, wasnt, insert, minutes, value, flow, save, normal, little, region, usually, bad, becomes, failure, replication, entirely, products, infrastructure, minute, person, average, servers, internet, looked, probably, campaigns, workloads, looks",8.2,7.4,175.5,-0.663,1069.0,47.9,9.8,275.5,11.4,22.2,15.5,52,108,34,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",4,4
183,"Any ETL, Data Quality, Data Governance professionals ?","Hi everyone,

I’m currently working as an IDQ and CDQ developer for a US-based project, with about 2 years of overall experience

I’m really passionate about growing in this space and want to deepen my knowledge, especially in data quality and data governance . 

I’ve recently started reading the DAMA DMBOK2 to build a strong foundation.

I’m here to connect with experienced professionals and like-minded individuals to learn, share insights, and get guidance on how to navigate and grow in this domain.

Any tips, resources, or advice would be truly appreciated. Looking forward to learning from all of you!

Thank you!
",8,1,2025-04-12 11:45:32,0,False,False,False,False,2025-04-12 11:45:32,11,Saturday,101.0,624,42.72,5,169,14.0,0,0,POSITIVE,0.9989985823631287,"['everyone', 'currently', 'working', 'idq', 'cdq', 'developer', 'usbased', 'project', 'years', 'overall', 'experience', 'really', 'passionate', 'growing', 'space', 'want', 'deepen', 'knowledge', 'especially', 'data', 'quality', 'data', 'governance', 'ive', 'recently', 'started', 'reading', 'dama', 'dmbok', 'build', 'strong', 'foundation', 'connect', 'experienced', 'professionals', 'likeminded', 'individuals', 'learn', 'share', 'insights', 'get', 'guidance', 'navigate', 'grow', 'domain', 'tips', 'resources', 'advice', 'would', 'truly', 'appreciated', 'looking', 'forward', 'learning', 'thank']",everyone currently working idq cdq developer usbased project years overall experience really passionate growing space want deepen knowledge especially data quality data governance ive recently started reading dama dmbok build strong foundation connect experienced professionals likeminded individuals learn share insights get guidance navigate grow domain tips resources advice would truly appreciated looking forward learning thank,High,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
184,Non IT background,"After a year of self teaching I managed to secure an internal career move to data engineering from finance 

What I am wondering is long term will my non IT background matter/discount me against other candidates? I have a degree in accountancy and I am a qualified accountant but I am considering doing a masters in data or computing if it will be beneficial longer term

Thanks",7,8,2025-04-12 13:54:29,0,False,False,False,False,2025-04-12 13:54:29,13,Saturday,67.0,378,37.47,2,109,0.0,0,0,NEGATIVE,-0.9606105089187622,"['year', 'self', 'teaching', 'managed', 'secure', 'internal', 'career', 'move', 'data', 'engineering', 'finance', 'wondering', 'long', 'term', 'non', 'background', 'matterdiscount', 'candidates', 'degree', 'accountancy', 'qualified', 'accountant', 'considering', 'masters', 'data', 'computing', 'beneficial', 'longer', 'term', 'thanks']",year self teaching managed secure internal career move data engineering finance wondering long term non background matterdiscount candidates degree accountancy qualified accountant considering masters data computing beneficial longer term thanks,High,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
185,Dilemma: SWE vs DE @ Big Tech,"I currently work at a Big Tech and have 3 YoE. My role is a mix of Full-Stack + Data Engineering. 

I want to keep preparing for interviews on the side, and to do that I need to know which role to aim for. 

Pros of SWE:
- more jobs positions 
- I have already invested 300 hours into DSA Leetcode. Don’t have to start DE prep from scratch
-Maybe better quality of work/pay(?)

Pros of DE:
- targeting a niche has always given me more callbacks
- if I practice a lot of sql, the interviews at FAANG could be gamed. FAANG do ask DSA but they barely scratch the surface

My thoughts:
Ideally I want to crack the SWE role at a FAANG as I like both roles equally but SWE pays 20% more. If I don’t get callbacks for SWE, then securing a similar pay through a DE role at FAANG is lucrative too. 
I’d be completely fine with doing DE, but I feel uneasy wasting the 100s of hours I spent on DSA. 

Applying for both jobs is sub optimal as I can only sink my time into SQL or DSA | system design or data modelling. 

What do you folks suggest? 
",3,6,2025-04-12 22:17:20,0,False,False,False,False,2025-04-12 22:17:20,22,Saturday,206.0,1036,78.38,11,269,11.2,0,0,NEGATIVE,-0.995376467704773,"['currently', 'work', 'big', 'tech', 'yoe', 'role', 'mix', 'fullstack', 'data', 'engineering', 'want', 'keep', 'preparing', 'interviews', 'side', 'need', 'know', 'role', 'aim', 'pros', 'swe', 'jobs', 'positions', 'already', 'invested', 'hours', 'dsa', 'leetcode', 'dont', 'start', 'prep', 'scratch', 'maybe', 'better', 'quality', 'workpay', 'pros', 'targeting', 'niche', 'always', 'given', 'callbacks', 'practice', 'lot', 'sql', 'interviews', 'faang', 'could', 'gamed', 'faang', 'ask', 'dsa', 'barely', 'scratch', 'surface', 'thoughts', 'ideally', 'want', 'crack', 'swe', 'role', 'faang', 'like', 'roles', 'equally', 'swe', 'pays', 'dont', 'get', 'callbacks', 'swe', 'securing', 'similar', 'pay', 'role', 'faang', 'lucrative', 'completely', 'fine', 'feel', 'uneasy', 'wasting', 'hours', 'spent', 'dsa', 'applying', 'jobs', 'sub', 'optimal', 'sink', 'time', 'sql', 'dsa', 'system', 'design', 'data', 'modelling', 'folks', 'suggest']",currently work big tech yoe role mix fullstack data engineering want keep preparing interviews side need know role aim pros swe jobs positions already invested hours dsa leetcode dont start prep scratch maybe better quality workpay pros targeting niche always given callbacks practice lot sql interviews faang could gamed faang ask dsa barely scratch surface thoughts ideally want crack swe role faang like roles equally swe pays dont get callbacks swe securing similar pay role faang lucrative completely fine feel uneasy wasting hours spent dsa applying jobs sub optimal sink time sql dsa system design data modelling folks suggest,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
186,Question about HDFS,"The course I'm taking is 10 years old so some information I'm finding is irrelevant, which prompted the following questions from me:

  
I'm learning about replication factors/rack awareness in HDFS and I'm curious about the current state of the world. How big are replication factors for massive companies today like, let's say, Uber? What about Amazon?

  
Moreover, do these tech giants even use Hadoop anymore or are they using a modernized version of it in 2025? Thank you for any insights.",3,10,2025-04-12 19:20:05,0,False,False,False,False,2025-04-12 19:20:05,19,Saturday,81.0,495,55.03,5,130,12.0,0,0,NEGATIVE,-0.9330361485481262,"['course', 'taking', 'years', 'old', 'information', 'finding', 'irrelevant', 'prompted', 'following', 'questions', 'learning', 'replication', 'factorsrack', 'awareness', 'hdfs', 'curious', 'current', 'state', 'world', 'big', 'replication', 'factors', 'massive', 'companies', 'today', 'like', 'lets', 'say', 'uber', 'amazon', 'moreover', 'tech', 'giants', 'even', 'use', 'hadoop', 'anymore', 'using', 'modernized', 'version', 'thank', 'insights']",course taking years old information finding irrelevant prompted following questions learning replication factorsrack awareness hdfs curious current state world big replication factors massive companies today like lets say uber amazon moreover tech giants even use hadoop anymore using modernized version thank insights,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
187,Mastering Spark Structured Streaming Integration with Azure Event Hubs,"Are you curious about building real-time streaming pipelines from popular streaming platforms like Azure Event Hubs? In this tutorial, I explain key Event Hubs concepts and demonstrate how to build Spark Structured Streaming pipelines interacting with Event Hubs. Check it out here: [https://youtu.be/wo9vhVBUKXI](https://youtu.be/wo9vhVBUKXI)",2,0,2025-04-12 15:43:22,0,False,False,False,False,2025-04-12 15:43:22,15,Saturday,43.0,343,31.58,3,80,11.9,1,0,POSITIVE,0.8180253505706787,"['curious', 'building', 'realtime', 'streaming', 'pipelines', 'popular', 'streaming', 'platforms', 'like', 'azure', 'event', 'hubs', 'tutorial', 'explain', 'key', 'event', 'hubs', 'concepts', 'demonstrate', 'build', 'spark', 'structured', 'streaming', 'pipelines', 'interacting', 'event', 'hubs', 'check', 'httpsyoutubewovhvbukxihttpsyoutubewovhvbukxi']",curious building realtime streaming pipelines popular streaming platforms like azure event hubs tutorial explain key event hubs concepts demonstrate build spark structured streaming pipelines interacting event hubs check httpsyoutubewovhvbukxihttpsyoutubewovhvbukxi,Mid,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
188,Which API system for my Postgres DWH?,"Hi everyone,

I am building a data warehouse for my company and because we have to process mostly spatial data I went with a postgres materialization. My stack is currently:

- dlt
- dbt
- dagster
- postgres

Now I have the use case that our developers at our company need some of the data for our software solutions to be integrated. And I would like to provide an API for easy access to the data. 

So I am wondering which solution is best for me. I have some experience in a private project with postgREST and found it pretty cool to directly use DB views and functions as endpoints for the API. But tools like FastAPI might be more mature for a production system. What would you recommend?



[View Poll](https://www.reddit.com/poll/1jxdch4)",3,0,2025-04-12 09:20:05,0,False,False,False,False,2025-04-12 09:20:05,9,Saturday,131.0,745,53.1,7,200,12.0,1,1,NEGATIVE,-0.9976711869239807,"['everyone', 'building', 'data', 'warehouse', 'company', 'process', 'mostly', 'spatial', 'data', 'went', 'postgres', 'materialization', 'stack', 'currently', 'dlt', 'dbt', 'dagster', 'postgres', 'use', 'case', 'developers', 'company', 'need', 'data', 'software', 'solutions', 'integrated', 'would', 'like', 'provide', 'api', 'easy', 'access', 'data', 'wondering', 'solution', 'best', 'experience', 'private', 'project', 'postgrest', 'found', 'pretty', 'cool', 'directly', 'use', 'views', 'functions', 'endpoints', 'api', 'tools', 'like', 'fastapi', 'might', 'mature', 'production', 'system', 'would', 'recommend', 'view', 'pollhttpswwwredditcompolljxdch']",everyone building data warehouse company process mostly spatial data went postgres materialization stack currently dlt dbt dagster postgres use case developers company need data software solutions integrated would like provide api easy access data wondering solution best experience private project postgrest found pretty cool directly use views functions endpoints api tools like fastapi might mature production system would recommend view pollhttpswwwredditcompolljxdch,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
189,I'm struggling to evaluate job offer and would appreciate outside opinions,"I've been searching for a new opportunity over the last few years (500+ applications) and have finally received an offer I'm strongly considering. I would really like to hear some outside opinions.

## Current position
- Analytics Lead
- $126k base, 10% bonus
- Tool stack: on-prem SQL Server, SSIS, Power BI, some Python/R
- Downsides: 
	- Incoherent/non-existent corporate data strategy
	- 3 days required in-office (~20-minute commute)
	- Lack of executive support for data and analytics
	- Data Scientist and Data Engineer roles have recently been eliminated
	- No clear path for additional growth or progression 
	- A significant part of the job involves training/mentoring several inexperienced analysts, which I don't enjoy
- Upsides: 
	- Very stable company (no risk of layoffs)
	- Very good relationship with direct manager

## New offer
- Senior Data Analyst
- $130k base, 10% bonus
- Tool stack: BigQuery, FiveTran, dbt / SQLMesh, Looker Studio, GSheets
- Downsides:
	- High-growth company, potentially volatile industry
- Upsides:
	- Fully remote
	- Working alongside experienced data engineers

Other info/significant factors:
- My current company paid for my MSDS degree, and they are within their right to claw back the entire ~$37k tuition if I leave.  I'm prepared to pay this, but it's a big factor in the decision.
- At this stage in my career, I'm putting a very high value on growth/development opportunities

Am I crazy to consider a lateral move that involves a significant amount of uncompensated risk, just for a potentially better learning and growth opportunity?",4,21,2025-04-12 21:49:54,0,False,False,False,False,2025-04-12 21:49:54,21,Saturday,254.0,1589,8.27,5,421,21.2,0,1,NEGATIVE,-0.9950438737869263,"['ive', 'searching', 'new', 'opportunity', 'last', 'years', 'applications', 'finally', 'received', 'offer', 'strongly', 'considering', 'would', 'really', 'like', 'hear', 'outside', 'opinions', 'current', 'position', 'analytics', 'lead', 'base', 'bonus', 'tool', 'stack', 'onprem', 'sql', 'server', 'ssis', 'power', 'pythonr', 'downsides', 'incoherentnonexistent', 'corporate', 'data', 'strategy', 'days', 'required', 'inoffice', 'minute', 'commute', 'lack', 'executive', 'support', 'data', 'analytics', 'data', 'scientist', 'data', 'engineer', 'roles', 'recently', 'eliminated', 'clear', 'path', 'additional', 'growth', 'progression', 'significant', 'part', 'job', 'involves', 'trainingmentoring', 'several', 'inexperienced', 'analysts', 'dont', 'enjoy', 'upsides', 'stable', 'company', 'risk', 'layoffs', 'good', 'relationship', 'direct', 'manager', 'new', 'offer', 'senior', 'data', 'analyst', 'base', 'bonus', 'tool', 'stack', 'bigquery', 'fivetran', 'dbt', 'sqlmesh', 'looker', 'studio', 'gsheets', 'downsides', 'highgrowth', 'company', 'potentially', 'volatile', 'industry', 'upsides', 'fully', 'remote', 'working', 'alongside', 'experienced', 'data', 'engineers', 'infosignificant', 'factors', 'current', 'company', 'paid', 'msds', 'degree', 'within', 'right', 'claw', 'back', 'entire', 'tuition', 'leave', 'prepared', 'pay', 'big', 'factor', 'decision', 'stage', 'career', 'putting', 'high', 'value', 'growthdevelopment', 'opportunities', 'crazy', 'consider', 'lateral', 'move', 'involves', 'significant', 'amount', 'uncompensated', 'risk', 'potentially', 'better', 'learning', 'growth', 'opportunity']",ive searching new opportunity last years applications finally received offer strongly considering would really like hear outside opinions current position analytics lead base bonus tool stack onprem sql server ssis power pythonr downsides incoherentnonexistent corporate data strategy days required inoffice minute commute lack executive support data analytics data scientist data engineer roles recently eliminated clear path additional growth progression significant part job involves trainingmentoring several inexperienced analysts dont enjoy upsides stable company risk layoffs good relationship direct manager new offer senior data analyst base bonus tool stack bigquery fivetran dbt sqlmesh looker studio gsheets downsides highgrowth company potentially volatile industry upsides fully remote working alongside experienced data engineers infosignificant factors current company paid msds degree within right claw back entire tuition leave prepared pay big factor decision stage career putting high value growthdevelopment opportunities crazy consider lateral move involves significant amount uncompensated risk potentially better learning growth opportunity,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
190,Struggling to resolve tickets - DE Course Recommendations?,"I’m looking for recommendations for a solid online course to learn Data Engineering. Less than a year ago, I started a new role as a BI developer. Most of my work involves creating data models and reports in Power BI using T-SQL and DAX, but lately I’ve been tasked with handling tickets related to reports showing incorrect data on the ETL side.

We use Wherescape for our ETL processes, but I’ve struggled to find good learning material for this tool. There's no formal training and everyone learns on the job. There’s so much to analyze during investigations, especially when reverse-engineering the problem.

I’m a visual learner, so I’d love recommendations for courses with videos and hands-on practice. Any suggestions? Thanks!

Edit: Most posts asking for course recommendations are 1 year older or more. Some links doesn't work anymore or are not found when I look it up. ",2,1,2025-04-12 21:39:00,1,False,2025-04-12 23:13:53,False,False,2025-04-12 21:39:00,21,Saturday,147.0,881,54.93,9,231,11.4,0,0,NEGATIVE,-0.9961994290351868,"['looking', 'recommendations', 'solid', 'online', 'course', 'learn', 'data', 'engineering', 'less', 'year', 'ago', 'started', 'new', 'role', 'developer', 'work', 'involves', 'creating', 'data', 'models', 'reports', 'power', 'using', 'tsql', 'dax', 'lately', 'ive', 'tasked', 'handling', 'tickets', 'related', 'reports', 'showing', 'incorrect', 'data', 'etl', 'side', 'use', 'wherescape', 'etl', 'processes', 'ive', 'struggled', 'find', 'good', 'learning', 'material', 'tool', 'theres', 'formal', 'training', 'everyone', 'learns', 'job', 'theres', 'much', 'analyze', 'investigations', 'especially', 'reverseengineering', 'problem', 'visual', 'learner', 'love', 'recommendations', 'courses', 'videos', 'handson', 'practice', 'suggestions', 'thanks', 'edit', 'posts', 'asking', 'course', 'recommendations', 'year', 'older', 'links', 'doesnt', 'work', 'anymore', 'found', 'look']",looking recommendations solid online course learn data engineering less year ago started new role developer work involves creating data models reports power using tsql dax lately ive tasked handling tickets related reports showing incorrect data etl side use wherescape etl processes ive struggled find good learning material tool theres formal training everyone learns job theres much analyze investigations especially reverseengineering problem visual learner love recommendations courses videos handson practice suggestions thanks edit posts asking course recommendations year older links doesnt work anymore found look,Mid,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
191,Debezium connector Sql server 2016,"I’m trying to get the Debezium SQL Server connector working with a SQL Server 2016 instance, but not having much luck. The official docs mention compatibility with 2017, 2019, and 2022—but nothing about 2016.

Is 2016 just not supported, or has anyone managed to get it working regardless?
Would love to hear if there are known limitations, workarounds, or specific gotchas for this version.",2,0,2025-04-12 14:47:33,1,False,False,False,False,2025-04-12 14:47:33,14,Saturday,64.0,391,55.24,4,104,12.6,0,0,NEGATIVE,-0.9993698000907898,"['trying', 'get', 'debezium', 'sql', 'server', 'connector', 'working', 'sql', 'server', 'instance', 'much', 'luck', 'official', 'docs', 'mention', 'compatibility', 'nothing', 'supported', 'anyone', 'managed', 'get', 'working', 'regardless', 'would', 'love', 'hear', 'known', 'limitations', 'workarounds', 'specific', 'gotchas', 'version']",trying get debezium sql server connector working sql server instance much luck official docs mention compatibility nothing supported anyone managed get working regardless would love hear known limitations workarounds specific gotchas version,Mid,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
192,Discovering data dependencies / lineage from excel workbooks,Hi r/dataengineering community. Trying to replace excel based reports that connect to databases and have in-built data transformation logic across worksheets. Is there a utility or platform you have used to help decipher and document the data dependencies / data lineage from excel?,2,0,2025-04-12 08:37:34,0,False,False,False,False,2025-04-12 08:37:34,8,Saturday,43.0,282,31.89,3,79,13.0,0,0,NEGATIVE,-0.9972303509712219,"['rdataengineering', 'community', 'trying', 'replace', 'excel', 'based', 'reports', 'connect', 'databases', 'inbuilt', 'data', 'transformation', 'logic', 'across', 'worksheets', 'utility', 'platform', 'used', 'help', 'decipher', 'document', 'data', 'dependencies', 'data', 'lineage', 'excel']",rdataengineering community trying replace excel based reports connect databases inbuilt data transformation logic across worksheets utility platform used help decipher document data dependencies data lineage excel,Mid,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
193,Help,"I'm using Airbyte Cloud because my PC doesn't have enough resources to install it. I have a Docker container running PostgreSQL on Airbyte Cloud. I want to set the PostgreSQL destination. Can anyone give me some guidance on how to do this? Should I create an SSH tunnel?

",1,1,2025-04-13 00:06:47,1,False,False,False,False,2025-04-13 00:06:47,0,Sunday,48.0,272,70.19,5,72,8.2,0,0,NEGATIVE,-0.9997380375862122,"['using', 'airbyte', 'cloud', 'doesnt', 'enough', 'resources', 'install', 'docker', 'container', 'running', 'postgresql', 'airbyte', 'cloud', 'want', 'set', 'postgresql', 'destination', 'anyone', 'give', 'guidance', 'create', 'ssh', 'tunnel']",using airbyte cloud doesnt enough resources install docker container running postgresql airbyte cloud want set postgresql destination anyone give guidance create ssh tunnel,Mid,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
194,Thoughts on Acryl vs other metadata platforms,"Hi all, I'm evaluating metadata management solutions for our data platform and would appreciate any thoughts from folks who've actually implemented these tools in production.

We're currently running into scaling issues with our in-house data catalog and I think we need something more robust for governance and lineage tracking.

I've narrowed it down to Acryl (DataHub) and Collate (openmetadata) as the main contenders. I know I should look at Collibra and Alation and maybe Unity Catalog?

For context, we're a mid-sized fintech (\~500 employees) with about 30 data engineers and scientists. We're AWS with Snowflake, Airflow for orchestration, and a growing number of ML models in production.

My question list is: 

1. How these tools handle machine-scale operations 
2. How painful was it to get set up?
3. For DataHub and openmetadata specifically - is the open source version viable or is the cloud version necessary?
4. Any unexpected limitations you've hit with any of these platforms?
5. Do you feel like these grow with you as we increasingly head into AI governance? 
6. How well they integrate with existing tools (Snowflake, dbt, Looker, etc.)

If anyone has switched from one solution to another, I'd love to hear why you made the change and whether it was worth it.

Sorry for the pick list of questions - the last post on this was years ago and I was hoping for some more insights. Thanks in advance for anyone's thoughts. ",0,0,2025-04-12 23:05:00,0,False,False,False,False,2025-04-12 23:05:00,23,Saturday,239.0,1442,56.45,16,377,11.9,0,0,NEGATIVE,-0.9969605803489685,"['evaluating', 'metadata', 'management', 'solutions', 'data', 'platform', 'would', 'appreciate', 'thoughts', 'folks', 'whove', 'actually', 'implemented', 'tools', 'production', 'currently', 'running', 'scaling', 'issues', 'inhouse', 'data', 'catalog', 'think', 'need', 'something', 'robust', 'governance', 'lineage', 'tracking', 'ive', 'narrowed', 'acryl', 'datahub', 'collate', 'openmetadata', 'main', 'contenders', 'know', 'look', 'collibra', 'alation', 'maybe', 'unity', 'catalog', 'context', 'midsized', 'fintech', 'employees', 'data', 'engineers', 'scientists', 'aws', 'snowflake', 'airflow', 'orchestration', 'growing', 'number', 'models', 'production', 'question', 'list', 'tools', 'handle', 'machinescale', 'operations', 'painful', 'get', 'set', 'datahub', 'openmetadata', 'specifically', 'open', 'source', 'version', 'viable', 'cloud', 'version', 'necessary', 'unexpected', 'limitations', 'youve', 'hit', 'platforms', 'feel', 'like', 'grow', 'increasingly', 'head', 'governance', 'well', 'integrate', 'existing', 'tools', 'snowflake', 'dbt', 'looker', 'etc', 'anyone', 'switched', 'one', 'solution', 'another', 'love', 'hear', 'made', 'change', 'whether', 'worth', 'sorry', 'pick', 'list', 'questions', 'last', 'post', 'years', 'ago', 'hoping', 'insights', 'thanks', 'advance', 'anyones', 'thoughts']",evaluating metadata management solutions data platform would appreciate thoughts folks whove actually implemented tools production currently running scaling issues inhouse data catalog think need something robust governance lineage tracking ive narrowed acryl datahub collate openmetadata main contenders know look collibra alation maybe unity catalog context midsized fintech employees data engineers scientists aws snowflake airflow orchestration growing number models production question list tools handle machinescale operations painful get set datahub openmetadata specifically open source version viable cloud version necessary unexpected limitations youve hit platforms feel like grow increasingly head governance well integrate existing tools snowflake dbt looker etc anyone switched one solution another love hear made change whether worth sorry pick list questions last post years ago hoping insights thanks advance anyones thoughts,Low,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
195,How to create changeStreams pipeline to bigquery,"I am building a streaming pipeline in GCP for work that works like this:

Cloud Run Service --> PubSub --> Dataflow --> BigQuery


My Cloud Run Service when it starts, it watches a collections with changeStreams and then published all changes into a PubSub topic. Dataflow then streams that messages into BQ.

The service runs in VPC connector where the linked IP is whitelisted in mongodb.

My issue is with my service! It keeps failing die to timeouts when trying to publish to pubsub after a few hours running.

Ive tried batching the publishing, extending the timeout, retries.


Any suggestion? Have you done something similar?",0,0,2025-04-12 20:51:58,0,False,False,False,False,2025-04-12 20:51:58,20,Saturday,105.0,632,65.12,7,149,8.8,0,0,NEGATIVE,-0.9989670515060425,"['building', 'streaming', 'pipeline', 'gcp', 'work', 'works', 'like', 'cloud', 'run', 'service', 'pubsub', 'dataflow', 'bigquery', 'cloud', 'run', 'service', 'starts', 'watches', 'collections', 'changestreams', 'published', 'changes', 'pubsub', 'topic', 'dataflow', 'streams', 'messages', 'service', 'runs', 'vpc', 'connector', 'linked', 'whitelisted', 'mongodb', 'issue', 'service', 'keeps', 'failing', 'die', 'timeouts', 'trying', 'publish', 'pubsub', 'hours', 'running', 'ive', 'tried', 'batching', 'publishing', 'extending', 'timeout', 'retries', 'suggestion', 'done', 'something', 'similar']",building streaming pipeline gcp work works like cloud run service pubsub dataflow bigquery cloud run service starts watches collections changestreams published changes pubsub topic dataflow streams messages service runs vpc connector linked whitelisted mongodb issue service keeps failing die timeouts trying publish pubsub hours running ive tried batching publishing extending timeout retries suggestion done something similar,Low,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
196,"I've been working on a query engine over semi-structured logs (think trino but for JSONs), would like to get feedback / feature ideas","[https://github.com/tontinton/miso](https://github.com/tontinton/miso)

Other than the obvious stuff like:

* Make it faster (benchmarking + improving implementation)
* Make it spool to disk to handle queries larger than memory
* Make it distributed to handle queries larger than memory / disk
* Implement a simple query language frontend for faster onboarding, something like KQL

Currently I only support [quickwit](https://quickwit.io/), and can pretty easily add elasticsearch support, but what other JSON databases would you think are the best fit? Datadog logs? MongoDB? Clickhouse jsons? Snowflake VARIANTs?

What features can a query engine that treats semi-structured data as a first class citizen have, that trino cannot?",0,1,2025-04-12 17:02:24,0,False,False,False,False,2025-04-12 17:02:24,17,Saturday,102.0,731,22.08,3,176,16.7,1,1,NEGATIVE,-0.9963619112968445,"['httpsgithubcomtontintonmisohttpsgithubcomtontintonmiso', 'obvious', 'stuff', 'like', 'make', 'faster', 'benchmarking', 'improving', 'implementation', 'make', 'spool', 'disk', 'handle', 'queries', 'larger', 'memory', 'make', 'distributed', 'handle', 'queries', 'larger', 'memory', 'disk', 'implement', 'simple', 'query', 'language', 'frontend', 'faster', 'onboarding', 'something', 'like', 'kql', 'currently', 'support', 'quickwithttpsquickwitio', 'pretty', 'easily', 'add', 'elasticsearch', 'support', 'json', 'databases', 'would', 'think', 'best', 'fit', 'datadog', 'logs', 'mongodb', 'clickhouse', 'jsons', 'snowflake', 'variants', 'features', 'query', 'engine', 'treats', 'semistructured', 'data', 'first', 'class', 'citizen', 'trino', 'cannot']",httpsgithubcomtontintonmisohttpsgithubcomtontintonmiso obvious stuff like make faster benchmarking improving implementation make spool disk handle queries larger memory make distributed handle queries larger memory disk implement simple query language frontend faster onboarding something like kql currently support quickwithttpsquickwitio pretty easily add elasticsearch support json databases would think best fit datadog logs mongodb clickhouse jsons snowflake variants features query engine treats semistructured data first class citizen trino cannot,Low,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
197,Data Engineering Employment,"I'm an Engineer with an MBA. I've spent 5 years at a steelplant and 5 years working in finance for the government.

In the past five years have been building data pipelines in Synapse off D365 data models that I have built with a vendor in SQL/Power BI. I have gained quite a bit of experience in this timeframe, but would actually like more data engineering experience.

Should I try to land a role in the data engineering department where I would get first hand experience in data engineering tools and frameworks or just keep doing what I am doing in Finance and learning as I go.

I make decent money for the city I live in, but I feel like the end to end would definitely help me land other roles in the future that would branch out from just financial reporting and data.

Especially in the capacity for remote work if for some reason company or job gets moved to another city.",0,8,2025-04-12 20:13:03,0,False,2025-04-12 20:30:53,False,False,2025-04-12 20:13:03,20,Saturday,164.0,883,64.64,7,237,12.3,0,0,NEGATIVE,-0.9899534583091736,"['engineer', 'mba', 'ive', 'spent', 'years', 'steelplant', 'years', 'working', 'finance', 'government', 'past', 'five', 'years', 'building', 'data', 'pipelines', 'synapse', 'data', 'models', 'built', 'vendor', 'sqlpower', 'gained', 'quite', 'bit', 'experience', 'timeframe', 'would', 'actually', 'like', 'data', 'engineering', 'experience', 'try', 'land', 'role', 'data', 'engineering', 'department', 'would', 'get', 'first', 'hand', 'experience', 'data', 'engineering', 'tools', 'frameworks', 'keep', 'finance', 'learning', 'make', 'decent', 'money', 'city', 'live', 'feel', 'like', 'end', 'end', 'would', 'definitely', 'help', 'land', 'roles', 'future', 'would', 'branch', 'financial', 'reporting', 'data', 'especially', 'capacity', 'remote', 'work', 'reason', 'company', 'job', 'gets', 'moved', 'another', 'city']",engineer mba ive spent years steelplant years working finance government past five years building data pipelines synapse data models built vendor sqlpower gained quite bit experience timeframe would actually like data engineering experience try land role data engineering department would get first hand experience data engineering tools frameworks keep finance learning make decent money city live feel like end end would definitely help land roles future would branch financial reporting data especially capacity remote work reason company job gets moved another city,Low,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
198,help with a research survey that im doing regarding big data please.,"Hi everyone! I'm conducting a university research survey on commonly used Big Data tools among students and professionals. If you work in data or tech, I’d really appreciate your input — it only takes 3 minutes! Thank you

[https://docs.google.com/forms/d/e/1FAIpQLScXK6CnNUHGR9UIEHUhX83kHoZGYuSunRE0foZgnew81nxxLg/viewform?usp=header](https://docs.google.com/forms/d/e/1FAIpQLScXK6CnNUHGR9UIEHUhX83kHoZGYuSunRE0foZgnew81nxxLg/viewform?usp=header)",0,0,2025-04-12 18:39:21,0,False,False,False,False,2025-04-12 18:39:21,18,Saturday,39.0,447,-26.03,3,100,11.9,1,0,NEGATIVE,-0.6963971257209778,"['everyone', 'conducting', 'university', 'research', 'survey', 'commonly', 'used', 'big', 'data', 'tools', 'among', 'students', 'professionals', 'work', 'data', 'tech', 'really', 'appreciate', 'input', 'takes', 'minutes', 'thank', 'httpsdocsgooglecomformsdefaipqlscxkcnnuhgruiehuhxkhozgyusunrefozgnewnxxlgviewformuspheaderhttpsdocsgooglecomformsdefaipqlscxkcnnuhgruiehuhxkhozgyusunrefozgnewnxxlgviewformuspheader']",everyone conducting university research survey commonly used big data tools among students professionals work data tech really appreciate input takes minutes thank httpsdocsgooglecomformsdefaipqlscxkcnnuhgruiehuhxkhozgyusunrefozgnewnxxlgviewformuspheaderhttpsdocsgooglecomformsdefaipqlscxkcnnuhgruiehuhxkhozgyusunrefozgnewnxxlgviewformuspheader,Low,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
199,Is this take-home assignment too large and complex ?,"I was given the following assignment as part of a job application. Would love to hear if people think this is reasonable or overkill for a take-home test:

**Assignment Summary:**

* Build a **Python data pipeline** and expose it via an **API**.
* The API must:
   * Accept a **venue ID**, **start date**, and **end date**.
   * Use Open-Meteo's historical weather API to fetch **hourly weather data** for the specified range and location.
   * Extract 10+ parameters (e.g., temperature, precipitation, snowfall, etc.).
   * Store the data in a **cloud-hosted database**.
   * Return success or error responses accordingly.
* Design the database schema for storing the weather data.
* Use **OpenAPI 3.0** to document the API.
* Deploy on **any cloud provider** (AWS, Azure, or GCP), including:
   * Database
   * API runtime
   * API Gateway or equivalent
* Set up **CI/CD pipeline** for the solution.
* Include a **README** with setup and testing instructions (Postman or Curl).
* Implement **QA checks in SQL** for data consistency.

Does this feel like a reasonable assignment for a take-home? How much time would you expect this to take?",87,105,2025-04-13 05:13:59,0,False,False,False,False,2025-04-13 05:13:59,5,Sunday,184.0,1141,43.9,16,294,11.8,0,0,NEGATIVE,-0.9987642765045166,"['given', 'following', 'assignment', 'part', 'job', 'application', 'would', 'love', 'hear', 'people', 'think', 'reasonable', 'overkill', 'takehome', 'test', 'assignment', 'summary', 'build', 'python', 'data', 'pipeline', 'expose', 'via', 'api', 'api', 'must', 'accept', 'venue', 'start', 'date', 'end', 'date', 'use', 'openmeteos', 'historical', 'weather', 'api', 'fetch', 'hourly', 'weather', 'data', 'specified', 'range', 'location', 'extract', 'parameters', 'temperature', 'precipitation', 'snowfall', 'etc', 'store', 'data', 'cloudhosted', 'database', 'return', 'success', 'error', 'responses', 'accordingly', 'design', 'database', 'schema', 'storing', 'weather', 'data', 'use', 'openapi', 'document', 'api', 'deploy', 'cloud', 'provider', 'aws', 'azure', 'gcp', 'including', 'database', 'api', 'runtime', 'api', 'gateway', 'equivalent', 'set', 'cicd', 'pipeline', 'solution', 'include', 'readme', 'setup', 'testing', 'instructions', 'postman', 'curl', 'implement', 'checks', 'sql', 'data', 'consistency', 'feel', 'like', 'reasonable', 'assignment', 'takehome', 'much', 'time', 'would', 'expect', 'take']",given following assignment part job application would love hear people think reasonable overkill takehome test assignment summary build python data pipeline expose via api api must accept venue start date end date use openmeteos historical weather api fetch hourly weather data specified range location extract parameters temperature precipitation snowfall etc store data cloudhosted database return success error responses accordingly design database schema storing weather data use openapi document api deploy cloud provider aws azure gcp including database api runtime api gateway equivalent set cicd pipeline solution include readme setup testing instructions postman curl implement checks sql data consistency feel like reasonable assignment takehome much time would expect take,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
200,How do my fellow on-prem DEs keep their sanity...,"...the joys of memory and compute resources seems to be a neverending suck 😭

We're building ETL pipelines, using Airflow in one K8s namespace and Spark in another (the latter having dedicated hardware). Most data workloads aren't really Spark-worthy as files are typically <20GB, and we keep hitting pain points where processes struggle in Airflow's memory (workers are 6Gi and 6 CPU, with a limit of 10GI; no KEDA or HPA). We are looking into more efficient data structures like DuckDB, Polars, etc or running ""mid-tier"" processes as separate K8s jobs but then we hit constraints like tools/libraries relying on Pandas use so we seem stuck with eager processes.

Case in point, I just learned that our teams are having to split files into smaller files of 125k records so Pydantic schema validation won't fail on memory. I looked into GX Core and see the main source options there again appear to be Pandas or Spark dataframes (yes, I'm going to try DuckDB through SQLAlchemy). I could bite the bullet and just say to go with Spark, but then our pipelines will be using Spark for QA and not for ETL which will be fun to keep clarifying. 

Sisyphus is the patron saint of Data Engineering... just sayin'

[Make it stoooooooooop!](https://preview.redd.it/qwikfhcpihue1.png?width=503&format=png&auto=webp&s=6565d874d8d2213835c172a8ed449b14cff8214a)

(there may be some internal sobbing/laughing whenever I see posts asking ""should I get into DE..."")",49,14,2025-04-12 23:08:19,0,False,False,False,False,2025-04-12 23:08:19,23,Saturday,227.0,1448,54.46,9,348,12.8,1,1,NEGATIVE,-0.9991188645362854,"['joys', 'memory', 'compute', 'resources', 'seems', 'neverending', 'suck', 'building', 'etl', 'pipelines', 'using', 'airflow', 'one', 'namespace', 'spark', 'another', 'latter', 'dedicated', 'hardware', 'data', 'workloads', 'arent', 'really', 'sparkworthy', 'files', 'typically', 'keep', 'hitting', 'pain', 'points', 'processes', 'struggle', 'airflows', 'memory', 'workers', 'cpu', 'limit', 'keda', 'hpa', 'looking', 'efficient', 'data', 'structures', 'like', 'duckdb', 'polars', 'etc', 'running', 'midtier', 'processes', 'separate', 'jobs', 'hit', 'constraints', 'like', 'toolslibraries', 'relying', 'pandas', 'use', 'seem', 'stuck', 'eager', 'processes', 'case', 'point', 'learned', 'teams', 'split', 'files', 'smaller', 'files', 'records', 'pydantic', 'schema', 'validation', 'wont', 'fail', 'memory', 'looked', 'core', 'see', 'main', 'source', 'options', 'appear', 'pandas', 'spark', 'dataframes', 'yes', 'going', 'try', 'duckdb', 'sqlalchemy', 'could', 'bite', 'bullet', 'say', 'spark', 'pipelines', 'using', 'spark', 'etl', 'fun', 'keep', 'clarifying', 'sisyphus', 'patron', 'saint', 'data', 'engineering', 'sayin', 'make', 'stoooooooooophttpspreviewredditqwikfhcpihuepngwidthformatpngautowebpsdddcaedbcffa', 'may', 'internal', 'sobbinglaughing', 'whenever', 'see', 'posts', 'asking', 'get']",joys memory compute resources seems neverending suck building etl pipelines using airflow one namespace spark another latter dedicated hardware data workloads arent really sparkworthy files typically keep hitting pain points processes struggle airflows memory workers cpu limit keda hpa looking efficient data structures like duckdb polars etc running midtier processes separate jobs hit constraints like toolslibraries relying pandas use seem stuck eager processes case point learned teams split files smaller files records pydantic schema validation wont fail memory looked core see main source options appear pandas spark dataframes yes going try duckdb sqlalchemy could bite bullet say spark pipelines using spark etl fun keep clarifying sisyphus patron saint data engineering sayin make stoooooooooophttpspreviewredditqwikfhcpihuepngwidthformatpngautowebpsdddcaedbcffa may internal sobbinglaughing whenever see posts asking get,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
201,Data Inserts best practices with Iceberg,"I receive various files at different intervals which are not defined. Can be every seconds, hour, daily, etc.

I don’t have any indication also of when something is finished. For example, it’s highly possible to have 100 files that would end up being 100% of my daily table, but I receive them scattered over 15min-30 when the data become available and my ingestion process ingest it. Can be 1 to 12 hours after the day is over.

Not that’s it’s also possible to have 10000 very small files per day.

I’m wondering how is this solves with Iceberg tables. Very newbie Iceberg guy here. Like I don’t see throughput write benchmark anywhere but I figure that rewriting the metadata files must be a big overhead if there’s a very large amount of files so inserting every times there’s a new one must not be the ideal solution.

I’ve read some medium post saying that there was a snapshot feature which track new files so you don’t have to do some fancy things to load them incrementally. But again if every insert is a query that change the metadata files it must be bad at some point.

Do you wait and usually build a process to store a list of files before inserting them or is this a feature build somewhere already in a doc I can’t find ?

Any help would be appreciated.

",18,2,2025-04-12 23:44:32,0,False,False,False,False,2025-04-12 23:44:32,23,Saturday,233.0,1272,61.87,13,342,11.4,0,0,NEGATIVE,-0.9944283962249756,"['receive', 'various', 'files', 'different', 'intervals', 'defined', 'every', 'seconds', 'hour', 'daily', 'etc', 'dont', 'indication', 'also', 'something', 'finished', 'example', 'highly', 'possible', 'files', 'would', 'end', 'daily', 'table', 'receive', 'scattered', 'min', 'data', 'become', 'available', 'ingestion', 'process', 'ingest', 'hours', 'day', 'thats', 'also', 'possible', 'small', 'files', 'per', 'day', 'wondering', 'solves', 'iceberg', 'tables', 'newbie', 'iceberg', 'guy', 'like', 'dont', 'see', 'throughput', 'write', 'benchmark', 'anywhere', 'figure', 'rewriting', 'metadata', 'files', 'must', 'big', 'overhead', 'theres', 'large', 'amount', 'files', 'inserting', 'every', 'times', 'theres', 'new', 'one', 'must', 'ideal', 'solution', 'ive', 'read', 'medium', 'post', 'saying', 'snapshot', 'feature', 'track', 'new', 'files', 'dont', 'fancy', 'things', 'load', 'incrementally', 'every', 'insert', 'query', 'change', 'metadata', 'files', 'must', 'bad', 'point', 'wait', 'usually', 'build', 'process', 'store', 'list', 'files', 'inserting', 'feature', 'build', 'somewhere', 'already', 'doc', 'cant', 'find', 'help', 'would', 'appreciated']",receive various files different intervals defined every seconds hour daily etc dont indication also something finished example highly possible files would end daily table receive scattered min data become available ingestion process ingest hours day thats also possible small files per day wondering solves iceberg tables newbie iceberg guy like dont see throughput write benchmark anywhere figure rewriting metadata files must big overhead theres large amount files inserting every times theres new one must ideal solution ive read medium post saying snapshot feature track new files dont fancy things load incrementally every insert query change metadata files must bad point wait usually build process store list files inserting feature build somewhere already doc cant find help would appreciated,High,0,0,"need, table, event, source, without, instead, every, replay, day, example, single, change, per, business, daily, based, keys, manually, model, keep, needs, events, doesnt, name, list, option, user, must, takes, history, often, truth, full, column, hour, address, requirements, date, reading, track, whenever, metrics, result, update, feature, primary, columns, meaning, pricing, structure, details, rows, simply, available, dedicated, country, means, facebook, error, missing, evolve, involves, names, size, domain, place, addresses, timestamp, parse, mean, slow, schemas, heres, return, dimensions, delete, state, times, arent, ground, errors, receive, min, added, geocoding, shape, sourcing, wasnt, insert, minutes, value, flow, save, normal, little, region, usually, bad, becomes, failure, replication, entirely, products, infrastructure, minute, person, average, servers, internet, looked, probably, campaigns, workloads, looks",8.2,7.4,175.5,-0.663,1069.0,47.9,9.8,275.5,11.4,22.2,15.5,52,108,34,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",4,4
202,is Microsoft fabric the right shortcut for a data analyst moving to data engineer ?,"I'm currently on my data engineering journey using AWS as my cloud platform. However, I’ve come across the Microsoft Fabric data engineering challenge. Should I pause my AWS learning to take the Fabric challenge? Is it worth switching focus?
",17,40,2025-04-13 06:48:20,0,False,False,False,False,2025-04-13 06:48:20,6,Sunday,39.0,242,61.53,4,64,9.5,0,0,NEGATIVE,-0.9978851675987244,"['currently', 'data', 'engineering', 'journey', 'using', 'aws', 'cloud', 'platform', 'however', 'ive', 'come', 'across', 'microsoft', 'fabric', 'data', 'engineering', 'challenge', 'pause', 'aws', 'learning', 'take', 'fabric', 'challenge', 'worth', 'switching', 'focus']",currently data engineering journey using aws cloud platform however ive come across microsoft fabric data engineering challenge pause aws learning take fabric challenge worth switching focus,High,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
203,Self-Healing Data Quality in DBT — Without Any Extra Tools,"I just published a practical breakdown of a method I call **Observe & Fix** — a simple way to manage data quality in DBT without breaking your pipelines or relying on external tools.

It’s a self-healing pattern that works entirely within DBT using native tests, macros, and logic — and it’s ideal for fixable issues like duplicates or nulls.

Includes examples, YAML configs, macros, and even when to alert via Elementary.

Would love feedback or to hear how others are handling this kind of pattern.

👉[Read the full post here ](https://medium.com/@baruchjacob/self-healing-pipelines-with-dbt-the-observe-fix-method-9d6b2da4eae3)",18,2,2025-04-13 19:41:47,0,False,False,False,False,2025-04-13 19:41:47,19,Sunday,91.0,631,45.15,5,150,11.6,1,0,POSITIVE,0.9813265204429626,"['published', 'practical', 'breakdown', 'method', 'call', 'observe', 'fix', 'simple', 'way', 'manage', 'data', 'quality', 'dbt', 'without', 'breaking', 'pipelines', 'relying', 'external', 'tools', 'selfhealing', 'pattern', 'works', 'entirely', 'within', 'dbt', 'using', 'native', 'tests', 'macros', 'logic', 'ideal', 'fixable', 'issues', 'like', 'duplicates', 'nulls', 'includes', 'examples', 'yaml', 'configs', 'macros', 'even', 'alert', 'via', 'elementary', 'would', 'love', 'feedback', 'hear', 'others', 'handling', 'kind', 'pattern', 'read', 'full', 'post', 'httpsmediumcombaruchjacobselfhealingpipelineswithdbttheobservefixmethoddbdaeae']",published practical breakdown method call observe fix simple way manage data quality dbt without breaking pipelines relying external tools selfhealing pattern works entirely within dbt using native tests macros logic ideal fixable issues like duplicates nulls includes examples yaml configs macros even alert via elementary would love feedback hear others handling kind pattern read full post httpsmediumcombaruchjacobselfhealingpipelineswithdbttheobservefixmethoddbdaeae,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
204,We built a natural language search tool for finding U.S. government datasets,"Hey everyone! My friend and I built [Crystal](https://askcrystal.info/search), a tool to help you search through 300,000+ datasets from [data.gov](http://data.gov) using plain English.

Example queries:

* *""Air quality in NYC after 2015""*
* *""Unemployment trends in Texas""*
* *""Obesity rates in Alabama""*

It finds and ranks the most relevant datasets, with clean summaries and download links.

We made it because searching [data.gov](http://data.gov) can be frustrating — we wanted something that feels more like asking a smart assistant than guessing keywords.

It’s in early alpha, but very usable. We’d love feedback on how useful it is for everyone's data analysis, and what features might make your work easier.

Try it out: [askcrystal.info/search](https://askcrystal.info/search)",13,2,2025-04-13 17:55:33,0,False,False,False,False,2025-04-13 17:55:33,17,Sunday,110.0,788,51.04,9,183,11.6,1,0,NEGATIVE,-0.9905799031257629,"['hey', 'everyone', 'friend', 'built', 'crystalhttpsaskcrystalinfosearch', 'tool', 'help', 'search', 'datasets', 'datagovhttpdatagov', 'using', 'plain', 'english', 'example', 'queries', 'air', 'quality', 'nyc', 'unemployment', 'trends', 'texas', 'obesity', 'rates', 'alabama', 'finds', 'ranks', 'relevant', 'datasets', 'clean', 'summaries', 'download', 'links', 'made', 'searching', 'datagovhttpdatagov', 'frustrating', 'wanted', 'something', 'feels', 'like', 'asking', 'smart', 'assistant', 'guessing', 'keywords', 'early', 'alpha', 'usable', 'wed', 'love', 'feedback', 'useful', 'everyones', 'data', 'analysis', 'features', 'might', 'make', 'work', 'easier', 'try', 'askcrystalinfosearchhttpsaskcrystalinfosearch']",hey everyone friend built crystalhttpsaskcrystalinfosearch tool help search datasets datagovhttpdatagov using plain english example queries air quality nyc unemployment trends texas obesity rates alabama finds ranks relevant datasets clean summaries download links made searching datagovhttpdatagov frustrating wanted something feels like asking smart assistant guessing keywords early alpha usable wed love feedback useful everyones data analysis features might make work easier try askcrystalinfosearchhttpsaskcrystalinfosearch,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
205,"Landed a Role with SQL/dbt, But Clueless About Data Modeling — Advice?","Hi everyone,  
I’m a 2025 new grad starting this May, and I’ll be working at a small start-up as an Analytics Engineer. I’ve gotten pretty solid at SQL as I’ve been grinding Leetcode questions for fun, and it really helped me land the job. During my internships, I also worked a lot with dbt, Snowflake, and Airflow, so I’m fairly comfortable with the tooling side of things. 

Where I’m struggling is data modeling—specifically the Kimball methodology, Star Schemas, and different types of dimensions and fact tables. I tried reading the Kimball book, but honestly, it felt super abstract without any hands-on practice. I get that real data modeling often involves trade-offs, business context, and actual stakeholder input, which isn’t easy to simulate on your own.

So my question is—how can a college student or new grad start building intuition and skills in data modeling? Are there any practical resources or projects I can work through to better understand this area? And if you have any general advice for someone entering the industry in this kind of role, I’d love to hear it.

Thanks a lot!",9,6,2025-04-13 21:12:25,0,False,False,False,False,2025-04-13 21:12:25,21,Sunday,185.0,1102,52.7,10,291,12.2,0,0,POSITIVE,0.9984258413314819,"['everyone', 'new', 'grad', 'starting', 'may', 'ill', 'working', 'small', 'startup', 'analytics', 'engineer', 'ive', 'gotten', 'pretty', 'solid', 'sql', 'ive', 'grinding', 'leetcode', 'questions', 'fun', 'really', 'helped', 'land', 'job', 'internships', 'also', 'worked', 'lot', 'dbt', 'snowflake', 'airflow', 'fairly', 'comfortable', 'tooling', 'side', 'things', 'struggling', 'data', 'modelingspecifically', 'kimball', 'methodology', 'star', 'schemas', 'different', 'types', 'dimensions', 'fact', 'tables', 'tried', 'reading', 'kimball', 'book', 'honestly', 'felt', 'super', 'abstract', 'without', 'handson', 'practice', 'get', 'real', 'data', 'modeling', 'often', 'involves', 'tradeoffs', 'business', 'context', 'actual', 'stakeholder', 'input', 'isnt', 'easy', 'simulate', 'question', 'ishow', 'college', 'student', 'new', 'grad', 'start', 'building', 'intuition', 'skills', 'data', 'modeling', 'practical', 'resources', 'projects', 'work', 'better', 'understand', 'area', 'general', 'advice', 'someone', 'entering', 'industry', 'kind', 'role', 'love', 'hear', 'thanks', 'lot']",everyone new grad starting may ill working small startup analytics engineer ive gotten pretty solid sql ive grinding leetcode questions fun really helped land job internships also worked lot dbt snowflake airflow fairly comfortable tooling side things struggling data modelingspecifically kimball methodology star schemas different types dimensions fact tables tried reading kimball book honestly felt super abstract without handson practice get real data modeling often involves tradeoffs business context actual stakeholder input isnt easy simulate question ishow college student new grad start building intuition skills data modeling practical resources projects work better understand area general advice someone entering industry kind role love hear thanks lot,High,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
206,Data modeling for analytics with legacy Schema-on-Read data lake?,"Most guides on data modeling and data pipelines seem to focus on greenfield projects.

But how do you deal with a legacy data lake where there's been years of data written into tables with no changes to original source-defined schemas?

I have hundreds of table schemas which analysts want to use but can't because they have to manually go through the data catalogue and find every column containing 'x' data or simply not bothering with some tables.

How do you tackle such a legacy mess of data? Say I want to create a Kimball model that models a persons fact table as the grain, and dimensions tables for biographical and employment data. Is my only choice to just manually inspect all the different tables to find which have the kind of column I need? Note here that there wasn't even a basic normalisation of column names enforced (""phone_num"", ""phone"", ""tel"", ""phone_number"" etc) and some of this data is already in OBT form with some containing up to a hundred sparsely populated columns.

Do I apply fuzzy matching to identify source columns? Use an LLM to build massive mapping dictionaries? What are some approaches or methods I should consider when tackling this so I'm not stuck scrolling through infinite print outs? There is a metadata catalogue with some columns having been given tags to identify its content, but these aren't consistent and also have high cardinality.

From the business perspective, they want completeness, so I can't strategically pick which tables to use and ignore the rest. Is there a way I should prioritize based on integrating the largest datasets first?

The tables are a mix of both static imports and a few daily pipelines. I'm primarily working in pyspark and spark SQL ",7,0,2025-04-13 14:51:05,1,False,False,False,False,2025-04-13 14:51:05,14,Sunday,291.0,1716,51.78,15,458,12.2,0,0,NEGATIVE,-0.999565064907074,"['guides', 'data', 'modeling', 'data', 'pipelines', 'seem', 'focus', 'greenfield', 'projects', 'deal', 'legacy', 'data', 'lake', 'theres', 'years', 'data', 'written', 'tables', 'changes', 'original', 'sourcedefined', 'schemas', 'hundreds', 'table', 'schemas', 'analysts', 'want', 'use', 'cant', 'manually', 'data', 'catalogue', 'find', 'every', 'column', 'containing', 'data', 'simply', 'bothering', 'tables', 'tackle', 'legacy', 'mess', 'data', 'say', 'want', 'create', 'kimball', 'model', 'models', 'persons', 'fact', 'table', 'grain', 'dimensions', 'tables', 'biographical', 'employment', 'data', 'choice', 'manually', 'inspect', 'different', 'tables', 'find', 'kind', 'column', 'need', 'note', 'wasnt', 'even', 'basic', 'normalisation', 'column', 'names', 'enforced', 'phonenum', 'phone', 'tel', 'phonenumber', 'etc', 'data', 'already', 'obt', 'form', 'containing', 'hundred', 'sparsely', 'populated', 'columns', 'apply', 'fuzzy', 'matching', 'identify', 'source', 'columns', 'use', 'llm', 'build', 'massive', 'mapping', 'dictionaries', 'approaches', 'methods', 'consider', 'tackling', 'stuck', 'scrolling', 'infinite', 'print', 'outs', 'metadata', 'catalogue', 'columns', 'given', 'tags', 'identify', 'content', 'arent', 'consistent', 'also', 'high', 'cardinality', 'business', 'perspective', 'want', 'completeness', 'cant', 'strategically', 'pick', 'tables', 'use', 'ignore', 'rest', 'way', 'prioritize', 'based', 'integrating', 'largest', 'datasets', 'first', 'tables', 'mix', 'static', 'imports', 'daily', 'pipelines', 'primarily', 'working', 'pyspark', 'spark', 'sql']",guides data modeling data pipelines seem focus greenfield projects deal legacy data lake theres years data written tables changes original sourcedefined schemas hundreds table schemas analysts want use cant manually data catalogue find every column containing data simply bothering tables tackle legacy mess data say want create kimball model models persons fact table grain dimensions tables biographical employment data choice manually inspect different tables find kind column need note wasnt even basic normalisation column names enforced phonenum phone tel phonenumber etc data already obt form containing hundred sparsely populated columns apply fuzzy matching identify source columns use llm build massive mapping dictionaries approaches methods consider tackling stuck scrolling infinite print outs metadata catalogue columns given tags identify content arent consistent also high cardinality business perspective want completeness cant strategically pick tables use ignore rest way prioritize based integrating largest datasets first tables mix static imports daily pipelines primarily working pyspark spark sql,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
207,Building a Real-Time Analytics Pipeline: Balancing Throughput and Latency,"Hey everyone,

I'm designing a system to process and analyze a continuous stream of data with a focus on both high throughput and low latency. I wanted to share my proposed architecture and get your insights.

1. The core components are: **Kafka:** Serving as the central nervous system for ingesting a massive amount of data reliably.
2. **Go Processor:** A consumer application written in Go, placed directly after Kafka, to perform initial, low-latency processing and filtering of the incoming data.
3. **Intermediate Queue (Redis Streams/NATS JetStream):** To decouple the low-latency processing from the potentially slower analytics and to provide buffering for data that needs further analysis.
4. **Analytics Consumer:** Responsible for the more intensive analytical tasks on the filtered data from the queue.
5. **WebSockets:** For pushing the processed insights to a frontend in real-time.

The idea is to leverage Kafka's throughput capabilities while using Go for quick initial processing. The queue acts as a buffer and allows us to be selective about the data sent for deeper analytics. Finally, WebSockets provide the real-time link to the user.

I built this keeping in mind these three principles

* **Separation of Concerns:** Each component has a specific responsibility.
* **Scalability:** Kafka handles ingestion, and individual consumers can be scaled independently.
* **Resilience:** The queue helps decouple processing stages.

Has anyone implemented a similar architecture? What were some of the challenges and lessons learned? Any recommendations for improvements or alternative approaches?

Looking forward to your feedback!",7,14,2025-04-13 13:35:41,1,False,False,False,False,2025-04-13 13:35:41,13,Sunday,244.0,1650,31.68,17,456,14.3,0,0,NEGATIVE,-0.9676582217216492,"['hey', 'everyone', 'designing', 'system', 'process', 'analyze', 'continuous', 'stream', 'data', 'focus', 'high', 'throughput', 'low', 'latency', 'wanted', 'share', 'proposed', 'architecture', 'get', 'insights', 'core', 'components', 'kafka', 'serving', 'central', 'nervous', 'system', 'ingesting', 'massive', 'amount', 'data', 'reliably', 'processor', 'consumer', 'application', 'written', 'placed', 'directly', 'kafka', 'perform', 'initial', 'lowlatency', 'processing', 'filtering', 'incoming', 'data', 'intermediate', 'queue', 'redis', 'streamsnats', 'jetstream', 'decouple', 'lowlatency', 'processing', 'potentially', 'slower', 'analytics', 'provide', 'buffering', 'data', 'needs', 'analysis', 'analytics', 'consumer', 'responsible', 'intensive', 'analytical', 'tasks', 'filtered', 'data', 'queue', 'websockets', 'pushing', 'processed', 'insights', 'frontend', 'realtime', 'idea', 'leverage', 'kafkas', 'throughput', 'capabilities', 'using', 'quick', 'initial', 'processing', 'queue', 'acts', 'buffer', 'allows', 'selective', 'data', 'sent', 'deeper', 'analytics', 'finally', 'websockets', 'provide', 'realtime', 'link', 'user', 'built', 'keeping', 'mind', 'three', 'principles', 'separation', 'concerns', 'component', 'specific', 'responsibility', 'scalability', 'kafka', 'handles', 'ingestion', 'individual', 'consumers', 'scaled', 'independently', 'resilience', 'queue', 'helps', 'decouple', 'processing', 'stages', 'anyone', 'implemented', 'similar', 'architecture', 'challenges', 'lessons', 'learned', 'recommendations', 'improvements', 'alternative', 'approaches', 'looking', 'forward', 'feedback']",hey everyone designing system process analyze continuous stream data focus high throughput low latency wanted share proposed architecture get insights core components kafka serving central nervous system ingesting massive amount data reliably processor consumer application written placed directly kafka perform initial lowlatency processing filtering incoming data intermediate queue redis streamsnats jetstream decouple lowlatency processing potentially slower analytics provide buffering data needs analysis analytics consumer responsible intensive analytical tasks filtered data queue websockets pushing processed insights frontend realtime idea leverage kafkas throughput capabilities using quick initial processing queue acts buffer allows selective data sent deeper analytics finally websockets provide realtime link user built keeping mind three principles separation concerns component specific responsibility scalability kafka handles ingestion individual consumers scaled independently resilience queue helps decouple processing stages anyone implemented similar architecture challenges lessons learned recommendations improvements alternative approaches looking forward feedback,High,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
208,Freelancing - Real Talk,"
Hey folks,
I’m a data/software engineer trying to break into freelancing, and honestly, I could use some advice. I’ve been focusing on niches like building ETL pipelines, automation tools, web scraping, data mining, and even playing around with RAG bots.

I’ve been on Upwork for about 2 months now and only landed one small scraping gig so far. My portfolio is pretty solid (or at least I think it is), but I’m not getting much traction, barely any invites.

So I’m wondering:

Is Upwork just super saturated for this kind of work right now?

Are there better platforms or communities for technical freelancing gigs (especially data-related)?

What worked for you when you were just starting out?

Is there a niche I should lean harder into?


Would love to hear from anyone who’s been through this or has some pointers. I'm open to harsh truths, hacks, or anything in between. Appreciate it!


",6,0,2025-04-13 21:54:58,1,False,False,False,False,2025-04-13 21:54:58,21,Sunday,151.0,897,64.61,10,224,10.6,0,0,NEGATIVE,-0.9973723888397217,"['hey', 'folks', 'datasoftware', 'engineer', 'trying', 'break', 'freelancing', 'honestly', 'could', 'use', 'advice', 'ive', 'focusing', 'niches', 'like', 'building', 'etl', 'pipelines', 'automation', 'tools', 'web', 'scraping', 'data', 'mining', 'even', 'playing', 'around', 'rag', 'bots', 'ive', 'upwork', 'months', 'landed', 'one', 'small', 'scraping', 'gig', 'far', 'portfolio', 'pretty', 'solid', 'least', 'think', 'getting', 'much', 'traction', 'barely', 'invites', 'wondering', 'upwork', 'super', 'saturated', 'kind', 'work', 'right', 'better', 'platforms', 'communities', 'technical', 'freelancing', 'gigs', 'especially', 'datarelated', 'worked', 'starting', 'niche', 'lean', 'harder', 'would', 'love', 'hear', 'anyone', 'whos', 'pointers', 'open', 'harsh', 'truths', 'hacks', 'anything', 'appreciate']",hey folks datasoftware engineer trying break freelancing honestly could use advice ive focusing niches like building etl pipelines automation tools web scraping data mining even playing around rag bots ive upwork months landed one small scraping gig far portfolio pretty solid least think getting much traction barely invites wondering upwork super saturated kind work right better platforms communities technical freelancing gigs especially datarelated worked starting niche lean harder would love hear anyone whos pointers open harsh truths hacks anything appreciate,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
209,I need assistance in optimizing this ADF workflow.,"[my\_pipeline](https://preview.redd.it/6xdqaeiikjue1.png?width=1792&format=png&auto=webp&s=7ca7ab2d25d73b1f4f7869c3927fd16c0246bb04)

Hello all! I'm excited to dive into **ADF** and try out some new things.

Here, you can see we have a copy data activity that transfers files from the source ADLS to the raw ADLS location. Then, we have a Lookup named **Lkp\_archivepath** which retrieves values from the SQL server, known as the Metastore. This will get values such as **archive\_path** and **archive\_delete\_flag** (typically it will be Y or N, and sometimes the parameter will be missing as well). After that, we have a copy activity that copies files from the source ADLS to the archive location. Now, I'm encountering an issue as I'm trying to introduce this archive delete flag concept.

If the **archive\_delete\_flag** is '**Y**', it should not delete the files from the source, but it should delete the files if the **archive\_delete\_flag** is '**N**', '' or NULL, depending on the Metastore values. How can I make this work?

Looking forward to your suggestions, thanks!",5,8,2025-04-13 06:02:54,0,False,False,False,False,2025-04-13 06:02:54,6,Sunday,158.0,1082,64.0,10,231,10.1,1,0,NEGATIVE,-0.9922130703926086,"['mypipelinehttpspreviewredditxdqaeiikjuepngwidthformatpngautowebpscaabddbffcfdcbb', 'hello', 'excited', 'dive', 'adf', 'try', 'new', 'things', 'see', 'copy', 'data', 'activity', 'transfers', 'files', 'source', 'adls', 'raw', 'adls', 'location', 'lookup', 'named', 'lkparchivepath', 'retrieves', 'values', 'sql', 'server', 'known', 'metastore', 'get', 'values', 'archivepath', 'archivedeleteflag', 'typically', 'sometimes', 'parameter', 'missing', 'well', 'copy', 'activity', 'copies', 'files', 'source', 'adls', 'archive', 'location', 'encountering', 'issue', 'trying', 'introduce', 'archive', 'delete', 'flag', 'concept', 'archivedeleteflag', 'delete', 'files', 'source', 'delete', 'files', 'archivedeleteflag', 'null', 'depending', 'metastore', 'values', 'make', 'work', 'looking', 'forward', 'suggestions', 'thanks']",mypipelinehttpspreviewredditxdqaeiikjuepngwidthformatpngautowebpscaabddbffcfdcbb hello excited dive adf try new things see copy data activity transfers files source adls raw adls location lookup named lkparchivepath retrieves values sql server known metastore get values archivepath archivedeleteflag typically sometimes parameter missing well copy activity copies files source adls archive location encountering issue trying introduce archive delete flag concept archivedeleteflag delete files source delete files archivedeleteflag null depending metastore values make work looking forward suggestions thanks,Mid,4,4,"files, api, tables, query, models, read, google, json, file, logic, write, case, csv, store, datasets, across, sources, metadata, created, schema, systems, large, platform, custom, script, changes, dataset, row, reporting, parquet, issues, ads, load, fields, analysis, cases, product, values, design, view, idea, layer, search, import, suggest, language, excel, app, blob, form, request, params, postgres, ingestion, raw, mysql, fact, processed, structures, either, separate, structured, connection, core, method, downstream, scripts, extract, maintain, send, hub, records, transactional, overhead, lineage, environment, object, external, pull, iot, order, typically, grid, hubs, validation, requires, mapping, batch, mechanism, location, updates, keeping, checkout, operations, runs, convert, optimized, avoid, matching, views, formats, matters, vpc, bring, reduce, migrate, consists, starts, reads, refresh, relationships, bronze, historical, tracking, developers, natural",8.0,7.1,169.4,-0.679,1041.7,44.5,9.5,267.7,10.7,24.1,17.9,49,102,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,7
210,Creating AWS Glue Connection for On-prem JDBC source,"There seems to be little to no documentation(or atleast I can't find any meaningful guides), that can help me establish a successful connection with a MySQL source. 
Either getting this VPC endpoint or NAT gateway error:

InvalidInputException: VPC S3 endpoint validation failed for SubnetId: subnet-XXX. VPC: vpc-XXX. Reason: Could not find S3 endpoint or NAT gateway for subnetId: subnet-XXX in Vpc vpc-XXX

Upon creating said endpoint and NAT gateway connection halts and provides Timeout after 5 or so minutes. My JDBC connection is able to successfully establish with either something like PyMySQL package on local machine, or in Glue notebooks with Spark JDBC connection. Any help would be great. ",3,1,2025-04-13 13:09:41,1,False,False,False,False,2025-04-13 13:09:41,13,Sunday,109.0,703,49.35,5,176,12.3,0,0,NEGATIVE,-0.9997143149375916,"['seems', 'little', 'documentationor', 'atleast', 'cant', 'find', 'meaningful', 'guides', 'help', 'establish', 'successful', 'connection', 'mysql', 'source', 'either', 'getting', 'vpc', 'endpoint', 'nat', 'gateway', 'error', 'invalidinputexception', 'vpc', 'endpoint', 'validation', 'failed', 'subnetid', 'subnetxxx', 'vpc', 'vpcxxx', 'reason', 'could', 'find', 'endpoint', 'nat', 'gateway', 'subnetid', 'subnetxxx', 'vpc', 'vpcxxx', 'upon', 'creating', 'said', 'endpoint', 'nat', 'gateway', 'connection', 'halts', 'provides', 'timeout', 'minutes', 'jdbc', 'connection', 'able', 'successfully', 'establish', 'either', 'something', 'like', 'pymysql', 'package', 'local', 'machine', 'glue', 'notebooks', 'spark', 'jdbc', 'connection', 'help', 'would', 'great']",seems little documentationor atleast cant find meaningful guides help establish successful connection mysql source either getting vpc endpoint nat gateway error invalidinputexception vpc endpoint validation failed subnetid subnetxxx vpc vpcxxx reason could find endpoint nat gateway subnetid subnetxxx vpc vpcxxx upon creating said endpoint nat gateway connection halts provides timeout minutes jdbc connection able successfully establish either something like pymysql package local machine glue notebooks spark jdbc connection help would great,Mid,4,4,"files, api, tables, query, models, read, google, json, file, logic, write, case, csv, store, datasets, across, sources, metadata, created, schema, systems, large, platform, custom, script, changes, dataset, row, reporting, parquet, issues, ads, load, fields, analysis, cases, product, values, design, view, idea, layer, search, import, suggest, language, excel, app, blob, form, request, params, postgres, ingestion, raw, mysql, fact, processed, structures, either, separate, structured, connection, core, method, downstream, scripts, extract, maintain, send, hub, records, transactional, overhead, lineage, environment, object, external, pull, iot, order, typically, grid, hubs, validation, requires, mapping, batch, mechanism, location, updates, keeping, checkout, operations, runs, convert, optimized, avoid, matching, views, formats, matters, vpc, bring, reduce, migrate, consists, starts, reads, refresh, relationships, bronze, historical, tracking, developers, natural",8.0,7.1,169.4,-0.679,1041.7,44.5,9.5,267.7,10.7,24.1,17.9,49,102,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,7
211,"I've built a ""Cursor for data"" app and looking for beta testers","Cipher42 is a ""Cursor for data"" which works by connecting to your database/data warehouse, indexing things like schema, metadata, recent used queries and then using it to provide better answers and making data analysts more productive. It took a lot of inspiration from cursor but for data related app cursor doesn't work as well as data analysis workloads are different by nature.",2,0,2025-04-13 21:58:22,0,False,False,False,False,2025-04-13 21:58:22,21,Sunday,62.0,381,31.55,2,104,0.0,0,0,NEGATIVE,-0.9864150881767273,"['cipher', 'cursor', 'data', 'works', 'connecting', 'databasedata', 'warehouse', 'indexing', 'things', 'like', 'schema', 'metadata', 'recent', 'used', 'queries', 'using', 'provide', 'better', 'answers', 'making', 'data', 'analysts', 'productive', 'took', 'lot', 'inspiration', 'cursor', 'data', 'related', 'app', 'cursor', 'doesnt', 'work', 'well', 'data', 'analysis', 'workloads', 'different', 'nature']",cipher cursor data works connecting databasedata warehouse indexing things like schema metadata recent used queries using provide better answers making data analysts productive took lot inspiration cursor data related app cursor doesnt work well data analysis workloads different nature,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
212,Data interpretation,"any book recommendations for data interpretation for ipucet bcom h paper
",2,0,2025-04-13 08:02:09,0,False,False,False,False,2025-04-13 08:02:09,8,Sunday,11.0,73,26.47,1,22,0.0,0,0,NEGATIVE,-0.9788328409194946,"['book', 'recommendations', 'data', 'interpretation', 'ipucet', 'bcom', 'paper']",book recommendations data interpretation ipucet bcom paper,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
213,Want opinion about Lambdas,"Hi all.
I'd love your opinion and experience about the data pipeline I'm working on.

The pipeline is for the RAG inference system.
The user would interact with the system through an API which triggers a Lambda.

The inference consists of  4 main functions-
1. Apply query guardrails
2. Fetch relevant chunks
3. Pass query and chunks to LLM and get response 
4. Apply source attribution (additional metadata related to the data) to the response 

I've assigned 1 AWS Lambda function to each component/function totalling to 4 lambdas in the pipeline.

Can the above mentioned functions be achieved under 30 secs if they're clubbed into 1 Lambda function?

Pls clarify in comments if this information is not sufficient to answer the question.

Also, please share any documentation that suggests which approach is better ( multiple lambdas or 1 lambda)

Thank you in advance!",2,7,2025-04-13 03:03:52,0,False,False,False,False,2025-04-13 03:03:52,3,Sunday,143.0,872,58.38,11,230,10.4,0,1,NEGATIVE,-0.9803685545921326,"['love', 'opinion', 'experience', 'data', 'pipeline', 'working', 'pipeline', 'rag', 'inference', 'system', 'user', 'would', 'interact', 'system', 'api', 'triggers', 'lambda', 'inference', 'consists', 'main', 'functions', 'apply', 'query', 'guardrails', 'fetch', 'relevant', 'chunks', 'pass', 'query', 'chunks', 'llm', 'get', 'response', 'apply', 'source', 'attribution', 'additional', 'metadata', 'related', 'data', 'response', 'ive', 'assigned', 'aws', 'lambda', 'function', 'componentfunction', 'totalling', 'lambdas', 'pipeline', 'mentioned', 'functions', 'achieved', 'secs', 'theyre', 'clubbed', 'lambda', 'function', 'pls', 'clarify', 'comments', 'information', 'sufficient', 'answer', 'question', 'also', 'please', 'share', 'documentation', 'suggests', 'approach', 'better', 'multiple', 'lambdas', 'lambda', 'thank', 'advance']",love opinion experience data pipeline working pipeline rag inference system user would interact system api triggers lambda inference consists main functions apply query guardrails fetch relevant chunks pass query chunks llm get response apply source attribution additional metadata related data response ive assigned aws lambda function componentfunction totalling lambdas pipeline mentioned functions achieved secs theyre clubbed lambda function pls clarify comments information sufficient answer question also please share documentation suggests approach better multiple lambdas lambda thank advance,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
214,"Developing, testing and deploying production grade data pipelines with AWS Glue","Serious question for data engineers working with AWS Glue: How do you actually structure and test production-grade pipelines.

For simple pipelines it's straight forward: just write everything in a single job using glue's editor, run and you're good to go, but for production data pipelines, how is the gap between the local code base that is modularized ( utils, libs, etc ) bridged with glue, that apparently needs everything to be bundled into jobs?

This is the first thing I am struggling to understand, my second dilemma is about testing jobs locally.  
How does local testing happen?

**->** if we will use glue's compute engine we run into the first question of: gap between code base and single jobs.

**->** if we use open source spark locally:

1. data can be too big to be processed locally, even if we are just testing, and this might be the reason we opted for serverless spark on the first place.

  
2. Glue’s customized Spark runtime behaves differently than open-source Spark, so local tests won’t fully match production behavior. This makes it hard to validate logic before deploying to Glue",1,4,2025-04-13 20:28:12,0,False,False,False,False,2025-04-13 20:28:12,20,Sunday,187.0,1110,59.33,9,277,12.3,0,0,NEGATIVE,-0.9978312849998474,"['serious', 'question', 'data', 'engineers', 'working', 'aws', 'glue', 'actually', 'structure', 'test', 'productiongrade', 'pipelines', 'simple', 'pipelines', 'straight', 'forward', 'write', 'everything', 'single', 'job', 'using', 'glues', 'editor', 'run', 'youre', 'good', 'production', 'data', 'pipelines', 'gap', 'local', 'code', 'base', 'modularized', 'utils', 'libs', 'etc', 'bridged', 'glue', 'apparently', 'needs', 'everything', 'bundled', 'jobs', 'first', 'thing', 'struggling', 'understand', 'second', 'dilemma', 'testing', 'jobs', 'locally', 'local', 'testing', 'happen', 'use', 'glues', 'compute', 'engine', 'run', 'first', 'question', 'gap', 'code', 'base', 'single', 'jobs', 'use', 'open', 'source', 'spark', 'locally', 'data', 'big', 'processed', 'locally', 'even', 'testing', 'might', 'reason', 'opted', 'serverless', 'spark', 'first', 'place', 'glues', 'customized', 'spark', 'runtime', 'behaves', 'differently', 'opensource', 'spark', 'local', 'tests', 'wont', 'fully', 'match', 'production', 'behavior', 'makes', 'hard', 'validate', 'logic', 'deploying', 'glue']",serious question data engineers working aws glue actually structure test productiongrade pipelines simple pipelines straight forward write everything single job using glues editor run youre good production data pipelines gap local code base modularized utils libs etc bridged glue apparently needs everything bundled jobs first thing struggling understand second dilemma testing jobs locally local testing happen use glues compute engine run first question gap code base single jobs use open source spark locally data big processed locally even testing might reason opted serverless spark first place glues customized spark runtime behaves differently opensource spark local tests wont fully match production behavior makes hard validate logic deploying glue,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
215,Thoughts on Acryl vs other metadata platforms,"Hi all, I'm evaluating metadata management solutions for our data platform and would appreciate any thoughts from folks who've actually implemented these tools in production.

We're currently running into scaling issues with our in-house data catalog and I think we need something more robust for governance and lineage tracking.

I've narrowed it down to Acryl (DataHub) and Collate (openmetadata) as the main contenders. I know I should look at Collibra and Alation and maybe Unity Catalog?

For context, we're a mid-sized fintech (\~500 employees) with about 30 data engineers and scientists. We're AWS with Snowflake, Airflow for orchestration, and a growing number of ML models in production.

My question list is: 

1. How these tools handle machine-scale operations 
2. How painful was it to get set up?
3. For DataHub and openmetadata specifically - is the open source version viable or is the cloud version necessary?
4. Any unexpected limitations you've hit with any of these platforms?
5. Do you feel like these grow with you as we increasingly head into AI governance? 
6. How well they integrate with existing tools (Snowflake, dbt, Looker, etc.)

If anyone has switched from one solution to another, I'd love to hear why you made the change and whether it was worth it.

Sorry for the pick list of questions - the last post on this was years ago and I was hoping for some more insights. Thanks in advance for anyone's thoughts. ",1,1,2025-04-12 23:05:00,0,False,False,False,False,2025-04-12 23:05:00,23,Saturday,239.0,1442,56.45,16,377,11.9,0,0,NEGATIVE,-0.9969605803489685,"['evaluating', 'metadata', 'management', 'solutions', 'data', 'platform', 'would', 'appreciate', 'thoughts', 'folks', 'whove', 'actually', 'implemented', 'tools', 'production', 'currently', 'running', 'scaling', 'issues', 'inhouse', 'data', 'catalog', 'think', 'need', 'something', 'robust', 'governance', 'lineage', 'tracking', 'ive', 'narrowed', 'acryl', 'datahub', 'collate', 'openmetadata', 'main', 'contenders', 'know', 'look', 'collibra', 'alation', 'maybe', 'unity', 'catalog', 'context', 'midsized', 'fintech', 'employees', 'data', 'engineers', 'scientists', 'aws', 'snowflake', 'airflow', 'orchestration', 'growing', 'number', 'models', 'production', 'question', 'list', 'tools', 'handle', 'machinescale', 'operations', 'painful', 'get', 'set', 'datahub', 'openmetadata', 'specifically', 'open', 'source', 'version', 'viable', 'cloud', 'version', 'necessary', 'unexpected', 'limitations', 'youve', 'hit', 'platforms', 'feel', 'like', 'grow', 'increasingly', 'head', 'governance', 'well', 'integrate', 'existing', 'tools', 'snowflake', 'dbt', 'looker', 'etc', 'anyone', 'switched', 'one', 'solution', 'another', 'love', 'hear', 'made', 'change', 'whether', 'worth', 'sorry', 'pick', 'list', 'questions', 'last', 'post', 'years', 'ago', 'hoping', 'insights', 'thanks', 'advance', 'anyones', 'thoughts']",evaluating metadata management solutions data platform would appreciate thoughts folks whove actually implemented tools production currently running scaling issues inhouse data catalog think need something robust governance lineage tracking ive narrowed acryl datahub collate openmetadata main contenders know look collibra alation maybe unity catalog context midsized fintech employees data engineers scientists aws snowflake airflow orchestration growing number models production question list tools handle machinescale operations painful get set datahub openmetadata specifically open source version viable cloud version necessary unexpected limitations youve hit platforms feel like grow increasingly head governance well integrate existing tools snowflake dbt looker etc anyone switched one solution another love hear made change whether worth sorry pick list questions last post years ago hoping insights thanks advance anyones thoughts,Mid,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
216,Help,"I'm using Airbyte Cloud because my PC doesn't have enough resources to install it. I have a Docker container running PostgreSQL on Airbyte Cloud. I want to set the PostgreSQL destination. Can anyone give me some guidance on how to do this? Should I create an SSH tunnel?

",0,1,2025-04-13 00:06:47,0,False,False,False,False,2025-04-13 00:06:47,0,Sunday,48.0,272,70.19,5,72,8.2,0,0,NEGATIVE,-0.9997380375862122,"['using', 'airbyte', 'cloud', 'doesnt', 'enough', 'resources', 'install', 'docker', 'container', 'running', 'postgresql', 'airbyte', 'cloud', 'want', 'set', 'postgresql', 'destination', 'anyone', 'give', 'guidance', 'create', 'ssh', 'tunnel']",using airbyte cloud doesnt enough resources install docker container running postgresql airbyte cloud want set postgresql destination anyone give guidance create ssh tunnel,Low,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
217,What to do and how to do???,"This is a photo of my notes (not OG rewrote later) about a meet at work about this said project. The project is about migration of ms sql server to snowflake. 

The code conversion will be done using Snowconvert. 

For historic data
1. The data extraction is done using a python script using bcp command and pyodbc library 
2. The converted code from snowconvert will be used in a python script again to create all the database objects. 
3. data extracted will be loaded into internal stage and then to table 

2 and 3 will use snowflake’s python connector 

For transitional data: 
1. Use ADF to store pipeline output into an Azure blob container 
2. Use external stage to utilise this blob and load data into table 


1. My question is if you have ADF for transitional data then why not use the same thing for historic data as well (I was given the task of historic data)
2. Is there a free way to handle this transitional data as well. It needs to be enterprise level (Also what is wrong with using VS Code extension) 
3. After I showed initial approach following things were asked by mentor/friend to incorporate in this to really sell my approach (He went home after giving me no clarification about how to do this and what even are they)
- validation of data on both sides 
- partition aware extraction 
- parallely extracting data (Idts it is even possible)

I request help on where to even start looking and rate my approach I am a fresh graduate and been on job for a month. 🙂‍↕️🙂‍↕️
",0,4,2025-04-13 18:30:34,0,False,False,False,False,2025-04-13 18:30:34,18,Sunday,271.0,1493,59.13,13,408,12.5,0,0,NEGATIVE,-0.9975302815437317,"['photo', 'notes', 'rewrote', 'later', 'meet', 'work', 'said', 'project', 'project', 'migration', 'sql', 'server', 'snowflake', 'code', 'conversion', 'done', 'using', 'snowconvert', 'historic', 'data', 'data', 'extraction', 'done', 'using', 'python', 'script', 'using', 'bcp', 'command', 'pyodbc', 'library', 'converted', 'code', 'snowconvert', 'used', 'python', 'script', 'create', 'database', 'objects', 'data', 'extracted', 'loaded', 'internal', 'stage', 'table', 'use', 'snowflakes', 'python', 'connector', 'transitional', 'data', 'use', 'adf', 'store', 'pipeline', 'output', 'azure', 'blob', 'container', 'use', 'external', 'stage', 'utilise', 'blob', 'load', 'data', 'table', 'question', 'adf', 'transitional', 'data', 'use', 'thing', 'historic', 'data', 'well', 'given', 'task', 'historic', 'data', 'free', 'way', 'handle', 'transitional', 'data', 'well', 'needs', 'enterprise', 'level', 'also', 'wrong', 'using', 'code', 'extension', 'showed', 'initial', 'approach', 'following', 'things', 'asked', 'mentorfriend', 'incorporate', 'really', 'sell', 'approach', 'went', 'home', 'giving', 'clarification', 'even', 'validation', 'data', 'sides', 'partition', 'aware', 'extraction', 'parallely', 'extracting', 'data', 'idts', 'even', 'possible', 'request', 'help', 'even', 'start', 'looking', 'rate', 'approach', 'fresh', 'graduate', 'job', 'month']",photo notes rewrote later meet work said project project migration sql server snowflake code conversion done using snowconvert historic data data extraction done using python script using bcp command pyodbc library converted code snowconvert used python script create database objects data extracted loaded internal stage table use snowflakes python connector transitional data use adf store pipeline output azure blob container use external stage utilise blob load data table question adf transitional data use thing historic data well given task historic data free way handle transitional data well needs enterprise level also wrong using code extension showed initial approach following things asked mentorfriend incorporate really sell approach went home giving clarification even validation data sides partition aware extraction parallely extracting data idts even possible request help even start looking rate approach fresh graduate job month,Low,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
218,Why Data Warehouses Were Created?,"The original data chaos actually started *before* spreadsheets were common. In the pre-ERP days, most business systems were siloed—HR, finance, sales, you name it—all running on their own. To report on anything meaningful, you had to extract data from each system, often manually. These extracts were pulled at different times, using different rules, and then stitched togethe. The result? Data quality issues. And to make matters worse, people were running these reports directly against transactional databases—systems that were supposed to be optimized for speed and reliability, not analytics. The reporting load bogged them down.

The problem was so painful for the businesses, so around the late 1980s, a few forward-thinking folks—most famously Bill Inmon—proposed a better way: a data warehouse.

To make matter even worse, in the late ’00s every department had its own spreadsheet empire. Finance had one version of “the truth,” Sales had another, and Marketing were inventing their own metrics. People would walk into meetings with totally different numbers for the same KPI.

The spreadsheet party had turned into a data chaos rave. There was no lineage, no source of truth—just lots of tab-switching and passive-aggressive email threads. It wasn’t just annoying—it was a risk. Businesses were making big calls on bad data. So data warehousing became common practice!

More about it: [https://www.corgineering.com/blog/How-Data-Warehouses-Were-Created](https://www.corgineering.com/blog/How-Data-Warehouses-Were-Created)

  
P.S. Thanks to u/rotr0102 I made the post at least 2x times better",37,14,2025-04-14 07:15:27,0,False,2025-04-14 19:01:58,False,False,2025-04-14 07:15:27,7,Monday,229.0,1602,50.12,18,382,10.9,1,0,NEGATIVE,-0.9993433356285095,"['original', 'data', 'chaos', 'actually', 'started', 'spreadsheets', 'common', 'preerp', 'days', 'business', 'systems', 'siloedhr', 'finance', 'sales', 'name', 'itall', 'running', 'report', 'anything', 'meaningful', 'extract', 'data', 'system', 'often', 'manually', 'extracts', 'pulled', 'different', 'times', 'using', 'different', 'rules', 'stitched', 'togethe', 'result', 'data', 'quality', 'issues', 'make', 'matters', 'worse', 'people', 'running', 'reports', 'directly', 'transactional', 'databasessystems', 'supposed', 'optimized', 'speed', 'reliability', 'analytics', 'reporting', 'load', 'bogged', 'problem', 'painful', 'businesses', 'around', 'late', 'forwardthinking', 'folksmost', 'famously', 'bill', 'inmonproposed', 'better', 'way', 'data', 'warehouse', 'make', 'matter', 'even', 'worse', 'late', 'every', 'department', 'spreadsheet', 'empire', 'finance', 'one', 'version', 'truth', 'sales', 'another', 'marketing', 'inventing', 'metrics', 'people', 'would', 'walk', 'meetings', 'totally', 'different', 'numbers', 'kpi', 'spreadsheet', 'party', 'turned', 'data', 'chaos', 'rave', 'lineage', 'source', 'truthjust', 'lots', 'tabswitching', 'passiveaggressive', 'email', 'threads', 'wasnt', 'annoyingit', 'risk', 'businesses', 'making', 'big', 'calls', 'bad', 'data', 'data', 'warehousing', 'became', 'common', 'practice', 'httpswwwcorgineeringcombloghowdatawarehouseswerecreatedhttpswwwcorgineeringcombloghowdatawarehouseswerecreated', 'thanks', 'urotr', 'made', 'post', 'least', 'times', 'better']",original data chaos actually started spreadsheets common preerp days business systems siloedhr finance sales name itall running report anything meaningful extract data system often manually extracts pulled different times using different rules stitched togethe result data quality issues make matters worse people running reports directly transactional databasessystems supposed optimized speed reliability analytics reporting load bogged problem painful businesses around late forwardthinking folksmost famously bill inmonproposed better way data warehouse make matter even worse late every department spreadsheet empire finance one version truth sales another marketing inventing metrics people would walk meetings totally different numbers kpi spreadsheet party turned data chaos rave lineage source truthjust lots tabswitching passiveaggressive email threads wasnt annoyingit risk businesses making big calls bad data data warehousing became common practice httpswwwcorgineeringcombloghowdatawarehouseswerecreatedhttpswwwcorgineeringcombloghowdatawarehouseswerecreated thanks urotr made post least times better,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
219,Roles when career shifting out of data engineering?,"To be specific, non-code heavy work. I think I’m one of the few data engineers who hates coding and developing. All our projects and clients so far have always asked us to use ADB in developing notebooks for ETL use, and I have never touched ADF -_-

Now I’m sick of it, developing ETL stuff using pyspark or sparksql is too stressful for me and I have 0 interest in data engineering right now. 

Anyone who has successfully left the DE field? What non-code role did you choose? I’d appreciate any suggestions especially for jobs that make use of some of the less-coding side of Data Engineering.

I see lots of people going for software eng because they love coding and some go ML or Data Scientist. Maybe i just want less tech-y work right now but yeah open to any suggestions. I’m also fine with sql, as long as it’s not to be used for developing sht lol",15,21,2025-04-14 04:50:00,0,False,False,False,False,2025-04-14 04:50:00,4,Monday,159.0,857,61.97,9,231,11.0,0,0,NEGATIVE,-0.9988046884536743,"['specific', 'noncode', 'heavy', 'work', 'think', 'one', 'data', 'engineers', 'hates', 'coding', 'developing', 'projects', 'clients', 'far', 'always', 'asked', 'use', 'adb', 'developing', 'notebooks', 'etl', 'use', 'never', 'touched', 'adf', 'sick', 'developing', 'etl', 'stuff', 'using', 'pyspark', 'sparksql', 'stressful', 'interest', 'data', 'engineering', 'right', 'anyone', 'successfully', 'left', 'field', 'noncode', 'role', 'choose', 'appreciate', 'suggestions', 'especially', 'jobs', 'make', 'use', 'lesscoding', 'side', 'data', 'engineering', 'see', 'lots', 'people', 'going', 'software', 'eng', 'love', 'coding', 'data', 'scientist', 'maybe', 'want', 'less', 'techy', 'work', 'right', 'yeah', 'open', 'suggestions', 'also', 'fine', 'sql', 'long', 'used', 'developing', 'sht', 'lol']",specific noncode heavy work think one data engineers hates coding developing projects clients far always asked use adb developing notebooks etl use never touched adf sick developing etl stuff using pyspark sparksql stressful interest data engineering right anyone successfully left field noncode role choose appreciate suggestions especially jobs make use lesscoding side data engineering see lots people going software eng love coding data scientist maybe want less techy work right yeah open suggestions also fine sql long used developing sht lol,High,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
220,What database did they use?,"ChatGPT can now remember all conversations you've had across all chat sessions. Google Gemini, I think, also implemented a similar feature about two months ago with *Personalization*—which provides help based on your search history.  

I’d like to hear from database engineers, database administrators, and other CS/IT professionals (as well as actual humans): What kind of database do you think they use? Relational, non-relational, vector, graph, data warehouse, data lake?  

*P.S. I know I could just do deep research on ChatGPT, Gemini, and Grok—but I want to hear from Redditors.",13,3,2025-04-14 21:51:54,0,False,False,False,False,2025-04-14 21:51:54,21,Monday,89.0,585,36.49,5,156,14.6,0,0,POSITIVE,0.9904264807701111,"['chatgpt', 'remember', 'conversations', 'youve', 'across', 'chat', 'sessions', 'google', 'gemini', 'think', 'also', 'implemented', 'similar', 'feature', 'two', 'months', 'ago', 'personalizationwhich', 'provides', 'help', 'based', 'search', 'history', 'like', 'hear', 'database', 'engineers', 'database', 'administrators', 'csit', 'professionals', 'well', 'actual', 'humans', 'kind', 'database', 'think', 'use', 'relational', 'nonrelational', 'vector', 'graph', 'data', 'warehouse', 'data', 'lake', 'know', 'could', 'deep', 'research', 'chatgpt', 'gemini', 'grokbut', 'want', 'hear', 'redditors']",chatgpt remember conversations youve across chat sessions google gemini think also implemented similar feature two months ago personalizationwhich provides help based search history like hear database engineers database administrators csit professionals well actual humans kind database think use relational nonrelational vector graph data warehouse data lake know could deep research chatgpt gemini grokbut want hear redditors,High,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
221,Event Sourcing as a creative tool for developers,"Hey, I think there are better use cases for event sourcing.  
  
Event sourcing is an architecture where you capture every change in your system as an immutable event, rather than just storing the latest state. Instead of only knowing what your data looks like now, you keep a full history of how it got there. In a simple crud app that would mean that every deleted, updated, and created entry is stored in your event source, that way when you replay your events you can recreate the state that the application was in at any given time.

Most developers see event sourcing as a kind of technical safety net: - Recovering from failures - Rebuilding corrupted read models - Auditability

Surviving schema changes without too much pain

And fair enough, replaying your event stream often feels like a stressful situation. Something broke, you need to fix it, and you’re crossing your fingers hoping everything rebuilds cleanly.

What if replaying your event history wasn’t just for emergencies? What if it was a normal, everyday part of building your system?

Instead of treating replay as a recovery mechanism, you treat it as a development tool — something you use to evolve your data models, improve your logic, and shape new views of your data over time. More excitingly, it means you can derive entirely new schemas from your event history whenever your needs change.

Your database stops being the single source of truth and instead becomes what it was always meant to be: a fast, convenient cache for your data, not the place where all your logic and assumptions are locked in.

With a full event history, you’re free to experiment with new read models, adapt your data structures without fear, and shape your data exactly to fit new purposes — like enriching fields, backfilling values, or building dedicated models for AI consumption. Replay becomes not about fixing what broke, but about continuously improving what you’ve built.

And this has big implications — especially when it comes to AI and MCP Servers.

Most application databases aren’t built for natural language querying or AI-powered insights. Their schemas are designed for transactions, not for understanding. Data is spread across normalized tables, with relationships and assumptions baked deeply into the structure.

But when you treat your event history as the source of truth, you can replay your events into purpose-built read models, specifically structured for AI consumption.

Need flat, denormalized tables for efficient semantic search? Done. Want to create a user-centric view with pre-joined context for better prompts? Easy. You’re no longer limited by your application’s schema — you shape your data to fit exactly how your AI needs to consume it.

And here’s where it gets really interesting: AI itself can help you explore your data history and discover what’s valuable.

Instead of guessing which fields to include, you can use AI to interrogate your raw events, spot gaps, surface patterns, and guide you in designing smarter read models. It’s a feedback loop: your AI doesn’t just query your data — it helps you shape it.

So instead of forcing your AI to wrestle with your transactional tables, you give it clean, dedicated models optimized for discovery, reasoning, and insight.

And the best part? You can keep iterating. As your AI use cases evolve, you simply adjust your flows and replay your events to reshape your models — no migrations, no backfills, no re-engineering.",14,11,2025-04-14 10:15:58,0,False,False,False,False,2025-04-14 10:15:58,10,Monday,570.0,3470,51.18,28,893,12.7,0,0,NEGATIVE,-0.9961470365524292,"['hey', 'think', 'better', 'use', 'cases', 'event', 'sourcing', 'event', 'sourcing', 'architecture', 'capture', 'every', 'change', 'system', 'immutable', 'event', 'rather', 'storing', 'latest', 'state', 'instead', 'knowing', 'data', 'looks', 'like', 'keep', 'full', 'history', 'got', 'simple', 'crud', 'app', 'would', 'mean', 'every', 'deleted', 'updated', 'created', 'entry', 'stored', 'event', 'source', 'way', 'replay', 'events', 'recreate', 'state', 'application', 'given', 'time', 'developers', 'see', 'event', 'sourcing', 'kind', 'technical', 'safety', 'net', 'recovering', 'failures', 'rebuilding', 'corrupted', 'read', 'models', 'auditability', 'surviving', 'schema', 'changes', 'without', 'much', 'pain', 'fair', 'enough', 'replaying', 'event', 'stream', 'often', 'feels', 'like', 'stressful', 'situation', 'something', 'broke', 'need', 'fix', 'youre', 'crossing', 'fingers', 'hoping', 'everything', 'rebuilds', 'cleanly', 'replaying', 'event', 'history', 'wasnt', 'emergencies', 'normal', 'everyday', 'part', 'building', 'system', 'instead', 'treating', 'replay', 'recovery', 'mechanism', 'treat', 'development', 'tool', 'something', 'use', 'evolve', 'data', 'models', 'improve', 'logic', 'shape', 'new', 'views', 'data', 'time', 'excitingly', 'means', 'derive', 'entirely', 'new', 'schemas', 'event', 'history', 'whenever', 'needs', 'change', 'database', 'stops', 'single', 'source', 'truth', 'instead', 'becomes', 'always', 'meant', 'fast', 'convenient', 'cache', 'data', 'place', 'logic', 'assumptions', 'locked', 'full', 'event', 'history', 'youre', 'free', 'experiment', 'new', 'read', 'models', 'adapt', 'data', 'structures', 'without', 'fear', 'shape', 'data', 'exactly', 'fit', 'new', 'purposes', 'like', 'enriching', 'fields', 'backfilling', 'values', 'building', 'dedicated', 'models', 'consumption', 'replay', 'becomes', 'fixing', 'broke', 'continuously', 'improving', 'youve', 'built', 'big', 'implications', 'especially', 'comes', 'mcp', 'servers', 'application', 'databases', 'arent', 'built', 'natural', 'language', 'querying', 'aipowered', 'insights', 'schemas', 'designed', 'transactions', 'understanding', 'data', 'spread', 'across', 'normalized', 'tables', 'relationships', 'assumptions', 'baked', 'deeply', 'structure', 'treat', 'event', 'history', 'source', 'truth', 'replay', 'events', 'purposebuilt', 'read', 'models', 'specifically', 'structured', 'consumption', 'need', 'flat', 'denormalized', 'tables', 'efficient', 'semantic', 'search', 'done', 'want', 'create', 'usercentric', 'view', 'prejoined', 'context', 'better', 'prompts', 'easy', 'youre', 'longer', 'limited', 'applications', 'schema', 'shape', 'data', 'fit', 'exactly', 'needs', 'consume', 'heres', 'gets', 'really', 'interesting', 'help', 'explore', 'data', 'history', 'discover', 'whats', 'valuable', 'instead', 'guessing', 'fields', 'include', 'use', 'interrogate', 'raw', 'events', 'spot', 'gaps', 'surface', 'patterns', 'guide', 'designing', 'smarter', 'read', 'models', 'feedback', 'loop', 'doesnt', 'query', 'data', 'helps', 'shape', 'instead', 'forcing', 'wrestle', 'transactional', 'tables', 'give', 'clean', 'dedicated', 'models', 'optimized', 'discovery', 'reasoning', 'insight', 'best', 'part', 'keep', 'iterating', 'use', 'cases', 'evolve', 'simply', 'adjust', 'flows', 'replay', 'events', 'reshape', 'models', 'migrations', 'backfills', 'reengineering']",hey think better use cases event sourcing event sourcing architecture capture every change system immutable event rather storing latest state instead knowing data looks like keep full history got simple crud app would mean every deleted updated created entry stored event source way replay events recreate state application given time developers see event sourcing kind technical safety net recovering failures rebuilding corrupted read models auditability surviving schema changes without much pain fair enough replaying event stream often feels like stressful situation something broke need fix youre crossing fingers hoping everything rebuilds cleanly replaying event history wasnt emergencies normal everyday part building system instead treating replay recovery mechanism treat development tool something use evolve data models improve logic shape new views data time excitingly means derive entirely new schemas event history whenever needs change database stops single source truth instead becomes always meant fast convenient cache data place logic assumptions locked full event history youre free experiment new read models adapt data structures without fear shape data exactly fit new purposes like enriching fields backfilling values building dedicated models consumption replay becomes fixing broke continuously improving youve built big implications especially comes mcp servers application databases arent built natural language querying aipowered insights schemas designed transactions understanding data spread across normalized tables relationships assumptions baked deeply structure treat event history source truth replay events purposebuilt read models specifically structured consumption need flat denormalized tables efficient semantic search done want create usercentric view prejoined context better prompts easy youre longer limited applications schema shape data fit exactly needs consume heres gets really interesting help explore data history discover whats valuable instead guessing fields include use interrogate raw events spot gaps surface patterns guide designing smarter read models feedback loop doesnt query data helps shape instead forcing wrestle transactional tables give clean dedicated models optimized discovery reasoning insight best part keep iterating use cases evolve simply adjust flows replay events reshape models migrations backfills reengineering,High,0,0,"need, table, event, source, without, instead, every, replay, day, example, single, change, per, business, daily, based, keys, manually, model, keep, needs, events, doesnt, name, list, option, user, must, takes, history, often, truth, full, column, hour, address, requirements, date, reading, track, whenever, metrics, result, update, feature, primary, columns, meaning, pricing, structure, details, rows, simply, available, dedicated, country, means, facebook, error, missing, evolve, involves, names, size, domain, place, addresses, timestamp, parse, mean, slow, schemas, heres, return, dimensions, delete, state, times, arent, ground, errors, receive, min, added, geocoding, shape, sourcing, wasnt, insert, minutes, value, flow, save, normal, little, region, usually, bad, becomes, failure, replication, entirely, products, infrastructure, minute, person, average, servers, internet, looked, probably, campaigns, workloads, looks",8.2,7.4,175.5,-0.663,1069.0,47.9,9.8,275.5,11.4,22.2,15.5,52,108,34,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",4,4
222,ETL for Ingesting S3 files and converting to Iceberg,"So, I'm currently working on a project (my first) to create a scalable data platform for a company. The whole thing structured around AWS, initially using DMS to migrate PostgreSQL data to S3 in parquet format (this is our raw datalake). Then using Glue jobs to read this data and create Iceberg tables which would be used in Athena queries and Quicksight. I've got a working Glue script for reading this data and perform upsert operations. Okay so now that I've given a bit of context of what I'm trying to do, let me tell you my problem.  
The client wants me to schedule this job to run every 15min or so for staging and most probably every hour for production. The data in the raw datalake is partitioned by date (for example: s3bucket/table\_name/2025/04/10/file.parquet). Now that I have to run this job every 15 min or so I'm not sure how to keep track of the files that have been processed and which haven't. Currently my script finds the current time and modifies the read command to use just the folder for the current date. But still, this means that I'll be reading all the files in the folder (processed already or not) every time the job runs during the day.   
I've looked around and found that using DynamoDB for keeping track of the files would be my best option but also found something related to Iceberg metadata files that could help me with this. I'm leaning towards the Iceberg option as I wanna make use of all its features but have too little information regarding this to implement. would absolutely appreciate it if someone could help me out with this.  
Has anyone worked with Iceberg in this matter? and if the iceberg solution isn't usable, could someone help me out with how to implement the DynamoDB way.",10,7,2025-04-14 12:15:20,0,False,False,False,False,2025-04-14 12:15:20,12,Monday,308.0,1736,67.59,15,443,11.1,0,1,NEGATIVE,-0.9969508647918701,"['currently', 'working', 'project', 'first', 'create', 'scalable', 'data', 'platform', 'company', 'whole', 'thing', 'structured', 'around', 'aws', 'initially', 'using', 'dms', 'migrate', 'postgresql', 'data', 'parquet', 'format', 'raw', 'datalake', 'using', 'glue', 'jobs', 'read', 'data', 'create', 'iceberg', 'tables', 'would', 'used', 'athena', 'queries', 'quicksight', 'ive', 'got', 'working', 'glue', 'script', 'reading', 'data', 'perform', 'upsert', 'operations', 'okay', 'ive', 'given', 'bit', 'context', 'trying', 'let', 'tell', 'problem', 'client', 'wants', 'schedule', 'job', 'run', 'every', 'min', 'staging', 'probably', 'every', 'hour', 'production', 'data', 'raw', 'datalake', 'partitioned', 'date', 'example', 'sbuckettablenamefileparquet', 'run', 'job', 'every', 'min', 'sure', 'keep', 'track', 'files', 'processed', 'havent', 'currently', 'script', 'finds', 'current', 'time', 'modifies', 'read', 'command', 'use', 'folder', 'current', 'date', 'still', 'means', 'ill', 'reading', 'files', 'folder', 'processed', 'already', 'every', 'time', 'job', 'runs', 'day', 'ive', 'looked', 'around', 'found', 'using', 'dynamodb', 'keeping', 'track', 'files', 'would', 'best', 'option', 'also', 'found', 'something', 'related', 'iceberg', 'metadata', 'files', 'could', 'help', 'leaning', 'towards', 'iceberg', 'option', 'wanna', 'make', 'use', 'features', 'little', 'information', 'regarding', 'implement', 'would', 'absolutely', 'appreciate', 'someone', 'could', 'help', 'anyone', 'worked', 'iceberg', 'matter', 'iceberg', 'solution', 'isnt', 'usable', 'could', 'someone', 'help', 'implement', 'dynamodb', 'way']",currently working project first create scalable data platform company whole thing structured around aws initially using dms migrate postgresql data parquet format raw datalake using glue jobs read data create iceberg tables would used athena queries quicksight ive got working glue script reading data perform upsert operations okay ive given bit context trying let tell problem client wants schedule job run every min staging probably every hour production data raw datalake partitioned date example sbuckettablenamefileparquet run job every min sure keep track files processed havent currently script finds current time modifies read command use folder current date still means ill reading files folder processed already every time job runs day ive looked around found using dynamodb keeping track files would best option also found something related iceberg metadata files could help leaning towards iceberg option wanna make use features little information regarding implement would absolutely appreciate someone could help anyone worked iceberg matter iceberg solution isnt usable could someone help implement dynamodb way,High,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
223,Need Advice on solution - Mapping Inconsistent Country Names to Standardized Values,"Hi Folks,

In my current project, we are ingesting a wide variety of external public datasets. One common issue we’re facing is that the **country names in these datasets are not standardized**. For example, we may encounter entries like **""Burma"" instead of ""Myanmar""**, or **""Islamic Republic of Iran"" instead of ""Iran""**.

My initial approach was to extract all unique country name variations and map them to a list of standard country names using logic such as CASE WHEN conditions or basic string-matching techniques.

However, my manager has suggested we leverage **AI/LLM-based models** to automate the mapping of these country names to a standardized list to handle new query points as well. 

I have a couple of concerns and would appreciate your thoughts:

1. **Is using AI/LLMs a suitable approach for this problem?**
2. **Can LLMs be fully reliable in these mappings, or is there a risk of incorrect matches?**
3. I was considering implementing a **feedback pipeline** that highlights any newly encountered or unmapped country names during data ingestion so we can review and incorporate logic to handle them in the code over time. Would this be a better or complementary solution?
4. Please suggest if there is some better approach.

Looking forward to your insights!",8,9,2025-04-14 08:02:29,0,False,False,False,False,2025-04-14 08:02:29,8,Monday,206.0,1280,54.02,12,334,12.3,0,0,NEGATIVE,-0.9988371729850769,"['folks', 'current', 'project', 'ingesting', 'wide', 'variety', 'external', 'public', 'datasets', 'one', 'common', 'issue', 'facing', 'country', 'names', 'datasets', 'standardized', 'example', 'may', 'encounter', 'entries', 'like', 'burma', 'instead', 'myanmar', 'islamic', 'republic', 'iran', 'instead', 'iran', 'initial', 'approach', 'extract', 'unique', 'country', 'name', 'variations', 'map', 'list', 'standard', 'country', 'names', 'using', 'logic', 'case', 'conditions', 'basic', 'stringmatching', 'techniques', 'however', 'manager', 'suggested', 'leverage', 'aillmbased', 'models', 'automate', 'mapping', 'country', 'names', 'standardized', 'list', 'handle', 'new', 'query', 'points', 'well', 'couple', 'concerns', 'would', 'appreciate', 'thoughts', 'using', 'aillms', 'suitable', 'approach', 'problem', 'llms', 'fully', 'reliable', 'mappings', 'risk', 'incorrect', 'matches', 'considering', 'implementing', 'feedback', 'pipeline', 'highlights', 'newly', 'encountered', 'unmapped', 'country', 'names', 'data', 'ingestion', 'review', 'incorporate', 'logic', 'handle', 'code', 'time', 'would', 'better', 'complementary', 'solution', 'please', 'suggest', 'better', 'approach', 'looking', 'forward', 'insights']",folks current project ingesting wide variety external public datasets one common issue facing country names datasets standardized example may encounter entries like burma instead myanmar islamic republic iran instead iran initial approach extract unique country name variations map list standard country names using logic case conditions basic stringmatching techniques however manager suggested leverage aillmbased models automate mapping country names standardized list handle new query points well couple concerns would appreciate thoughts using aillms suitable approach problem llms fully reliable mappings risk incorrect matches considering implementing feedback pipeline highlights newly encountered unmapped country names data ingestion review incorporate logic handle code time would better complementary solution please suggest better approach looking forward insights,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
224,Has anyone used Cube.js for operational (non-BI) use cases?,"The semantic layer in Cube looks super useful — defining metrics, dimensions, and joins in one place is a dream. But most use cases I’ve seen are focused on BI dashboards and analytics.

I’m wondering if anyone here has used Cube for more *operational* or *app-level* read scenarios — like powering parts of an internal tool, or building a unified read API across microservices (via Cube's GraphQL support). All read-only, but not just charts — more like structured data fetching.

Any war stories, performance considerations, or architectural tips? Curious if it holds up well when the use case isn't classic OLAP.

Thanks!",7,0,2025-04-14 14:59:21,0,False,False,False,False,2025-04-14 14:59:21,14,Monday,102.0,624,54.73,6,163,13.3,0,0,NEGATIVE,-0.9958095550537109,"['semantic', 'layer', 'cube', 'looks', 'super', 'useful', 'defining', 'metrics', 'dimensions', 'joins', 'one', 'place', 'dream', 'use', 'cases', 'ive', 'seen', 'focused', 'dashboards', 'analytics', 'wondering', 'anyone', 'used', 'cube', 'operational', 'applevel', 'read', 'scenarios', 'like', 'powering', 'parts', 'internal', 'tool', 'building', 'unified', 'read', 'api', 'across', 'microservices', 'via', 'cubes', 'graphql', 'support', 'readonly', 'charts', 'like', 'structured', 'data', 'fetching', 'war', 'stories', 'performance', 'considerations', 'architectural', 'tips', 'curious', 'holds', 'well', 'use', 'case', 'isnt', 'classic', 'olap', 'thanks']",semantic layer cube looks super useful defining metrics dimensions joins one place dream use cases ive seen focused dashboards analytics wondering anyone used cube operational applevel read scenarios like powering parts internal tool building unified read api across microservices via cubes graphql support readonly charts like structured data fetching war stories performance considerations architectural tips curious holds well use case isnt classic olap thanks,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
225,How do I document existing Pipelines?,There is lot of pipelines working in our Azure Data Factory. There is json files available for those. I am new in the team and there not very well details about those pipelines. And my boss wants me to create something which will describe how pipelines working. And looking for how do i Document those so for future anyone new in our team can understand what have done. ,3,7,2025-04-14 15:00:15,0,False,False,False,False,2025-04-14 15:00:15,15,Monday,68.0,370,74.59,5,96,8.8,0,0,NEGATIVE,-0.9973899722099304,"['lot', 'pipelines', 'working', 'azure', 'data', 'factory', 'json', 'files', 'available', 'new', 'team', 'well', 'details', 'pipelines', 'boss', 'wants', 'create', 'something', 'describe', 'pipelines', 'working', 'looking', 'document', 'future', 'anyone', 'new', 'team', 'understand', 'done']",lot pipelines working azure data factory json files available new team well details pipelines boss wants create something describe pipelines working looking document future anyone new team understand done,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
226,Advice on data warehouse design for ERP Integration with Power BI,"Hi everyone!

I’d like to ask for your advice on designing a relational data warehouse fed from our ERP system. We plan to use Power BI as our reporting tool, and all departments in the company will rely on it for analytics.

The challenge is that teams from different departments expect **the data to be fully related and ready** to use when building dashboards, minimizing the need for additional modeling. We’re struggling to determine the best approach to meet these expectations.

What would you recommend?

Should all dimensions and facts be pre-related in the data warehouse, even if it adds complexity?

Creating separate data models in Power BI for different departments/use cases?

Handling all relationships in the data warehouse and exposing them via curated datasets?

Should we empower Power BI users to create their own data models, or enforce strict governance with documented relationships?

Thanks in advance for your insights! ",5,2,2025-04-14 07:55:22,1,False,False,False,False,2025-04-14 07:55:22,7,Monday,150.0,946,47.79,10,259,13.3,0,0,NEGATIVE,-0.9841214418411255,"['everyone', 'like', 'ask', 'advice', 'designing', 'relational', 'data', 'warehouse', 'fed', 'erp', 'system', 'plan', 'use', 'power', 'reporting', 'tool', 'departments', 'company', 'rely', 'analytics', 'challenge', 'teams', 'different', 'departments', 'expect', 'data', 'fully', 'related', 'ready', 'use', 'building', 'dashboards', 'minimizing', 'need', 'additional', 'modeling', 'struggling', 'determine', 'best', 'approach', 'meet', 'expectations', 'would', 'recommend', 'dimensions', 'facts', 'prerelated', 'data', 'warehouse', 'even', 'adds', 'complexity', 'creating', 'separate', 'data', 'models', 'power', 'different', 'departmentsuse', 'cases', 'handling', 'relationships', 'data', 'warehouse', 'exposing', 'via', 'curated', 'datasets', 'empower', 'power', 'users', 'create', 'data', 'models', 'enforce', 'strict', 'governance', 'documented', 'relationships', 'thanks', 'advance', 'insights']",everyone like ask advice designing relational data warehouse fed erp system plan use power reporting tool departments company rely analytics challenge teams different departments expect data fully related ready use building dashboards minimizing need additional modeling struggling determine best approach meet expectations would recommend dimensions facts prerelated data warehouse even adds complexity creating separate data models power different departmentsuse cases handling relationships data warehouse exposing via curated datasets empower power users create data models enforce strict governance documented relationships thanks advance insights,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
227,Databricks Pain Points?,"Hi everyone,

My team is working on some tooling to build some user friendly ways to do things in Databricks. Our initial focus is around entity resolution, creating a simple tool that can evaluate the data in unity catalog and deduplicate tables, create identity graphs, etc.

I'm trying to get some insights from people who use Databricks day-to-day to figure out what other kinds of capabilities we'd want this thing to have if we want users to try it out. 

Some examples I have gotten from other venues so far:

* Cost optimization
* Annotating or using advanced features of Unity Catalog can't be done from the UI and users would like being able to do it without having to write a bunch of SQL
* Figuring out which libraries to use in notebooks for a specific use case

This is just an open call for input here. If you use Databricks all the time, what kind of stuff annoys you about it or is confusing?

For the record, this tool are building will be open source and this isn't an ad. The eventual tool will be free to use, I am just looking for broader input into how to make it as useful as possible.

Thanks!",2,3,2025-04-14 18:37:22,0,False,False,False,False,2025-04-14 18:37:22,18,Monday,207.0,1118,50.4,7,307,13.9,0,0,NEGATIVE,-0.9878329038619995,"['everyone', 'team', 'working', 'tooling', 'build', 'user', 'friendly', 'ways', 'things', 'databricks', 'initial', 'focus', 'around', 'entity', 'resolution', 'creating', 'simple', 'tool', 'evaluate', 'data', 'unity', 'catalog', 'deduplicate', 'tables', 'create', 'identity', 'graphs', 'etc', 'trying', 'get', 'insights', 'people', 'use', 'databricks', 'daytoday', 'figure', 'kinds', 'capabilities', 'wed', 'want', 'thing', 'want', 'users', 'try', 'examples', 'gotten', 'venues', 'far', 'cost', 'optimization', 'annotating', 'using', 'advanced', 'features', 'unity', 'catalog', 'cant', 'done', 'users', 'would', 'like', 'able', 'without', 'write', 'bunch', 'sql', 'figuring', 'libraries', 'use', 'notebooks', 'specific', 'use', 'case', 'open', 'call', 'input', 'use', 'databricks', 'time', 'kind', 'stuff', 'annoys', 'confusing', 'record', 'tool', 'building', 'open', 'source', 'isnt', 'eventual', 'tool', 'free', 'use', 'looking', 'broader', 'input', 'make', 'useful', 'possible', 'thanks']",everyone team working tooling build user friendly ways things databricks initial focus around entity resolution creating simple tool evaluate data unity catalog deduplicate tables create identity graphs etc trying get insights people use databricks daytoday figure kinds capabilities wed want thing want users try examples gotten venues far cost optimization annotating using advanced features unity catalog cant done users would like able without write bunch sql figuring libraries use notebooks specific use case open call input use databricks time kind stuff annoys confusing record tool building open source isnt eventual tool free use looking broader input make useful possible thanks,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
228,How do managed services work with vendors like ClickHouse?,"  
**Context:**  
New to data engineering. New to the cloud too. I am in charge of doing trade studies on various storage solutions for my new company. I'm gathering requirements for the system, then pricing out options that meet those requirements. At the end of all my research, I have to present my trade studies so leadership can decide how to spend their cash.  

**Question:**  
I am seeing a lot of companies that do ""managed services"" that are not native to a cloud provider like AWS. For example, I see that ClickHouse offers managed services that piggy back off of AWS or other cloud providers. 

Do they have an AWS account that they provision with their software on ec2 instances (or something), and then they give you access to it? Or do they act as consultants who come in and install ClickHouse on your own AWS account? 







",2,13,2025-04-14 20:34:56,0,False,False,False,False,2025-04-14 20:34:56,20,Monday,146.0,843,71.95,9,209,11.0,0,0,NEGATIVE,-0.9860650300979614,"['context', 'new', 'data', 'engineering', 'new', 'cloud', 'charge', 'trade', 'studies', 'various', 'storage', 'solutions', 'new', 'company', 'gathering', 'requirements', 'system', 'pricing', 'options', 'meet', 'requirements', 'end', 'research', 'present', 'trade', 'studies', 'leadership', 'decide', 'spend', 'cash', 'question', 'seeing', 'lot', 'companies', 'managed', 'services', 'native', 'cloud', 'provider', 'like', 'aws', 'example', 'see', 'clickhouse', 'offers', 'managed', 'services', 'piggy', 'back', 'aws', 'cloud', 'providers', 'aws', 'account', 'provision', 'software', 'instances', 'something', 'give', 'access', 'act', 'consultants', 'come', 'install', 'clickhouse', 'aws', 'account']",context new data engineering new cloud charge trade studies various storage solutions new company gathering requirements system pricing options meet requirements end research present trade studies leadership decide spend cash question seeing lot companies managed services native cloud provider like aws example see clickhouse offers managed services piggy back aws cloud providers aws account provision software instances something give access act consultants come install clickhouse aws account,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
229,"If you've been curious about what a feature store is and if you actually need one, this post might help","I've worked as both a data and ML engineer and feature stores tend to be an interesting subject. I think they're often misunderstood and quite frankly, not needed for many companies. I wanted to write the blog post to solidify my thoughts and thought it might be helpful for others here.",2,0,2025-04-14 18:14:31,0,False,False,False,False,2025-04-14 18:14:31,18,Monday,51.0,287,71.14,3,73,10.5,0,0,POSITIVE,0.9916974306106567,"['ive', 'worked', 'data', 'engineer', 'feature', 'stores', 'tend', 'interesting', 'subject', 'think', 'theyre', 'often', 'misunderstood', 'quite', 'frankly', 'needed', 'many', 'companies', 'wanted', 'write', 'blog', 'post', 'solidify', 'thoughts', 'thought', 'might', 'helpful', 'others']",ive worked data engineer feature stores tend interesting subject think theyre often misunderstood quite frankly needed many companies wanted write blog post solidify thoughts thought might helpful others,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
230,Databricks geographic coding on the cheap?,"We're migrating a bunch of geography data from local SQL Server to Azure Databricks.  Locally, we use ArcGIS to match latitude/longitude to city,state locations, and pay a fixed cost for the subscription.  We're looking for a way to do the same work on Databricks, but are having a tough time finding a cost effective ""all-you-can-eat"" way to do it.  We can't just install ArcGIS there to use or current sub.

Any ideas how to best do this geocoding work on Databricks, without breaking the bank?",2,1,2025-04-14 14:33:42,0,False,False,False,False,2025-04-14 14:33:42,14,Monday,85.0,496,62.68,5,126,11.2,0,0,NEGATIVE,-0.9994930028915405,"['migrating', 'bunch', 'geography', 'data', 'local', 'sql', 'server', 'azure', 'databricks', 'locally', 'use', 'arcgis', 'match', 'latitudelongitude', 'citystate', 'locations', 'pay', 'fixed', 'cost', 'subscription', 'looking', 'way', 'work', 'databricks', 'tough', 'time', 'finding', 'cost', 'effective', 'allyoucaneat', 'way', 'cant', 'install', 'arcgis', 'use', 'current', 'sub', 'ideas', 'best', 'geocoding', 'work', 'databricks', 'without', 'breaking', 'bank']",migrating bunch geography data local sql server azure databricks locally use arcgis match latitudelongitude citystate locations pay fixed cost subscription looking way work databricks tough time finding cost effective allyoucaneat way cant install arcgis use current sub ideas best geocoding work databricks without breaking bank,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
231,dbt sqlmesh migration,Has anyone migrated their dbt cloud to sqlmesh? If so what tools did you use? How many models? How much time did take? Biggest pain points?,2,3,2025-04-14 08:38:03,0,False,False,False,False,2025-04-14 08:38:03,8,Monday,26.0,139,91.58,5,33,6.7,0,0,NEGATIVE,-0.9994675517082214,"['anyone', 'migrated', 'dbt', 'cloud', 'sqlmesh', 'tools', 'use', 'many', 'models', 'much', 'time', 'take', 'biggest', 'pain', 'points']",anyone migrated dbt cloud sqlmesh tools use many models much time take biggest pain points,Mid,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
232,Help with possible skill expansion or clarification on current role,"So after about 25 years of experience in what was considered DBA, I am now unemployed due to the federal job cuts and it seems DBA just isn't a role anymore. I am currently working on getting a cloud certification but the rest of my skills seem to be mixed and I am hoping someone has a more specific role I would fit into. I am also hoping to expand my skills into some newer technology but I have no clue where to even start. 

Current skills are:

Expert level SQL

Some knowledge of Azure and AWS

Python, PowerShell, GIT, .NET, C#, Idera, Vcentre, Oracle, BI, and ETL with some other minor things mixed in. 

Where should I go from here? What role could this be considered? What other skills could I gain some knowledge on?",2,1,2025-04-14 06:08:14,0,False,False,False,False,2025-04-14 06:08:14,6,Monday,136.0,728,71.14,8,191,10.4,0,0,NEGATIVE,-0.999589741230011,"['years', 'experience', 'considered', 'dba', 'unemployed', 'due', 'federal', 'job', 'cuts', 'seems', 'dba', 'isnt', 'role', 'anymore', 'currently', 'working', 'getting', 'cloud', 'certification', 'rest', 'skills', 'seem', 'mixed', 'hoping', 'someone', 'specific', 'role', 'would', 'fit', 'also', 'hoping', 'expand', 'skills', 'newer', 'technology', 'clue', 'even', 'start', 'current', 'skills', 'expert', 'level', 'sql', 'knowledge', 'azure', 'aws', 'python', 'powershell', 'git', 'net', 'idera', 'vcentre', 'oracle', 'etl', 'minor', 'things', 'mixed', 'role', 'could', 'considered', 'skills', 'could', 'gain', 'knowledge']",years experience considered dba unemployed due federal job cuts seems dba isnt role anymore currently working getting cloud certification rest skills seem mixed hoping someone specific role would fit also hoping expand skills newer technology clue even start current skills expert level sql knowledge azure aws python powershell git net idera vcentre oracle etl minor things mixed role could considered skills could gain knowledge,Mid,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
233,Files to be processed in sequence on S3 bucket.,"What is the best possible solution to process the files in an S3 bucket in a sequential order. 

Use case is that an external systems generates CSV files and dump them on to S3 buckets. These CSV files consists of data from few  Oracle tables. These files needs to be processed in a sequential order in order to maintain the referential integrity of the data while loading into the Postgres RDS. If the files are not processed in an order, the load errors out with the reference data doesn't exist. What is a best solution to process the files on a S3 bucket in an order? ",1,3,2025-04-14 16:47:37,0,False,False,False,False,2025-04-14 16:47:37,16,Monday,106.0,572,61.97,6,159,11.5,0,0,NEGATIVE,-0.9970746040344238,"['best', 'possible', 'solution', 'process', 'files', 'bucket', 'sequential', 'order', 'use', 'case', 'external', 'systems', 'generates', 'csv', 'files', 'dump', 'buckets', 'csv', 'files', 'consists', 'data', 'oracle', 'tables', 'files', 'needs', 'processed', 'sequential', 'order', 'order', 'maintain', 'referential', 'integrity', 'data', 'loading', 'postgres', 'rds', 'files', 'processed', 'order', 'load', 'errors', 'reference', 'data', 'doesnt', 'exist', 'best', 'solution', 'process', 'files', 'bucket', 'order']",best possible solution process files bucket sequential order use case external systems generates csv files dump buckets csv files consists data oracle tables files needs processed sequential order order maintain referential integrity data loading postgres rds files processed order load errors reference data doesnt exist best solution process files bucket order,Mid,4,4,"files, api, tables, query, models, read, google, json, file, logic, write, case, csv, store, datasets, across, sources, metadata, created, schema, systems, large, platform, custom, script, changes, dataset, row, reporting, parquet, issues, ads, load, fields, analysis, cases, product, values, design, view, idea, layer, search, import, suggest, language, excel, app, blob, form, request, params, postgres, ingestion, raw, mysql, fact, processed, structures, either, separate, structured, connection, core, method, downstream, scripts, extract, maintain, send, hub, records, transactional, overhead, lineage, environment, object, external, pull, iot, order, typically, grid, hubs, validation, requires, mapping, batch, mechanism, location, updates, keeping, checkout, operations, runs, convert, optimized, avoid, matching, views, formats, matters, vpc, bring, reduce, migrate, consists, starts, reads, refresh, relationships, bronze, historical, tracking, developers, natural",8.0,7.1,169.4,-0.679,1041.7,44.5,9.5,267.7,10.7,24.1,17.9,49,102,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,7
234,NoSQL Database for Ticketing System,"We're working on a uni project where we need to design the database for an Ticketing system that will support around 7,000 users. Under normal circumstances, I'd definitely go with a relational database. But we're *required* to use multiple **NoSQL** databases instead. Any suggestions for NoSQL Databases?",0,4,2025-04-14 17:21:23,0,False,False,False,False,2025-04-14 17:21:23,17,Monday,47.0,306,42.58,4,86,12.6,0,0,NEGATIVE,-0.9982751607894897,"['working', 'uni', 'project', 'need', 'design', 'database', 'ticketing', 'system', 'support', 'around', 'users', 'normal', 'circumstances', 'definitely', 'relational', 'database', 'required', 'use', 'multiple', 'nosql', 'databases', 'instead', 'suggestions', 'nosql', 'databases']",working uni project need design database ticketing system support around users normal circumstances definitely relational database required use multiple nosql databases instead suggestions nosql databases,Low,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
235,Any success story from Microsoft Feature Stores?,"The idea is great: build once and use everywhere. But for MS Feature Store, it requires a single flat file as source for any given feature set. 

That means if I need multiple data sources, I need write code to connect to the various data sources, merge them, flatten them into a single file -- all of them done outside of Feature Stores.

For me, it creates inefficiency as the raw flattened file is created solely for the purpose of transformation within feature store. 

Plus when there is a mismatch in granularity or non-overlapping domain, I have to create different flattened files for different feature sets. That seems to be more hassles than whatever merit it may bring.

  
I would love to hear from your success stories before I put in more effort. 

",0,1,2025-04-14 16:31:25,0,False,False,False,False,2025-04-14 16:31:25,16,Monday,133.0,763,60.75,7,196,11.2,0,0,NEGATIVE,-0.9988011121749878,"['idea', 'great', 'build', 'use', 'everywhere', 'feature', 'store', 'requires', 'single', 'flat', 'file', 'source', 'given', 'feature', 'set', 'means', 'need', 'multiple', 'data', 'sources', 'need', 'write', 'code', 'connect', 'various', 'data', 'sources', 'merge', 'flatten', 'single', 'file', 'done', 'outside', 'feature', 'stores', 'creates', 'inefficiency', 'raw', 'flattened', 'file', 'created', 'solely', 'purpose', 'transformation', 'within', 'feature', 'store', 'plus', 'mismatch', 'granularity', 'nonoverlapping', 'domain', 'create', 'different', 'flattened', 'files', 'different', 'feature', 'sets', 'seems', 'hassles', 'whatever', 'merit', 'may', 'bring', 'would', 'love', 'hear', 'success', 'stories', 'put', 'effort']",idea great build use everywhere feature store requires single flat file source given feature set means need multiple data sources need write code connect various data sources merge flatten single file done outside feature stores creates inefficiency raw flattened file created solely purpose transformation within feature store plus mismatch granularity nonoverlapping domain create different flattened files different feature sets seems hassles whatever merit may bring would love hear success stories put effort,Low,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
236,Recommendations for a new grad,"Hello all, I am looking for some advice on the reason of data engineering/data science (yes I know they are different). I will be graduating in May with a degree in Physics. During my time in school, I have spent considerable time doing independent study for Python, MATLAB, Java, and SQL. Due to financial constraints I am not able to pay for a certification course for these languages but I have taken free exams to get some sort of certificate that says I know what I'm talking about. I have grown to not really want to work in a lab setting, but rather a role working with numbers and data points in the abstract. So I'm looking for a role in analyzing data or creating infrastructure for data management. Do you all have any advice for a new head trying to break into the industry? Anything would be greatly appreciated.",0,6,2025-04-14 14:21:26,0,False,2025-04-14 14:31:31,False,False,2025-04-14 14:21:26,14,Monday,150.0,825,52.39,8,233,11.5,0,0,NEGATIVE,-0.9983668923377991,"['hello', 'looking', 'advice', 'reason', 'data', 'engineeringdata', 'science', 'yes', 'know', 'different', 'graduating', 'may', 'degree', 'physics', 'time', 'school', 'spent', 'considerable', 'time', 'independent', 'study', 'python', 'matlab', 'java', 'sql', 'due', 'financial', 'constraints', 'able', 'pay', 'certification', 'course', 'languages', 'taken', 'free', 'exams', 'get', 'sort', 'certificate', 'says', 'know', 'talking', 'grown', 'really', 'want', 'work', 'lab', 'setting', 'rather', 'role', 'working', 'numbers', 'data', 'points', 'abstract', 'looking', 'role', 'analyzing', 'data', 'creating', 'infrastructure', 'data', 'management', 'advice', 'new', 'head', 'trying', 'break', 'industry', 'anything', 'would', 'greatly', 'appreciated']",hello looking advice reason data engineeringdata science yes know different graduating may degree physics time school spent considerable time independent study python matlab java sql due financial constraints able pay certification course languages taken free exams get sort certificate says know talking grown really want work lab setting rather role working numbers data points abstract looking role analyzing data creating infrastructure data management advice new head trying break industry anything would greatly appreciated,Low,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
237,How do you improve Data Quality?,I always get different answer from different people on this.,0,17,2025-04-14 12:05:56,0,False,False,False,False,2025-04-14 12:05:56,12,Monday,10.0,60,52.87,1,17,0.0,0,0,POSITIVE,0.9900774955749512,"['always', 'get', 'different', 'answer', 'different', 'people']",always get different answer different people,Low,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
238,Fact Tables: The Backbone of Your Data Warehouse,"Check out the new blog about Fact Tables   
[https://medium.com/@adityasharmah27/fact-tables-the-backbone-of-your-data-warehouse-9a3014cc20c3](https://medium.com/@adityasharmah27/fact-tables-the-backbone-of-your-data-warehouse-9a3014cc20c3)",0,0,2025-04-14 10:54:13,0,False,False,False,False,2025-04-14 10:54:13,10,Monday,9.0,240,-242.23,1,47,0.0,1,0,NEGATIVE,-0.9507795572280884,"['check', 'new', 'blog', 'fact', 'tables', 'httpsmediumcomadityasharmahfacttablesthebackboneofyourdatawarehouseaccchttpsmediumcomadityasharmahfacttablesthebackboneofyourdatawarehouseaccc']",check new blog fact tables httpsmediumcomadityasharmahfacttablesthebackboneofyourdatawarehouseaccchttpsmediumcomadityasharmahfacttablesthebackboneofyourdatawarehouseaccc,Low,4,4,"files, api, tables, query, models, read, google, json, file, logic, write, case, csv, store, datasets, across, sources, metadata, created, schema, systems, large, platform, custom, script, changes, dataset, row, reporting, parquet, issues, ads, load, fields, analysis, cases, product, values, design, view, idea, layer, search, import, suggest, language, excel, app, blob, form, request, params, postgres, ingestion, raw, mysql, fact, processed, structures, either, separate, structured, connection, core, method, downstream, scripts, extract, maintain, send, hub, records, transactional, overhead, lineage, environment, object, external, pull, iot, order, typically, grid, hubs, validation, requires, mapping, batch, mechanism, location, updates, keeping, checkout, operations, runs, convert, optimized, avoid, matching, views, formats, matters, vpc, bring, reduce, migrate, consists, starts, reads, refresh, relationships, bronze, historical, tracking, developers, natural",8.0,7.1,169.4,-0.679,1041.7,44.5,9.5,267.7,10.7,24.1,17.9,49,102,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,7
239,Khatabook (YC S18) replaced Mixpanel and cut its analytics cost by 90%,"Khatabook, a leading Indian fintech company (YC 18), replaced Mixpanel with Mitzu and Segment with RudderStack to manage its massive scale of over 4 billion monthly events, achieving a 90% reduction in both data ingestion and analytics costs. By adopting a warehouse-native architecture centered on Snowflake, Khatabook enabled real-time, self-service analytics across teams while maintaining 100% data accuracy.",0,1,2025-04-14 10:03:50,0,False,False,False,False,2025-04-14 10:03:50,10,Monday,58.0,412,8.2,2,115,0.0,0,0,POSITIVE,0.9930111765861511,"['khatabook', 'leading', 'indian', 'fintech', 'company', 'replaced', 'mixpanel', 'mitzu', 'segment', 'rudderstack', 'manage', 'massive', 'scale', 'billion', 'monthly', 'events', 'achieving', 'reduction', 'data', 'ingestion', 'analytics', 'costs', 'adopting', 'warehousenative', 'architecture', 'centered', 'snowflake', 'khatabook', 'enabled', 'realtime', 'selfservice', 'analytics', 'across', 'teams', 'maintaining', 'data', 'accuracy']",khatabook leading indian fintech company replaced mixpanel mitzu segment rudderstack manage massive scale billion monthly events achieving reduction data ingestion analytics costs adopting warehousenative architecture centered snowflake khatabook enabled realtime selfservice analytics across teams maintaining data accuracy,Low,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
240,One of the best Fivetran alternative,"If you're urgently looking for a Fivetran alternative, this might help

Been seeing a lot of people here caught off guard by the new Fivetran pricing. If you're in eCommerce and relying on platforms like Shopify, Amazon, TikTok, or Walmart, the shift to MAR-based billing makes things really hard to predict and for a lot of teams, hard to justify.

If you’re in that boat and actively looking for alternatives, this might be helpful.

**Daton**, built by Saras Analytics, is an ETL tool specifically created for eCommerce. That focus has made a big difference for a lot of teams we’ve worked with recently who needed something that aligns better with how eComm brands operate and grow.

Here are a few reasons teams are choosing it when moving off Fivetran:

**Flat, predictable pricing**  
There’s no MAR billing. You’re not getting charged more just because your campaigns performed well or your syncs ran more often. Pricing is clear and stable, which helps a lot for brands trying to manage budgets while scaling.

**Retail-first coverage**  
Daton supports all the platforms most eComm teams rely on. Amazon, Walmart, Shopify, TikTok, Klaviyo and more are covered with production-grade connectors and logic that understands how retail data actually works.

**Built-in reporting**  
Along with pipelines, Daton includes Pulse, a reporting layer with dashboards and pre-modeled metrics like CAC, LTV, ROAS, and SKU performance. This means you can skip the BI setup phase and get straight to insights.

**Custom connectors without custom pricing**  
If you use a platform that’s not already integrated, the team will build it for you. No surprise fees. They also take care of API updates so your pipelines keep running without extra effort.

**Support that’s actually helpful**  
You’re not stuck waiting in a ticket queue. Teams get hands-on onboarding and responsive support, which is a big deal when you’re trying to migrate pipelines quickly and with minimal friction.

Most eComm brands start with a stack of tools. Shopify for the storefront, a few ad platforms, email, CRM, and so on. Over time, that stack evolves. You might switch CRMs, change ad platforms, or add new tools. But Shopify stays. It grows with you. Daton is designed with the same mindset. You shouldn't have to rethink your data infrastructure every time your business changes. It’s built to scale with your brand.

If you're currently evaluating options or trying to avoid a painful renewal, Daton might be worth looking into. I work with the Saras team and happy to help , here's the link if you want to checkout [https://www.sarasanalytics.com/saras-daton](https://www.sarasanalytics.com/saras-daton)

Hope this helps !",0,7,2025-04-14 10:07:58,0,False,False,False,False,2025-04-14 10:07:58,10,Monday,428.0,2700,65.01,29,646,9.8,1,0,NEGATIVE,-0.9990242719650269,"['youre', 'urgently', 'looking', 'fivetran', 'alternative', 'might', 'help', 'seeing', 'lot', 'people', 'caught', 'guard', 'new', 'fivetran', 'pricing', 'youre', 'ecommerce', 'relying', 'platforms', 'like', 'shopify', 'amazon', 'tiktok', 'walmart', 'shift', 'marbased', 'billing', 'makes', 'things', 'really', 'hard', 'predict', 'lot', 'teams', 'hard', 'justify', 'youre', 'boat', 'actively', 'looking', 'alternatives', 'might', 'helpful', 'daton', 'built', 'saras', 'analytics', 'etl', 'tool', 'specifically', 'created', 'ecommerce', 'focus', 'made', 'big', 'difference', 'lot', 'teams', 'weve', 'worked', 'recently', 'needed', 'something', 'aligns', 'better', 'ecomm', 'brands', 'operate', 'grow', 'reasons', 'teams', 'choosing', 'moving', 'fivetran', 'flat', 'predictable', 'pricing', 'theres', 'mar', 'billing', 'youre', 'getting', 'charged', 'campaigns', 'performed', 'well', 'syncs', 'ran', 'often', 'pricing', 'clear', 'stable', 'helps', 'lot', 'brands', 'trying', 'manage', 'budgets', 'scaling', 'retailfirst', 'coverage', 'daton', 'supports', 'platforms', 'ecomm', 'teams', 'rely', 'amazon', 'walmart', 'shopify', 'tiktok', 'klaviyo', 'covered', 'productiongrade', 'connectors', 'logic', 'understands', 'retail', 'data', 'actually', 'works', 'builtin', 'reporting', 'along', 'pipelines', 'daton', 'includes', 'pulse', 'reporting', 'layer', 'dashboards', 'premodeled', 'metrics', 'like', 'cac', 'ltv', 'roas', 'sku', 'performance', 'means', 'skip', 'setup', 'phase', 'get', 'straight', 'insights', 'custom', 'connectors', 'without', 'custom', 'pricing', 'use', 'platform', 'thats', 'already', 'integrated', 'team', 'build', 'surprise', 'fees', 'also', 'take', 'care', 'api', 'updates', 'pipelines', 'keep', 'running', 'without', 'extra', 'effort', 'support', 'thats', 'actually', 'helpful', 'youre', 'stuck', 'waiting', 'ticket', 'queue', 'teams', 'get', 'handson', 'onboarding', 'responsive', 'support', 'big', 'deal', 'youre', 'trying', 'migrate', 'pipelines', 'quickly', 'minimal', 'friction', 'ecomm', 'brands', 'start', 'stack', 'tools', 'shopify', 'storefront', 'platforms', 'email', 'crm', 'time', 'stack', 'evolves', 'might', 'switch', 'crms', 'change', 'platforms', 'add', 'new', 'tools', 'shopify', 'stays', 'grows', 'daton', 'designed', 'mindset', 'shouldnt', 'rethink', 'data', 'infrastructure', 'every', 'time', 'business', 'changes', 'built', 'scale', 'brand', 'youre', 'currently', 'evaluating', 'options', 'trying', 'avoid', 'painful', 'renewal', 'daton', 'might', 'worth', 'looking', 'work', 'saras', 'team', 'happy', 'help', 'heres', 'link', 'want', 'checkout', 'httpswwwsarasanalyticscomsarasdatonhttpswwwsarasanalyticscomsarasdaton', 'hope', 'helps']",youre urgently looking fivetran alternative might help seeing lot people caught guard new fivetran pricing youre ecommerce relying platforms like shopify amazon tiktok walmart shift marbased billing makes things really hard predict lot teams hard justify youre boat actively looking alternatives might helpful daton built saras analytics etl tool specifically created ecommerce focus made big difference lot teams weve worked recently needed something aligns better ecomm brands operate grow reasons teams choosing moving fivetran flat predictable pricing theres mar billing youre getting charged campaigns performed well syncs ran often pricing clear stable helps lot brands trying manage budgets scaling retailfirst coverage daton supports platforms ecomm teams rely amazon walmart shopify tiktok klaviyo covered productiongrade connectors logic understands retail data actually works builtin reporting along pipelines daton includes pulse reporting layer dashboards premodeled metrics like cac ltv roas sku performance means skip setup phase get straight insights custom connectors without custom pricing use platform thats already integrated team build surprise fees also take care api updates pipelines keep running without extra effort support thats actually helpful youre stuck waiting ticket queue teams get handson onboarding responsive support big deal youre trying migrate pipelines quickly minimal friction ecomm brands start stack tools shopify storefront platforms email crm time stack evolves might switch crms change platforms add new tools shopify stays grows daton designed mindset shouldnt rethink data infrastructure every time business changes built scale brand youre currently evaluating options trying avoid painful renewal daton might worth looking work saras team happy help heres link want checkout httpswwwsarasanalyticscomsarasdatonhttpswwwsarasanalyticscomsarasdaton hope helps,Low,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
241,US job search 2025 results,"Currently Senior DE at medium size global e-commerce tech company, looking for new job. Prepped for like 2 months Jan and Feb, and then started applying and interviewing. Here are the numbers:

Total apps: 107. 6 companies reached out for at least a phone screen. 5.6% conversion ratio.

The 6 companies where the following:

|Company|Role|Interviews|
|:-|:-|:-|
|Meta|Data Engineer|HR and then LC tech screening. Rejected after screening|
|Amazon|Data Engineer 1|Take home tech screening then LC type tech screening. Rejected after second screening|
|Root|Senior Data Engineer|HR then HM. Got rejected after HM|
|Kin|Senior Data Engineer|Only HR, got rejected after.|
|Clipboard Health|Data Engineer|Online take home screening, fairly easy but got rejected after.|
|Disney Streaming|Senior Data Engineer|Passed HR and HM interviews. Declined technical screening loop.|

At the end of the day, my current company offered me a good package to stay as well as a team change to a more architecture type role. Considering my current role salary is decent and fully remote, declined Disneys loop since I was going to be making the same while having to move to work on site in a HCOL city.

PS. Im a US Citizen.",68,19,2025-04-15 14:47:13,0,False,False,False,False,2025-04-15 14:47:13,14,Tuesday,188.0,1205,50.33,15,310,12.0,0,0,NEGATIVE,-0.9979427456855774,"['currently', 'senior', 'medium', 'size', 'global', 'ecommerce', 'tech', 'company', 'looking', 'new', 'job', 'prepped', 'like', 'months', 'jan', 'feb', 'started', 'applying', 'interviewing', 'numbers', 'total', 'apps', 'companies', 'reached', 'least', 'phone', 'screen', 'conversion', 'ratio', 'companies', 'following', 'companyroleinterviews', 'metadata', 'engineerhr', 'tech', 'screening', 'rejected', 'screening', 'amazondata', 'engineer', 'take', 'home', 'tech', 'screening', 'type', 'tech', 'screening', 'rejected', 'second', 'screening', 'rootsenior', 'data', 'engineerhr', 'got', 'rejected', 'kinsenior', 'data', 'engineeronly', 'got', 'rejected', 'clipboard', 'healthdata', 'engineeronline', 'take', 'home', 'screening', 'fairly', 'easy', 'got', 'rejected', 'disney', 'streamingsenior', 'data', 'engineerpassed', 'interviews', 'declined', 'technical', 'screening', 'loop', 'end', 'day', 'current', 'company', 'offered', 'good', 'package', 'stay', 'well', 'team', 'change', 'architecture', 'type', 'role', 'considering', 'current', 'role', 'salary', 'decent', 'fully', 'remote', 'declined', 'disneys', 'loop', 'since', 'going', 'making', 'move', 'work', 'site', 'hcol', 'city', 'citizen']",currently senior medium size global ecommerce tech company looking new job prepped like months jan feb started applying interviewing numbers total apps companies reached least phone screen conversion ratio companies following companyroleinterviews metadata engineerhr tech screening rejected screening amazondata engineer take home tech screening type tech screening rejected second screening rootsenior data engineerhr got rejected kinsenior data engineeronly got rejected clipboard healthdata engineeronline take home screening fairly easy got rejected disney streamingsenior data engineerpassed interviews declined technical screening loop end day current company offered good package stay well team change architecture type role considering current role salary decent fully remote declined disneys loop since going making move work site hcol city citizen,High,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
242,Greenfield: Do you go DWH or DL/DLH?,"If you're building a data platform from scratch today, do you start with a DWH on RDBMS? Or Data Lake[House] on object storage with something like Iceberg?

I'm assuming the near dominance of Oracle/DB2/SQL Server of > ~10 years ago has shifted? And Postgres has entered the mix as a serious option? But are people building data lakes/lakehouses from the outset, or only once they breach the size of what a DWH can reliably/cost-effectively do?",28,69,2025-04-15 17:07:52,0,False,False,False,False,2025-04-15 17:07:52,17,Tuesday,75.0,444,64.91,5,113,9.4,0,0,NEGATIVE,-0.9989582300186157,"['youre', 'building', 'data', 'platform', 'scratch', 'today', 'start', 'dwh', 'rdbms', 'data', 'lakehouse', 'object', 'storage', 'something', 'like', 'iceberg', 'assuming', 'near', 'dominance', 'oracledbsql', 'server', 'years', 'ago', 'shifted', 'postgres', 'entered', 'mix', 'serious', 'option', 'people', 'building', 'data', 'lakeslakehouses', 'outset', 'breach', 'size', 'dwh', 'reliablycosteffectively']",youre building data platform scratch today start dwh rdbms data lakehouse object storage something like iceberg assuming near dominance oracledbsql server years ago shifted postgres entered mix serious option people building data lakeslakehouses outset breach size dwh reliablycosteffectively,High,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
243,Address & Name matching technique,"Context: 
I have a dataset of company owned products like: Name: Company A, Address: 5th avenue, Product: A. 
Company A inc, Address: New york, Product B. 
Company A inc. , Address, 5th avenue New York, product C. 

I have 400 million entries like these. As you can see, addresses and names are in inconsistent formats. 
I have another dataset that will be me ground truth for companies. It has a clean name for the company along with it’s parsed address. 

The objective is to match the records from the table with inconsistent formats to the ground truth, so that each product is linked to a clean company. 



Questions and help: 
- i was thinking to use google geocoding api to parse the addresses and get geocoding. Then use the geocoding to perform distance search between my my addresses and ground truth BUT i don’t have the geocoding in the ground truth dataset. So, i would like to find another method to match parsed addresses without using geocoding. 

- Ideally, i would like to be able to input my parsed address and the name (maybe along with some other features like industry of activity) and get returned the top matching candidates from the ground truth dataset with a score between 0 and 1. Which approach would you suggest that fits big size datasets? 

- The method should be able to handle cases were one of my addresses could be: company A, address: Washington (meaning an approximate address that is just a city for example, sometimes the country is not even specified). I will receive several parsed addresses from this candidate as Washington is vague. What is the best practice in such cases? As the google api won’t return a single result, what can i do?

- My addresses are from all around the world, do you know if google api can handle the whole world? Would a language model be better at parsing for some regions? 

Help would be very much appreciated, thank you guys. 
",7,11,2025-04-15 10:13:54,0,False,False,False,False,2025-04-15 10:13:54,10,Tuesday,334.0,1902,64.0,21,488,11.1,0,0,NEGATIVE,-0.9906371235847473,"['context', 'dataset', 'company', 'owned', 'products', 'like', 'name', 'company', 'address', 'avenue', 'product', 'company', 'inc', 'address', 'new', 'york', 'product', 'company', 'inc', 'address', 'avenue', 'new', 'york', 'product', 'million', 'entries', 'like', 'see', 'addresses', 'names', 'inconsistent', 'formats', 'another', 'dataset', 'ground', 'truth', 'companies', 'clean', 'name', 'company', 'along', 'parsed', 'address', 'objective', 'match', 'records', 'table', 'inconsistent', 'formats', 'ground', 'truth', 'product', 'linked', 'clean', 'company', 'questions', 'help', 'thinking', 'use', 'google', 'geocoding', 'api', 'parse', 'addresses', 'get', 'geocoding', 'use', 'geocoding', 'perform', 'distance', 'search', 'addresses', 'ground', 'truth', 'dont', 'geocoding', 'ground', 'truth', 'dataset', 'would', 'like', 'find', 'another', 'method', 'match', 'parsed', 'addresses', 'without', 'using', 'geocoding', 'ideally', 'would', 'like', 'able', 'input', 'parsed', 'address', 'name', 'maybe', 'along', 'features', 'like', 'industry', 'activity', 'get', 'returned', 'top', 'matching', 'candidates', 'ground', 'truth', 'dataset', 'score', 'approach', 'would', 'suggest', 'fits', 'big', 'size', 'datasets', 'method', 'able', 'handle', 'cases', 'one', 'addresses', 'could', 'company', 'address', 'washington', 'meaning', 'approximate', 'address', 'city', 'example', 'sometimes', 'country', 'even', 'specified', 'receive', 'several', 'parsed', 'addresses', 'candidate', 'washington', 'vague', 'best', 'practice', 'cases', 'google', 'api', 'wont', 'return', 'single', 'result', 'addresses', 'around', 'world', 'know', 'google', 'api', 'handle', 'whole', 'world', 'would', 'language', 'model', 'better', 'parsing', 'regions', 'help', 'would', 'much', 'appreciated', 'thank', 'guys']",context dataset company owned products like name company address avenue product company inc address new york product company inc address avenue new york product million entries like see addresses names inconsistent formats another dataset ground truth companies clean name company along parsed address objective match records table inconsistent formats ground truth product linked clean company questions help thinking use google geocoding api parse addresses get geocoding use geocoding perform distance search addresses ground truth dont geocoding ground truth dataset would like find another method match parsed addresses without using geocoding ideally would like able input parsed address name maybe along features like industry activity get returned top matching candidates ground truth dataset score approach would suggest fits big size datasets method able handle cases one addresses could company address washington meaning approximate address city example sometimes country even specified receive several parsed addresses candidate washington vague best practice cases google api wont return single result addresses around world know google api handle whole world would language model better parsing regions help would much appreciated thank guys,High,0,0,"need, table, event, source, without, instead, every, replay, day, example, single, change, per, business, daily, based, keys, manually, model, keep, needs, events, doesnt, name, list, option, user, must, takes, history, often, truth, full, column, hour, address, requirements, date, reading, track, whenever, metrics, result, update, feature, primary, columns, meaning, pricing, structure, details, rows, simply, available, dedicated, country, means, facebook, error, missing, evolve, involves, names, size, domain, place, addresses, timestamp, parse, mean, slow, schemas, heres, return, dimensions, delete, state, times, arent, ground, errors, receive, min, added, geocoding, shape, sourcing, wasnt, insert, minutes, value, flow, save, normal, little, region, usually, bad, becomes, failure, replication, entirely, products, infrastructure, minute, person, average, servers, internet, looked, probably, campaigns, workloads, looks",8.2,7.4,175.5,-0.663,1069.0,47.9,9.8,275.5,11.4,22.2,15.5,52,108,34,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",4,4
244,Dataverse vs. Azure SQL DB,"Thank you everyone with all of your helpful insights from my initial post! Just as the title states, I'm an intern looking to weigh the pros and cons of using Dataverse vs an Azure SQL Database (After many back and forths with IT, we've landed at these two options that were approved by our company). 

Our team plans to use Microsoft Power Apps to collect data and are now trying to figure out where to store the data. Upon talking with my supervisor, they plan to have data exported from this database to use for data analysis in SAS or RStudio, in addition to the Microsoft Power App.

What would be the better or ideal solution for this? Thank you!
Edit: Also, they want to store images as well. Any ideas on how and where to store them?",5,4,2025-04-15 03:35:41,0,False,2025-04-15 04:44:35,False,False,2025-04-15 03:35:41,3,Tuesday,138.0,741,59.94,7,203,11.5,0,0,POSITIVE,0.9544152021408081,"['thank', 'everyone', 'helpful', 'insights', 'initial', 'post', 'title', 'states', 'intern', 'looking', 'weigh', 'pros', 'cons', 'using', 'dataverse', 'azure', 'sql', 'database', 'many', 'back', 'forths', 'weve', 'landed', 'two', 'options', 'approved', 'company', 'team', 'plans', 'use', 'microsoft', 'power', 'apps', 'collect', 'data', 'trying', 'figure', 'store', 'data', 'upon', 'talking', 'supervisor', 'plan', 'data', 'exported', 'database', 'use', 'data', 'analysis', 'sas', 'rstudio', 'addition', 'microsoft', 'power', 'app', 'would', 'better', 'ideal', 'solution', 'thank', 'edit', 'also', 'want', 'store', 'images', 'well', 'ideas', 'store']",thank everyone helpful insights initial post title states intern looking weigh pros cons using dataverse azure sql database many back forths weve landed two options approved company team plans use microsoft power apps collect data trying figure store data upon talking supervisor plan data exported database use data analysis sas rstudio addition microsoft power app would better ideal solution thank edit also want store images well ideas store,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
245,How has Business Intelligence Analytics changed the way you make decisions at work?,"I’ve been diving deep into how companies use Business Intelligence Analytics to not just track KPIs but actually transform how they operate day to day. It’s crazy how powerful real-time dashboards and predictive models have become. imagine optimizing customer experiences before they even ask for it or spotting a supply chain delay before it even happens. Curious to hear how others are using BI analytics in your field Have tools like tableau, Power BI, or even simple CRM dashboards helped your team make better decisions or is it all still gut feeling and spreadsheets?  P.S. I found an article that simplified this topic pretty well. If anyones curious I’ll drop the link below. Not a promotion just thought it broke things down nicely https://instalogic.in/blog/the-role-of-business-intelligence-analytics-what-is-it-and-why-does-it-matter/",4,6,2025-04-15 02:01:57,0,False,False,False,False,2025-04-15 02:01:57,2,Tuesday,124.0,846,36.59,7,224,13.0,1,0,POSITIVE,0.9533692002296448,"['ive', 'diving', 'deep', 'companies', 'use', 'business', 'intelligence', 'analytics', 'track', 'kpis', 'actually', 'transform', 'operate', 'day', 'day', 'crazy', 'powerful', 'realtime', 'dashboards', 'predictive', 'models', 'become', 'imagine', 'optimizing', 'customer', 'experiences', 'even', 'ask', 'spotting', 'supply', 'chain', 'delay', 'even', 'happens', 'curious', 'hear', 'others', 'using', 'analytics', 'field', 'tools', 'like', 'tableau', 'power', 'even', 'simple', 'crm', 'dashboards', 'helped', 'team', 'make', 'better', 'decisions', 'still', 'gut', 'feeling', 'spreadsheets', 'found', 'article', 'simplified', 'topic', 'pretty', 'well', 'anyones', 'curious', 'ill', 'drop', 'link', 'promotion', 'thought', 'broke', 'things', 'nicely', 'httpsinstalogicinblogtheroleofbusinessintelligenceanalyticswhatisitandwhydoesitmatter']",ive diving deep companies use business intelligence analytics track kpis actually transform operate day day crazy powerful realtime dashboards predictive models become imagine optimizing customer experiences even ask spotting supply chain delay even happens curious hear others using analytics field tools like tableau power even simple crm dashboards helped team make better decisions still gut feeling spreadsheets found article simplified topic pretty well anyones curious ill drop link promotion thought broke things nicely httpsinstalogicinblogtheroleofbusinessintelligenceanalyticswhatisitandwhydoesitmatter,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
246,How would you handle the ingestion of thousands of files ?,"Hello,
I’m facing a philosophical question at work and I can’t find an answer that would put my brain at ease. 

Basically we work with Databricks and Pyspark for ingestion and transformation.

We have a new data provider that sends crypted and zipped files to an s3 bucket. There are a couple of thousands of files (2 years of historic).

We wanted to use dataloader from databricks. It’s basically a spark stream that scans folders, finds the files that you never ingested (it keeps track in a table) and reads the new files only and write them.
The problem is that dataloader doesn’t handle encrypted and zipped files (json files inside).

We can’t unzip files permanently. 

My coworker proposed that we use the autoloader to find the files (that it can do) and in that spark stream use the for each batch method to apply a lambda that does:
- get the file name (current row)
-decrypt and unzip
-hash the files (to avoid duplicates in case of failure)
-open the unzipped file using spark
-save in the final table using spark 

I argued that it’s not the right place to do all that and since it’s not the use case of autoloader it’s not a good practice, he argues that spark is distributed and that’s the only thing we care since it allows us to do what we need quickly even though it’s hard to debug (and we need to pass the s3 credentials to each executor using the lambda…)

I proposed a homemade solution which isn’t the most optimal, but it seems better and easier to maintain which is:
- use boto paginator to find files
- decrypt and unzip each file 
- write then json in the team bucket/folder
-create a monitoring table in which we save the file name, hash, status (ok/ko) and exceptions if there are any

He argues that this is not efficient since it’ll only use one single node cluster and not parallelised. 

I never encountered such use case before and I’m kind of stuck, I read a lot of literature but everything seems very generic. 

Edit: we only receive 2 to 3 files daily per data feed (150mo per file on average) but we have 2 years of historical data which amounts to around 1000 files. So we need 1 run for all the historic then a daily run. 
Every feed ingested is a class instantiation (a job on a cluster with a config) so it doesn’t matter if we have 10 feeds. 

Edit2: 1000 files roughly summed to 130go after unzipping. Not sure of average zip/json file though. 

What do you people think of this? Any advices ?
Thank you ",5,28,2025-04-15 18:15:12,0,False,2025-04-15 19:09:42,False,False,2025-04-15 18:15:12,18,Tuesday,449.0,2452,60.18,16,622,12.2,0,1,NEGATIVE,-0.9971766471862793,"['hello', 'facing', 'philosophical', 'question', 'work', 'cant', 'find', 'answer', 'would', 'put', 'brain', 'ease', 'basically', 'work', 'databricks', 'pyspark', 'ingestion', 'transformation', 'new', 'data', 'provider', 'sends', 'crypted', 'zipped', 'files', 'bucket', 'couple', 'thousands', 'files', 'years', 'historic', 'wanted', 'use', 'dataloader', 'databricks', 'basically', 'spark', 'stream', 'scans', 'folders', 'finds', 'files', 'never', 'ingested', 'keeps', 'track', 'table', 'reads', 'new', 'files', 'write', 'problem', 'dataloader', 'doesnt', 'handle', 'encrypted', 'zipped', 'files', 'json', 'files', 'inside', 'cant', 'unzip', 'files', 'permanently', 'coworker', 'proposed', 'use', 'autoloader', 'find', 'files', 'spark', 'stream', 'use', 'batch', 'method', 'apply', 'lambda', 'get', 'file', 'name', 'current', 'row', 'decrypt', 'unzip', 'hash', 'files', 'avoid', 'duplicates', 'case', 'failure', 'open', 'unzipped', 'file', 'using', 'spark', 'save', 'final', 'table', 'using', 'spark', 'argued', 'right', 'place', 'since', 'use', 'case', 'autoloader', 'good', 'practice', 'argues', 'spark', 'distributed', 'thats', 'thing', 'care', 'since', 'allows', 'need', 'quickly', 'even', 'though', 'hard', 'debug', 'need', 'pass', 'credentials', 'executor', 'using', 'lambda', 'proposed', 'homemade', 'solution', 'isnt', 'optimal', 'seems', 'better', 'easier', 'maintain', 'use', 'boto', 'paginator', 'find', 'files', 'decrypt', 'unzip', 'file', 'write', 'json', 'team', 'bucketfolder', 'create', 'monitoring', 'table', 'save', 'file', 'name', 'hash', 'status', 'okko', 'exceptions', 'argues', 'efficient', 'since', 'itll', 'use', 'one', 'single', 'node', 'cluster', 'parallelised', 'never', 'encountered', 'use', 'case', 'kind', 'stuck', 'read', 'lot', 'literature', 'everything', 'seems', 'generic', 'edit', 'receive', 'files', 'daily', 'per', 'data', 'feed', 'per', 'file', 'average', 'years', 'historical', 'data', 'amounts', 'around', 'files', 'need', 'run', 'historic', 'daily', 'run', 'every', 'feed', 'ingested', 'class', 'instantiation', 'job', 'cluster', 'config', 'doesnt', 'matter', 'feeds', 'edit', 'files', 'roughly', 'summed', 'unzipping', 'sure', 'average', 'zipjson', 'file', 'though', 'people', 'think', 'advices', 'thank']",hello facing philosophical question work cant find answer would put brain ease basically work databricks pyspark ingestion transformation new data provider sends crypted zipped files bucket couple thousands files years historic wanted use dataloader databricks basically spark stream scans folders finds files never ingested keeps track table reads new files write problem dataloader doesnt handle encrypted zipped files json files inside cant unzip files permanently coworker proposed use autoloader find files spark stream use batch method apply lambda get file name current row decrypt unzip hash files avoid duplicates case failure open unzipped file using spark save final table using spark argued right place since use case autoloader good practice argues spark distributed thats thing care since allows need quickly even though hard debug need pass credentials executor using lambda proposed homemade solution isnt optimal seems better easier maintain use boto paginator find files decrypt unzip file write json team bucketfolder create monitoring table save file name hash status okko exceptions argues efficient since itll use one single node cluster parallelised never encountered use case kind stuck read lot literature everything seems generic edit receive files daily per data feed per file average years historical data amounts around files need run historic daily run every feed ingested class instantiation job cluster config doesnt matter feeds edit files roughly summed unzipping sure average zipjson file though people think advices thank,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
247,How much does your org spend on ETL tools monthly?,"Looking for a general estimate on how much companies spend on tools like Airbyte, Fivetran, Stitch, etc, per month?



[View Poll](https://www.reddit.com/poll/1jznyyv)",3,17,2025-04-15 09:55:46,0,False,False,False,False,2025-04-15 09:55:46,9,Tuesday,21.0,167,24.78,1,39,0.0,1,0,NEGATIVE,-0.9988571405410767,"['looking', 'general', 'estimate', 'much', 'companies', 'spend', 'tools', 'like', 'airbyte', 'fivetran', 'stitch', 'etc', 'per', 'month', 'view', 'pollhttpswwwredditcompolljznyyv']",looking general estimate much companies spend tools like airbyte fivetran stitch etc per month view pollhttpswwwredditcompolljznyyv,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
248,Spark sql vs Redshift tiebreaker rules during sorting,"I’m looking to move some of my teams etl away from redshift and on to AWS glue. 

I’m noticing that the spark sql data frames don’t sort back in the same order in the case of having nulls vs redshift. 

My hope was to port over the Postgres sql to spark sql and end up with very similar output. 

Unfortunately it’s looking like it’s off. 
For instance if I have a window function for row count, the same query assigns the numbers to different rows in spark. 

What is the best path forward to get the sorting the same?",5,3,2025-04-15 01:06:55,0,False,False,False,False,2025-04-15 01:06:55,1,Tuesday,99.0,519,71.65,6,136,9.3,0,1,NEGATIVE,-0.997014045715332,"['looking', 'move', 'teams', 'etl', 'away', 'redshift', 'aws', 'glue', 'noticing', 'spark', 'sql', 'data', 'frames', 'dont', 'sort', 'back', 'order', 'case', 'nulls', 'redshift', 'hope', 'port', 'postgres', 'sql', 'spark', 'sql', 'end', 'similar', 'output', 'unfortunately', 'looking', 'like', 'instance', 'window', 'function', 'row', 'count', 'query', 'assigns', 'numbers', 'different', 'rows', 'spark', 'best', 'path', 'forward', 'get', 'sorting']",looking move teams etl away redshift aws glue noticing spark sql data frames dont sort back order case nulls redshift hope port postgres sql spark sql end similar output unfortunately looking like instance window function row count query assigns numbers different rows spark best path forward get sorting,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
249,PowerAutomate as an ETL Tool,"Hi!

This is a problem I am facing in my current job right now. We have a lot of RPA requirements and 300's of CSV's and Excel files are manually obtained from some interfaces and mail and customer only works with excels including reporting and operational changes are being done manually by hand.

The thing is we don't have any data. We plan to implement Power Automate to grab these files from the said interfaces. But as some of you know, PowerAutomate has SQL Connectors. 

Do you think it is ok to write files directly to a database with PowerAutomate? Have any of you experience in this? Thanks.",3,12,2025-04-15 07:42:17,0,False,False,False,False,2025-04-15 07:42:17,7,Tuesday,108.0,602,64.3,7,167,12.3,0,0,NEGATIVE,-0.9972583055496216,"['problem', 'facing', 'current', 'job', 'right', 'lot', 'rpa', 'requirements', 'csvs', 'excel', 'files', 'manually', 'obtained', 'interfaces', 'mail', 'customer', 'works', 'excels', 'including', 'reporting', 'operational', 'changes', 'done', 'manually', 'hand', 'thing', 'dont', 'data', 'plan', 'implement', 'power', 'automate', 'grab', 'files', 'said', 'interfaces', 'know', 'powerautomate', 'sql', 'connectors', 'think', 'write', 'files', 'directly', 'database', 'powerautomate', 'experience', 'thanks']",problem facing current job right lot rpa requirements csvs excel files manually obtained interfaces mail customer works excels including reporting operational changes done manually hand thing dont data plan implement power automate grab files said interfaces know powerautomate sql connectors think write files directly database powerautomate experience thanks,Mid,4,4,"files, api, tables, query, models, read, google, json, file, logic, write, case, csv, store, datasets, across, sources, metadata, created, schema, systems, large, platform, custom, script, changes, dataset, row, reporting, parquet, issues, ads, load, fields, analysis, cases, product, values, design, view, idea, layer, search, import, suggest, language, excel, app, blob, form, request, params, postgres, ingestion, raw, mysql, fact, processed, structures, either, separate, structured, connection, core, method, downstream, scripts, extract, maintain, send, hub, records, transactional, overhead, lineage, environment, object, external, pull, iot, order, typically, grid, hubs, validation, requires, mapping, batch, mechanism, location, updates, keeping, checkout, operations, runs, convert, optimized, avoid, matching, views, formats, matters, vpc, bring, reduce, migrate, consists, starts, reads, refresh, relationships, bronze, historical, tracking, developers, natural",8.0,7.1,169.4,-0.679,1041.7,44.5,9.5,267.7,10.7,24.1,17.9,49,102,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,7
250,Suggest best sources to master DBMS,"I recently joined as an intern in an organisation. They assigned me database technology, and they wanted me to learn everything about database and database management systems in the span of 5 months.
They suggested to me a book to learn from but it's difficult to learn from that book. 
I have an intermediate knowledge on Oracle SQL and Oracle PL/SQL. 
I want to gain much knowledge on Database and DBMS.

So i request people out there who have knowledge on databases to suggest the best sources(preffered free) to learn from scratch to advanced as soon as possible.",3,3,2025-04-15 02:13:21,0,False,False,False,False,2025-04-15 02:13:21,2,Tuesday,98.0,567,54.93,6,152,13.0,0,0,NEGATIVE,-0.9908662438392639,"['recently', 'joined', 'intern', 'organisation', 'assigned', 'database', 'technology', 'wanted', 'learn', 'everything', 'database', 'database', 'management', 'systems', 'span', 'months', 'suggested', 'book', 'learn', 'difficult', 'learn', 'book', 'intermediate', 'knowledge', 'oracle', 'sql', 'oracle', 'plsql', 'want', 'gain', 'much', 'knowledge', 'database', 'dbms', 'request', 'people', 'knowledge', 'databases', 'suggest', 'best', 'sourcespreffered', 'free', 'learn', 'scratch', 'advanced', 'soon', 'possible']",recently joined intern organisation assigned database technology wanted learn everything database database management systems span months suggested book learn difficult learn book intermediate knowledge oracle sql oracle plsql want gain much knowledge database dbms request people knowledge databases suggest best sourcespreffered free learn scratch advanced soon possible,Mid,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
251,SAP Databricks,"Curious if anyone is brave enough to leave Azure/AWS Databricks for SAP Databricks? Or if you are an SAP shop would you choose that over pure Databricks. From past experiences with SAP I’ve never been a fan of anything they do outside ERP. Personally, I believe you should separate yourself as much as possible for future contract negotiations. Also the risk of limited people singing up and you have a bunch of half baked integrations.",2,2,2025-04-15 23:21:01,0,False,False,False,False,2025-04-15 23:21:01,23,Tuesday,75.0,436,64.71,5,112,11.2,0,0,NEGATIVE,-0.9985112547874451,"['curious', 'anyone', 'brave', 'enough', 'leave', 'azureaws', 'databricks', 'sap', 'databricks', 'sap', 'shop', 'would', 'choose', 'pure', 'databricks', 'past', 'experiences', 'sap', 'ive', 'never', 'fan', 'anything', 'outside', 'erp', 'personally', 'believe', 'separate', 'much', 'possible', 'future', 'contract', 'negotiations', 'also', 'risk', 'limited', 'people', 'singing', 'bunch', 'half', 'baked', 'integrations']",curious anyone brave enough leave azureaws databricks sap databricks sap shop would choose pure databricks past experiences sap ive never fan anything outside erp personally believe separate much possible future contract negotiations also risk limited people singing bunch half baked integrations,Mid,6,6,"new, dont, know, job, time, company, even, help, much, really, best, could, current, seems, people, still, since, question, given, sure, first, big, hear, say, found, kind, questions, possible, free, tried, thank, companies, anything, end, cant, everything, wondering, post, understand, ago, give, practice, part, someone, back, hard, field, theres, please, never, advance, got, technical, real, isnt, made, move, last, type, path, maybe, ask, version, community, didnt, guys, may, surrogate, small, already, appreciated, together, level, department, stuck, came, world, though, enough, various, always, days, makes, month, thing, mind, look, edit, information, said, context, become, old, input, break, wrong, clear, finance, hours, book, needed, considering, fear, spend, long, complete, actual, scratch, feels, answer, today, yes, rate, fast, noticed, regarding, hand, switch, matter, becoming, pros, interesting, says, finally, seen, kimball, asking, technology, situation, cannot, wants, taking, blah, exactly, reasons, wonder, title, encounter, outside, away, explore, startup, stats, interviews, lets, live, number, numbers, fine, risk, somewhere, strong, covered, analysts, sometimes, decide, deep, guy, rejected, consider, generally, city, ideally, tell, scope, macbooks, positions, personally, tend, documentation, finding, decisions, put, demand, took, contract, consistent, stable, links, lack",8.1,7.9,149.8,-0.638,910.6,50.2,8.6,235.0,10.3,18.8,13.7,67,135,54,"I've built an interactive demo for CDC to help explain how it works.

The app currently shows the transaction log-based and query-based CDC approaches.

Change Data Capture (CDC) is a design pattern that tracks changes (inserts, updates, deletes) in a database and makes those changes available to downstream systems in real-time or near real-time.

CDC is super useful for a variety of use cases:

\- Real-time data replication between operational databases and data warehouses or lakehouses

\- Keeping analytics systems up to date without full batch reloads

\- Synchronizing data across microservices or distributed systems

\- Feeding event-driven architectures by turning database changes into event streams

\- Maintaining materialized views or derived tables with fresh data

\- Simplifying ETL/ELT pipelines by processing only changed records

And many more!

Let me know what you think and if there's any functionality missing that could be interesting to showcase.",5,2
252,"Are complex data types (JSON, BSON, MAP, LIST, etc.) commonly used in Parquet?","Hey folks,

I'm building a tool to convert between Parquet and other formats (CSV, JSON, etc.).  You can see it here: [https://dataconverter.io/tools/parquet](https://dataconverter.io/tools/parquet)

Progress has been very good so far.  The question now is how far into complex Parquet types to go – given than many of the target formats don't have an equivalent type.

How often do you come across Parquet files with complex or nested structures?  And what are you mostly seeing?

I'd appreciate any insight you can share.",3,1,2025-04-15 22:53:16,0,False,False,False,False,2025-04-15 22:53:16,22,Tuesday,80.0,523,51.55,7,134,8.0,1,0,POSITIVE,0.9950219392776489,"['hey', 'folks', 'building', 'tool', 'convert', 'parquet', 'formats', 'csv', 'json', 'etc', 'see', 'httpsdataconverteriotoolsparquethttpsdataconverteriotoolsparquet', 'progress', 'good', 'far', 'question', 'far', 'complex', 'parquet', 'types', 'given', 'many', 'target', 'formats', 'dont', 'equivalent', 'type', 'often', 'come', 'across', 'parquet', 'files', 'complex', 'nested', 'structures', 'mostly', 'seeing', 'appreciate', 'insight', 'share']",hey folks building tool convert parquet formats csv json etc see httpsdataconverteriotoolsparquethttpsdataconverteriotoolsparquet progress good far question far complex parquet types given many target formats dont equivalent type often come across parquet files complex nested structures mostly seeing appreciate insight share,Mid,4,4,"files, api, tables, query, models, read, google, json, file, logic, write, case, csv, store, datasets, across, sources, metadata, created, schema, systems, large, platform, custom, script, changes, dataset, row, reporting, parquet, issues, ads, load, fields, analysis, cases, product, values, design, view, idea, layer, search, import, suggest, language, excel, app, blob, form, request, params, postgres, ingestion, raw, mysql, fact, processed, structures, either, separate, structured, connection, core, method, downstream, scripts, extract, maintain, send, hub, records, transactional, overhead, lineage, environment, object, external, pull, iot, order, typically, grid, hubs, validation, requires, mapping, batch, mechanism, location, updates, keeping, checkout, operations, runs, convert, optimized, avoid, matching, views, formats, matters, vpc, bring, reduce, migrate, consists, starts, reads, refresh, relationships, bronze, historical, tracking, developers, natural",8.0,7.1,169.4,-0.679,1041.7,44.5,9.5,267.7,10.7,24.1,17.9,49,102,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,7
253,Looking for advice or resources on folder structure for a Data Engineering project,"Hey everyone,  
I’m working on a Data Engineering project and I want to make sure I’m organizing everything properly from the start. I'm looking for **best practices, lessons learned, or even examples of folder structures** used in real-world data engineering projects.

Would really appreciate:

* Any **advice or personal experience** on what worked well (or didn’t) for you
* **Blog posts, GitHub repos, YouTube videos**, or other resources that walk through good project structure
* Recommendations for organizing things like ETL pipelines, raw vs processed data, scripts, configs, notebooks, etc.

Thanks in advance — trying to avoid a mess later by doing things right early on!",2,4,2025-04-15 16:15:01,0,False,False,False,False,2025-04-15 16:15:01,16,Tuesday,106.0,683,37.13,4,177,14.9,0,0,POSITIVE,0.9914548397064209,"['hey', 'everyone', 'working', 'data', 'engineering', 'project', 'want', 'make', 'sure', 'organizing', 'everything', 'properly', 'start', 'looking', 'best', 'practices', 'lessons', 'learned', 'even', 'examples', 'folder', 'structures', 'used', 'realworld', 'data', 'engineering', 'projects', 'would', 'really', 'appreciate', 'advice', 'personal', 'experience', 'worked', 'well', 'didnt', 'blog', 'posts', 'github', 'repos', 'youtube', 'videos', 'resources', 'walk', 'good', 'project', 'structure', 'recommendations', 'organizing', 'things', 'like', 'etl', 'pipelines', 'raw', 'processed', 'data', 'scripts', 'configs', 'notebooks', 'etc', 'thanks', 'advance', 'trying', 'avoid', 'mess', 'later', 'things', 'right', 'early']",hey everyone working data engineering project want make sure organizing everything properly start looking best practices lessons learned even examples folder structures used realworld data engineering projects would really appreciate advice personal experience worked well didnt blog posts github repos youtube videos resources walk good project structure recommendations organizing things like etl pipelines raw processed data scripts configs notebooks etc thanks advance trying avoid mess later things right early,Mid,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
254,Use the output of a cell in a Databricks notebook in another cell,"Hi,
I have a Notebook A containing multiple SQL scripts in multiple cells.  I am trying to use the output of specific cells of Notebook_A in another notebook. Eg: count of records returned in cell2 of notebook_a in the python Notebook_B.

Kindly suggest on the feasible ways to implement the above.",2,5,2025-04-15 09:24:21,0,False,False,False,False,2025-04-15 09:24:21,9,Tuesday,51.0,298,58.48,4,81,11.2,0,0,NEGATIVE,-0.9924968481063843,"['notebook', 'containing', 'multiple', 'sql', 'scripts', 'multiple', 'cells', 'trying', 'use', 'output', 'specific', 'cells', 'notebooka', 'another', 'notebook', 'count', 'records', 'returned', 'cell', 'notebooka', 'python', 'notebookb', 'kindly', 'suggest', 'feasible', 'ways', 'implement']",notebook containing multiple sql scripts multiple cells trying use output specific cells notebooka another notebook count records returned cell notebooka python notebookb kindly suggest feasible ways implement,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
255,Tracking Ongoing tasks for the team,"My team is involved in Project development work that fits perfectly in the agile framework, but we also have some ongoing tasks related to platform administration, monitoring support, continuous enhancement of security, etc. These tasks do not fit well in the agile process. How do others track such tasks and measure progress on them? Do you use specific tools for this?",2,1,2025-04-15 03:21:03,0,False,False,False,False,2025-04-15 03:21:03,3,Tuesday,61.0,371,47.49,4,102,12.6,0,0,NEGATIVE,-0.9985561966896057,"['team', 'involved', 'project', 'development', 'work', 'fits', 'perfectly', 'agile', 'framework', 'also', 'ongoing', 'tasks', 'related', 'platform', 'administration', 'monitoring', 'support', 'continuous', 'enhancement', 'security', 'etc', 'tasks', 'fit', 'well', 'agile', 'process', 'others', 'track', 'tasks', 'measure', 'progress', 'use', 'specific', 'tools']",team involved project development work fits perfectly agile framework also ongoing tasks related platform administration monitoring support continuous enhancement security etc tasks fit well agile process others track tasks measure progress use specific tools,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
256,Parquet Nested Type to JSON in C++/Rust,"Hi Reddit community! This is my first Reddit post and I’m hoping I could get some help with this task I’m stuck with please!

I read a parquet file and store it in an arrow table. I want to read a parquet complex/nested column and convert it into a JSON object. I use C++ so I’m searching for libraries/tools preferably in C++ but if not, then I can try to integrate it with rust. 
What I want to do:
Say there is a parquet column in my file of type (arbitrary, just to showcase complexity): List(Struct(List(Struct(int,string,List(Struct(int, bool)))), bool))
I want to process this into a JSON object (or a json formatted string, then I can convert that into a json object). I do not want to flatten it out for my current use case. 

What I have found so far:
1. Parquet's inbuilt toString functions don’t really work with structs (they’re just good for debugging)
2. haven’t found anything in C++ that would do this without me having to writing a custom recursive logic, even with rapidjson
3. tried Polars with Rust but didn’t get a Json yet. 

I know I can get write my custom logic to create a json formatted string, but there must be some existing libraries that do this? I've been asked to not write a custom code because they're difficult to maintain and easy to break :)

Appreciate any help!",2,14,2025-04-15 02:36:14,0,False,False,False,False,2025-04-15 02:36:14,2,Tuesday,235.0,1302,70.13,13,319,9.7,0,1,NEGATIVE,-0.9988327622413635,"['reddit', 'community', 'first', 'reddit', 'post', 'hoping', 'could', 'get', 'help', 'task', 'stuck', 'please', 'read', 'parquet', 'file', 'store', 'arrow', 'table', 'want', 'read', 'parquet', 'complexnested', 'column', 'convert', 'json', 'object', 'use', 'searching', 'librariestools', 'preferably', 'try', 'integrate', 'rust', 'want', 'say', 'parquet', 'column', 'file', 'type', 'arbitrary', 'showcase', 'complexity', 'liststructliststructintstringliststructint', 'bool', 'bool', 'want', 'process', 'json', 'object', 'json', 'formatted', 'string', 'convert', 'json', 'object', 'want', 'flatten', 'current', 'use', 'case', 'found', 'far', 'parquets', 'inbuilt', 'tostring', 'functions', 'dont', 'really', 'work', 'structs', 'theyre', 'good', 'debugging', 'havent', 'found', 'anything', 'would', 'without', 'writing', 'custom', 'recursive', 'logic', 'even', 'rapidjson', 'tried', 'polars', 'rust', 'didnt', 'get', 'json', 'yet', 'know', 'get', 'write', 'custom', 'logic', 'create', 'json', 'formatted', 'string', 'must', 'existing', 'libraries', 'ive', 'asked', 'write', 'custom', 'code', 'theyre', 'difficult', 'maintain', 'easy', 'break', 'appreciate', 'help']",reddit community first reddit post hoping could get help task stuck please read parquet file store arrow table want read parquet complexnested column convert json object use searching librariestools preferably try integrate rust want say parquet column file type arbitrary showcase complexity liststructliststructintstringliststructint bool bool want process json object json formatted string convert json object want flatten current use case found far parquets inbuilt tostring functions dont really work structs theyre good debugging havent found anything would without writing custom recursive logic even rapidjson tried polars rust didnt get json yet know get write custom logic create json formatted string must existing libraries ive asked write custom code theyre difficult maintain easy break appreciate help,Mid,4,4,"files, api, tables, query, models, read, google, json, file, logic, write, case, csv, store, datasets, across, sources, metadata, created, schema, systems, large, platform, custom, script, changes, dataset, row, reporting, parquet, issues, ads, load, fields, analysis, cases, product, values, design, view, idea, layer, search, import, suggest, language, excel, app, blob, form, request, params, postgres, ingestion, raw, mysql, fact, processed, structures, either, separate, structured, connection, core, method, downstream, scripts, extract, maintain, send, hub, records, transactional, overhead, lineage, environment, object, external, pull, iot, order, typically, grid, hubs, validation, requires, mapping, batch, mechanism, location, updates, keeping, checkout, operations, runs, convert, optimized, avoid, matching, views, formats, matters, vpc, bring, reduce, migrate, consists, starts, reads, refresh, relationships, bronze, historical, tracking, developers, natural",8.0,7.1,169.4,-0.679,1041.7,44.5,9.5,267.7,10.7,24.1,17.9,49,102,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,7
257,AI for data and analytics,"We just launched **Seda.** You can connect your data and ask questions in plain English, write and fix SQL with AI, build dashboards instantly, ask about data lineage, and auto-document your tables and metrics. We’re opening up early access now at [seda.ai](https://www.seda.ai/). It works with Postgres, Snowflake, Redshift, BigQuery, dbt, and more.",0,1,2025-04-16 00:40:23,0,False,False,False,False,2025-04-16 00:40:23,0,Wednesday,52.0,350,66.74,4,80,10.1,1,0,POSITIVE,0.9535940885543823,"['launched', 'seda', 'connect', 'data', 'ask', 'questions', 'plain', 'english', 'write', 'fix', 'sql', 'build', 'dashboards', 'instantly', 'ask', 'data', 'lineage', 'autodocument', 'tables', 'metrics', 'opening', 'early', 'access', 'sedaaihttpswwwsedaai', 'works', 'postgres', 'snowflake', 'redshift', 'bigquery', 'dbt']",launched seda connect data ask questions plain english write fix sql build dashboards instantly ask data lineage autodocument tables metrics opening early access sedaaihttpswwwsedaai works postgres snowflake redshift bigquery dbt,Low,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
258,"Data Governance, a safe role in the near future?","What’s your take on the Data Governance role when it comes to job security and future opportunities, especially with how fast technology is changing, tasks getting automated, new roles popping up, and some jobs becoming obsolete?",0,3,2025-04-16 00:26:34,0,False,False,False,False,2025-04-16 00:26:34,0,Wednesday,36.0,229,26.48,1,62,0.0,0,0,NEGATIVE,-0.9677839279174805,"['whats', 'take', 'data', 'governance', 'role', 'comes', 'job', 'security', 'future', 'opportunities', 'especially', 'fast', 'technology', 'changing', 'tasks', 'getting', 'automated', 'new', 'roles', 'popping', 'jobs', 'becoming', 'obsolete']",whats take data governance role comes job security future opportunities especially fast technology changing tasks getting automated new roles popping jobs becoming obsolete,Low,2,2,"work, ive, python, working, project, cloud, looking, aws, currently, well, etl, snowflake, tools, dbt, databricks, used, learning, anyone, love, airflow, solution, hey, right, resources, personal, platforms, worked, especially, pyspark, whats, specific, testing, going, focus, months, development, services, wanted, microsoft, far, governance, suggestions, folks, implement, next, try, fabric, great, mainly, improve, later, stack, hope, lake, cost, thoughts, knowledge, future, pretty, weve, recommend, docker, fully, linux, speed, recommendations, willing, remote, amazon, early, handson, seem, learned, easier, setting, step, making, mostly, stuff, airbyte, objectives, definitely, online, integrate, potentially, security, implemented, kafka, considered, client, repo, plus, basis, portfolio, practices, proper, technologies, redshift, couple, courses, rdataengineering, guidance, reason, languages, budget, fun, training, managed, uses, dialects, orchestration, developing, tips, obviously, task, github, three, sort, experienced, examples, allowed, involved, comfortable, majority, joined, athena, numpy, gain, endtoend, computer, gotten, familiar, unity, interest, gap, devops, onprem, sap, automated, xgboost, reasonable, terraform, offers, challenge, comes, exploring",8.5,7.2,153.1,-0.642,934.2,49.4,8.7,241.0,10.6,19.9,14.2,63,133,50,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",3,5
259,Does Microsoft Purview has MDM feature?,"I know Purview is a data governance tool but does it has any MDM functionality.  From the article it seems it has integration with third party MDM solution partners such as CluedIn, profisee but I am not very clear whether or not it can do MDM by itself. 

One of my client's budget is very slim and they wanted to implement MDM. Do you think Microsoft Data Services (MDS) is an option but it looks very old to me and it seems to require a dedicated SQL server license. ",1,0,2025-04-15 23:10:56,1,False,False,False,False,2025-04-15 23:10:56,23,Tuesday,89.0,470,57.3,4,132,12.6,0,1,NEGATIVE,-0.9997676014900208,"['know', 'purview', 'data', 'governance', 'tool', 'mdm', 'functionality', 'article', 'seems', 'integration', 'third', 'party', 'mdm', 'solution', 'partners', 'cluedin', 'profisee', 'clear', 'whether', 'mdm', 'one', 'clients', 'budget', 'slim', 'wanted', 'implement', 'mdm', 'think', 'microsoft', 'data', 'services', 'mds', 'option', 'looks', 'old', 'seems', 'require', 'dedicated', 'sql', 'server', 'license']",know purview data governance tool mdm functionality article seems integration third party mdm solution partners cluedin profisee clear whether mdm one clients budget slim wanted implement mdm think microsoft data services mds option looks old seems require dedicated sql server license,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
260,How do you handle datetime dimentions ?,"I had a small “argument” at the office today. I am building a fact table to aggregate session metrics from our Google Analytics environment. One of the columns is the of course the session’s datetime. There are multiple reports and dashboards that do analysis at hour granularity. Ex : “What hour are visitors from this source more likely to buy hour product?”

To address this, I creates a date and time dimention. Today, the Data Specialist had an argument with me and said this is suboptimal and a single timestamp dimention should have been created. I though this makes no sense since it would result in extreme redudancy : you would have multiple minute rows for a single day for example. 

Now I am questioning my skills as he is a specialist and teorically knows better. I am failing to understand how a single timestamp table is better than seperates time and date dimentions",1,7,2025-04-15 22:39:58,0,False,False,False,False,2025-04-15 22:39:58,22,Tuesday,154.0,883,56.05,10,239,11.6,0,0,NEGATIVE,-0.9965014457702637,"['small', 'argument', 'office', 'today', 'building', 'fact', 'table', 'aggregate', 'session', 'metrics', 'google', 'analytics', 'environment', 'one', 'columns', 'course', 'sessions', 'datetime', 'multiple', 'reports', 'dashboards', 'analysis', 'hour', 'granularity', 'hour', 'visitors', 'source', 'likely', 'buy', 'hour', 'product', 'address', 'creates', 'date', 'time', 'dimention', 'today', 'data', 'specialist', 'argument', 'said', 'suboptimal', 'single', 'timestamp', 'dimention', 'created', 'though', 'makes', 'sense', 'since', 'would', 'result', 'extreme', 'redudancy', 'would', 'multiple', 'minute', 'rows', 'single', 'day', 'example', 'questioning', 'skills', 'specialist', 'teorically', 'knows', 'better', 'failing', 'understand', 'single', 'timestamp', 'table', 'better', 'seperates', 'time', 'date', 'dimentions']",small argument office today building fact table aggregate session metrics google analytics environment one columns course sessions datetime multiple reports dashboards analysis hour granularity hour visitors source likely buy hour product address creates date time dimention today data specialist argument said suboptimal single timestamp dimention created though makes sense since would result extreme redudancy would multiple minute rows single day example questioning skills specialist teorically knows better failing understand single timestamp table better seperates time date dimentions,Mid,0,0,"need, table, event, source, without, instead, every, replay, day, example, single, change, per, business, daily, based, keys, manually, model, keep, needs, events, doesnt, name, list, option, user, must, takes, history, often, truth, full, column, hour, address, requirements, date, reading, track, whenever, metrics, result, update, feature, primary, columns, meaning, pricing, structure, details, rows, simply, available, dedicated, country, means, facebook, error, missing, evolve, involves, names, size, domain, place, addresses, timestamp, parse, mean, slow, schemas, heres, return, dimensions, delete, state, times, arent, ground, errors, receive, min, added, geocoding, shape, sourcing, wasnt, insert, minutes, value, flow, save, normal, little, region, usually, bad, becomes, failure, replication, entirely, products, infrastructure, minute, person, average, servers, internet, looked, probably, campaigns, workloads, looks",8.2,7.4,175.5,-0.663,1069.0,47.9,9.8,275.5,11.4,22.2,15.5,52,108,34,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",4,4
261,Best setup report builder within SaaS?,"Hi everyone,

We've built a CRM and are looking to implement a report builder in our app.

We are exploring the best solutions for our needs and it seems like we have two paths we could take:

* Option A: Build the front-end/query builder ourselves and hit read-only replica
* Option B: Build the front-end/query builder ourselves and hit a data warehouse we've built using a key-base replication mechanism on BigQuery/Snowflake, etc..
* Option C: Use third party tools like Explo etc...

About the app:

* Our stack is React, Rails, Postgres.
* Our most used table (contacts) have 20,000,000 rows
* Some of our users have custom fields

We're trying to build something scalable but most importantly not spend months in this project.  
As a result, I'm wondering about the viability of Option A vs. Option B.  
  
One important point is how to manage custom fields that our users created on some objects.

We were thinking about, for contacts for example, we were thinking about simply running with joins across the following tables

* contacts
* contacts\_custom\_fields
* companies (and any other related 1:1 table so we can query fields from related 1:1 objects)
* contacts\_calculated\_fields (materialized view to compute values from 1:many relationship like # of deals the contacts is on)

So the two questions are:

* Would managing all this on the read-only be viable for our volume and a good starting point or will we hit the performance limits soon given our volume?
* Is managing custom fields this way the right way?",1,2,2025-04-15 19:28:35,0,False,False,False,False,2025-04-15 19:28:35,19,Tuesday,257.0,1529,43.97,9,401,14.7,0,0,NEGATIVE,-0.9910603165626526,"['everyone', 'weve', 'built', 'crm', 'looking', 'implement', 'report', 'builder', 'app', 'exploring', 'best', 'solutions', 'needs', 'seems', 'like', 'two', 'paths', 'could', 'take', 'option', 'build', 'frontendquery', 'builder', 'hit', 'readonly', 'replica', 'option', 'build', 'frontendquery', 'builder', 'hit', 'data', 'warehouse', 'weve', 'built', 'using', 'keybase', 'replication', 'mechanism', 'bigquerysnowflake', 'etc', 'option', 'use', 'third', 'party', 'tools', 'like', 'explo', 'etc', 'app', 'stack', 'react', 'rails', 'postgres', 'used', 'table', 'contacts', 'rows', 'users', 'custom', 'fields', 'trying', 'build', 'something', 'scalable', 'importantly', 'spend', 'months', 'project', 'result', 'wondering', 'viability', 'option', 'option', 'one', 'important', 'point', 'manage', 'custom', 'fields', 'users', 'created', 'objects', 'thinking', 'contacts', 'example', 'thinking', 'simply', 'running', 'joins', 'across', 'following', 'tables', 'contacts', 'contactscustomfields', 'companies', 'related', 'table', 'query', 'fields', 'related', 'objects', 'contactscalculatedfields', 'materialized', 'view', 'compute', 'values', 'many', 'relationship', 'like', 'deals', 'contacts', 'two', 'questions', 'would', 'managing', 'readonly', 'viable', 'volume', 'good', 'starting', 'point', 'hit', 'performance', 'limits', 'soon', 'given', 'volume', 'managing', 'custom', 'fields', 'way', 'right', 'way']",everyone weve built crm looking implement report builder app exploring best solutions needs seems like two paths could take option build frontendquery builder hit readonly replica option build frontendquery builder hit data warehouse weve built using keybase replication mechanism bigquerysnowflake etc option use third party tools like explo etc app stack react rails postgres used table contacts rows users custom fields trying build something scalable importantly spend months project result wondering viability option option one important point manage custom fields users created objects thinking contacts example thinking simply running joins across following tables contacts contactscustomfields companies related table query fields related objects contactscalculatedfields materialized view compute values many relationship like deals contacts two questions would managing readonly viable volume good starting point hit performance limits soon given volume managing custom fields way right way,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
262,Doing a Hard Delete in Fivetran,"Wondering if doing a hard delete in fivetran is possible without a dbt connector. I did my initial sync, go to transformations and can't figure out how to just add a sql statement to run after each sync.",1,0,2025-04-15 19:02:28,0,False,False,False,False,2025-04-15 19:02:28,19,Tuesday,38.0,203,60.65,2,58,0.0,0,0,NEGATIVE,-0.9989781379699707,"['wondering', 'hard', 'delete', 'fivetran', 'possible', 'without', 'dbt', 'connector', 'initial', 'sync', 'transformations', 'cant', 'figure', 'add', 'sql', 'statement', 'run', 'sync']",wondering hard delete fivetran possible without dbt connector initial sync transformations cant figure add sql statement run sync,Mid,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
263,Airflow or Prefect,"I've just started a data engineering project where I’m building a data pipeline using DuckDB and DBT, but I’m a bit unsure whether to go with Airflow or Prefect for orchestration. Any suggestions?",1,3,2025-04-15 18:44:36,1,False,False,False,False,2025-04-15 18:44:36,18,Tuesday,33.0,196,37.98,1,54,0.0,0,0,NEGATIVE,-0.9995250701904297,"['ive', 'started', 'data', 'engineering', 'project', 'building', 'data', 'pipeline', 'using', 'duckdb', 'dbt', 'bit', 'unsure', 'whether', 'airflow', 'prefect', 'orchestration', 'suggestions']",ive started data engineering project building data pipeline using duckdb dbt bit unsure whether airflow prefect orchestration suggestions,Mid,5,5,"engineering, experience, years, role, engineer, thanks, everyone, projects, year, learn, software, advice, skills, appreciate, feel, engineers, career, getting, take, started, recently, course, analyst, bit, starting, hello, position, tech, ill, share, roles, background, curious, exam, helpful, science, worth, employees, else, previous, hoping, past, developer, senior, pay, general, offer, modeling, forward, honestly, plan, industry, growth, thought, solid, less, screening, dagster, least, yrs, research, taken, offered, whether, transition, supposed, perspective, lead, towards, growing, scientist, certification, stay, anymore, moved, java, leave, consulting, grow, feeling, switching, expect, spent, market, salary, higher, junior, term, weeks, coding, opportunity, transitioning, constantly, told, infra, family, basics, videos, head, swe, expertise, statistics, overall, dba, opinion, decent, accepted, program, strategy, planning, sections, financial, mention, sub",9.5,8.8,169.5,-0.653,1031.9,52.7,9.7,266.1,11.1,20.2,15.4,60,115,33,"Hi all!

I am reviewing for interviews and the following question come to mind.

If **surrogate keys** are supposed to be unique identifiers that don't have real world meaning AND if **primary keys** are supposed to reliably identify and distinguish between each individual record (which also don't have real world meaning), then why will someone use a surrogate key? Wouldn't using primary keys be the same? Is there any case in which surrogate keys are the way to go?

P.S: Both surrogate and primary keys are auto generated by DB. Right?

P.S.1: I understand that a surrogate key doesn't necessarily have to be the a primary key, so considering that both have no real meaning outside the DB, then I wonder what the purpose of surrogate keys are.

P.S.2: At work (in different projects), we mainly use natural keys for analytical workloads and primary keys for uniquely identifying a given row. So I am wondering on which kind of cases/projects these surrogate keys will fit. 

",1,1
264,My Experience in preparing Azure Data Engineer Associate DP-203.,"So I recently appeared for the DP-203 certification by Microsoft and want to share my learnings and strategy that I followed to crack the exam.

As you all must already be knowing that this exam is labelled as “**Intermediate**” by Microsoft themselves which is perfect in my opinion. This exam does test you in the various concepts that are required for a data engineer to  master in his/her career. 

Having said that, it is not too hard to crack the exam but at the same time also not as easy as appearing for AZ-900. 

DP-203 is aimed at testing the understanding of data related concepts and various tools Microsoft has offered in its suite to make your life easier. Some topics include SQL, Modern Data Warehousing, Python, PySpark, Azure Data Factory, Azure Synapse Analytics, Azure Stream Analytics, Azure EventHubs, Azure Data Lake Storage and last but not the least Azure Databricks. You can go through the complete set of topics this exam focuses on here - [https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/?practice-assessment-type=certification#certification-take-the-exam](https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/?practice-assessment-type=certification#certification-take-the-exam)



**Courses:**

I had just taken this one course for DP-203 by Alan Rodrigues *(This is not a paid promotion. I just thought that these resources were good to refer to)* and this is a 24 hour long course which has covered all the important and core concepts clearly and precisely. What I loved the most about this course is that it is a complete hands-on course. One more thing is that the instructor very rarely mentions anything as “this has already been covered in the previous sections”. If there is anything that we are using in the current section he makes sure to give a quick background on what has been covered in the earlier sections. Why this is so important is because we tend to forget some things and by just getting a refresher in a couple of sentences we are up to speed. 

For those of you who don’t know, Microsoft offers access to majority resources if not all for FREE credit worth $200 for 30 days. So you simply have to sign up on their portal (insert link) and get access to all of them for 30 days. If you are residing in another country then convert dollars to your local currency. That is how much worth of free credit you will get for 30 days. 

**For example -** 

I live in India. 

1 $ = 87.789 INR 

So I got FREE credits worth 87.789 X 200 = Rs 17,557

Even when I appeared for the exam (Feb 8th, 2025) I hardly got 3-4 questions from the mock tests. But don’t get disheartened. Be sure you are consistent with your learning path and take notes whenever required. As I mentioned earlier, the exam is not very hard.

**Link -** [https://www.udemy.com/course/data-engineering-on-microsoft-azure/learn/lecture/44817315?start=40#overview](https://www.udemy.com/course/data-engineering-on-microsoft-azure/learn/lecture/44817315?start=40#overview)



**Mock Tests Resources:**

So I had referred a couple of resources for taking the mocks which I have mentioned below. *(This is not a paid promotion. I just thought that these resources were good to refer to.)*



1. **Udemy Practice Tests -** [https://www.udemy.com/course/practice-exams-microsoft-azure-dp-203-data-engineering/?couponCode=KEEPLEARNING](https://www.udemy.com/course/practice-exams-microsoft-azure-dp-203-data-engineering/?couponCode=KEEPLEARNING)
2. **Microsoft Practice Assessments -** [https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/practice/assessment?assessment-type=practice&assessmentId=49&practice-assessment-type=certification](https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/practice/assessment?assessment-type=practice&assessmentId=49&practice-assessment-type=certification)
3. [https://www.examtopics.com/exams/microsoft/dp-203/](https://www.examtopics.com/exams/microsoft/dp-203/)





**DO’s:**



1. Make sure that if and whenever possible you do hands-on for all the sections and videos that have been covered in the Udemy course as I am 100% sure that you will encounter certain errors and would have to explore and solve the errors by yourself. This will build a sense of confidence and achievement after being able to run the pipelines or code all by yourself. (Also don’t forget to delete or pause resources whenever needed so that you get a hang of it and don’t lose out on money. The instructor does tell you when to do so.)
2. Let’s be very practical, nobody remembers all the resolutions or solutions to every single issue or problem faced in the past. We tend to forget things over time and hence it is very important to document everything that you think is useful and would be important in the future. Maintain an excel sheet and create two columns “**Errors” and “Learnings/Resolution**” so that next time you encounter the same issue you already have a solution and don’t waste time. 
3. Watch and practice at least 5-10 videos daily. This way you can complete all the videos in a month and then go back and rewatch lessons you thought were hard. Then you can start giving practice tests. 





**DON'Ts:**



1. By heart all the MCQs or answers to the questions. 
2. Refer to many resources so much so that you will get overwhelmed and not be able to focus on preparation.
3. Even refer to multiple courses from different websites.





**Conclusion:**

All in all, just make sure you do your hands on, practice regularly, give a timeline for yourself, don’t mug up things, don’t by heart things, make sure you use limited but quality resources for learning and practice. I am sure that by following these things you will be able to crack the exam in the first attempt itself. ",1,1,2025-04-15 18:34:58,0,False,False,False,False,2025-04-15 18:34:58,18,Tuesday,832.0,5838,44.03,44,1405,11.7,1,0,POSITIVE,0.9744135141372681,"['recently', 'appeared', 'certification', 'microsoft', 'want', 'share', 'learnings', 'strategy', 'followed', 'crack', 'exam', 'must', 'already', 'knowing', 'exam', 'labelled', 'intermediate', 'microsoft', 'perfect', 'opinion', 'exam', 'test', 'various', 'concepts', 'required', 'data', 'engineer', 'master', 'hisher', 'career', 'said', 'hard', 'crack', 'exam', 'time', 'also', 'easy', 'appearing', 'aimed', 'testing', 'understanding', 'data', 'related', 'concepts', 'various', 'tools', 'microsoft', 'offered', 'suite', 'make', 'life', 'easier', 'topics', 'include', 'sql', 'modern', 'data', 'warehousing', 'python', 'pyspark', 'azure', 'data', 'factory', 'azure', 'synapse', 'analytics', 'azure', 'stream', 'analytics', 'azure', 'eventhubs', 'azure', 'data', 'lake', 'storage', 'last', 'least', 'azure', 'databricks', 'complete', 'set', 'topics', 'exam', 'focuses', 'httpslearnmicrosoftcomenuscredentialscertificationsazuredataengineerpracticeassessmenttypecertificationcertificationtaketheexamhttpslearnmicrosoftcomenuscredentialscertificationsazuredataengineerpracticeassessmenttypecertificationcertificationtaketheexam', 'courses', 'taken', 'one', 'course', 'alan', 'rodrigues', 'paid', 'promotion', 'thought', 'resources', 'good', 'refer', 'hour', 'long', 'course', 'covered', 'important', 'core', 'concepts', 'clearly', 'precisely', 'loved', 'course', 'complete', 'handson', 'course', 'one', 'thing', 'instructor', 'rarely', 'mentions', 'anything', 'already', 'covered', 'previous', 'sections', 'anything', 'using', 'current', 'section', 'makes', 'sure', 'give', 'quick', 'background', 'covered', 'earlier', 'sections', 'important', 'tend', 'forget', 'things', 'getting', 'refresher', 'couple', 'sentences', 'speed', 'dont', 'know', 'microsoft', 'offers', 'access', 'majority', 'resources', 'free', 'credit', 'worth', 'days', 'simply', 'sign', 'portal', 'insert', 'link', 'get', 'access', 'days', 'residing', 'another', 'country', 'convert', 'dollars', 'local', 'currency', 'much', 'worth', 'free', 'credit', 'get', 'days', 'example', 'live', 'india', 'inr', 'got', 'free', 'credits', 'worth', 'even', 'appeared', 'exam', 'feb', 'hardly', 'got', 'questions', 'mock', 'tests', 'dont', 'get', 'disheartened', 'sure', 'consistent', 'learning', 'path', 'take', 'notes', 'whenever', 'required', 'mentioned', 'earlier', 'exam', 'hard', 'link', 'httpswwwudemycomcoursedataengineeringonmicrosoftazurelearnlecturestartoverviewhttpswwwudemycomcoursedataengineeringonmicrosoftazurelearnlecturestartoverview', 'mock', 'tests', 'resources', 'referred', 'couple', 'resources', 'taking', 'mocks', 'mentioned', 'paid', 'promotion', 'thought', 'resources', 'good', 'refer', 'udemy', 'practice', 'tests', 'httpswwwudemycomcoursepracticeexamsmicrosoftazuredpdataengineeringcouponcodekeeplearninghttpswwwudemycomcoursepracticeexamsmicrosoftazuredpdataengineeringcouponcodekeeplearning', 'microsoft', 'practice', 'assessments', 'httpslearnmicrosoftcomenuscredentialscertificationsazuredataengineerpracticeassessmentassessmenttypepracticeassessmentidpracticeassessmenttypecertificationhttpslearnmicrosoftcomenuscredentialscertificationsazuredataengineerpracticeassessmentassessmenttypepracticeassessmentidpracticeassessmenttypecertification', 'httpswwwexamtopicscomexamsmicrosoftdphttpswwwexamtopicscomexamsmicrosoftdp', 'dos', 'make', 'sure', 'whenever', 'possible', 'handson', 'sections', 'videos', 'covered', 'udemy', 'course', 'sure', 'encounter', 'certain', 'errors', 'would', 'explore', 'solve', 'errors', 'build', 'sense', 'confidence', 'achievement', 'able', 'run', 'pipelines', 'code', 'also', 'dont', 'forget', 'delete', 'pause', 'resources', 'whenever', 'needed', 'get', 'hang', 'dont', 'lose', 'money', 'instructor', 'tell', 'lets', 'practical', 'nobody', 'remembers', 'resolutions', 'solutions', 'every', 'single', 'issue', 'problem', 'faced', 'past', 'tend', 'forget', 'things', 'time', 'hence', 'important', 'document', 'everything', 'think', 'useful', 'would', 'important', 'future', 'maintain', 'excel', 'sheet', 'create', 'two', 'columns', 'errors', 'learningsresolution', 'next', 'time', 'encounter', 'issue', 'already', 'solution', 'dont', 'waste', 'time', 'watch', 'practice', 'least', 'videos', 'daily', 'way', 'complete', 'videos', 'month', 'back', 'rewatch', 'lessons', 'thought', 'hard', 'start', 'giving', 'practice', 'tests', 'donts', 'heart', 'mcqs', 'answers', 'questions', 'refer', 'many', 'resources', 'much', 'get', 'overwhelmed', 'able', 'focus', 'preparation', 'even', 'refer', 'multiple', 'courses', 'different', 'websites', 'conclusion', 'make', 'sure', 'hands', 'practice', 'regularly', 'give', 'timeline', 'dont', 'mug', 'things', 'dont', 'heart', 'things', 'make', 'sure', 'use', 'limited', 'quality', 'resources', 'learning', 'practice', 'sure', 'following', 'things', 'able', 'crack', 'exam', 'first', 'attempt']",recently appeared certification microsoft want share learnings strategy followed crack exam must already knowing exam labelled intermediate microsoft perfect opinion exam test various concepts required data engineer master hisher career said hard crack exam time also easy appearing aimed testing understanding data related concepts various tools microsoft offered suite make life easier topics include sql modern data warehousing python pyspark azure data factory azure synapse analytics azure stream analytics azure eventhubs azure data lake storage last least azure databricks complete set topics exam focuses httpslearnmicrosoftcomenuscredentialscertificationsazuredataengineerpracticeassessmenttypecertificationcertificationtaketheexamhttpslearnmicrosoftcomenuscredentialscertificationsazuredataengineerpracticeassessmenttypecertificationcertificationtaketheexam courses taken one course alan rodrigues paid promotion thought resources good refer hour long course covered important core concepts clearly precisely loved course complete handson course one thing instructor rarely mentions anything already covered previous sections anything using current section makes sure give quick background covered earlier sections important tend forget things getting refresher couple sentences speed dont know microsoft offers access majority resources free credit worth days simply sign portal insert link get access days residing another country convert dollars local currency much worth free credit get days example live india inr got free credits worth even appeared exam feb hardly got questions mock tests dont get disheartened sure consistent learning path take notes whenever required mentioned earlier exam hard link httpswwwudemycomcoursedataengineeringonmicrosoftazurelearnlecturestartoverviewhttpswwwudemycomcoursedataengineeringonmicrosoftazurelearnlecturestartoverview mock tests resources referred couple resources taking mocks mentioned paid promotion thought resources good refer udemy practice tests httpswwwudemycomcoursepracticeexamsmicrosoftazuredpdataengineeringcouponcodekeeplearninghttpswwwudemycomcoursepracticeexamsmicrosoftazuredpdataengineeringcouponcodekeeplearning microsoft practice assessments httpslearnmicrosoftcomenuscredentialscertificationsazuredataengineerpracticeassessmentassessmenttypepracticeassessmentidpracticeassessmenttypecertificationhttpslearnmicrosoftcomenuscredentialscertificationsazuredataengineerpracticeassessmentassessmenttypepracticeassessmentidpracticeassessmenttypecertification httpswwwexamtopicscomexamsmicrosoftdphttpswwwexamtopicscomexamsmicrosoftdp dos make sure whenever possible handson sections videos covered udemy course sure encounter certain errors would explore solve errors build sense confidence achievement able run pipelines code also dont forget delete pause resources whenever needed get hang dont lose money instructor tell lets practical nobody remembers resolutions solutions every single issue problem faced past tend forget things time hence important document everything think useful would important future maintain excel sheet create two columns errors learningsresolution next time encounter issue already solution dont waste time watch practice least videos daily way complete videos month back rewatch lessons thought hard start giving practice tests donts heart mcqs answers questions refer many resources much get overwhelmed able focus preparation even refer multiple courses different websites conclusion make sure hands practice regularly give timeline dont mug things dont heart things make sure use limited quality resources learning practice sure following things able crack exam first attempt,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
265,Spark UI DAG,Just wanted ro understand if after doing an union I want to write to S3 as parquet.  Why do I see 76 task ? Is it because union actually partitioned the data ? I tried doing salting after union still I see 76 tasks for a given stage. Perhaps I see it is read parquet I am guessing something to do with committed whixh creates a temporary folder before writing to s3. Any help is appreciated. Please note I don't have access to the spark UI to debug the DAG. I have manged to give print statements and that I where I am trying to  corelate. ,1,0,2025-04-15 18:03:08,0,False,False,False,False,2025-04-15 18:03:08,18,Tuesday,106.0,540,75.2,8,148,8.1,0,0,NEGATIVE,-0.9944717884063721,"['wanted', 'understand', 'union', 'want', 'write', 'parquet', 'see', 'task', 'union', 'actually', 'partitioned', 'data', 'tried', 'salting', 'union', 'still', 'see', 'tasks', 'given', 'stage', 'perhaps', 'see', 'read', 'parquet', 'guessing', 'something', 'committed', 'whixh', 'creates', 'temporary', 'folder', 'writing', 'help', 'appreciated', 'please', 'note', 'dont', 'access', 'spark', 'debug', 'dag', 'manged', 'give', 'print', 'statements', 'trying', 'corelate']",wanted understand union want write parquet see task union actually partitioned data tried salting union still see tasks given stage perhaps see read parquet guessing something committed whixh creates temporary folder writing help appreciated please note dont access spark debug dag manged give print statements trying corelate,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
266,"bigquery/sheet/tableau, need for advice","Hello everyone,

I recently joined a project that uses BigQuery for data storage, dbt for transformations, and Tableau for dashboarding. I'd like some advice on improving our current setup.

# Current Architecture

* Data pipelines run transformations using dbt
* Data from BigQuery is synchronized to Google Sheets
* Tableau reports connect to these Google Sheets (not directly to BigQuery)
* Users can modify tracking values directly in Google Sheets

# The Problems

1. **Manual Process**: Currently, the Google Sheets and Tableau connections are created manually during development
2. **Authentication Issues**: In development, Tableau connects using the individual developer's account credentials
3. **Orchestration Concerns**: We have Google Cloud Composer for orchestration, but the Google Sheets synchronization happens separately

# Questions

1. What's the best way to automate the creation and configuration of Google Sheets in this workflow? Is there a Terraform approach or another IaC solution?
2. How should we properly manage connection strings in tableau between environments, especially when moving from development (using personal accounts) to production?

Any insights from those who have worked with similar setups would be greatly appreciated!",1,0,2025-04-15 15:24:07,0,False,False,False,False,2025-04-15 15:24:07,15,Tuesday,182.0,1265,28.33,10,335,14.7,0,0,NEGATIVE,-0.9971157312393188,"['hello', 'everyone', 'recently', 'joined', 'project', 'uses', 'bigquery', 'data', 'storage', 'dbt', 'transformations', 'tableau', 'dashboarding', 'like', 'advice', 'improving', 'current', 'setup', 'current', 'architecture', 'data', 'pipelines', 'run', 'transformations', 'using', 'dbt', 'data', 'bigquery', 'synchronized', 'google', 'sheets', 'tableau', 'reports', 'connect', 'google', 'sheets', 'directly', 'bigquery', 'users', 'modify', 'tracking', 'values', 'directly', 'google', 'sheets', 'problems', 'manual', 'process', 'currently', 'google', 'sheets', 'tableau', 'connections', 'created', 'manually', 'development', 'authentication', 'issues', 'development', 'tableau', 'connects', 'using', 'individual', 'developers', 'account', 'credentials', 'orchestration', 'concerns', 'google', 'cloud', 'composer', 'orchestration', 'google', 'sheets', 'synchronization', 'happens', 'separately', 'questions', 'whats', 'best', 'way', 'automate', 'creation', 'configuration', 'google', 'sheets', 'workflow', 'terraform', 'approach', 'another', 'iac', 'solution', 'properly', 'manage', 'connection', 'strings', 'tableau', 'environments', 'especially', 'moving', 'development', 'using', 'personal', 'accounts', 'production', 'insights', 'worked', 'similar', 'setups', 'would', 'greatly', 'appreciated']",hello everyone recently joined project uses bigquery data storage dbt transformations tableau dashboarding like advice improving current setup current architecture data pipelines run transformations using dbt data bigquery synchronized google sheets tableau reports connect google sheets directly bigquery users modify tracking values directly google sheets problems manual process currently google sheets tableau connections created manually development authentication issues development tableau connects using individual developers account credentials orchestration concerns google cloud composer orchestration google sheets synchronization happens separately questions whats best way automate creation configuration google sheets workflow terraform approach another iac solution properly manage connection strings tableau environments especially moving development using personal accounts production insights worked similar setups would greatly appreciated,Mid,1,1,"using, sql, azure, pipelines, database, pipeline, built, create, set, processing, storage, running, server, local, duckdb, power, handle, performance, support, memory, simple, queries, bigquery, complex, databases, via, key, users, glue, features, works, functions, directly, application, processes, catalog, setup, realtime, reports, lambda, connector, pandas, add, arrow, main, manage, gateway, efficient, tableau, function, including, transformation, integration, specifically, gcp, apis, connect, stream, dashboard, postgresql, workflows, capabilities, gizmosql, frontend, test, faster, automate, supports, transformations, easily, check, streaming, output, engine, initial, shared, serverless, document, embedded, perform, interactive, format, querying, visualization, syntax, builder, robust, dependencies, compute, dezoomcamp, connectors, powerful, queue, cloudbased, locally, instance, workflow, flask, apache, cli, library, notebooks, machine, libraries, provide, allows, sheets, studio, essentially, looker, airbnb, scale, extraction, sqlite, direct, popular, existing, host, readme, daton, sqlflow, bunch, environments, adopt, include, relevant, download, relying, proposed, frequently, tested, loads, effort, challenges, alternatives, larger, ipc, designing, graph, native, runtime, potential, cicd, analytical, mongodb, scalable, handling, central",9.4,7.8,156.9,-0.624,962.2,47.5,9.0,247.7,10.7,20.5,15.7,63,122,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",2,3
267,Is it possible to generate an open-table/metadata store that combines multiple data sources?,"I've recently learned about open-table paradigm, which if I am interpreting correctly, is essentially a mechanism for storing metadata so that the data associated with it can be efficiently looked up and retrieved. (Please correct this understanding if it is wrong). 

My question is whether or not you could have a single metadata store or open-table that combines metadata from two different storage solutions, so that you could query both from a single CLI tool using SQL like syntax? 

And as a follow on question... I've learned about and played with AWS Athena in an online course. It uses Glue Crawler to somehow discover metadata. Is this based on an open-table paradigm? Or a different technology? ",1,3,2025-04-15 14:39:22,0,False,False,False,False,2025-04-15 14:39:22,14,Tuesday,116.0,707,56.76,8,187,12.4,0,0,NEGATIVE,-0.994753360748291,"['ive', 'recently', 'learned', 'opentable', 'paradigm', 'interpreting', 'correctly', 'essentially', 'mechanism', 'storing', 'metadata', 'data', 'associated', 'efficiently', 'looked', 'retrieved', 'please', 'correct', 'understanding', 'wrong', 'question', 'whether', 'could', 'single', 'metadata', 'store', 'opentable', 'combines', 'metadata', 'two', 'different', 'storage', 'solutions', 'could', 'query', 'single', 'cli', 'tool', 'using', 'sql', 'like', 'syntax', 'follow', 'question', 'ive', 'learned', 'played', 'aws', 'athena', 'online', 'course', 'uses', 'glue', 'crawler', 'somehow', 'discover', 'metadata', 'based', 'opentable', 'paradigm', 'different', 'technology']",ive recently learned opentable paradigm interpreting correctly essentially mechanism storing metadata data associated efficiently looked retrieved please correct understanding wrong question whether could single metadata store opentable combines metadata two different storage solutions could query single cli tool using sql like syntax follow question ive learned played aws athena online course uses glue crawler somehow discover metadata based opentable paradigm different technology,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
268,API Help,"Hello, I am working on a personal ETL project with a beginning goal of trying to ingest data from Google Books API and batch insert into pg.  


Currently I have a script that cleans the API result into a list which is then inserted into pg. But, I have many repeat values each time I run this query, resulting in no data being inserted into pg.

I also notice that I get very random books that are not at all on topic for what I specific with my query parameters. e.g. title='data' and author=' '. 

  
I am wondering if anybody knows how to get only relevant data with API calls, as well as non duplicate value with each run of the script (eg persistent pagination).

  
Example of a \~320 book query.

In the first result I get somewhat data-related books. However, in the second result i get results such as: ""Homoeopathic Journal of Obstetrics, Gynaecology and Paedology"".

I understand that this is a broad query, but when I specify I end up getting very few book results(\~40-80), which is surprising because I figured a Google API would have more data. 

I may be doing this wrong, but any advice is very much appreciated.

    ❯ python3 apiClean.py
    The selfLink we get data from: https://www.googleapis.com/books/v1/volumes?q=data+inauthor:&startIndex=0&maxResults=40&printType=books&fields=items(selfLink)&key=AIzaSyDirSZjmIfQTvYgCnUZ0BhbIlrKRF8qxHw
    
    ...
    
    The selfLink we get data from: https://www.googleapis.com/books/v1/volumes?q=data+inauthor:&startIndex=240&maxResults=40&printType=books&fields=items(selfLink)&key=AIzaSyDirSZjmIfQTvYgCnUZ0BhbIlrKRF8qxHw
    
    size of result rv:320",1,1,2025-04-15 14:30:37,0,False,False,False,False,2025-04-15 14:30:37,14,Tuesday,225.0,1620,38.42,14,397,12.0,1,1,NEGATIVE,-0.9378097653388977,"['hello', 'working', 'personal', 'etl', 'project', 'beginning', 'goal', 'trying', 'ingest', 'data', 'google', 'books', 'api', 'batch', 'insert', 'currently', 'script', 'cleans', 'api', 'result', 'list', 'inserted', 'many', 'repeat', 'values', 'time', 'run', 'query', 'resulting', 'data', 'inserted', 'also', 'notice', 'get', 'random', 'books', 'topic', 'specific', 'query', 'parameters', 'titledata', 'author', 'wondering', 'anybody', 'knows', 'get', 'relevant', 'data', 'api', 'calls', 'well', 'non', 'duplicate', 'value', 'run', 'script', 'persistent', 'pagination', 'example', 'book', 'query', 'first', 'result', 'get', 'somewhat', 'datarelated', 'books', 'however', 'second', 'result', 'get', 'results', 'homoeopathic', 'journal', 'obstetrics', 'gynaecology', 'paedology', 'understand', 'broad', 'query', 'specify', 'end', 'getting', 'book', 'results', 'surprising', 'figured', 'google', 'api', 'would', 'data', 'may', 'wrong', 'advice', 'much', 'appreciated', 'python', 'apicleanpy', 'selflink', 'get', 'data', 'httpswwwgoogleapiscombooksvvolumesqdatainauthorstartindexmaxresultsprinttypebooksfieldsitemsselflinkkeyaizasydirszjmifqtvygcnuzbhbilrkrfqxhw', 'selflink', 'get', 'data', 'httpswwwgoogleapiscombooksvvolumesqdatainauthorstartindexmaxresultsprinttypebooksfieldsitemsselflinkkeyaizasydirszjmifqtvygcnuzbhbilrkrfqxhw', 'size', 'result']",hello working personal etl project beginning goal trying ingest data google books api batch insert currently script cleans api result list inserted many repeat values time run query resulting data inserted also notice get random books topic specific query parameters titledata author wondering anybody knows get relevant data api calls well non duplicate value run script persistent pagination example book query first result get somewhat datarelated books however second result get results homoeopathic journal obstetrics gynaecology paedology understand broad query specify end getting book results surprising figured google api would data may wrong advice much appreciated python apicleanpy selflink get data httpswwwgoogleapiscombooksvvolumesqdatainauthorstartindexmaxresultsprinttypebooksfieldsitemsselflinkkeyaizasydirszjmifqtvygcnuzbhbilrkrfqxhw selflink get data httpswwwgoogleapiscombooksvvolumesqdatainauthorstartindexmaxresultsprinttypebooksfieldsitemsselflinkkeyaizasydirszjmifqtvygcnuzbhbilrkrfqxhw size result,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
269,Need advice - Informatica production support,"Hi , i have working as a informatica production support where i need to monitor ETL jobs on daily basis and report the bottlenecks to the developer to fix the issue and im getting $9.5k/year with 5 YOE. rightnow its kind of boring and planning to move to informatica powercenter admin position since its not opensource its hard for me to self learn myself. just want to know any opensource tools related to data integration that has high in demand for administrator role would be great. ",1,4,2025-04-15 14:20:01,0,False,False,False,False,2025-04-15 14:20:01,14,Tuesday,86.0,487,49.86,4,132,12.6,0,0,NEGATIVE,-0.9988116025924683,"['working', 'informatica', 'production', 'support', 'need', 'monitor', 'etl', 'jobs', 'daily', 'basis', 'report', 'bottlenecks', 'developer', 'fix', 'issue', 'getting', 'kyear', 'yoe', 'rightnow', 'kind', 'boring', 'planning', 'move', 'informatica', 'powercenter', 'admin', 'position', 'since', 'opensource', 'hard', 'self', 'learn', 'want', 'know', 'opensource', 'tools', 'related', 'data', 'integration', 'high', 'demand', 'administrator', 'role', 'would', 'great']",working informatica production support need monitor etl jobs daily basis report bottlenecks developer fix issue getting kyear yoe rightnow kind boring planning move informatica powercenter admin position since opensource hard self learn want know opensource tools related data integration high demand administrator role would great,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
270,Database design problem for many to many data relationship...need suggestions,"I have to come up with a database design working on postgres. I have to migrate at the end almost trillions volumes of data into a postgres DB wherein CRUD operations can be run most efficiently. The data present is in the form of a many to many relationship. How the data looks is:  
  
In my old data base i have a value T1 which is connected to on average 700 values (like x1,x2,x3...x700). Here in the old DB we are saving 700 records of this connection. Similarly other values like T2,T3,T100 all have multiple connections each having a separate row

Use case:  
We need to make updates,deletions and inserts to both values of T and values of X  
for example,  
I am given That value T1 instead of 700 connections of X has now 800 connections...so i must update or insert all the new connections corresponding to T1  
And like wise if I am given , we need to update all T values X1 (say X1 has 200 connection of T) i need to insert/update or delete T values associated with X1.

  
for now, I was thinking of aggregating my data in the form of a jsonb column  
where  
Column T             Column X (jsonb)  
T1                           {""value"":\[X1,X2,X3.....X700\]}

But i will have to create another similar table where i keep column T as jsonb. Since any updates in one table needs to be synced to the other any errors may cause it to be out of sync.

Also the time taken to read and update a jsonb row will be high

Any other suggestions on how i should think about creating schema for my problem?",1,2,2025-04-15 09:41:51,0,False,False,False,False,2025-04-15 09:41:51,9,Tuesday,274.0,1509,63.22,11,394,11.7,0,0,NEGATIVE,-0.9980979561805725,"['come', 'database', 'design', 'working', 'postgres', 'migrate', 'end', 'almost', 'trillions', 'volumes', 'data', 'postgres', 'wherein', 'crud', 'operations', 'run', 'efficiently', 'data', 'present', 'form', 'many', 'many', 'relationship', 'data', 'looks', 'old', 'data', 'base', 'value', 'connected', 'average', 'values', 'like', 'xxxx', 'old', 'saving', 'records', 'connection', 'similarly', 'values', 'like', 'ttt', 'multiple', 'connections', 'separate', 'row', 'use', 'case', 'need', 'make', 'updatesdeletions', 'inserts', 'values', 'values', 'example', 'given', 'value', 'instead', 'connections', 'connectionsso', 'must', 'update', 'insert', 'new', 'connections', 'corresponding', 'like', 'wise', 'given', 'need', 'update', 'values', 'say', 'connection', 'need', 'insertupdate', 'delete', 'values', 'associated', 'thinking', 'aggregating', 'data', 'form', 'jsonb', 'column', 'column', 'column', 'jsonb', 'valuexxxx', 'create', 'another', 'similar', 'table', 'keep', 'column', 'jsonb', 'since', 'updates', 'one', 'table', 'needs', 'synced', 'errors', 'may', 'cause', 'sync', 'also', 'time', 'taken', 'read', 'update', 'jsonb', 'row', 'high', 'suggestions', 'think', 'creating', 'schema', 'problem']",come database design working postgres migrate end almost trillions volumes data postgres wherein crud operations run efficiently data present form many many relationship data looks old data base value connected average values like xxxx old saving records connection similarly values like ttt multiple connections separate row use case need make updatesdeletions inserts values values example given value instead connections connectionsso must update insert new connections corresponding like wise given need update values say connection need insertupdate delete values associated thinking aggregating data form jsonb column column column jsonb valuexxxx create another similar table keep column jsonb since updates one table needs synced errors may cause sync also time taken read update jsonb row high suggestions think creating schema problem,Mid,3,3,"data, use, would, like, get, want, also, way, etc, make, better, something, different, one, things, lot, see, team, trying, build, building, think, architecture, tool, spark, might, process, run, approach, good, find, able, around, start, thats, analytics, youre, system, done, similar, insights, two, feedback, solutions, many, multiple, open, easy, another, issue, quite, jobs, however, code, actually, warehouse, tasks, problem, access, management, teams, clickhouse, useful, dashboards, within, moving, iceberg, common, creating, others, costs, related, tests, come, production, high, fit, sense, drop, super, options, prefer, helps, opensource, clean, results, along, necessary, following, thinking, several, basically, gets, link, internal, point, second, pain, basic, let, base, whole, report, due, books, points, asked, quality, week, fivetran, limited, rather, apply, struggle, smaller, side, expensive, call, certain, entire, goal, theyre, install, important, understanding, match, service, website, top, figure, interested, ways, saas, writing, hit, painful, significant, written, pass, struggling, historic, wed, require, wont, oracle, blog, half, youve, marketing, seeing, purpose, ingest, stage, low, entry, party, adf, managing, required, ingesting, perhaps, windows, analyze, concepts, warehouses, stat, ecommerce, fix, designed, normally, regularly, video, businesses, rest, quickly, sharing, cons, present, email, topic, discussions, ever, obvious, ready, benchmark, visual, standard",8.0,7.2,144.7,-0.636,885.0,43.9,8.3,228.1,10.1,20.7,13.7,68,144,59,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,5
271,"Event sourcing isn’t about storing history. it’s about replaying it.
Discussion","Replay isn’t just about fixing broken systems. It’s about rethinking how we build them in the first place. If your data architecture is driven by immutable events instead of current state, then replay stops being a recovery mechanism and starts becoming a way to continuously reshape, refine, and evolve your system with zero fear of breaking things.

Let’s talk about replay :)

**Event sourcing is misunderstood**  
For most developers, event sourcing shows up as a safety mechanism. It’s there to recover from a failure, rebuild a read model, trace an audit trail, or get through a schema change without too much pain. Replay is something you reach for in the rare cases when things go sideways.

That’s how it’s typically treated. A fallback. Something reactive.

But that lens is narrow. It frames replay as an emergency tool instead of something more fundamental. When events are treated as the source of truth, replay can become a normal, repeatable part of development. Not just a way to recover, but a way to refine.

**What if replay wasn’t just for emergencies?**  
What if it was a routine, even joyful, part of building your system?

Instead of treating replay as a recovery mechanism, you treat it as a development tool. Something you use to evolve your data models, improve your business logic, and shape entirely new views of your data over time. And more excitingly, it means you can derive entirely new schemas from your event history whenever your needs change.

**Why replay is so hard in most setups**  
Here’s the catch. In most event-sourced systems, **events are emitted after your app logic runs**. Your API gets the request, updates the database, and only then emits a change event. That event is a side effect, not the source of truth.

So when you want to replay, it gets tricky. You need replay-safe logic. You need to carefully version events. You need infrastructure to reprocess historical data. And you have to make absolutely sure you’re not double-applying anything.

That’s why replay often feels fragile. It’s not that the idea is bad. It’s just hard to pull off.

**But what if you flip the model?**  
What if events come first, not last?

That’s the approach we took.

A user action, like creating a user, updating an address, or assigning a tag, sends an event. That event is immediately appended to an immutable event store, **and only then** is it passed along to the application API to validate and store in the database.

Suddenly your database isn’t your source of truth. It’s just a read model. A fast, disposable output of your event stream.

So when you want to evolve your logic or reshape your data structure, all you have to do is update your flow, delete the old database, and press replay.

That’s it.

No migrations.  
No fragile ETL jobs.  
No one-off backfills.  
Just replay your history into the new shape.

**Your data becomes fluid**  
Say you’re running an e-commerce platform, and six months in, you realize you never tracked the discount code a customer used at checkout. It wasn’t part of the original schema. Normally, this would mean a migration, a painful manual backfill (if the data even still exists), or writing a fragile script to stitch it in later, assuming you’re lucky enough to recover it.

But with a full event history, you don’t need to hack anything.

You just update your flow logic to extract the discount code from the original checkout events. Then replay them.

Within minutes, your entire dataset is updated. The new field is populated everywhere it should have been, as if it had been there from day one.

**Your database becomes what it was always meant to be**  
A cache.  
Not a source of truth.  
Something you can throw away and rebuild without fear.  
You stop treating your schema like a delicate glass sculpture and start treating it like software.

**Replay unlocks AI-native data (with MCP Servers)**  
Most application databases are optimized for transactions, not understanding. They’re normalized, rigid, and shaped around application logic, not meaning. That’s fine for serving an app. But for AI? Nope.

Language models thrive on context. They need denormalized, readable structures. They need relationships spelled out. They need the why, not just the what.

When you have an event history, not just state but actions and intent. You can replay those events into entirely new shapes. You can build read models that are tailored specifically for AI: flattened tables for semantic search, user-centric structures for chat interfaces, agent-friendly layouts for reasoning.

And it’s not just one-and-done. You can reshape your models over and over as your use cases evolve. No migrations. No backfills. Just a new flow and a replay.

What is even more interesting is that with the help of MCP Servers AI can help you do this. By interrogating the event history with natural language prompts, it can suggest new model structures, flag gaps, and uncover meaning you didn’t plan for. It’s a feedback loop: replay helps AI make sense of your data, and AI helps you decide how to replay.

And none of this works without events that store intent. Current state is just a snapshot. Events tell the story.

**So, why doesn’t everyone build this way?**  
Because it’s hard. You need immutable storage. Replay-safe logic. Tools to build and maintain read models. Schema evolution support. Observability. Infrastructure to safely reprocess everything.

The architecture has been around for a while — Martin Fowler helped popularize event sourcing nearly two decades ago. But most teams ran into the same issue: implementing it well was too complex for everyday use.

That’s the reason behind the Flowcore Platform To make this kind of architecture not just possible, but effortless. Flowcore handles the messy parts. The ingestion, the immutability, the reprocessing, the flow management, the replay. So you can just build. You send an event, define what you want done with it, and replay it whenever you need to improve.",0,3,2025-04-15 15:55:44,0,False,2025-04-15 16:09:47,False,False,2025-04-15 15:55:44,15,Tuesday,995.0,6011,68.06,85,1530,9.8,0,0,NEGATIVE,-0.9901841282844543,"['replay', 'isnt', 'fixing', 'broken', 'systems', 'rethinking', 'build', 'first', 'place', 'data', 'architecture', 'driven', 'immutable', 'events', 'instead', 'current', 'state', 'replay', 'stops', 'recovery', 'mechanism', 'starts', 'becoming', 'way', 'continuously', 'reshape', 'refine', 'evolve', 'system', 'zero', 'fear', 'breaking', 'things', 'lets', 'talk', 'replay', 'event', 'sourcing', 'misunderstood', 'developers', 'event', 'sourcing', 'shows', 'safety', 'mechanism', 'recover', 'failure', 'rebuild', 'read', 'model', 'trace', 'audit', 'trail', 'get', 'schema', 'change', 'without', 'much', 'pain', 'replay', 'something', 'reach', 'rare', 'cases', 'things', 'sideways', 'thats', 'typically', 'treated', 'fallback', 'something', 'reactive', 'lens', 'narrow', 'frames', 'replay', 'emergency', 'tool', 'instead', 'something', 'fundamental', 'events', 'treated', 'source', 'truth', 'replay', 'become', 'normal', 'repeatable', 'part', 'development', 'way', 'recover', 'way', 'refine', 'replay', 'wasnt', 'emergencies', 'routine', 'even', 'joyful', 'part', 'building', 'system', 'instead', 'treating', 'replay', 'recovery', 'mechanism', 'treat', 'development', 'tool', 'something', 'use', 'evolve', 'data', 'models', 'improve', 'business', 'logic', 'shape', 'entirely', 'new', 'views', 'data', 'time', 'excitingly', 'means', 'derive', 'entirely', 'new', 'schemas', 'event', 'history', 'whenever', 'needs', 'change', 'replay', 'hard', 'setups', 'heres', 'catch', 'eventsourced', 'systems', 'events', 'emitted', 'app', 'logic', 'runs', 'api', 'gets', 'request', 'updates', 'database', 'emits', 'change', 'event', 'event', 'side', 'effect', 'source', 'truth', 'want', 'replay', 'gets', 'tricky', 'need', 'replaysafe', 'logic', 'need', 'carefully', 'version', 'events', 'need', 'infrastructure', 'reprocess', 'historical', 'data', 'make', 'absolutely', 'sure', 'youre', 'doubleapplying', 'anything', 'thats', 'replay', 'often', 'feels', 'fragile', 'idea', 'bad', 'hard', 'pull', 'flip', 'model', 'events', 'come', 'first', 'last', 'thats', 'approach', 'took', 'user', 'action', 'like', 'creating', 'user', 'updating', 'address', 'assigning', 'tag', 'sends', 'event', 'event', 'immediately', 'appended', 'immutable', 'event', 'store', 'passed', 'along', 'application', 'api', 'validate', 'store', 'database', 'suddenly', 'database', 'isnt', 'source', 'truth', 'read', 'model', 'fast', 'disposable', 'output', 'event', 'stream', 'want', 'evolve', 'logic', 'reshape', 'data', 'structure', 'update', 'flow', 'delete', 'old', 'database', 'press', 'replay', 'thats', 'migrations', 'fragile', 'etl', 'jobs', 'oneoff', 'backfills', 'replay', 'history', 'new', 'shape', 'data', 'becomes', 'fluid', 'say', 'youre', 'running', 'ecommerce', 'platform', 'six', 'months', 'realize', 'never', 'tracked', 'discount', 'code', 'customer', 'used', 'checkout', 'wasnt', 'part', 'original', 'schema', 'normally', 'would', 'mean', 'migration', 'painful', 'manual', 'backfill', 'data', 'even', 'still', 'exists', 'writing', 'fragile', 'script', 'stitch', 'later', 'assuming', 'youre', 'lucky', 'enough', 'recover', 'full', 'event', 'history', 'dont', 'need', 'hack', 'anything', 'update', 'flow', 'logic', 'extract', 'discount', 'code', 'original', 'checkout', 'events', 'replay', 'within', 'minutes', 'entire', 'dataset', 'updated', 'new', 'field', 'populated', 'everywhere', 'day', 'one', 'database', 'becomes', 'always', 'meant', 'cache', 'source', 'truth', 'something', 'throw', 'away', 'rebuild', 'without', 'fear', 'stop', 'treating', 'schema', 'like', 'delicate', 'glass', 'sculpture', 'start', 'treating', 'like', 'software', 'replay', 'unlocks', 'ainative', 'data', 'mcp', 'servers', 'application', 'databases', 'optimized', 'transactions', 'understanding', 'theyre', 'normalized', 'rigid', 'shaped', 'around', 'application', 'logic', 'meaning', 'thats', 'fine', 'serving', 'app', 'nope', 'language', 'models', 'thrive', 'context', 'need', 'denormalized', 'readable', 'structures', 'need', 'relationships', 'spelled', 'need', 'event', 'history', 'state', 'actions', 'intent', 'replay', 'events', 'entirely', 'new', 'shapes', 'build', 'read', 'models', 'tailored', 'specifically', 'flattened', 'tables', 'semantic', 'search', 'usercentric', 'structures', 'chat', 'interfaces', 'agentfriendly', 'layouts', 'reasoning', 'oneanddone', 'reshape', 'models', 'use', 'cases', 'evolve', 'migrations', 'backfills', 'new', 'flow', 'replay', 'even', 'interesting', 'help', 'mcp', 'servers', 'help', 'interrogating', 'event', 'history', 'natural', 'language', 'prompts', 'suggest', 'new', 'model', 'structures', 'flag', 'gaps', 'uncover', 'meaning', 'didnt', 'plan', 'feedback', 'loop', 'replay', 'helps', 'make', 'sense', 'data', 'helps', 'decide', 'replay', 'none', 'works', 'without', 'events', 'store', 'intent', 'current', 'state', 'snapshot', 'events', 'tell', 'story', 'doesnt', 'everyone', 'build', 'way', 'hard', 'need', 'immutable', 'storage', 'replaysafe', 'logic', 'tools', 'build', 'maintain', 'read', 'models', 'schema', 'evolution', 'support', 'observability', 'infrastructure', 'safely', 'reprocess', 'everything', 'architecture', 'around', 'martin', 'fowler', 'helped', 'popularize', 'event', 'sourcing', 'nearly', 'two', 'decades', 'ago', 'teams', 'ran', 'issue', 'implementing', 'well', 'complex', 'everyday', 'use', 'thats', 'reason', 'behind', 'flowcore', 'platform', 'make', 'kind', 'architecture', 'possible', 'effortless', 'flowcore', 'handles', 'messy', 'parts', 'ingestion', 'immutability', 'reprocessing', 'flow', 'management', 'replay', 'build', 'send', 'event', 'define', 'want', 'done', 'replay', 'whenever', 'need', 'improve']",replay isnt fixing broken systems rethinking build first place data architecture driven immutable events instead current state replay stops recovery mechanism starts becoming way continuously reshape refine evolve system zero fear breaking things lets talk replay event sourcing misunderstood developers event sourcing shows safety mechanism recover failure rebuild read model trace audit trail get schema change without much pain replay something reach rare cases things sideways thats typically treated fallback something reactive lens narrow frames replay emergency tool instead something fundamental events treated source truth replay become normal repeatable part development way recover way refine replay wasnt emergencies routine even joyful part building system instead treating replay recovery mechanism treat development tool something use evolve data models improve business logic shape entirely new views data time excitingly means derive entirely new schemas event history whenever needs change replay hard setups heres catch eventsourced systems events emitted app logic runs api gets request updates database emits change event event side effect source truth want replay gets tricky need replaysafe logic need carefully version events need infrastructure reprocess historical data make absolutely sure youre doubleapplying anything thats replay often feels fragile idea bad hard pull flip model events come first last thats approach took user action like creating user updating address assigning tag sends event event immediately appended immutable event store passed along application api validate store database suddenly database isnt source truth read model fast disposable output event stream want evolve logic reshape data structure update flow delete old database press replay thats migrations fragile etl jobs oneoff backfills replay history new shape data becomes fluid say youre running ecommerce platform six months realize never tracked discount code customer used checkout wasnt part original schema normally would mean migration painful manual backfill data even still exists writing fragile script stitch later assuming youre lucky enough recover full event history dont need hack anything update flow logic extract discount code original checkout events replay within minutes entire dataset updated new field populated everywhere day one database becomes always meant cache source truth something throw away rebuild without fear stop treating schema like delicate glass sculpture start treating like software replay unlocks ainative data mcp servers application databases optimized transactions understanding theyre normalized rigid shaped around application logic meaning thats fine serving app nope language models thrive context need denormalized readable structures need relationships spelled need event history state actions intent replay events entirely new shapes build read models tailored specifically flattened tables semantic search usercentric structures chat interfaces agentfriendly layouts reasoning oneanddone reshape models use cases evolve migrations backfills new flow replay even interesting help mcp servers help interrogating event history natural language prompts suggest new model structures flag gaps uncover meaning didnt plan feedback loop replay helps make sense data helps decide replay none works without events store intent current state snapshot events tell story doesnt everyone build way hard need immutable storage replaysafe logic tools build maintain read models schema evolution support observability infrastructure safely reprocess everything architecture around martin fowler helped popularize event sourcing nearly two decades ago teams ran issue implementing well complex everyday use thats reason behind flowcore platform make kind architecture possible effortless flowcore handles messy parts ingestion immutability reprocessing flow management replay build send event define want done replay whenever need improve,Low,0,0,"need, table, event, source, without, instead, every, replay, day, example, single, change, per, business, daily, based, keys, manually, model, keep, needs, events, doesnt, name, list, option, user, must, takes, history, often, truth, full, column, hour, address, requirements, date, reading, track, whenever, metrics, result, update, feature, primary, columns, meaning, pricing, structure, details, rows, simply, available, dedicated, country, means, facebook, error, missing, evolve, involves, names, size, domain, place, addresses, timestamp, parse, mean, slow, schemas, heres, return, dimensions, delete, state, times, arent, ground, errors, receive, min, added, geocoding, shape, sourcing, wasnt, insert, minutes, value, flow, save, normal, little, region, usually, bad, becomes, failure, replication, entirely, products, infrastructure, minute, person, average, servers, internet, looked, probably, campaigns, workloads, looks",8.2,7.4,175.5,-0.663,1069.0,47.9,9.8,275.5,11.4,22.2,15.5,52,108,34,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",4,4
272,Kimball's Approach In Harry Potter Style,"Checkout this amazing post about Kimball's Approach 

[https://medium.com/@adityasharmah27/kimballs-approach-the-sorcerer-s-stone-of-data-warehousing-9658f292eeb4](https://medium.com/@adityasharmah27/kimballs-approach-the-sorcerer-s-stone-of-data-warehousing-9658f292eeb4)",0,2,2025-04-15 10:05:17,0,False,False,False,False,2025-04-15 10:05:17,10,Tuesday,8.0,272,-308.89,1,48,0.0,1,0,POSITIVE,0.9978705644607544,"['checkout', 'amazing', 'post', 'kimballs', 'approach', 'httpsmediumcomadityasharmahkimballsapproachthesorcerersstoneofdatawarehousingfeebhttpsmediumcomadityasharmahkimballsapproachthesorcerersstoneofdatawarehousingfeeb']",checkout amazing post kimballs approach httpsmediumcomadityasharmahkimballsapproachthesorcerersstoneofdatawarehousingfeebhttpsmediumcomadityasharmahkimballsapproachthesorcerersstoneofdatawarehousingfeeb,Low,4,4,"files, api, tables, query, models, read, google, json, file, logic, write, case, csv, store, datasets, across, sources, metadata, created, schema, systems, large, platform, custom, script, changes, dataset, row, reporting, parquet, issues, ads, load, fields, analysis, cases, product, values, design, view, idea, layer, search, import, suggest, language, excel, app, blob, form, request, params, postgres, ingestion, raw, mysql, fact, processed, structures, either, separate, structured, connection, core, method, downstream, scripts, extract, maintain, send, hub, records, transactional, overhead, lineage, environment, object, external, pull, iot, order, typically, grid, hubs, validation, requires, mapping, batch, mechanism, location, updates, keeping, checkout, operations, runs, convert, optimized, avoid, matching, views, formats, matters, vpc, bring, reduce, migrate, consists, starts, reads, refresh, relationships, bronze, historical, tracking, developers, natural",8.0,7.1,169.4,-0.679,1041.7,44.5,9.5,267.7,10.7,24.1,17.9,49,102,44,"[https://github.com/turbolytics/sql-flow](https://github.com/turbolytics/sql-flow)

**The goal of SQLFlow is to bring the simplicity of DuckDB to streaming data.**

SQLFlow is a high-performance stream processing engine that simplifies building data pipelines by enabling you to define them using just SQL. Think of SQLFLow as a lightweight, modern Flink.

SQLFlow models stream-processing as SQL queries using the [DuckDB SQL dialect](https://duckdb.org/docs/sql/introduction.html). Express your entire stream processing pipeline—ingestion, transformation, and enrichment—as a single SQL statement and configuration file.

Process [10's of thousands of events per second](https://sql-flow.com/docs/tutorials/clickhouse-sink) on a single machine with low memory overhead, using Python, DuckDB, Arrow and Confluent Python Client.

Tap into the DuckDB ecosystem of tools and libraries to build your stream processing applications. SQLFlow supports [parquet](https://sql-flow.com/docs/tutorials/s3-parquet-sink), csv, json and [iceberg](https://sql-flow.com/docs/tutorials/iceberg-sink). Read data from Kafka.",6,7

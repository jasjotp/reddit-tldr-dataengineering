id,title,selftext,score,num_comments,author,created_utc,url,upvote_ratio,over_18,edited,spoiler,stickied
1jr15ej,What's the non-technical biggest barrier you face at work?,"What‚Äôs currently challenging for me is getting access to things.

I design a data pipeline, present it to the team that will benefit from it, and everyone gets super excited.

Then I reach out to the internal department or an external party to either grant me admin access to the platform I need, or to help me obtain an API.

A week goes by‚Äînothing. I follow up via email. Eventually, someone replies and says it's not possible to give me admin credentials. Fine. So I ask, ‚ÄúCan you help me get the API instead? It‚Äôs very straightforward.‚Äù

Another week goes by‚Äîstill nothing. I send another follow-up‚Ä¶

Now the other person is kind of frustrated (because I‚Äôm asking them to do something slightly different, even though I‚Äôm offering guidance).

What follows is just a back-and-forth with long, frustrating waiting periods in between. Meanwhile, the team I presented the pipeline or project to starts getting frustrated with me and probably thinks I‚Äôm full of crap.

Once I finally get the damn API or whatever access I needed, I complete the project in 1‚Äì2 days but delayed by weeks or even months.

Aaaaaaah!",48,17,sirtuinsenolytic,2025-04-04 02:23:50,https://www.reddit.com/r/dataengineering/comments/1jr15ej/whats_the_nontechnical_biggest_barrier_you_face/,0,False,False,False,False
1jr68kn,Are Hyperscalers becoming more expensive in Europe due to the tariffs?,"Hi,

With the recent tariffs in mind, are cloud providers like AWS, Azure, and Google Cloud becoming more expensive for European companies? And what about other techs like Snowflake or Databricks ‚Äì are they affected too?

Would it be wise for European businesses to consider open-source alternatives, both for cost and strategic independence?

And from a personal perspective: should we, as employees, expand our skill sets toward open-source tech stacks to stay future-proof?",30,26,Ok-Inspection3886,2025-04-04 07:31:32,https://www.reddit.com/r/dataengineering/comments/1jr68kn/are_hyperscalers_becoming_more_expensive_in/,0,False,False,False,False
1jr70yg,Which tool do you use to move data from the cloud to Snowflake?,"Hey, r/dataengineering 

I‚Äôm working on a project where I need to move data from our cloud-hosted databases into Snowflake, and I‚Äôm trying to figure out the best tool for the job. Ideally, I‚Äôd like something that‚Äôs cost-effective and scales well. 

If you‚Äôve done this before, what did you use?
Would love to hear about your experience‚Äîhow reliable it is, how much it roughly costs, and any pros/cons you‚Äôve noticed. Appreciate any insights!

[View Poll](https://www.reddit.com/poll/1jr70yg)",7,12,Many-Tart-7661,2025-04-04 08:30:40,https://www.reddit.com/r/dataengineering/comments/1jr70yg/which_tool_do_you_use_to_move_data_from_the_cloud/,0,False,False,False,False
1jrfp85,Data Engineer Consulting Rate?,"I currently work as a mid-level DE (3y) and I‚Äôve recently been offered an opportunity in Consulting. I‚Äôm clueless what rate I should ask for. Should it be 25% more than what I currently earn? 50% more? Double!? 

I know that leaping into consulting means compromising job stability and higher expectations for deliveries, so I want to ask for a much higher rate without high or low balling a ridiculous offer. Does someone have experience going from DE to consultant DE? Thanks!",5,16,ActRepresentative378,2025-04-04 16:11:28,https://www.reddit.com/r/dataengineering/comments/1jrfp85/data_engineer_consulting_rate/,0,False,False,False,False
1jre0v2,Logging in Spark applications.,"Hi guys, i am moving to on-prem managed Spark applications with Kuberenetes. I am wondering what do u use for logging? I am talking about Python and PySpark. Do u setup log4j? Or just use Python's logging library for application? What is the standard here? I have not seen much about log4j within PySpark.",5,2,Hot_While_6471,2025-04-04 15:01:43,https://www.reddit.com/r/dataengineering/comments/1jre0v2/logging_in_spark_applications/,0,False,False,False,False
1jrdue4,Anyone know of any vscode linter for sql that can accommodate pyspark sql?,"In pyspark 3.4 you can write sql as 

spark.sql(SELECT * FROM {df_input}, df_input = df_input) 

The popular sql linters I tried SQL Formatter and and Prettier SQL Vscode currently does not accommodate{}. Does anyone know of any linters that does? Thank you",5,0,AUGcodon,2025-04-04 14:54:19,https://www.reddit.com/r/dataengineering/comments/1jrdue4/anyone_know_of_any_vscode_linter_for_sql_that_can/,1,False,False,False,False
1jr3z1u,Faster way to view + debug data,"Hi r/dataengineering!

  
I wanted to share a project that I have been working on.¬†It's an intuitive data editor where you can interact with local and remote data (e.g. Athena & BigQuery). For several important tasks, it can speed you up by 10x or more. (see website for more)

  
For data engineering specifically, this would be really useful in debugging pipelines, cleaning local or remote data, and being able to easy create new tables within data warehouses etc.

I know this could be a lot faster than having to type everything out, especially if you're just poking around. I personally find myself using this before trying any manual work.

Also, for those doing complex queries, you can split them up and work with the frame visually and add queries when needed. Super useful for when you want to iteratively build an analysis or new frame¬†***without writing a super long query.***

  
As for data size, it can handle local data up to around 1B rows, and remote data is only limited by your data warehouse.

  
You don't have to migrate *anything* either.

  
If you're interested, you can check it out here: [https://www.cocoalemana.com](https://www.cocoalemana.com)

  
I'd love to hear about your workflow, and see what we can change to make it cover more data engineering use cases.

  
Cheers!

[Coco Alemana](https://preview.redd.it/02wogjj72rse1.jpg?width=3820&format=pjpg&auto=webp&s=0905bd40927b4dd7e80521568982ebe82994a5fe)

",4,3,Impressive_Run8512,2025-04-04 05:00:10,https://www.reddit.com/r/dataengineering/comments/1jr3z1u/faster_way_to_view_debug_data/,0,False,False,False,False
1jr6a6h,How to stream results of a complex SQL query,"Hello,

I'm writing you because I have a problem with a side project and maybe here somebody can help me. I have to run a complex query with a potentially high number of results and it takes a lot of time. However, for my project I don't need all the results to be showed together, perhaps after some hours/days. It would be much more useful to get a stream of the partial results in real time. How can I achieve this? I would prefer to use free software, however please suggest me any solution you have in mind.

Thank you in advance!",3,12,forevernevermore_,2025-04-04 07:35:07,https://www.reddit.com/r/dataengineering/comments/1jr6a6h/how_to_stream_results_of_a_complex_sql_query/,0,False,False,False,False
1jr23mk,How do I get out of this rut,"I‚Äôm currently about the finish an early career rotational program with a top 10 bank. The rotation I am currently on and where the company is placing me post program (I tried to get placed somewhere else) is as a data engineer on a data delivery team. When I was advertised this rotation and the team I was told pretty specifically we would be using all the relevant technologies and I would be very hands on keyboard building pipelines with python , configuring cloud services and snowflake, being a part of data modeling. Mind you I‚Äôm not completely new I have experience with all this in personal projects and previous work experience as a SWE and researcher in college. 

Turns out all of that was a lie. I later learned there is an army of contractors that do the actual work. I was stuck with analyzing .egp files and other SAS files documenting it and handing off to consultants to rebuild in Talend to ingest into snowflake. The only tech that I use is Visio and Word.

I coped with that by saying after I‚Äôm out of the program I‚Äôll get to do the actual work. But I had a conversation with my manager today about what my role will be post program. He basically said there are a lot more of these SAS procedures they are porting over to talend and snowflake and I‚Äôll be documenting them and handing over to contractors so they can implement the new process. Honestly that is all really quick and easy to do because there isn‚Äôt that much complicated business logic for the LOBs we support just joins and the occasional aggregation so most days I‚Äôm not doing anything.

When I told him I would really like to be involved in the technical work or the data modeling , he said that is not my job anymore and that is what we pay the contractors to do so I can‚Äôt do it. Almost made it seem like I should be grateful and he is doing me a favor somehow.

It just feels like I was misled or even outright lied to about the position. We don‚Äôt use any of the technologies that were advertised (Drag and drop/low code tools seem like fake engineering), I don‚Äôt get to be hands on keyboard at all. Just seems like there really I no growth or opportunity in this role. I would leave but I took relocation and a signing bonus for this and if I leave too early I owe it back. I also can‚Äôt internally transfer anywhere for a year after starting my new role.

I guess my rant is just to ask what should I be doing in this situation? I work on personal projects and open source and I have gotten a few certs in the downtime at work but I don‚Äôt know if it‚Äôs enough to make sure my skills don‚Äôt atrophy while I wait out my repayment period. I consider myself a somewhat technical guy but I have been boxed into a non technical role.

",3,6,anonymous_0618615740,2025-04-04 03:13:53,https://www.reddit.com/r/dataengineering/comments/1jr23mk/how_do_i_get_out_of_this_rut/,0,False,2025-04-04 09:08:09,False,False
1jr1r2t,"Built a real-time e-commerce data pipeline with Kinesis, Spark, Redshift & QuickSight ‚Äî looking for feedback","I recently completed a real-time ETL pipeline project as part of my data engineering portfolio, and I‚Äôd love to share it here and get some feedback from the community.

# What it does:

* Streams transactional data using **Amazon Kinesis**
* Backs up raw data in **S3** (Parquet format)
* Processes and transforms data with **Apache Spark**
* Loads the transformed data into **Redshift Serverless**
* Orchestrates the pipeline with **Apache Airflow (Docker)**
* Visualizes insights through a **QuickSight dashboard**

# Key Metrics Visualized:

* Total Revenue
* Orders Over Time
* Average Order Value
* Top Products
* Revenue by Category (donut chart)

I built this to practice real-time ingestion, transformation, and visualization in a scalable, production-like setup using AWS-native services.

# GitHub Repo:

[https://github.com/amanuel496/real-time-ecommerce-etl-pipeline](https://github.com/amanuel496/real-time-ecommerce-etl-pipeline)

If you have any thoughts on how to improve the architecture, scale it better, or handle ops/monitoring more effectively, I‚Äôd love to hear your input.

Thanks!",5,6,MysteriousRide5284,2025-04-04 02:55:36,https://www.reddit.com/r/dataengineering/comments/1jr1r2t/built_a_realtime_ecommerce_data_pipeline_with/,0,False,False,False,False
1jramqt,"Airbyte Connector Builder now supports GraphQL, Async Requests and Custom Components","Hello, Marcos from the Airbyte Team.

For those who may not be familiar, Airbyte is an open-source data integration (EL) platform with over 500 connectors for APIs, databases, and file storage.

In our last release we added several new features to our no-code Connector Builder:

* [GraphQL Support](https://docs.airbyte.com/connector-development/config-based/understanding-the-yaml-file/request-options#graphql-request-injection): In addition to REST, you can now make requests to GraphQL APIs (and properly handle pagination!)
* [Async Data Requests](https://docs.airbyte.com/connector-development/connector-builder-ui/async-streams): There are some reporting APIs that do not return responses immediately. For instance, with Google Ads.¬† You can now request a custom report from these sources and wait for the report to be processed and downloaded.
* [Custom Python Code Components](https://docs.airbyte.com/connector-development/connector-builder-ui/custom-components): We recognize that some APIs behave uniquely‚Äîfor example, by returning records as key-value pairs instead of arrays or by not ordering data correctly. To address these cases, our open-source platform now supports custom Python components that extend the capabilities of the no-code framework without blocking you from building your connector.

We believe these updates will make connector development faster and more accessible, helping you get the most out of your data integration projects.

We understand there are discussions about the trade-offs between no-code and low-code solutions. At Airbyte, transitioning from fully coded connectors to a low-code approach allowed us to maintain a large connector catalog using standard components.¬† We were also able to create a better build and test process directly in the UI. Users frequently give us the feedback that the no-code connector Builder enables less technical users to create and ship connectors. This reduces the workload on senior data engineers allowing them to focus on critical data pipelines.

Something else that has been top of mind is speed and performance. With a robust and stable connector framework, the engineering team has been dedicating significant resources to introduce concurrency to enhance sync speed. You can read this[ blog post](https://airbyte.com/blog/improving-connector-sync-speed-up-to-10x-faster) about how the team implemented concurrency in the Klaviyo connector, resulting in a speed increase of about 10x for syncs.

I hope you like the news! Let me know if you want to discuss any missing features or provide feedback about Airbyte.",3,2,marcos_airbyte,2025-04-04 12:26:58,https://www.reddit.com/r/dataengineering/comments/1jramqt/airbyte_connector_builder_now_supports_graphql/,0,False,False,False,False
1jrm80y,Marketing Report & Fivetran,"Fishing for advice as I'm sure many have been here before. I came from DE at a SaaS company where I was more focused on the infra but now I'm in a role much close to the business and currently working with marketing. I'm sure this could make the Top-5 all time repeated DE tasks. A daily marketing report showing metrics like Spend, cost-per-click, engagement rate, cost-add-to-cart, cost-per-traffic... etc. These are per campaign based on various data sources like GA4, Google Ads, Facebook Ads, TikTok etc. Data updates once a day.

It should be obvious I'm not writing API connectors for a dozen different services. I'm just one person doing this and have many other things to do. I have Fivetran up and running getting the data I need but MY GOD is it ever expensive for something that seems like it should be simple, infrequent & low volume. It comes with a ton of build in reports that I don't even need sucking rows and bloating the bill. I can't seem to get what I need without pulling millions of event rows which costs a fortune to do.

Are there other similar but (way) cheaper solutions are out there? I know of others but any recommendations for this specific purpose?",2,5,bcsamsquanch,2025-04-04 20:45:27,https://www.reddit.com/r/dataengineering/comments/1jrm80y/marketing_report_fivetran/,0,False,False,False,False
1jrd8po,PII Obfuscation in Databricks,"Hi Data Champs,

I have been recently given chance to explore PII obfuscation technique in databricks.

I proposed using sql aes_encryption or python fernet for PII column level encryption before landing to bronze.

And use column masking on delta tables which has built in logic for group membership check and decryption so to avoid the overhead of a new view per table.

My HDE was more interested in sql approach than the fernet but fernet offers built in key rotation out of the box.

Has anyone used aes_encryption 
Is it secure, easy to work with and relatively more robust.

From my experience for data type other than binary like long, int, double it needs to be first converted to binary (don‚Äôt like it)

Apart from that usual error here and there for padding and generic error when decrypting sometimes.

So given the choice what will be your architecture 

What you will prefer, what you don‚Äôt and why

I am open to DM if you wanna üí¨ ",2,1,Intelligent-Mind8510,2025-04-04 14:28:31,https://www.reddit.com/r/dataengineering/comments/1jrd8po/pii_obfuscation_in_databricks/,0,False,2025-04-04 14:33:56,False,False
1jqzz3y,General question about data consulting,"Let's say there's a data consulting company working within a certain industry (e.g., utilities or energy). How do they gain access to their clients' databases if they want to perform ETL or other services? How about working with their data in a cloud setting (e.g., AWS)? What is the usual process for that? Is the consulting company responsible for setting and managing AWS costs, etc.?",2,9,No7-Francesco88,2025-04-04 01:23:44,https://www.reddit.com/r/dataengineering/comments/1jqzz3y/general_question_about_data_consulting/,0,False,False,False,False
1jrohrl,Question about file sync,"Pardon the noob question. I'm building a simple ETL process using Airflow on a remote Linux server and need a way for users to upload input files and download processed files.

I would prefer a method that is easy to use for users like a shared drive (like Google Drive).

I've considered Syncthing, and in the worst case, SFTP access. What solutions do you typically use or recommend for this? Thanks!",3,1,CraftedLove,2025-04-04 22:25:11,https://www.reddit.com/r/dataengineering/comments/1jrohrl/question_about_file_sync/,1,False,False,False,False
1jrd0k2,Great Expectations Implementation,"Our company is implementing data quality testing and we are interested in borrowing from the Great Expectations suite of open source tests. I've read mostly negative reviews of the initial implementation of Great Expectations, but am curious if anyone else set up a much more lightweight configuration?

Ultimately, we plan to use the GX python code to run tests on data in Snowflake and then make the results available in Snowflake. Has anyone done something similar to this?",1,2,HAKOC534,2025-04-04 14:18:53,https://www.reddit.com/r/dataengineering/comments/1jrd0k2/great_expectations_implementation/,0,False,False,False,False
1jrc62b,Can you call an aimless star schema a data mart?,"So,

  
as always that's for the insight from other people, I find a lot of these discussions around points very entertaining and very helpful!

I'm having an argument with someone who is several levels above me. This might sound petty so I apologise in advance. It centres around the definition of a Mart. Our Mart is a single Fact with around 20 dimensions. The Fact is extremely wide and deep. Indeed we usually put it into a de normalised table for reporting. To me this isn't a MART as it isn't based on requirements but rather a star schema that supposedly servers multiple purposed or potential purposes. When engaged on requirements the person leans on there experience in the domain and says a user probable wants to do X, Y and Z. I've never seen anything written down. Constantly that report also defers to Kimball methodology and how this follows them closely. My take on the book is that these things need to be based of requirement, business requirements. 

My questions is, is it fair to say that a data mart needs to have requirements and ideally a business domain in mind or else its just a star schema?

Yes this  is  very theoretical... yes I probable need a hobby but look there hasn't been a decent RTS game in years and its friday!!!

Have a good weekend everyone",1,3,ObjectiveAssist7177,2025-04-04 13:41:31,https://www.reddit.com/r/dataengineering/comments/1jrc62b/can_you_call_an_aimless_star_schema_a_data_mart/,0,False,False,False,False
1jrbgqt,Data Engineering Performance -  Authors,I having worked in BI and transitioned to DE have followed best practices reading books by authors like Ralph Kimball in BI. Is there someone in DE with a similar level of reputation. I am not looking for specific technologies but rather want to pick up DE fundamentals especially in the performance and optimization space.,1,1,Amar_K1,2025-04-04 13:08:48,https://www.reddit.com/r/dataengineering/comments/1jrbgqt/data_engineering_performance_authors/,0,False,False,False,False
1jravos,Unstructured Data,"I see this has been asked prior but I didn't see a clear answer. We have a smallish database (glorified spreadsheet) where one field contains text. It houses details regarding customers, etc calling in for various issues. For various reasons (in-house) they want to keep using the simple app (it's a SharePoint List). I can easily download the data to a CSV file, for example, but is there a fairly simple method (AI?) to make sense of this data and correlate it? Maybe a creative prompt? Or is there a tool for this? (I'm not a software engineer). Thanks!",1,5,Top_Sink9871,2025-04-04 12:39:42,https://www.reddit.com/r/dataengineering/comments/1jravos/unstructured_data/,0,False,False,False,False
1jr6oc9,Do you need statistics to land a DE job?,"As the title suggests. Even if stats are not used on the job, will having stats qualifications give me an edge in the hiring process?",1,20,Normal-Bandicoot-180,2025-04-04 08:03:59,https://www.reddit.com/r/dataengineering/comments/1jr6oc9/do_you_need_statistics_to_land_a_de_job/,0,False,False,False,False
1jr05id,[Seeking Guidance] Aspiring GCP Data Engineer ‚Äì Will Work Pro Bono for Hands-On Experience!,"Hey r/dataengineering community,  

I‚Äôm deep into prepping for the Google Cloud Professional Data Engineer cert and want to transition from theory to real-world projects. To ace the exam and build job-ready skills, I‚Äôm looking for:  

- Hands-on opportunities (pro bono!) to work with GCP tools like BigQuery, Dataflow, Pub/Sub, Cloud Composer, etc.  
- Mentorship or collaboration on data pipelines, workflow optimization, or cloud architecture projects.  
- Open-source/community projects needing an extra pair of hands.  

Why me? I‚Äôm motivated, detail-oriented, and eager to learn. I‚Äôll treat your project like my own!  

If you‚Äôre working on anything data-related in GCP - or know someone who is - I‚Äôd hugely appreciate a chance to contribute (or even just advice on where to start). Comment/DM me, and thanks for being an awesome community!  

P.S. Upvotes for visibility help a ton! üôè",1,3,aiqdec,2025-04-04 01:33:00,https://www.reddit.com/r/dataengineering/comments/1jr05id/seeking_guidance_aspiring_gcp_data_engineer_will/,0,False,False,False,False
1jrmmcn,AI agent for complex query,"https://www.reddit.com/r/AI_Agents/s/iKnUXMLoxZ

",0,1,Future_Scar_7875,2025-04-04 21:02:34,https://www.reddit.com/r/dataengineering/comments/1jrmmcn/ai_agent_for_complex_query/,0,False,False,False,False
1jrd286,Just wanted to share a recent win that made our whole team feel pretty good.,"We worked with this e-commerce client last month (kitchen products company, can't name names) who was dealing with data chaos.

When they came to us, their situation was rough. Dashboards taking forever to load, some poor analyst manually combining data from 5 different sources, and their CEO breathing down everyone's neck for daily conversion reports. Classic spreadsheet hell that we've all seen before.

We spent about two weeks redesigning their entire data architecture. Built them a proper [**data warehouse solution** ](https://datafortune.com/services/enterprise-data-management/data-warehouse/)with automated ETL pipelines that consolidated everything into one central location. Created some logical data models and connected it all to their existing BI tools.

The transformation was honestly pretty incredible to watch. Reports that used to take hours now run in seconds. Their analyst actually took a vacation for the first time in a year. And we got this really nice email from their CTO saying we'd ""changed how they make decisions"" which gave us all the warm fuzzies.

It's projects like these that remind us why we got into this field in the first place. There's something so satisfying about taking a messy data situation and turning it into something clean and efficient that actually helps people do their jobs better.",0,9,DataMaster2025,2025-04-04 14:20:44,https://www.reddit.com/r/dataengineering/comments/1jrd286/just_wanted_to_share_a_recent_win_that_made_our/,0,False,False,False,False

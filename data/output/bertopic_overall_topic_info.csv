Topic,Count,Name,Representation,Representative_Docs
-1,380,-1_the_to_and_in,"['the', 'to', 'and', 'in', 'data', 'of', 'for', 'is', 'it', 'you']","['Hey r/dataengineering, I need your help to find a solution to my dumpster fire and potentially save a soul (or two)).\n\nI\'m working together with an older dev who has been put on a project and it\'s a mess left behind by contractors. I noticed he\'s on some kind of PIP thing, and the project has a set deadline which is not realistic. It could be both of us are set up to fail. The code is the worst I have seen in my ten years in the field. No tests, no docs, a mix of prod and test, infra mixed with application code, a misunderstanding of how classes and scope work, etc.\n\nThe project itself is a ""library"" that syncing databricks with data from an external source. We query the external source and insert data into databricks, and every once in a while query the source again for changes (for sake of discussion, lets assume these are page reads per user) which need to be done incrementally. We also frequently submit new jobs to the external source with the same project. what we ingest from the source is not a lot of data, usually under 1 million rows and rarely over 100k a day. \n\nRoughly 75% of the code is doing computation in python for databricks, where they first pull out the dataframe and then filter it down with python and spark. The remaining 25% is code to wrap the API on the external source. All code lives in databricks and is mostly vanilla python. It is called from a notebook. (...)\n\nMy only idea is that the ""library"" should be split instead of having to do everything. The ingestion part of the source can be handled by dbt and we can make that work first. The part that holds the logic to manipulate the dataframes and submit new jobs to the external api is buggy and I feel it needs to be gradually rewritten, but we need to double the features to this part of the code base if we are to make the deadline. \n\nI\'m already pushing back on the deadline and I\'m pulling in another DE to work on this, but I am wondering what my technical approach should be.', ""Hi,\n\nI'm looking for sincere advice.\n\nI'm basically a data/analytics engineer. My tasks generally are like this\n\n1. put configurations so that the source dataset can ingest and preprocess into aws s3 in correct file format. I've noticed sometimes filepath names randomly change without warning which would cause configs to change so I would have to be cognizant of that. \n\n2. the s3 output is then put into a mapping tool (which in my experience is super slow and frequently annoying to use) we have to map source -> our schema\n\n3. once you update things in the mapping tool, it SHOULD export automatically to S3 and show in production environment after refresh, which is usually. However, keyword should. There are times where my data didn't show up and it turned out I have to 'manually export' a file to S3 without being made aware beforehand which files require manual export and which ones occur automatically through our pipeline\n\n4. I then usually have to develop a SQL view that combines data from various sources for different purposes\n\nThe issues I'm facing lately....\n\nA colleague left end of last year and I've noticed that my workload has dramatically changed. I've been given tasks that I can only assume were once hers from another colleague. The thing is the tasks I'm given:\n\n1. Have zero documentation. I have no clue what the task is meant to accomplish\n\n2. I have very vague understanding of the source data \n\n3. Just go off of an either previously completed script, which sometimes suffers from major issues (too many subqueries, thousands of lines of code). Try to realistically manage how/if to refactor vs. using same code and 'coming back to it later' if I have time constraints. After using similar code, randomly realize the requirements of old script changed b/c my data doesn't populate in which I have to ask my boss what the issue \n\n4. Me and my boss have to navigate various excel sheets and communication to play 'guess work' as to what the requirements are so we can get something out \n\n5. Review them with the colleague who assigned it to me who points out things are wrong OR randomly changes the requirements that causes me to make more changes and then expresses frustration 'this is unacceptable', 'this is getting delayed', 'I am getting frustrated' continuously that is making me uncomfortable in asking questions.\n\nI do not directly interact with the stakeholders. The colleague I just mentioned is the person who does and translates requirements back. I really, honestly have no clue what is going through the stakeholders mind or how they intend to use the product. All I frequently hear is that 'they are not happy', 'I am frustrated', 'this is too slow'. I am expected to get things out within few hours to 1-2 business days. This doesn't give me enough time to ensure if I made many mistakes in the process. I will take accountability that I have made some mistakes in this process by fixing things then not checking and ensuring things are as expected that caused further delays. Overall, I am under constant pressure to churn things out ASAP and I'm struggling to keep up and feel like many mistakes are a result of the pressure to do things fast.\n\nI have told my boss and colleague in detail (even wrote it up) that it would be helpful for me to: 1. just have 1-2 sentences as to what this project is trying to accomplish 2. better documentation.  People have agreed with me but they have not really done much b/c everybody is too busy to document since once one project is done, I'm pulled into the next. I personally am observing a technical debt problem here, but I am new to my job and new to data engineering (was previously in a different analytics role) so I am trying to figure out if this is a me issue and where I can take accountability or this speaks to broader issues with my team and I should consider another job. I am honestly thinking about starting the job search again in a few months, but I am quite discouraged with my current experience and starting to notice signs of burnout."", 'Hey everyone, \n\nwe are a company that relies heavy on a so called no-code middleware that combines many different aspects of typical data engineering stuff into one big platform. However we have found ourselves (finally) in the situation that we need to migrate to a lets say more fundamental tech stack that relies more on knowledge about programming, databases and sql. I wanted to ask if someone has been in the same situation and what their experiences have been. Our only option right now is to migrate for business reasons and it will happen, the only question is what we are going to use and how we will use it. \n\n**Background:**  \nWe use this platform as our main ""engine"" or tool to map various business proccess. The platform includes creation and management of various kinds of ""connectors"" including Http, as2, mail, x400 and whatnot. You can then create profiles that can get fetch and transform data based on what comes in by one of the connectors and load the data directly into your database, create files or do whatever the business logic requires. The platform provides a comprehensive amount of logging and administration. In my honest opinion, that is quite a lot that this tool can offer. Does anyone know any kind of other tool that can do the same? I heard about Apache Airflow or Apache Nifi but only on the surface. \n\nThe same platform we are using right now has another software solution for building database entities on top of its own database structure to create ""input masks"" for users to create, change or read data and also apply business logic. We use this tool to provide whole platforms and even ""build"" basic websites. \n\nWhat would be the best tech stack to migrate to if your goal was to cover all of the above? I mean there probably is not an all in one solution but that is not what we are looking for right now. If you said to me that for example apache nifi in combination with python would be enough to cover everything our middleware provided would be more than enough for me.  \n  \nWhat is essential for us is also a good logging capability. We need to make sure that whatever data flows are happening or have happended is comprehensible in case of errors or questions. \n\nFor input masks and simple web platforms we are currently using C# Blazor and have multiple projects that are working very well, which we could also migrate to. ']"
0,146,0_and_in_data_to,"['and', 'in', 'data', 'to', 'my', 'the', 'for', 'of', 'engineering', 'is']","['Hello, a few months ago I graduated for a ""Data Science in Business"" MSc degree in France (Paris) and I started looking for a job as a Junior Data Scientist, I kept my options open by applying in different sectors, job types and regions in France, even in Europe in general as I am fluent in both French and English. Today, it\'s been almost 8 months since I started applying (even before I graduated), but without success. During my internship as a data scientist in the retail sector, I found myself doing some ""data engineering"" tasks like working a lot on the cloud (GCP) and doing a lot of SQL in Bigquery, I know it\'s not much compared to what a real data engineer does on his daily tasks, but it was a new thing for me and I enjoyed doing it. At the end of my internship, I learned that unlike internships in the US, where it\'s considered a trial period to get hired, here in France it\'s considered more like a way to get some work done for cheap... well, especially in big companies. I understand that it\'s not always like that, but that\'s what I\'ve noticed from many students.\n\nAnyway, during those few months after the internship, I started learning tools like Spark, AWS, and some of Airflow. I\'m thinking that maybe I have a better chance to get a job in data engineering, because a lot of people say that it\'s getting harder and harder to find a job as a data scientist, especially for juniors. So is this a good idea for me? Because it\'s been like 3-4 months applying for Data Engineering jobs, still nothing. If so, is there more I need to learn? Or should I stick to Data Science profil, and look in other places, like Germany for example?\n\nSorry for making this post long, but I wanted to give the big picture first.', 'I recently finished my degree in Computer Science and worked part-time throughout my studies, including on many personal projects in the data domain. I’m very confident in my technical skills: I can (and have) built large systems and my own SaaS projects. I know all the ins and outs of the basic data-engineering tools, SQL, Python, Pandas, PySpark, and have experience with the entire software-engineering stack (Docker, CI/CD, Kubernetes, even front-end). I also have a solid grasp of statistics.\n\nAbout a year ago, I was hired at a company that had previously outsourced all IT to external firms. I got the job through the CEO of a company where I’d interned previously. He’s now the CTO of this new company and is building the entire IT department from scratch. The reason he was hired is to transform this traditional company, whose industry is being significantly disrupted by tech, into a “tech” company. You can really tell the CEO cares about that: in a little over one year, we’ve grown to 15+ developers, and the culture has changed a lot.\n\nI now have the privilege of being trusted with the responsibility of building the entire data infrastructure from scratch. I have total authority over all tech decisions, although I don’t have much experience with how mature data teams operate. Since I’m a total open-source nerd and we’re based in Europe, we want to rely on as few American cloud providers as possible, I’ve set up the current infrastructure like this:\n\n* **Airflow** (running in our Kubernetes cluster)\n* **ClickHouse DWH** (also running in our Kubernetes cluster)\n* **Spark** (you guessed it, running in our cluster)\n* **Goose** for SQL migrations in our warehouse\n\nSome conceptual decisions I’ve made so far:\n\n1. Data ingestion from different sources (Salesforce, multiple products, etc.) runs through Airflow, using simple Pandas scripts to load into the DWH (about 200 k rows per day).\n2. ClickHouse is our DWH, and Spark connects to ClickHouse so that all analytics runs through Spark against ClickHouse. If you have any tips on how to structure the different data layers (Ingestion/datamart etc), please!\n\nWhat I want to implement next are typical software-engineering practices, dev/prod environments, testing, etc. As I mentioned, I have a lot of experience in classical SWE within corporate environments, so I want to apply as much from that as possible. In my research, I’ve found that you basically just copy the entire environment for dev and prod, which makes sense, but sounds expensive computing wise. We will soon start hiring additional DE/DA/DS.\n\nMy question is: What technical or organizational decisions do you think are important and valuable? What have you seen work (or not work) in your experience as a data engineer? Are there problems you only discover once your team has grown? I want to get in front of those issues as early as possible. Like I said, I have a lot of experience in how to build SWE projects in a corporate environment. Any things I am not thinking about that will sooner or later come to haunt me in my DE team? Any tips on how to setup my DWH architecture? How does your DWH look conceptually?', 'I graduated last August with a bachelors degree in Math from a good university. The job market already sucked then and it sucked even more considering I only had one internship and it was not related to my field. I ended up getting a job as a data analyst through networking, but it was a basically an extended internship and I now work in the IT department doing basic IT things and some data engineering.\n\nMy company wants me to move to another state and I have already done some work there for the past 3 months but I do not want to continue working in IT. I can also tell that the company I work for is going to shit at least in regards to the IT department given how many experienced people we have lost in the past year.\n\nAfter thinking about it, I would rather be a full time ETL developer or data engineer. I actually have a part time gig as a data engineer for a startup but it is not enough to cover the bills right now.\n\n**My question is how dumb would it be for me to quit my current job and work on getting certifications (I found some stuff on coursera but I am open to other ideas) to learn things like databricks, T-SQL, SSIS, SSRS, etc?** I have about one year of experience under my belt as a data analyst for a small company but I only really used Cognos Analytics, Python, and Excel.\n\nI have about 6 months of expenses saved up where I could not work at all but with my part time gig and maybe some other low wage job I could make it last like a year and a half.\n\nEDIT: I did not make it clear but I currently have a side job as a microsoft fabric data engineer and while the program has bad reviews on reddit, I am still learning Power BI, Azure, PySpark, Databricks, and some other stuff. It actually has covered my expenses for the past three months (if I did not have my full time job) but it might not be consistent. I am mostly wondering if quitting my current job which is basically as an IT helpdesk technician and still doing this side job while also getting certifications from Microsoft, Tableau, etc would allow me to get some kind of legit data engineering job in the near future. I was also thinking of making my own website and listing some of my own side projects and things I have worked on for this data engineering job.']"
1,55,1_to_iceberg_the_and,"['to', 'iceberg', 'the', 'and', 'table', 'data', 'is', 'tables', 'parquet', 'files']","[""So, I'm currently working on a project (my first) to create a scalable data platform for a company. The whole thing structured around AWS, initially using DMS to migrate PostgreSQL data to S3 in parquet format (this is our raw datalake). Then using Glue jobs to read this data and create Iceberg tables which would be used in Athena queries and Quicksight. I've got a working Glue script for reading this data and perform upsert operations. Okay so now that I've given a bit of context of what I'm trying to do, let me tell you my problem.  \nThe client wants me to schedule this job to run every 15min or so for staging and most probably every hour for production. The data in the raw datalake is partitioned by date (for example: s3bucket/table\\_name/2025/04/10/file.parquet). Now that I have to run this job every 15 min or so I'm not sure how to keep track of the files that have been processed and which haven't. Currently my script finds the current time and modifies the read command to use just the folder for the current date. But still, this means that I'll be reading all the files in the folder (processed already or not) every time the job runs during the day.   \nI've looked around and found that using DynamoDB for keeping track of the files would be my best option but also found something related to Iceberg metadata files that could help me with this. I'm leaning towards the Iceberg option as I wanna make use of all its features but have too little information regarding this to implement. would absolutely appreciate it if someone could help me out with this.  \nHas anyone worked with Iceberg in this matter? and if the iceberg solution isn't usable, could someone help me out with how to implement the DynamoDB way."", 'Noob questions incoming!\n\n**Context:**   \nI\'m designing my project\'s storage and data pipelines, but am new to data engineering. I\'m trying to understand the ins and outs of various solutions for the task of reading/writing diverse types of very large data.   \n  \nFrom a theoretical standpoint, I understand that Iceberg is a standard for organizing metadata about files. Metadata organized to the Iceberg standard allows for the creation of ""Iceberg tables"" that can be queried with a familiar SQL-like syntax.\n\nI\'m trying to understand how this would fit into a real world scenario... For example, lets say I use object storage, and there are a bunch of pre-existing parquet files and maybe some images in there. Could be anything...\n\n**Question 1:**  \nHow is the metadata/tables initially generated for all this existing data? I know AWS has the Glue Crawler. Is something like that used? \n\nOr do you have to manually create the tables, and then somehow point the tables to the correct parquet files that contain the data associated with that table?\n\n**Question 2:**  \nOkay, now assume I have object storage and metadata/tables all generated for files in storage. Someone comes along and drops a new parquet file into some bucket. I\'m assuming that I would need some orchestration utility that is monitoring my storage and kicking off some script to add the new data to the appropriate tables? Or is it done some other way?\n\n**Question 3:**   \nI assume that there are query engines out there that are implemented to the Iceberg standard for creating and reading Iceberg metadata/tables, and fetching data based on those tables. For example, I\'ve read that SparkQL and Trino have Iceberg ""connectors"". So essentially the power of Iceberg can\'t be leveraged if your tech stack doesn\'t implement compliant readers/writers? How prolific are Iceberg compatible query engines?\n\n', 'I\'m fairly new to the idea of ETL even though I\'ve read about and followed it for years; however, the implementation is what I have a question about.\n\nOur needs have migrated towards the idea of Spark so I\'m thinking of building our pipeline in Scala.  I\'ve used it on and off in the past so it\'s not a foreign language for me.\n\nHowever, the question I have is should I build our workflow and hard code it from A-Z (data ingestion, create or replace, populate tables) outside of snowflake, or is it better practice to have it fragmented and saved as snowflake worksheets?  My aim with this change would be strongly typed services that can\'t be ""accidentally"" fired off.\n\nI\'m thinking the pipeline would be more of a spot instance that is fired off with certain configs with the A-Z only allowed for certain logins.  There aren\'t many people on the team but there are people working with tables that have drop permissions (not from me) and I just want to be prepared for disasters and recovery.\n\nIt\'s like a mini-dream whereas I\'m in full control of the data and ingestion pipelines but everything is sql currently.  Therefore, we are building from scratch right now and the Scala system would mainly be a disaster recovery so made to repopulate tables, or to ingest a new set of raw data to be transformed and loaded (updates).\n\nThis is a non-profit so I don\'t want to load them up with huge bills (databricks) so I do want to do most of the stuff myself with the help of apache.  I understand there are numerous options but essentially it\'s going to be like this\n\nScala server -> Apache Spark -> ML Categorization From Spark -> Snowflake\n\nSince we are ingesting data I figured we should mix in the machine learning while transforming and processing to save on time and headaches.\n\nWHY I DIDN\'T CHOOSE SNOWPARK:  \nAfter looking over snowpark I see it as a great gateway for people either needing pure speed, or those who are newer to software engineering and needing a box to be in.  I\'m well-versed in pandas, numpy, etc. so I wanted to be able to break the mold at any point.  I know this may not be preferable for snowflake people but I have about a decade of experience writing complex software systems, and I didn\'t want vendor lock-in so I hope that can be respected to some extent.  If I am blatantly wrong then please let me know how snowpark is better.  \n\n  \nNote: I do see snowpark offers Scala (or something like that); however, the point isn\'t solely to use Scala, I come from Golang and want a sturdy pipeline that won\'t run into breaking changes and make it a JVM shop.\n\nAny other advice from engineers here on other things I should recommend would be greatly appreciated as well.  Scraping is a huge concern, which is why I chose Golang off the bat, but scraping new data can\'t objectively be the main priority, I feel like there are other things that I might be unaware of.  Maybe a checklist of things that I can make sure we have just so we don\'t run into major issues then I catch the blame shift.\n\nTherefore, please be gentle I am not the most well-versed in data engineering but I do see it as a fascinating discipline that I\'d like to find a niche in if possible.']"
2,48,2_data_the_and_ai,"['data', 'the', 'and', 'ai', 'to', 'it', 'for', 'we', 'that', 'of']","[""TLDR; My company wants to replace our pipelines with some all-in-one “AI agent” platform\n\nI’m a lone data engineer in a mid-size retail/logistics company that runs SAP ERP (moving to HANA soon). Historically, every department pulled SAP data into Excel, calculated things manually, and got conflicting numbers. I was hired into a small analytics unit to centralize this. I’ve automated data pulls from SAP exports, APIs, scrapers, and built pipelines into SQL Server. It’s traceable, consistent, and used regularly.\n\nNow, our new CEO wants to “centralize everything” and “go AI-driven” by bringing in a no-name platform that offers:\n\n\\- Limited source connectors for a basic data lake/warehouse setup\n\n\\- A simple SQL interface + visualization tools\n\n\\- And the worst of it all: an AI agent PER DEPARTMENT\n\nEach department will have its own AI “instance” with manually provided business context. Example: “This is how finance defines tenure,” or “Sales counts revenue like this.” Then managers are supposed to just ask the AI for a metric, and it will generate SQL and return the result. Supposedly, this will replace 95–97% of reporting, instantly (and the CTO/CEO believe it).\n\nObviously, I’m extremely skeptical:\n\n\\- Even with perfect prompts and context, if the underlying data is inconsistent (e.g. rehire dates in free text, missing fields, label mismatches), the AI will silently get it wrong.\n\n\\- There’s no way to audit mistakes, so if a number looks off, it’s unclear who’s accountable. If a manager believes it, it may go unchallenged.\n\n\\- The answer to every flaw from them is: “the context was insufficient” or “you didn’t prompt it right.” That’s not sustainable or realistic\n\n\\- Also some people (probs including me) will have to manage and maintain all the departmental context logic, deal with messy results, and take the blame when AI gets it wrong.\n\n\\- Meanwhile, we already have a working, auditable, centralized system that could scale better with a real warehouse and a few more hires. They just don't want to hire a team or I have to convince them somehow (bc they think that this is a cheaper, more efficient alternative).\n\nI’m still relatively new in this company and I feel like I’m not taken seriously, but I want to push back before we go too far, I'll switch jobs probably soon anyway but I'm actually concerned about my team.\n\nHow do I convince the management that this is a bad idea?"", ""Hi all, I'm evaluating metadata management solutions for our data platform and would appreciate any thoughts from folks who've actually implemented these tools in production.\n\nWe're currently running into scaling issues with our in-house data catalog and I think we need something more robust for governance and lineage tracking.\n\nI've narrowed it down to Acryl (DataHub) and Collate (openmetadata) as the main contenders. I know I should look at Collibra and Alation and maybe Unity Catalog?\n\nFor context, we're a mid-sized fintech (\\~500 employees) with about 30 data engineers and scientists. We're AWS with Snowflake, Airflow for orchestration, and a growing number of ML models in production.\n\nMy question list is: \n\n1. How these tools handle machine-scale operations \n2. How painful was it to get set up?\n3. For DataHub and openmetadata specifically - is the open source version viable or is the cloud version necessary?\n4. Any unexpected limitations you've hit with any of these platforms?\n5. Do you feel like these grow with you as we increasingly head into AI governance? \n6. How well they integrate with existing tools (Snowflake, dbt, Looker, etc.)\n\nIf anyone has switched from one solution to another, I'd love to hear why you made the change and whether it was worth it.\n\nSorry for the pick list of questions - the last post on this was years ago and I was hoping for some more insights. Thanks in advance for anyone's thoughts. "", ""Hi all, I'm evaluating metadata management solutions for our data platform and would appreciate any thoughts from folks who've actually implemented these tools in production.\n\nWe're currently running into scaling issues with our in-house data catalog and I think we need something more robust for governance and lineage tracking.\n\nI've narrowed it down to Acryl (DataHub) and Collate (openmetadata) as the main contenders. I know I should look at Collibra and Alation and maybe Unity Catalog?\n\nFor context, we're a mid-sized fintech (\\~500 employees) with about 30 data engineers and scientists. We're AWS with Snowflake, Airflow for orchestration, and a growing number of ML models in production.\n\nMy question list is: \n\n1. How these tools handle machine-scale operations \n2. How painful was it to get set up?\n3. For DataHub and openmetadata specifically - is the open source version viable or is the cloud version necessary?\n4. Any unexpected limitations you've hit with any of these platforms?\n5. Do you feel like these grow with you as we increasingly head into AI governance? \n6. How well they integrate with existing tools (Snowflake, dbt, Looker, etc.)\n\nIf anyone has switched from one solution to another, I'd love to hear why you made the change and whether it was worth it.\n\nSorry for the pick list of questions - the last post on this was years ago and I was hoping for some more insights. Thanks in advance for anyone's thoughts. ""]"
3,35,3_the_to_and_ad,"['the', 'to', 'and', 'ad', 'data', 'from', 'for', 'of', 'field', 'that']","[""Fishing for advice as I'm sure many have been here before. I came from DE at a SaaS company where I was more focused on the infra but now I'm in a role much close to the business and currently working with marketing. I'm sure this could make the Top-5 all time repeated DE tasks. A daily marketing report showing metrics like Spend, cost-per-click, engagement rate, cost-add-to-cart, cost-per-traffic... etc. These are per campaign based on various data sources like GA4, Google Ads, Facebook Ads, TikTok etc. Data updates once a day.\n\nIt should be obvious I'm not writing API connectors for a dozen different services. I'm just one person doing this and have many other things to do. I have Fivetran up and running getting the data I need but MY GOD is it ever expensive for something that seems like it should be simple, infrequent & low volume. It comes with a ton of build in reports that I don't even need sucking rows and bloating the bill. I can't seem to get what I need without pulling millions of event rows which costs a fortune to do.\n\nAre there other similar but (way) cheaper solutions are out there? I know of others but any recommendations for this specific purpose?"", ""We have a python integration set up where we pull data from Google Ads and Facebook Marketing into our data warehouse. We're pulling data about all 3 hierarchy tiers and some daily metrics:\n\n1. Campaigns (id, name, start time, stop time)\n2. Ad Groups/Ad Sets (id, name)\n3. Ads (id, name, URL)\n4. Metrics (clicks, impressions, spend) for the previous day\n\nFor the Google Ads API, you basically send a SQL query and the return time is like a tenth of a second.\n\nFor Facebook, we see returns times in the minutes, especially on the Ads piece. Was hoping to get an idea of how others might have successfully set up a process to get this data from Facebook in a more timely fashion, and possibly without hitting the rate limiting threshold.\n\nNot the exact code we're using - I can get it off my work system tomorrow - but the gist:\n\n    from facebook_business.adobjects.adaccount import AdAccount\n    from facebook_business.adobjects.campaign import Campaign\n    from facebook_business.adobjects.ad import AdSet\n    from facebook_business.adobjects.ad import Ad\n    from facebook_business.adobjects.adcreative import AdCreative\n    campaigns = AdAccount('act_123456789').get_campaigns(\n        params={},\n        fields=[Campaign.Field.id,Campaign.Field.name,Campaign.Field.start_time,Campaign.Field.stop_time]\n    )\n    adsets= AdAccount('act_123456789').get_ad_sets(\n        params={},\n        fields=[AdSet.Field.id,AdSet.Field.name]\n    )\n    ads = AdAccount('act_123456789').get_ads(\n        params={},\n        fields=[Ad.Field.id,Ad.Field.name,Ad.Field.creative]\n    )\n    object_urls = AdAccount('act_123456789').get_ad_creatives(\n        params={},\n        fields=[AdCreative.Field.object_story_spec]\n    )\n    asset_urls = AdAccount('act_123456789').get_ad_creatives(\n        params={},\n        fields=[AdCreative.Field.asset_feed_spec]\n    )\n\nWe then have to do some joining between ads/object\\_urls/asset\\_urls to match the Ad with the destination URL if the ad is clicked on.\n\nThe performance is so slow, that I hope we are doing it wrong. I was never able to get the batch call to work and I'm not sure how to improve things.\n\nSincerely a data analyst who crosses over into data engineering because our data engineers don't know python."", ""We have a python integration set up where we pull data from Google Ads and Facebook Marketing into our data warehouse. We're pulling data about all 3 hierarchy tiers and some daily metrics:\n\n1. Campaigns (id, name, start time, stop time)\n2. Ad Groups/Ad Sets (id, name)\n3. Ads (id, name, URL)\n4. Metrics (clicks, impressions, spend) for the previous day\n\nFor the Google Ads API, you basically send a SQL query and the return time is like a tenth of a second.\n\nFor Facebook, we see returns times in the minutes, especially on the Ads piece. Was hoping to get an idea of how others might have successfully set up a process to get this data from Facebook in a more timely fashion, and possibly without hitting the rate limiting threshold.\n\nNot the exact code we're using - I can get it off my work system tomorrow - but the gist:\n\n    from facebook_business.adobjects.adaccount import AdAccount\n    from facebook_business.adobjects.campaign import Campaign\n    from facebook_business.adobjects.ad import AdSet\n    from facebook_business.adobjects.ad import Ad\n    from facebook_business.adobjects.adcreative import AdCreative\n    campaigns = AdAccount('act_123456789').get_campaigns(\n        params={},\n        fields=[Campaign.Field.id,Campaign.Field.name,Campaign.Field.start_time,Campaign.Field.stop_time]\n    )\n    adsets= AdAccount('act_123456789').get_ad_sets(\n        params={},\n        fields=[AdSet.Field.id,AdSet.Field.name]\n    )\n    ads = AdAccount('act_123456789').get_ads(\n        params={},\n        fields=[Ad.Field.id,Ad.Field.name,Ad.Field.creative]\n    )\n    object_urls = AdAccount('act_123456789').get_ad_creatives(\n        params={},\n        fields=[AdCreative.Field.object_story_spec]\n    )\n    asset_urls = AdAccount('act_123456789').get_ad_creatives(\n        params={},\n        fields=[AdCreative.Field.asset_feed_spec]\n    )\n\nWe then have to do some joining between ads/object\\_urls/asset\\_urls to match the Ad with the destination URL if the ad is clicked on.\n\nThe performance is so slow, that I hope we are doing it wrong. I was never able to get the batch call to work and I'm not sure how to improve things.\n\nSincerely a data analyst who crosses over into data engineering because our data engineers don't know python.""]"
4,32,4_airflow_the_and_data,"['airflow', 'the', 'and', 'data', 'in', 'for', 'to', 'pipeline', 'of', 'containers']","[""My company has a few clients and I am tasked with organizing our schemas so that each client has their own schema. I am mostly the only one working on ETL pipelines, but there are 1-2 devs who can split time between data and software, and our CTO who is mainly working on admin stuff but does help out with engineering from time to time. We deal with highly sensitive healthcare data. Our apps right now use mongo for our backend db, but a separate database for analytics. In the past we only required ETL pipelines for 2 clients, but as we are expanding analytics to our other clients we need to create ETL pipelines at scale. That also means making changes to our current dev process.\n\n  \nRight now both our production and preproduction data is stored in one single instance. Also, we only have one EC2 instance that houses our ETL pipeline for both clients AND our preproduction environment. My vision is to have two database instances (one for production data, one for preproduction data that can be used for testing both changes in the products and also our data pipelines) which are both HIPAA compliant. Also, to have two separate EC2 instances (and in the far future K8s); one for production ready code and one for preproduction code to test features, new data requests, etc.\n\n  \nMy question is what is best practice: keep ALL ETL code for each client in one single repo and separate out in folders based on clients, or have separate repos, one for core ETL that loads parent tables and shared tables and then separate repos for each client? The latter seems like the safer bet, but just so much overhead if I'm the only one working on it. But I also want to build at scale seeing that we may be experiencing more growth than we imagine.\n\n  \nIf it helps, right now our ETL pipelines are built in Python/SQL and scheduled via cron jobs. Currently exploring the use of dagster and dbt, but I do have some other client-facing analytics projects I gotta get done first."", '# ✅ Introduction: What If Software Just Worked… Everywhere?\n\nHave you ever built something that ran perfectly on your computer—only to watch it crash on someone else’s machine?  \nThat age-old problem has a modern solution: **containers**.\n\nIn this post, you\'ll discover **what containers are**, **how they work**, and **why they\'re revolutionizing software development, cloud computing, and DevOps workflows**. Whether you\'re a curious beginner, a developer, or a tech leader, this is your ultimate beginner-to-pro guide.\n\n# 🧠 What Are Containers? (In One Clear Sentence)\n\n**A container** is a lightweight, portable unit that packages your application **along with everything it needs to run**—so it behaves exactly the same across different computers, servers, and environments.\n\n# 🔁 Think of it as:\n\n>\n\n# 🔍 Why Do Containers Matter in 2025?\n\nContainers are at the **heart of modern app development**. They power everything from **microservices** to **cloud-native architectures** and **CI/CD pipelines**. Giants like Netflix, Google, Spotify, and Airbnb use containers to deploy faster, scale instantly, and recover from failures in seconds.\n\n# 📈 Real-world impact:\n\n* **Faster deployments**\n* **Lower costs**\n* **Zero configuration issues**\n* **Massive scalability**\n\n# 📦 How Containers Actually Work (Simplified)\n\nLet’s break it down:\n\n1. You write an app.\n2. You create a **container image** using tools like **Docker**.\n3. This image includes your app **+ libraries, code, settings, runtime, dependencies**.\n4. You launch the image → it becomes a **container** running independently on any system.\n\n✅ The secret sauce?\n\n>\n\n# ⚖️ Containers vs. Virtual Machines (VMs)\n\n|Feature|Containers 🐳|Virtual Machines 🖥️|\n|:-|:-|:-|\n|OS Overhead|Minimal|Full OS per VM|\n|Startup Time|Seconds|Minutes|\n|Portability|Extremely portable|Less portable|\n|Isolation|Process-level|Full hardware-level|\n|Performance|High|Moderate|\n\n>\n\n# 🛠️ Popular Tools You Should Know\n\n* **Docker** – The most popular container platform\n* **Kubernetes** – Manages containers at scale\n* **Podman** – A secure alternative to Docker\n* **Containerd** – A high-performance container runtime\n\n# 🧙 Real-Life Analogy: Cooking with Containers\n\nImagine you\'re a chef. You have a recipe that works perfectly in *your* kitchen.\n\nBut when you travel to a new city, you\'re in someone else\'s kitchen:\n\n* No garlic?\n* Oven too hot?\n* Tools missing?\n\n🤯 The result: your food is inconsistent.\n\nNow imagine carrying **your entire kitchen inside a suitcase**—with your spices, knives, and perfect oven. You unpack it anywhere, cook your signature dish, and it **tastes exactly the same.**\n\nThat’s what containers do for software.  \n**Portable, repeatable, reliable**.\n\n# 🎯 Benefits of Using Containers\n\n✔ **Run anywhere** – Cloud, local, Windows, Linux—it just works.  \n✔ **Speed up development** – Spin up dev environments in seconds.  \n✔ **Improve collaboration** – “Works on my machine” becomes “Works everywhere.”  \n✔ **Simplify deployment** – One artifact to deploy across all environments.  \n✔ **Scale effortlessly** – Run hundreds of containers in parallel with tools like Kubernetes.\n\n# ⚠️ Common Myths & Mistakes\n\n|❌ Myth|✅ Truth|\n|:-|:-|\n|""Containers are like VMs""|They share the OS kernel, not the entire OS|\n|""Containers are secure by default""|You must harden container environments|\n|""Containers can store app data""|**statelessvolumes**Containers are   unless you use  |\n\n# 💡 Real-World Use Cases\n\n* **Microservices architecture** – Break large apps into small, independently deployable services.\n* **CI/CD pipelines** – Use containers to test and deploy code faster.\n* **Hybrid cloud** – Move workloads across clouds seamlessly.\n* **Dev environments** – Onboard new developers in minutes.\n\n# 🚀 Final Takeaway\n\n**Containers have become the building blocks of modern software.**  \nThey’re not just a trend—they’re a necessity for teams that want to move fast, stay agile, and build reliable systems in today’s cloud-first world.\n\nIf you\'re not using containers yet, you\'re leaving speed, consistency, and innovation on the table.\n\n# 🔗 Want to Go Deeper?\n\n📘 Learn Docker: [https://docker.com/get-started]()  \n📘 Kubernetes basics: [https://kubernetes.io/docs/home/]()  \n📘 Free labs & practice: [https://play-with-docker.com](https://play-with-docker.com)\n\n# ✨ Bonus: Quick SEO FAQ\n\n**Q: What is a container in simple terms?**  \nA: It\'s a portable unit that packages your app and its dependencies to run consistently on any system.\n\n**Q: How are containers different from virtual machines?**  \nA: Containers share the host OS kernel and are faster and more lightweight than VMs.\n\n**Q: What are the benefits of containerization?**  \nA: Portability, speed, scalability, environment consistency, and cost efficiency.\n\n**Q: Is Docker the same as a container?**  \nA: Docker is a **tool** used to create and manage containers.', 'In a recent blog, the team at La\xa0Poste (France’s postal service) shared how they redesigned their real-time package tracking pipeline from a monolithic app into a modular microservice architecture. The goal was to provide more accurate ETA predictions for deliveries while making the system easier to scale and monitor in production. They describe splitting the pipeline into multiple decoupled stages (using Pathway – an open-source streaming ETL engine) connected via Delta Lake storage and Kafka. This revamped design not only improved performance and reliability, but also significantly cut costs (the blog cites a 50% reduction in total cost of ownership for the IoT data platform and a projected 16% drop in fleet capital expenditures, which is huge). Below I’ll outline the architecture, key decisions, and trade-offs from the blog in an engineering-focused way.\n\nFrom Monolith to Microservices: Originally, a single streaming pipeline handled everything: data cleansing, ETA calculation, and maybe some basic monitoring. That monolith worked for a prototype, but it became hard to extend – for instance, adding continuous evaluation of prediction accuracy or integrating new models would make the one pipeline much more complex and fragile. The team decided to decouple the concerns into separate pipelines (microservices) that communicate through shared data layers. This is analogous to breaking a big application into microservices – here each Pathway pipeline is a lightweight service focused on one part of the workflow.\n\nThey ended up with four main pipeline components:\n\n1. Data Acquisition & Cleaning: Ingest raw telemetry from delivery vehicles and clean it. IoT devices on trucks emit location updates (latitude/longitude, speed, timestamp, etc.) to a Kafka topic. This first pipeline reads from Kafka, applies a schema, and filters out bad data (e.g. GPS (0,0) errors, duplicates, out-of-order events). The cleaned, normalized data is then written to a Delta Lake table as the “prepared data” store. Delta Lake was used here to persist the stream in a queryable table format (every incoming event gets appended as a new row). This makes the downstream processing simpler and the intermediate data reusable. (Notably, they chose Delta Lake over something like chaining another Kafka topic for the clean data – a design choice we’ll discuss more below.)\n\n2. ETA Prediction: This stage consumes two things – the cleaned vehicle data (from that Delta table) and incoming ETA requests. ETA request events come as another stream (Kafka topic) containing a delivery request ID, the target destination, the assigned vehicle ID, and a timestamp. The topic is partitioned by vehicle ID so all requests for the same vehicle are ordered (ensuring the sequence of stops is handled correctly). The Pathway pipeline joins each request with the latest state of the corresponding vehicle from the clean data, then computes an estimated arrival time. The blog kept the prediction logic straightforward (e.g., basically using current location to estimate travel time to the destination), since the focus was architecture. The important part is that this service is stateless with respect to historical data – it relies on the up-to-date clean data table as its source of truth for vehicle positions. Once an ETA is computed for a request, the result is written out to two places: a Kafka topic (so that whoever requested the ETA gets the answer in real-time) and another Delta Lake table storing all predictions (for later analysis).\n\n3. Ground Truth Extraction: This pipeline waits for deliveries to actually be completed, so they can record the real arrival times (“ground truth” data for model evaluation). It reads the same prepared data table (vehicle telemetry) and the requests stream/table to know what destinations were expected. The logic here tracks each vehicle’s journey and identifies when a vehicle has reached the delivery location for a request (and has no further pending deliveries for that request). When it detects a completed delivery, it logs the actual time of arrival for that specific order. Each of these actual arrival records is written to a ground-truth Delta Lake table. This component runs asynchronously from the prediction one – an order might be delivered 30 minutes after the prediction was made, but by isolating this in its own service, the system can handle that naturally without slowing down predictions. Essentially, the ground truth job is doing a continuous join between live positions and the list of active delivery requests, looking for matches to signal completion.\n\n4. Evaluation & Monitoring: The final stage joins the predictions with their corresponding ground truths to measure accuracy. It reads from the predictions Delta table and the ground truths table, linking records by request ID (each record pairs a predicted arrival time with the actual arrival time for a delivery). The pipeline then computes error metrics. For example, it can calculate the difference in minutes between predicted and actual delivery time for each order. These per-delivery error records are extremely useful for analytics – the blog mentions calculating overall Mean Absolute Error (MAE) and also segmenting error by how far in advance the prediction was made (predictions made closer to the delivery tend to be more accurate). Rather than hard-coding any specific aggregation in the pipeline, the approach was to output the raw prediction-vs-actual data into a PostgreSQL database (or even just a CSV file), and then use external tools or dashboards for deeper analysis and alerting. By doing so, they keep the streaming pipeline focused and let data analysts iterate on metrics in a familiar environment. (One cool extension: because everything is modular, they can add an alerting microservice that monitors this error data stream in real-time – e.g. trigger a Slack alert if error spikes – without impacting the other components.)\n\nKey Architectural Decisions:\n\nDecoupling via Delta Lake Tables: A standout decision was to connect these microservice pipelines using Delta Lake as the intermediate store. Instead of passing intermediate data via queues or Kafka topics, each stage writes its output to a durable table that the next stage reads. For example, the clean telemetry is a Delta table that both the Prediction and Ground Truth services read from. This has several benefits in a data engineering context:\n\nData Reusability & Observability: Because intermediate results are in tables, it’s easy to query or snapshot them at any time. If predictions look off, engineers can examine the cleaned data table to trace back anomalies. In a pure streaming hand-off (e.g. Kafka topic chaining), debugging would be harder – you’d have to attach consumers or replay logs to inspect events. Here, Delta gives a persistent history you can query with Spark/Pandas, etc.\n\nMultiple Consumers: Many pipelines can read the same prepared dataset in parallel. The La\xa0Poste use case leveraged this to have two different processes (prediction and ground truth) independently consuming the prepared\\_data table. Kafka could also multicast to multiple consumers, but those consumers would each need to handle data cleaning or maintaining state. With the Delta approach, the heavy lifting (cleaning) is done once and all consumers get a consistent view of the results.\n\nFailure Recovery: If one pipeline crashes or needs to be redeployed, the downstream pipelines don’t lose data – the intermediate state is stored in Delta. They can simply pick up from the last processed record by reading the table. There’s less worry about Kafka retention or exactly-once delivery mechanics between services, since the data lake serves as a reliable buffer and single source of truth.\n\nOf course, there are trade-offs. Writing to a data lake introduces some latency (micro-batch writes of files) compared to an in-memory event stream. It also costs storage – effectively duplicating data that in a pure streaming design might be transient. The blog specifically calls out the issue of many small files: frequent Delta commits (especially for high-volume streams) create lots of tiny parquet files and transaction log entries, which can degrade read performance over time. The team mitigated this by partitioning the Delta tables (e.g. by date) and periodically compacting small files. Partitioning by a day or similar key means new data accumulates in a separate folder each day, which keeps the number of files per partition manageable and makes it easier to run vacuum/compaction on older partitions. With these maintenance steps (partition + compact + clean old metadata), they report that the Delta-based approach remains efficient even for continuous, long-running pipelines. It’s a case of trading some complexity in storage management for a lot of flexibility in pipeline design.\n\nSchema Management & Versioning: With data passing through tables, keeping schemas in sync became an important consideration. If the schema of the cleaned data table changes (say they add a new column from the IoT feed), then the downstream Pathway jobs reading that table must be updated to expect that schema. The blog notes this as an increased maintenance overhead compared to a monolith. They likely addressed it by versioning their data schemas and coordinating deployments – e.g. update the writing pipeline to add new columns in parallel with updating readers, or use schema evolution features of Delta Lake. On the plus side, using Delta Lake made some aspects of schema handling easier: Pathway automatically stores each table’s schema in the Delta log, so when a job reads the table it can fetch the schema and apply it without manual definitions. This reduces code duplication and errors. Still, any intentional schema changes require careful planning across multiple services. This is just the nature of microservices – you gain modularity at the cost of more coordination.\n\nIndependent Scaling & Fault Isolation: A big reason for the microservice approach was scalability and reliability in production. Each pipeline can be scaled horizontally on its own. For example, if ETA requests volume spikes, they could scale out just the Prediction service (Pathway supports parallel processing within a job as well, but logically separating it is an extra layer of scalability). Meanwhile, the data cleaning service might be CPU-bound and need its own scaling considerations, separate from the evaluation service which might be lighter. In a monolithic pipeline, you’d have to scale the whole thing as one unit, even if only one part is the bottleneck. By splitting them, only the hot spots get more resources. Likewise, if the evaluation pipeline fails due to, say, a bug or out-of-memory error, it doesn’t bring down the ingestion or prediction pipelines – they keep running and data accumulates in the tables. The ops team can fix and redeploy the evaluation job and catch up on the stored data. This isolation is crucial for a production system where you want to minimize downtime and avoid one component’s failure cascading into an outage of the whole feature.\n\nPipeline Extensibility: The modular design also opened up new capabilities with minimal effort. The case study highlights a few:\n\nThey can easily plug in an anomaly detection/alerting service that reads the continuous error metrics (from the evaluation stage) and sends notifications if something goes wrong (e.g., if predictions suddenly become very inaccurate, indicating a possible model issue or data drift).\n\nThey can do offline model retraining or improvement by leveraging the historical data collected. Since they’re storing all cleaned inputs and outcomes, they have a high-quality dataset to train next-generation models. The blog mentions using the accumulated Delta tables of inputs and ground truths to experiment with improved prediction algorithms offline.\n\nThey can perform A/B testing of prediction strategies by running two prediction pipelines in parallel. For example, run the current model on half the vehicles and a new model on a subset of vehicles (perhaps by partitioning the Kafka requests by transport\\_unit\\_id hash). Because the infrastructure supports multiple pipelines reading the same input and writing results, this is straightforward – you just add another Pathway service, maybe writing its predictions to a different topic/table, and compare the evaluation metrics in the end. In a monolithic system, A/B testing could be really cumbersome or require building that logic into the single pipeline.\n\n  \nOperational Insights: On the operations side, the team did have to invest in coordinated deployments and monitoring for multiple services. There are four Pathway processes to deploy (plus Kafka, plus maybe the Delta Lake storage on S3 or HDFS, and the Postgres DB for results). Automated deploy pipelines and containerization likely help here (the blog doesn’t go deep into it, but it’s implied that there’s added complexity). Monitoring needs to cover each component’s health as well as end-to-end latency. The payoff is that each component is simpler by itself and can be updated or rolled back independently. For instance, deploying a new model in the Prediction service doesn’t require touching the ingestion or evaluation code at all – reducing risk. The scaling benefits were already mentioned: Pathway allows configuring parallelism for each pipeline, and because of the microservice separation, they only scale the parts that need it. This kind of targeted scaling can be more cost-efficient.\n\nThe La\xa0Poste case is a compelling example of applying software engineering best practices (modularity, fault isolation, clear data contracts) to a streaming data pipeline. It demonstrates how breaking a pipeline into microservices can yield significant improvements in maintainability and extensibility for data engineering workflows. Of course, as the authors caution, this isn’t a silver bullet – one should adopt such complexity only when the benefits (scaling, flexibility, etc.) outweigh the overhead. In their scenario of continuously improving an ETA prediction service, the trade-off made sense and paid off.\n\nI found this architecture interesting, especially the use of Delta Lake as a communication layer between streaming jobs – it’s a hybrid approach that combines real-time processing with durable data lake storage. It raises some great discussion points: e.g., would you have used message queues (Kafka topics) between each stage instead, and how would that compare? How do others handle schema evolution across pipeline stages in production? The post provides a concrete case study to think about these questions. If you want to dive deeper or see code snippets of how Pathway implements these connectors (Kafka read/write, Delta Lake integration, etc.), I recommend checking out the original blog and the Pathway GitHub. Links below. Happy to hear others’ thoughts on this design!']"
5,28,5_azure_to_data_the,"['azure', 'to', 'data', 'the', 'and', 'databricks', 'function', 'from', 'power', 'with']","[""Hi! I’m working on a FinOps initiative to improve cloud cost visibility and attribution across departments and projects in our data platform. We do tagging production workflows on department level and can get a decent view in Azure Cost Analysis by filtering on tags like department: X. But I am struggling to bring Databricks into that picture — especially when it comes to SQL Serverless Warehouses.\n\nMy goal is to be able to print out: total project cost = azure stuff + sql serverless.\n\n**Questions**:\n\n**1. Tagging Databricks SQL Warehouses for Attribution**\n\nIs creating a separate SQL Warehouse per department/project the only way to track department/project usage or is there any other way? \n\n\n\n**2. Joining Azure + Databricks Costs**\n\nIs there a clean way to join usage data from Azure Cost Analysis with Databricks billing data (e.g., from system.billing.usage)?\n\nI'd love to get a unified view of total cost per department or project — Azure Cost has most of it, but not SQL serverless warehouse usage or Vector Search or Model Serving. \n\n\n\n**3. Sharing Cost** \n\nFor those of you doing this well — how do you present project-level cost data to stakeholders like departments or customers?"", ""Hei! I’m designing a solution to pull daily survey data from an external API and load it into Power BI Service in a secure and automated way. Here’s the main idea:\n\n\t•\tUse an Azure Function to fetch paginated API data and store it in Azure Blob Storage (daily-partitioned .json files).\n\n\t•\tPower BI connects to the Blob container, dynamically loads the latest file/folder, and refreshes on schedule.\n\n\t•\tNo API calls happen inside Power BI Service (to avoid dynamic data source limitations). I was trying to do normal built-in GET API from Power BI Service but it doesn't accept dynamic data sources (Power BI Desktop works well, no issues) as API usually does.\n\n\t•\tEverything is designed with data protection and scalability in mind — future-compatible with Fabric Lakehouse.\nP/S: The reason we are forced to go with this solution without using Fabric architecture because it requires cost-effective solution and Fabric integration is planning to be deployed in our organization (potentially project starts from November)\n\nLooking for feedback on:\n\n\t•\tAnything I might be missing?\n\n\t•\tAny more robust or elegant approaches?\n\n\t•\tWould love to hear if anyone’s done something similar."", 'Hi all,\n\nI currently am working on a new structure to save sensor data coming from Azure Iot Hub in Azure to store it into Azure Blob Storage for historical data, and Clickhouse for hot data with TTL (around half year). The sensor data is coming from different entities (e.g building1, boat1, boat2) and should be partioned by entity. The data we’re processing daily is around 300-2 million records per day.\n\nI know Azure Iot Hub is essentially a built-in Azure Hub. I had a few questions since I’ve tried multiple solutions. \n\n1. Normal message routing to Azure Blob\nIssue: no custom partitioning on file structure (e.g entityid/timestamp_sensor/) it requires you to use the enqueued time. And there is no dead letter queue for fallback\n\n2. IoT hub -> Azure Functions -> Blob Storage & Clickhouse\nIssue: this should work correctly but I have not that much experience in Azure Functions, I tried creating a function with the IoT Hub template but it seems I need to also have an Event Hubs namespace which is not what I want. HTTP trigger is also not what I want. I don’t find any good documentation on it aswell. I know I can maybe use Event Hubs trigger and use the Iot Hub connection string but I didn’t manage to do this yet.\n\n3. IoT hub -> Event Grid \nSomeone suggested using Event Grid, however to my knowledge Event Grid is not used for telemetry data despite there being an option for. Is this beneficial? I don’t really know what the flow would be since you can’t use Event Grid to send data to Clickhouse. You would still need an Azure Functions.\n\n4. IoT Hub -> Event Grid -> Event Hubs -> Azure Functions -> Azure Blob & Clickhouse\nThis one seemed the most appealing to me but I don’t know if it’s the smartest, it can get expensive (maybe).\nBut the idea here is that we use Event Grid for batching the data and to have a dead letter queue.\nArrived in Event Hubs we use an Azure Function to send the data to blob storage and clickhouse.\n\nThe only problem is I might need some delay to sending to Clickhouse & Blob Storage (around maybe every 15 minutes) to reduce the risks of memory usage in Clickhouse and to reduce costs.\n\nCan someone help me out? Am I forgetting something crucial? I am a graduated data scientist, however I have no in depth experience with Azure.\n\n\n']"
6,27,6_dbt_to_the_sql,"['dbt', 'to', 'the', 'sql', 'and', 'lineage', 'it', 'for', 'with', 'sequor']","['I have had eyes on dbt for years. I think it helps with well-organized processes and clean code. I have never used it further than a PoC though because my company uses a lot of Python for data processing. Some of it could be replaced with SQL but some of it is text processing with Python NLP libraries which I wouldn’t know how to do in SQL. And dbt Python models are only available for some cloud database services while we use Postgres on-prem, so no go here.\n\nNow finally for the question: can you point me to software/frameworks that\n- allow Python code execution\n- build a DAG like dbt and only execute what is required\n- offer versioning where you could „go back in time“ to obtain the state of data like it was half a year before\n- offer a graphical view of the DAG\n- offer data lineage \n- help with project structure and are not overly complicated \n\nIt should be open source software, no GUI required. If we would use dbt, we would be dbt-core users.\n\nThanks for hints!', ""Hey dbt folks,\n\nI'm a data engineer and use dbt on a day-to-day basis, my team and I were struggling to find a good open-source tool for user-friendly column-level lineage visualization that we could use daily, similar to what commercial solutions like dbt Cloud offer. So, I decided to start building one...\n\nhttps://reddit.com/link/1jnh7pu/video/wcl9lru6zure1/player\n\nYou can find the repo [here](https://github.com/Fszta/dbt-column-lineage), and the package on [pypi](https://pypi.org/project/dbt-col-lineage/0.1.1/)\n\n**Under the hood**\n\nBasically, it works by combining dbt's manifest and catalog with some compiled SQL parsing magic (big shoutout to sqlglot!).\n\nI've built it as a CLI, keeping the syntax similar to dbt-core, with upstream and downstream selectors.\n\n    dbt-col-lineage --select stg_transactions.amount+ --format html\n\nRight now, it supports:\n\n* Interactive HTML visualizations\n* DOT graph images\n* Simple text output in the console\n\n**What's next ?**\n\n* Focus on compatibility with more SQL dialects\n* Improve the parser to handle complex syntax specific to certain dialects\n* Making the UI less... basic. It's kinda rough right now, plus some information could be added such as materialization type, col typing etc\n\nFeel free to drop any feedback or open an issue on the [repo](https://github.com/Fszta/dbt-column-lineage/tree/main)! It's still super early, and any help for testing on other dialects would be awesome. It's only been tested on projects using Snowflake, DuckDB, and SQLite adapters so far."", ""Hey dbt folks,\n\nI'm a data engineer and use dbt on a day-to-day basis, my team and I were struggling to find a good open-source tool for user-friendly column-level lineage visualization that we could use daily, similar to what commercial solutions like dbt Cloud offer. So, I decided to start building one...\n\nhttps://reddit.com/link/1jnh7pu/video/wcl9lru6zure1/player\n\nYou can find the repo [here](https://github.com/Fszta/dbt-column-lineage), and the package on [pypi](https://pypi.org/project/dbt-col-lineage/0.1.1/)\n\n**Under the hood**\n\nBasically, it works by combining dbt's manifest and catalog with some compiled SQL parsing magic (big shoutout to sqlglot!).\n\nI've built it as a CLI, keeping the syntax similar to dbt-core, with upstream and downstream selectors.\n\n    dbt-col-lineage --select stg_transactions.amount+ --format html\n\nRight now, it supports:\n\n* Interactive HTML visualizations\n* DOT graph images\n* Simple text output in the console\n\n**What's next ?**\n\n* Focus on compatibility with more SQL dialects\n* Improve the parser to handle complex syntax specific to certain dialects\n* Making the UI less... basic. It's kinda rough right now, plus some information could be added such as materialization type, col typing etc\n\nFeel free to drop any feedback or open an issue on the [repo](https://github.com/Fszta/dbt-column-lineage/tree/main)! It's still super early, and any help for testing on other dialects would be awesome. It's only been tested on projects using Snowflake, DuckDB, and SQLite adapters so far.""]"
7,26,7_the_to_keys_table,"['the', 'to', 'keys', 'table', 'in', 'schema', 'redshift', 'is', 'surrogate', 'and']","[""I'm setting up a Glue (Spark) to Redshift pipeline with incremental SQL loads, and while fact tables are straightforward (just append new records), dimension tables are more complex to be honest - I have a few questions regarding the practical implementation of a star schema data warehouse model ?   \n  \nFirst, avoiding duplicates, transactional facts won't have this issue because they will be unique, but for dimensions it is not the case,  do you pre-filter in Spark (reads existing Redshift dim tables and ensure new chunks of dim tables are new records) or just dump everything to Redshift and let it deduplicate (let Redshift handle upinserts)?   \n  \nSecond, surrogate keys, they have to be globally unique across all the table because they will serve as primary keys, do you generate them in Spark (risk collisions across job runs) or use Redshift IDENTITY for example?   \n  \nThird, SCD Type 2: implement change detection in Spark (comparing new vs old records) or handle it in Redshift (with MERGE/triggers)? Would love to hear real-world experiences on what actually scales, especially for large dimensions (10M+ rows) - how do you balance the Spark vs Redshift work while keeping everything consistent?\n\nLast but not least I want to know how to ensure fact tables are properly pointing to dimension tables, do we fill the foreign key column in spark before loading to redshift? \n\nPS: if you have any learning resources with practical implementations and best practices in place please provide them, because I feel the majority of the info on the web is theoretical.   \nThank you in advance."", 'I am currently implementing a Data Warehouse using Glue and Redshift, a star schema dimensional model to be exact. \n\n  \nAnd I think of the data transformations, that need to be done before having the clean fact and dimension tables in the data warehouse, as two types:\n\n  \n\\* Transformations related to the logic or business itself, eg. drop irrelevant columns, create new columns etc,   \n \\* Transformations that are purely related to the structure of a table, eg. the surrogate key column, the foreign key columns that we need to add to fact tables, etc  \nFor the second type, from what I understood from mt research, it can be done in Glue or Redshift, but apparently it will be more complicated to do it in Glue?  \n\nTake the example of surrogate keys, they will be Primary keys later on, and therefore if we will generate them in Glue, we have to ensure their uniqueness, this is feasible for the same job run, but if you want to ensure uniqueness across the entire table, you need to load the entire surrogate key column from Redshift and ensure that the newly generated ones in the job are unique.  \n\n\nI find this type of question recurrent in almost everything related to the structure of the data warehouse, from surrogate keys, to foreign keys, to SCD type 2.\n\nPlease if you have any thoughts or suggestions feel free to comment them.  \nThanks :)', 'Serious question to those who have done some data warehousing where Spark/Glue is the transformation engine, bonus if the data warehouse is Redshift.\n\nThis is my first time putting a data warehouse in place, and , I am doing so with AWS Glue and Redshift. The data load is incremental.\n\nWhile in theory dimensional modeling ( star schemas to be exact ) is not hard, I am finding a hard time implementing the actual model.\n\nI want to know how are these dimensional modeling concepts are actually implemented, the following is  my thoughts about how I understand some theoretical concepts and the way I find gaps between them and the actual practice.\n\n**Avoiding duplicates in both fact and dimension tables**\xa0–does this happen in the Spark job or Redshift itself? \n\nI feel like for transactional fact tables it is not a problem, but for dimensions, it is not straight forward: you need to insure uniqueness of entries for all the table not just the chunk you loaded during this run and this raises the above question, whether it is done in Spark, and in this case we will need to somehow load the dimension table  in dataframes so that we can filter new data loads, or in redshidt, and in this case we just load everything new to Redshift and delegate upserts and duplication checks to Redshift.\n\n  \nAnd speaking of uniqueness of entries in dimension tables ( I know it is getting long, bear with me, we are almost there xD) , we have to also allow exceptions, because when dealing with **SCD type 2,** we must allow duplicate entries and update the old ones to be depricated, so again how is this exception implemented practically? \n\n**Surrogate keys**\xa0– Generate in Spark (eg. UUIDs/hashes?) or rely on Redshift\xa0`IDENTITY` for example?\n\nSurrogate keys are going to serve as primary keys for both our fact and dimension tables, so they have to be unique, again do we generate them in Spark then load to, Redshift or do we just make Redshift handle these for us and not worry about uniqueness? \n\n**Fact-dim integrity**\xa0– Resolve FKs in Spark or after loading to Redshift?\n\nAnother concern arises when talking about surrogate keys, each fact table has to point to its dimensions with FKs, which in reality will be the surrogate keys of the dimensions, so these columns need to be filled with the right values, I am wondering whether this is done in Spark, and in this case we will have to again load the dimensions from Redshift in Spark dataframes and extract the right values of FKs, or can this be done in Reshift????\n\nIf you have any thoughts or insights please feel free to share them, litterally anything can help at this point xD']"
8,26,8_azure_microsoft_dp_exam,"['azure', 'microsoft', 'dp', 'exam', 'to', 'the', '203', 'on', 'and', 'practice']","[""Has anyone had much luck with finding roles in NZ or AU which have a heavy reliance on the types of orchestration frameworks above?\n\nI understand most businesses will always just go for the out of the box, click and forget approach, or the option from the big providers like Azure, Aws, Gcp, etc.\n\nHowever, I'm more interested in finding a company building it open source or at least managed outside of a big platform. \n\nI've found d it really hard to crack into those roles, they seem to just reject anyone without years of experience using the tool in question, so I've been building my own projects while using little bits of them at various jobs like managed airflow in azure or GCP.\n\nI just find data engineering tasks within the big platforms, especially azure, a bit stale, it'll get much worse with fabric too. GCP isn't to bad, I've not used much in aws besides S3 with snowflake or glue and redshift."", 'i took the Azure DP-203 last week — of course, it’s retiring literally tomorrow. But I figured it is indeed a very broad certification and so it can give a ""grounding"" scope in Azure D.E.\n\nAlso, I think it\'s still super early to go full Fabric (DP-600 or even DP-700), because the job demand is still not really there. Most jobs still demand strong grounding in Azure services even in the wake of Fabric adoption (POCing…).\n\nSo of course here, it’s retiring literally tomorrow unfortunately. I have passed the exam with a high score (900+). Also, I have worked (during internship) directly with MS Fabric only. So I would say some skills actually transfer quite nicely (ex: ADF ~ FDF).\n\n---\n\n### Some notes on resources for future exams:\n\nI have relied primarily on [@tybulonazure](https://www.youtube.com/@tybulonazure)’s excellent YouTube channel (DP-203 playlist). It’s really great (watch on 1.8x – 2x speed).  \nNow going back to Fabric, I have seen he has pivoted to Fabric-centric content — also a great news!\n\nI also used the official “Guide” book (2024 version), which I found to be a surprisingly good way of structuring your learning. I hope equivalents for Fabric will be similar (TBS…).\n\n---\n\nThe labs on Microsoft Learn are honestly **poorly designed** for what they offer.  \n**Tip:** @tybul has video labs too — *use these*.  \nAnd for the exams, always focus on **conceptual understanding**, not rote memorization.\n\nAnother **important (and mostly ignored)** tip:  \nFocus on the **“best practices”** sections of Azure services in Microsoft Learn — I’ve read a lot of MS documentation, and those parts are often more helpful on the exam than the main pages.\n\n---\n\n**Examtopics** is obviously very helpful — but **read the comments**, they’re essential!\n\n---\n\nFinally, I do think it’s a shame it’s retiring — because the “traditional” Azure environment knowledge seems to be a sort of industry standard for companies. Also, the Fabric pricing model seems quite aggressive.\n\nSo for juniors, it would have been really good to still be able to have this background knowledge as a base layer.', ""So I recently appeared for the DP-203 certification by Microsoft and want to share my learnings and strategy that I followed to crack the exam.\n\nAs you all must already be knowing that this exam is labelled as “**Intermediate**” by\xa0Microsoft themselves which is perfect in my opinion. This exam does test you in the various concepts that are required for a data engineer to\xa0 master in his/her career. \n\nHaving said that, it is not too hard to crack the exam but at the same time also not as easy as appearing for AZ-900. \n\nDP-203 is aimed at testing the understanding of data related concepts and various tools Microsoft has offered in its suite to make your life easier. Some topics include SQL, Modern Data Warehousing, Python, PySpark, Azure Data Factory, Azure Synapse Analytics, Azure Stream Analytics, Azure EventHubs, Azure Data Lake Storage and last but not the least Azure Databricks. You can go through the complete set of topics this exam focuses on here - [https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/?practice-assessment-type=certification#certification-take-the-exam](https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/?practice-assessment-type=certification#certification-take-the-exam)\n\n\n\n**Courses:**\n\nI had just taken this one course for DP-203 by Alan Rodrigues *(This is not a paid promotion. I just thought that these resources were good to refer to)* and this is a 24 hour long course which has covered all the important and core concepts clearly and precisely. What I loved the most about this course is that it is a complete hands-on course. One more thing is that the instructor very rarely mentions anything as “this has already been covered in the previous sections”. If there is anything that we are using in the current section he makes sure to give a quick background on what has been covered in the earlier sections. Why this is so important is because we tend to forget some things and by just getting a refresher in a couple of sentences we are up to speed. \n\nFor those of you who don’t know, Microsoft offers access to majority resources if not all for FREE credit worth $200 for 30 days. So you simply have to sign up on their portal (insert link) and get access to all of them for 30 days. If you are residing in another country then convert dollars to your local currency. That is how much worth of free credit you will get for 30 days. \n\n**For example -** \n\nI live in India. \n\n1 $ = 87.789 INR \n\nSo I got FREE credits worth 87.789 X 200 = Rs 17,557\n\nEven when I appeared for the exam (Feb 8th, 2025) I hardly got 3-4 questions from the mock tests. But don’t get disheartened. Be sure you are consistent with your learning path and take notes whenever required. As I mentioned earlier, the exam is not very hard.\n\n**Link -** [https://www.udemy.com/course/data-engineering-on-microsoft-azure/learn/lecture/44817315?start=40#overview](https://www.udemy.com/course/data-engineering-on-microsoft-azure/learn/lecture/44817315?start=40#overview)\n\n\n\n**Mock Tests Resources:**\n\nSo I had referred a couple of resources for taking the mocks which I have mentioned below. *(This is not a paid promotion. I just thought that these resources were good to refer to.)*\n\n\n\n1. **Udemy Practice Tests -** [https://www.udemy.com/course/practice-exams-microsoft-azure-dp-203-data-engineering/?couponCode=KEEPLEARNING](https://www.udemy.com/course/practice-exams-microsoft-azure-dp-203-data-engineering/?couponCode=KEEPLEARNING)\n2. **Microsoft Practice Assessments -** [https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/practice/assessment?assessment-type=practice&assessmentId=49&practice-assessment-type=certification](https://learn.microsoft.com/en-us/credentials/certifications/azure-data-engineer/practice/assessment?assessment-type=practice&assessmentId=49&practice-assessment-type=certification)\n3. [https://www.examtopics.com/exams/microsoft/dp-203/](https://www.examtopics.com/exams/microsoft/dp-203/)\n\n\n\n\n\n**DO’s:**\n\n\n\n1. Make sure that if and whenever possible you do hands-on for all the sections and videos that have been covered in the Udemy course as I am 100% sure that you will encounter certain errors and would have to explore and solve the errors by yourself. This will build a sense of confidence and achievement after being able to run the pipelines or code all by yourself. (Also don’t forget to delete or pause resources whenever needed so that you get a hang of it and don’t lose out on money. The instructor does tell you when to do so.)\n2. Let’s be very practical, nobody remembers all the resolutions or solutions to every single issue or problem faced in the past. We tend to forget things over time and hence it is very important to document everything that you think is useful and would be important in the future. Maintain an excel sheet and create two columns “**Errors” and “Learnings/Resolution**” so that next time you encounter the same issue you already have a solution and don’t waste time. \n3. Watch and practice at least 5-10 videos daily. This way you can complete all the videos in a month and then go back and rewatch lessons you thought were hard. Then you can start giving practice tests. \n\n\n\n\n\n**DON'Ts:**\n\n\n\n1. By heart all the MCQs or answers to the questions. \n2. Refer to many resources so much so that you will get overwhelmed and not be able to focus on preparation.\n3. Even refer to multiple courses from different websites.\n\n\n\n\n\n**Conclusion:**\n\nAll in all, just make sure you do your hands on, practice regularly, give a timeline for yourself, don’t mug up things, don’t by heart things, make sure you use limited but quality resources for learning and practice. I am sure that by following these things you will be able to crack the exam in the first attempt itself. ""]"
9,21,9_kafka_the_and_streaming,"['kafka', 'the', 'and', 'streaming', 'data', 'to', 'user', 'this', 'of', 'flink']","[""Hi folks. My company is trying to switch some batch jobs to streaming. The current method is that the data are streaming data through Kafka, then there's a Spark streaming job that consumes the data and appends them to a raw table (with schema defined, so not 100% raw). Then we have some scheduled batch jobs (also Spark) that read data from the raw table, transform the data, load them into destination tables, and show them in the dashboards. We use Databricks for storage (Unity catalog) and compute (Spark), but use something else for dashboards.\n\nNow we are trying to switch these scheduled batch jobs into streaming, since the incoming data are already streaming anyway, why not make use of it and turn our dashboards into realtime. It makes sense from business perspective too.\n\nHowever, we've been facing some difficulty in rewriting the transformation jobs from batch to streaming. Turns out, Spark streaming doesn't support some imporant operations in batch. Here are a few that I've found so far:\n\n1. Spark streaming doesn't support window function (e.g. : ROW\\_NUMBER() OVER (...)). Our batch transformations have a lot of these.\n2. Joining streaming dataframes is more complicated, as you have to deal with windows and watermarks (I guess this is important for dealing with unbounded data). So it breaks many joining logic in the batch jobs.\n3. Aggregations are also more complicated. For example you can't do this: raw\\_df -> get aggregated df from raw\\_df -> join aggregated\\_df with raw\\_df\n\nSo far I have been working around these limitations by using Foreachbatch and using intermediary tables (Databricks delta table). However, I'm starting to question this approach, as the pipelines get more complicated. Another method would be refactoring the entire transformation queries to conform both the business logic and streaming limitations, which is probably not feasible in our scenario.\n\nHave any of you encountered such scenario and how did you deal with it? Or maybe do you have some suggestions or ideas? Thanks in advance."", '""**Flink DataStream API - Scalable Event Processing for Supplier Stats**""!\n\nHaving explored the lightweight power of Kafka Streams, we now level up to a full-fledged distributed processing engine: **Apache Flink**. This post dives into the foundational DataStream API, showcasing its power for stateful, event-driven applications.\n\nIn this deep dive, you\'ll learn how to:\n\n* Implement sophisticated event-time processing with Flink\'s native **Watermarks**.\n* Gracefully handle late-arriving data using Flink’s elegant **Side Outputs** feature.\n* Perform stateful aggregations with custom **AggregateFunction** and **WindowFunction**.\n* Consume Avro records and sink aggregated results back to Kafka.\n* Visualize the entire pipeline, from source to sink, using **Kpow** and **Factor House Local**.\n\nThis is post 4 of 5, demonstrating the control and performance you get with Flink\'s core API. If you\'re ready to move beyond the basics of stream processing, this one\'s for you!\n\nRead the full article here: https://jaehyeon.me/blog/2025-06-10-kotlin-getting-started-flink-datastream/\n\nIn the final post, we\'ll see how Flink\'s Table API offers a much more declarative way to achieve the same result. Your feedback is always appreciated!\n\n🔗 **Catch up on the series**:\n1. Kafka Clients with JSON\n2. Kafka Clients with Avro\n3. Kafka Streams for Supplier Stats', ""Ready to explore the world of Kafka, Flink, data pipelines, and real-time analytics without the headache of complex cloud setups or resource contention?\n\n🚀 Introducing the **NEW Factor House Local Labs** – your personal sandbox for building and experimenting with sophisticated data streaming architectures, all on your local machine!\n\nWe've designed these hands-on labs to take you from foundational concepts to building complete, reactive applications:\n\n🔗 **Explore the Full Suite of Labs Now:**\n[https://github.com/factorhouse/examples/tree/main/fh-local-labs](https://github.com/factorhouse/examples/tree/main/fh-local-labs)\n\n**Here's what you can get hands-on with:**\n\n*   💧 **Lab 1 - Streaming with Confidence:**\n    *   Learn to produce and consume Avro data using Schema Registry. This lab helps you ensure data integrity and build robust, schema-aware Kafka streams.\n\n*   🔗 **Lab 2 - Building Data Pipelines with Kafka Connect:**\n    *   Discover the power of Kafka Connect! This lab shows you how to stream data from sources to sinks (e.g., databases, files) efficiently, often without writing a single line of code.\n\n*   🧠 **Labs 3, 4, 5 - From Events to Insights:**\n    *   Unlock the potential of your event streams! Dive into building real-time analytics applications using powerful stream processing techniques. You'll work on transforming raw data into actionable intelligence.\n\n*   🏞️ **Labs 6, 7, 8, 9, 10 - Streaming to the Data Lake:**\n    *   Build modern data lake foundations. These labs guide you through ingesting Kafka data into highly efficient and queryable formats like Parquet and Apache Iceberg, setting the stage for powerful batch and ad-hoc analytics.\n\n*   💡 **Labs 11, 12 - Bringing Real-Time Analytics to Life:**\n    *   See your data in motion! You'll construct reactive client applications and dashboards that respond to live data streams, providing immediate insights and visualizations.\n\n**Why dive into these labs?**\n*   **Demystify Complexity:** Break down intricate data streaming concepts into manageable, hands-on steps.\n*   **Skill Up:** Gain practical experience with essential tools like Kafka, Flink, Spark, Kafka Connect, Iceberg, and Pinot.\n*   **Experiment Freely:** Test, iterate, and innovate on data architectures locally before deploying to production.\n*   **Accelerate Learning:** Fast-track your journey to becoming proficient in real-time data engineering.\n\nStop just dreaming about real-time data – start *building* it! Clone the repo, pick your adventure, and transform your understanding of modern data systems.\n""]"
10,21,10_to_python_the_and,"['to', 'python', 'the', 'and', 'for', 'excel', 'in', 'my', 'data', 'with']","[""Hey there :)\n\n  \nI hope I find myself in the right subreddit for this as I am trying to **engineer** my computer to push around some **data** ;) \n\nI'm currently working on a project to fully automate the processing of test results for a scientific study with students. \n\nThe workflow consists of several stages:\n\n1. **Data Extraction:** The test data is extracted from a local SQL database.\n2. **SPSS Processing:** The extracted data is then processed using SPSS with a custom-built syntax (legacy). This step generates multiple files from the data. I have been looking into how I can transition this syntax to a python script, so this step might be cut later.\n3. **Python Automation:** A Python script takes over the further processing. It reads the files, splits the data per class, inserts it into pre-designed Excel reporting templates.\n4. **File Upload:** The files are then automatically uploaded to a self-hosted Nextcloud instance.\n5. **Notification:** Once the workflow is complete, a notification  \n\nI have been thinking about different ways to implement this. Right now the inputs and outputs for the different steps are still done manually. \n\nAt work I have been using Jenkins lately and I think it feels natural  to do it in Jenkins and just describe the whole workflow in a pipeline with different stages to run. Besides that I have some experience with AWS Lambda and n8n but I am not sure if they would be helpful with this task.\n\nI´m not that experienced setting up such workflows as my work background is more in Infosec, so please forgive my uneducated guesses about how I best go about this :D Just trying not to take decisions that will be problematic later.\n\n\n\nGreetings from Germany"", 'Hey r/dataengineering,\n\nI’m 6 months into learning Python, SQL and DE.\n\nFor my current work (non-related to DE) I need to process an Excel file with 10k+ rows of product listings (boats, ATVs, snowmobiles) for a classifieds platform (like Craigslist/OLX).\n\nI already have about 10-15 scripts in Python I often use on that Excel file which made my work tremendously easier. And I thought it would be logical to make the whole process automated in a full pipeline with Airflow, normalization, validation, reporting etc.\n\nHere’s my plan:\n\n**Extract**\n\n- load Excel (local or cloud) using pandas\n\n**Transform**\n\n- create a 3NF SQL DB\n\n- validate data, check unique IDs, validate years columns, check for empty/broken data, check constency, data types fix invalid addresses etc)\n\n- run obligatory business-logic scripts (validate addresses, duplicate rows if needed, check for dealerships and many more)\n\n- query final rows via joins, export to data/transformed.xlsx\n\n**Load**\n\n- upload final Excel via platform’s API\n- archive versioned files on my VPS\n\n**Report**\n\n- send Telegram message with row counts, category/address summaries, Matplotlib graphs, and attached Excel\n- error logs for validation failures\n\n**Testing**\n\n- pytest unit tests for each stage (e.g., Excel parsing, normalization, API uploads).\n\nPlanning to use Airflow to manage the pipeline as a DAG, with tasks for each ETL stage and retries for API failures but didn’t think that through yet.\n\nAs experienced data engineers what strikes you first as bad design or bad idea here? How can I improve it as a project for my portfolio?\n\nThank you in advance!', 'Hey r/dataengineering,\n\nI’m 6 months into learning Python, SQL and DE. \n\nFor my current work (non-related to DE) I need to process an Excel file with 10k+ rows of product listings (boats, ATVs, snowmobiles) for a classifieds platform (like Craigslist/OLX). \n\nI already have about 10-15 scripts in Python I often use on that Excel file which made my work tremendously easier. And I thought it would be logical to make the whole process automated in a full pipeline with Airflow, normalization, validation, reporting etc.\n\nHere’s my plan:\n\n1. Extract:  \n- load Excel (local or cloud) using pandas\n\n2. Transform:  \n- create a 3NF SQL DB\n- validate data, check unique IDs, validate years columns, check for empty/broken data, check constency, data types fix invalid addresses etc)\n- run obligatory business-logic scripts (validate addresses, duplicate rows if needed, check for dealerships and many more)\n- query final rows via joins, export to data/transformed.xlsx\n\n3. Load\n   - upload final Excel via platform’s API\n   - archive versioned files on my VPS\n\n4. Report\n   - send Telegram message with row counts, category/address summaries, Matplotlib graphs, and attached Excel.  \n   - error logs for validation failures\n\n5. Testing\n   - pytest unit tests for each stage (e.g., Excel parsing, normalization, API uploads).  \n\nPlanning to use Airflow to manage the pipeline as a DAG, with tasks for each ETL stage and retries for API failures but didn’t think that through yet.\n\nAs experienced data engineers what strikes you first as bad design or bad idea here? How can I improve it as a project for my portfolio?\n\nThanks in advance!']"
11,20,11_to_for_and_my,"['to', 'for', 'and', 'my', 'that', 'on', 'of', 'the', 'platforms', 'with']","[""Hey Everyone\n\nI'm a self-taught solo engineer/developer (with university + multi-year professional software engineer experience) developing a solution for a growing problem I've noticed many organizations are facing: managing and optimizing spending across multiple AI and LLM platforms (OpenAI, Anthropic, Cohere, Midjourney, etc.).\n\n# The Problem I'm Research / Attempting to Address:\n\nFrom my own research and conversations with various teams, I'm seeing consistent challenges:\n\n* No centralized way to track spending across multiple AI providers\n* Difficulty attributing costs to specific departments, projects, or use cases\n* Inconsistent billing cycles creating budgeting headaches\n* Unexpected cost spikes with limited visibility into their causes\n* Minimal tools for forecasting AI spending as usage scales\n\n# My Proposed Solution\n\nBuilding a platform-agnostic billing management solution that would:\n\n* Provide a unified dashboard for all AI platform spending\n* Enable project/team attribution for better cost allocation\n* Offer usage analytics to identify optimization opportunities\n* Include customizable alerts for budget management\n* Generate forecasts based on historical usage patterns\n\n# I Need Your Input:\n\nBefore I go too deep into development, I want to make sure I'm building something that genuinely solves problems:\n\n1. What features would be most valuable for your organization?\n2. What platforms beyond the major LLM providers should we support?\n3. How would you ideally integrate this with your existing systems?\n4. What reporting capabilities are most important to you?\n5. How do you currently handle this challenge (manual spreadsheets, custom tools, etc.)?\n\nSeriously would love your insights and/or recommendations of other projects I could build because I'm pretty good at launching MVPs extremely quickly (few hours to 1 week MAX)."", ""I am a data/functional analyst looking to get a better grasp on 'end-to-end data management and processing' (in quotations as that's the term my direct lead used when we aligned regarding my career objectives for the year, which definitely matches with my current interests).\n\nGiven this objective, my own interest in gaining a better grasp on general principles and functions of Data Engineering (and later on, Data Science), as well as a personal project I'm looking to finish for myself/my own portfolio by the end of the year, I'm looking for a cloud-based data pipeline solution for me to spend my role's training stipend on -- though I'm willing (and allowed) to use some of my personal funds on this as well since it's both for career progression and personal fun.\n\nI started looking for small-scale subscriptions for some of the platforms we've previously used at work (AWS, Databricks), but the consensus I've gotten is that cost-wise, I'm potentially better off with multiple smaller platforms that I can just weave together, instead of going for platforms normally designed for enterprise use. And so I come here for help!\n\n*(Though if this is not the right community for me to ask this, please let me know where instead)*\n\n# Budget and Scope\n\n* Base budget is a $200 stipend from our department, which refreshes at the end of the year\n* I'm willing to add another $200-300/year, if necessary\n* I'm from South-East Asia (converting our local currency, in case regional pricing matters)\n* I prefer platforms/solutions that come with upfront costs instead of per-pull/use costs, so that's it's easier to manage the finances involved\n* Languages I'm comfortable with are Python and SQL, since these are what I mainly use for work, but willing to learn basics of new languages where necessary \n* Since I will be using my personal computer, which is not that powerful currently, I am looking for a pipeline/platform that I can use completely via cloud (including running training models) \n\n# Learning Objectives \n\n*(aka things I want my platform/s to be able to do)*\n\n**1. Data Gathering/Ingestion/Validation/Transformation/Storage**\n\n* Obviously not too familiar with this step currently, so I ended up lumping a bunch of them together, but essentially ETL\n* In terms of data I intend to use, I have three main sources I intend to work my way through, from simple to complex: **(1)** will be starting with ingesting CSV files I get from third-party free data providers like Kaggle, **(2)** then moving on to structured databases from sports stat sites such as BBREF, before  eventually work my way to **(3)** automating data collection from publicly available video games I normally play (Dota 2, Path of Exile -- if it matters)\n* I want to be able to load these into SQL tables that I can regularly query form\n\n**2. Data Querying and Extract Cloud Storage**\n\n* I want to then be able to set it up so that I can easily query and then store-via-cloud what I need.\n* To make my intention clear, most of my work experience is with AWS Athena and S3, so I hope to get platforms that function similarly.\n\n**3. Cloud-Based Data Processing, Machine Learning, and Visualization**\n\n* I currently do majority of my post-query data processing and modelling locally on my work laptop on Jupyter (via the Anaconda distribution), but one of my key objectives is learning to do all of these on cloud (especially since my peronal computer I'll be using would obviously not be as powerful as what I use for work)\n* I definitely prefer Notebook-like environments, so perhaps something like AWS EMR\n* My main experience at this level is mainly with Python (using specific packages such as Pandas, Numpy, Matplotlib, Numpy, Sklearn, etc.), but I'm looking to do more PySpark as well\n\n**4. EXTRA/OPTIONAL: Dashboard Creation and Hosting**\n\n* If I can get a platform/pipeline that will allow me to host interactive dashboards for me to just embed in my portfolio, that would be a plus, but I am very easily willing to drop this should it not fit my budget\n\n#  Final Remarks\n\n* Want to learn ETL, and cloud-based data processing on a personal data pipeline and processing platform/solution that also has SQL capabilities -- in line with my career and personal learning objectives for the year \n* Tried looking into personal subscriptions for 360 solutions like Databricks and AWS Sagemaker Unified Studio, but was told that for what I want I might be better off with patching together T2 solutions or something along those lines -- but I imagine this would be much more tricky to set up "", ""I am a data/functional analyst looking to get a better grasp on 'end-to-end data management and processing' (in quotations as that's the term my direct lead used when we aligned regarding my career objectives for the year, which definitely matches with my current interests).\n\nGiven this objective, my own interest in gaining a better grasp on general principles and functions of Data Engineering (and later on, Data Science), as well as a personal project I'm looking to finish for myself/my own portfolio by the end of the year, I'm looking for a cloud-based data pipeline solution for me to spend my role's training stipend on -- though I'm willing (and allowed) to use some of my personal funds on this as well since it's both for career progression and personal fun.\n\nI started looking for small-scale subscriptions for some of the platforms we've previously used at work (AWS, Databricks), but the consensus I've gotten is that cost-wise, I'm potentially better off with multiple smaller platforms that I can just weave together, instead of going for platforms normally designed for enterprise use. And so I come here for help!\n\n*(Though if this is not the right community for me to ask this, please let me know where instead)*\n\n# Budget and Scope\n\n* Base budget is a $200 stipend from our department, which refreshes at the end of the year\n* I'm willing to add another $200-300/year, if necessary\n* I'm from South-East Asia (converting our local currency, in case regional pricing matters)\n* I prefer platforms/solutions that come with upfront costs instead of per-pull/use costs, so that's it's easier to manage the finances involved\n* Languages I'm comfortable with are Python and SQL, since these are what I mainly use for work, but willing to learn basics of new languages where necessary \n* Since I will be using my personal computer, which is not that powerful currently, I am looking for a pipeline/platform that I can use completely via cloud (including running training models) \n\n# Learning Objectives \n\n*(aka things I want my platform/s to be able to do)*\n\n**1. Data Gathering/Ingestion/Validation/Transformation/Storage**\n\n* Obviously not too familiar with this step currently, so I ended up lumping a bunch of them together, but essentially ETL\n* In terms of data I intend to use, I have three main sources I intend to work my way through, from simple to complex: **(1)** will be starting with ingesting CSV files I get from third-party free data providers like Kaggle, **(2)** then moving on to structured databases from sports stat sites such as BBREF, before  eventually work my way to **(3)** automating data collection from publicly available video games I normally play (Dota 2, Path of Exile -- if it matters)\n* I want to be able to load these into SQL tables that I can regularly query form\n\n**2. Data Querying and Extract Cloud Storage**\n\n* I want to then be able to set it up so that I can easily query and then store-via-cloud what I need.\n* To make my intention clear, most of my work experience is with AWS Athena and S3, so I hope to get platforms that function similarly.\n\n**3. Cloud-Based Data Processing, Machine Learning, and Visualization**\n\n* I currently do majority of my post-query data processing and modelling locally on my work laptop on Jupyter (via the Anaconda distribution), but one of my key objectives is learning to do all of these on cloud (especially since my peronal computer I'll be using would obviously not be as powerful as what I use for work)\n* I definitely prefer Notebook-like environments, so perhaps something like AWS EMR\n* My main experience at this level is mainly with Python (using specific packages such as Pandas, Numpy, Matplotlib, Numpy, Sklearn, etc.), but I'm looking to do more PySpark as well\n\n**4. EXTRA/OPTIONAL: Dashboard Creation and Hosting**\n\n* If I can get a platform/pipeline that will allow me to host interactive dashboards for me to just embed in my portfolio, that would be a plus, but I am very easily willing to drop this should it not fit my budget\n\n#  Final Remarks\n\n* Want to learn ETL, and cloud-based data processing on a personal data pipeline and processing platform/solution that also has SQL capabilities -- in line with my career and personal learning objectives for the year \n* Tried looking into personal subscriptions for 360 solutions like Databricks and AWS Sagemaker Unified Studio, but was told that for what I want I might be better off with patching together T2 solutions or something along those lines -- but I imagine this would be much more tricky to set up ""]"
12,19,12_files_to_the_we,"['files', 'to', 'the', 'we', 'and', 'memory', 'file', 'it', 'is', 'that']","['I\'m working on some data pipelines for a new source of data for our data lake, and right now we really only have one path to get the data up to the cloud. Going to do some hand-waving here only because I can\'t control this part of the process (for now), but a process is extracting data from our mainframe system as text (csv), and then compressing the data, and then copying it out to a cloud storage account in S3.\n\nWhy compress it? Well, it does compress well; we see around \\~30% space saved and the data size is not small; we\'re going from roughly 15GB per extract to down to 4.5GB. These are averages; some days are smaller, some are larger, but it\'s in this ballpark. Part of the reason for the compression is to save us some bandwidth and time in the file copy.\n\nSo now, I have a spark job to ingest the data into our raw layer, and it\'s taking longer than I \\*feel\\* it should take. I know that there\'s some overhead to reading compressed .gzip (I feel like I read somewhere once that it has to read the entire file on a single thread first). So the reads and then ultimately the writes to our tables are taking a while, longer than we\'d like, for the data to be available for our consumers.\n\nThe debate we\'re having now is where do we want to ""eat"" the time:\n\n* Upload uncompressed files (vs compressed) so longer times in the file transfer\n* Add a step to decompress the files before we read them\n* Or just continue to have slower ingestion in our pipelines\n\nMy argument is that we can\'t beat physics; we are going to have to accept some length of time with any of these options. I just feel as an organization, we\'re over-indexing on a solution. So I\'m curious which ones of these you\'d prefer? And for the title: ', '**Context:** I joined a finance consultancy a few years ago and noticed that most people in my department are frustrated with the current ""software"" our engineering team has built over decades (yes - not years, decades). The issue is that the software consists of a bundle of Python scripts that repeatedly read large CSV files whenever a user interacts with it. The master CSV file ranges in size from 20MB to 1GB.\n\nFor example, if a user wants to select an option from a dropdown menu, clicking the dropdown triggers the reading and aggregation of a 1GB file, after which the frontend is ""returned"" 20 strings (the dropdown options). By ""returned,"" I mean that a new CSV file is created somewhere on the user\'s local file system, and the ""frontend"" picks it up. I tested a similar functionality using a Flask REST API, and once the CSV file is loaded into virtual memory, the process takes only 100ms—compared to the current design, which takes a full minute (some of it is due to scripts needing to sort out dependencies, validation, etc.). However, our engineering team refuses to adopt web-based communication, arguing that it\'s not worth the effort. The idea of using a cloud-based relational database is essentially taboo; it has to be either CSV files or Python’s pickle dumps on each user\'s local system.\n\nI have some experience in software engineering, so I made it my mission to redesign this legacy monster—with the blessing of a senior manager. So far, the transition has gone incredibly well. Last year, I did a soft launch of a small subset of features, and within days, every person in my department was using it.\n\n**Question:** My current design requires users to set up a virtual environment and run an installation script that sorts out any environment variables, dependencies, etc. Each time they want to start the software, they must run a local Flask API, which interacts with a React TypeScript frontend. When the Flask API starts, it loads all necessary files into memory, does validation and other things (takes around a minute). After that, every subsequent request is easy and takes on average 100 to 200 ms. However, I dislike that each user needs a fully configured environment. Version control is also a headache since every user must manually run an update script.\n\nI’d like to move my Flask API to the cloud so that either:\n\n1. **A single server serves all colleagues**, or\n2. **Each colleague gets a dedicated node/pod.**\n\nThe problem with a single server is that it would quickly run out of virtual memory if 100+ colleagues loaded large datasets simultaneously. The problem with one node per colleague is the complexity—it would require Kubernetes (K8s) or AWS Fargate, along with an orchestrator to manage node creation and termination, which is a significant engineering effort.\n\nI then considered making my Flask API stateless: storing large datasets in S3, using DynamoDB for file mapping, and loading data into virtual memory on every request. I converted some sample datasets to Parquet, reducing their size significantly (down to \\~10MB), but I worry about added latency. Repeatedly reading the same data (given that each user makes 1–10 requests per minute) seems highly inefficient.\n\nAm I missing any alternatives? Based on this, a local Flask API still seems like the best option—unless I want to pay for an expensive 64GB vRAM EC2 instance or invest significant time in building a node-per-user architecture.\n\nThanks!', 'Hello,\nI’m facing a philosophical question at work and I can’t find an answer that would put my brain at ease. \n\nBasically we work with Databricks and Pyspark for ingestion and transformation.\n\nWe have a new data provider that sends crypted and zipped files to an s3 bucket. There are a couple of thousands of files (2 years of historic).\n\nWe wanted to use dataloader from databricks. It’s basically a spark stream that scans folders, finds the files that you never ingested (it keeps track in a table) and reads the new files only and write them.\nThe problem is that dataloader doesn’t handle encrypted and zipped files (json files inside).\n\nWe can’t unzip files permanently. \n\nMy coworker proposed that we use the autoloader to find the files (that it can do) and in that spark stream use the for each batch method to apply a lambda that does:\n- get the file name (current row)\n-decrypt and unzip\n-hash the files (to avoid duplicates in case of failure)\n-open the unzipped file using spark\n-save in the final table using spark \n\nI argued that it’s not the right place to do all that and since it’s not the use case of autoloader it’s not a good practice, he argues that spark is distributed and that’s the only thing we care since it allows us to do what we need quickly even though it’s hard to debug (and we need to pass the s3 credentials to each executor using the lambda…)\n\nI proposed a homemade solution which isn’t the most optimal, but it seems better and easier to maintain which is:\n- use boto paginator to find files\n- decrypt and unzip each file \n- write then json in the team bucket/folder\n-create a monitoring table in which we save the file name, hash, status (ok/ko) and exceptions if there are any\n\nHe argues that this is not efficient since it’ll only use one single node cluster and not parallelised. \n\nI never encountered such use case before and I’m kind of stuck, I read a lot of literature but everything seems very generic. \n\nEdit: we only receive 2 to 3 files daily per data feed (150mo per file on average) but we have 2 years of historical data which amounts to around 1000 files. So we need 1 run for all the historic then a daily run. \nEvery feed ingested is a class instantiation (a job on a cluster with a config) so it doesn’t matter if we have 10 feeds. \n\nEdit2: 1000 files roughly summed to 130go after unzipping. Not sure of average zip/json file though. \n\nWhat do you people think of this? Any advices ?\nThank you ']"
13,17,13_spark_https_is_com,"['spark', 'https', 'is', 'com', 'and', 'it', 'the', 'of', 'rust', 'polars']","['I’ve benchmarked it. For use cases in my specific industry it’s something like x5, x7 more efficient in computation. It looks like it’s pretty revolutionary in terms of cost savings. It’s faster and cheaper.\n\nThe problem is PySpark is like using a missile to kill a worm. In what I’ve seen, it’s totally overpowered for what’s actually needed. It starts spinning up clusters and workers and all the tasks. \n\nI’m not saying it’s not useful. It’s needed and crucial for huge workloads but most of the time huge workloads are not actually what’s needed.\n\nSpark is perfect with big datasets and when huge data lake where complex computation is needed. It’s a marvel and will never fully disappear for that.\n\nAlso Polars syntax and API is very nice to use. It’s written to use only one node. \n\nBy comparison Pandas syntax is not as nice (my opinion). \n\nAnd it’s computation is objectively less efficient.  It’s simply worse than Polars in nearly every metric in efficiency terms.\n\nI cant publish the stats because it’s in my company enterprise solution but search on open Github other people are catching on and publishing metrics.\n\nPolars uses Lazy execution, a Rust based computation (Polars is a Dataframe library for Rust). Plus Apache Arrow data format.\n\nIt’s pretty clear it occupies that middle ground where Spark is still needed for 10GB/ terabyte / 10-15 million row+ datasets. \n\nPandas is useful for small scripts (Excel, Csv) or hobby projects but Polars can do everything Pandas can do and faster and more efficiently.\n\nSpake is always there for the those use cases where you need high performance but don’t need to call in artillery. \n\nIts syntax means if you know Spark is pretty seamless to learn.\n\nI predict as well there’s going to be massive porting to Polars for ancestor input datasets.\n\nYou can use Polars for the smaller inputs that get used further on and keep Spark for the heavy workloads. The problem is converting to different data frames object types and data formats is tricky. Polars is very new.\n\nMany legacy stuff in Pandas over 500k rows where costs is an increasing factor or cloud expensive stuff is also going to see it being used.\n\n', 'In this opinionated article I am going to explain why I believe we have reached peak Spark usage and why it is only downhill from here.\n\n# Before Spark\n\nSome will remember that 12 years ago [Pig](https://pig.apache.org), [Hive](https://hive.apache.org), [Sqoop](https://sqoop.apache.org), [HBase](https://hbase.apache.org) and MapReduce were all the rage. Many of us were under the spell of [Hadoop](https://hadoop.apache.org) during those times.\n\n# Enter Spark\n\nThe brilliant [Matei Zaharia](https://www.linkedin.com/in/mateizaharia/) started working on Spark sometimes before 2010 already, but adoption really only began after 2013.  \nThe lazy evaluation and memory leveraging as well as other [innovative features](https://www.youtube.com/watch?v=w0Tisli7zn4&t=97s) were a huge leap forward and I was dying to try this new promising technology.  \nMy then CTO was visionary enough to understand the potential and for years since, I, along with many others, ripped the benefits of an only improving Spark.\n\n# The Loosers\n\nHow many of you recall companies like [Hortonworks](https://hortonworks.com/wp-content/uploads/2013/11/Webinar.HDP2_.20131112.pdf) and [Cloudera](https://www.cloudera.com/about.html)? Hortonworks and Cloudera merged after both becoming public, only to be taken private a few years later. Cloudera still exists, but not much more than that.\n\nThose companies were yesterday’s [Databricks](https://www.databricks.com) and they bet big on the Hadoop ecosystem and not so much on Spark.\n\n# Hunting decisions\n\nIn creating Spark, Matei did what any pragmatist would have done, he piggybacked on the existing Hadoop ecosystem. This allowed Spark not to be built from scratch in isolation, but integrate nicely in the Hadoop ecosystem and supporting tools.\n\nThere is just one problem with the Hadoop ecosystem…it’s exclusively **JVM based**. This decision has fed and made rich thousands of consultants and engineers that have fought with the [GC](https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)) and inconsistent memory issues for years…and still does. The JVM is a solid choice, safe choice, but despite more than 10 years passing and Databricks having the plethora of resources it has, some of Spark\'s core issues with managing memory and performance just can\'t be fixed.\n\n# The writing is on the wall\n\nChange is coming, and few are noticing it ([some do](https://nolanlawson.com/2024/10/20/why-im-skeptical-of-rewriting-javascript-tools-in-faster-languages/?utm_source=chatgpt.com)). This change is happening in all sorts of supporting tools and frameworks.\n\nWhat do [uv](https://docs.astral.sh/uv/), [Pydantic](https://docs.pydantic.dev/latest/), [Deno](https://deno.com), [Rolldown](https://rolldown.rs) and the Linux kernel all have in common that no one cares about...for now? They all have a Rust backend or have an increasingly large Rust footprint. These handful of examples are just the tip of the iceberg.\n\nRust is the most prominent example and the forerunner of a set of languages that offer performance, a completely different memory model and some form of usability that is hard to find in market leaders such as C and C++. There is also Zig which similar to Rust, and a bunch of other languages that can be found in TIOBE\'s top 100.\n\nThe examples I gave above are all of tools for which the primary target are not Rust engineers but Python or JavaScipt. Rust and other languages that allow easy interoperability are increasingly being used as an efficient reliable backend for frameworks targeted at completely different audiences.\n\nThere\'s going to be less of ""by Python developers for Python developers"" looking forward.\n\n# Nothing is forever\n\nSpark is here to stay for many years still, hey, Hive is still being used and maintained, but I belive that peak adoption has been reached, there\'s nowhere to go from here than downhill. Users don\'t have much to expect in terms of performance and usability looking forward.\n\nOn the other hand, frameworks like [Daft](https://www.getdaft.io) offer a completely different experience working with data, no strange JVM error messages, no waiting for things to boot, just bliss. Maybe it\'s not Daft that is going to be the next best thing, but it\'s inevitable that Spark will be overthroned.\n\n# Adapt\n\nDatabricks better be ahead of the curve on this one.  \nInstead of using scaremongering marketing gimmicks like labelling the use of engines other than Spark as *Allow External Data Access*, it better ride with the wave.', 'In this opinionated article I am going to explain why I believe we have reached peak Spark usage and why it is only downhill from here.\n\n# Before Spark\n\nSome will remember that 12 years ago [Pig](https://pig.apache.org), [Hive](https://hive.apache.org), [Sqoop](https://sqoop.apache.org), [HBase](https://hbase.apache.org) and MapReduce were all the rage. Many of us were under the spell of [Hadoop](https://hadoop.apache.org) during those times.\n\n# Enter Spark\n\nThe brilliant [Matei Zaharia](https://www.linkedin.com/in/mateizaharia/) started working on Spark sometimes before 2010 already, but adoption really only began after 2013.  \nThe lazy evaluation and memory leveraging as well as other [innovative features](https://www.youtube.com/watch?v=w0Tisli7zn4&t=97s) were a huge leap forward and I was dying to try this new promising technology.  \nMy then CTO was visionary enough to understand the potential and for years since, I, along with many others, ripped the benefits of an only improving Spark.\n\n# The Loosers\n\nHow many of you recall companies like [Hortonworks](https://hortonworks.com/wp-content/uploads/2013/11/Webinar.HDP2_.20131112.pdf) and [Cloudera](https://www.cloudera.com/about.html)? Hortonworks and Cloudera merged after both becoming public, only to be taken private a few years later. Cloudera still exists, but not much more than that.\n\nThose companies were yesterday’s [Databricks](https://www.databricks.com) and they bet big on the Hadoop ecosystem and not so much on Spark.\n\n# Hunting decisions\n\nIn creating Spark, Matei did what any pragmatist would have done, he piggybacked on the existing Hadoop ecosystem. This allowed Spark not to be built from scratch in isolation, but integrate nicely in the Hadoop ecosystem and supporting tools.\n\nThere is just one problem with the Hadoop ecosystem…it’s exclusively **JVM based**. This decision has fed and made rich thousands of consultants and engineers that have fought with the [GC](https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)) and inconsistent memory issues for years…and still does. The JVM is a solid choice, safe choice, but despite more than 10 years passing and Databricks having the plethora of resources it has, some of Spark\'s core issues with managing memory and performance just can\'t be fixed.\n\n# The writing is on the wall\n\nChange is coming, and few are noticing it ([some do](https://nolanlawson.com/2024/10/20/why-im-skeptical-of-rewriting-javascript-tools-in-faster-languages/?utm_source=chatgpt.com)). This change is happening in all sorts of supporting tools and frameworks.\n\nWhat do [uv](https://docs.astral.sh/uv/), [Pydantic](https://docs.pydantic.dev/latest/), [Deno](https://deno.com), [Rolldown](https://rolldown.rs) and the Linux kernel all have in common that no one cares about...for now? They all have a Rust backend or have an increasingly large Rust footprint. These handful of examples are just the tip of the iceberg.\n\nRust is the most prominent example and the forerunner of a set of languages that offer performance, a completely different memory model and some form of usability that is hard to find in market leaders such as C and C++. There is also Zig which similar to Rust, and a bunch of other languages that can be found in TIOBE\'s top 100.\n\nThe examples I gave above are all of tools for which the primary target are not Rust engineers but Python or JavaScipt. Rust and other languages that allow easy interoperability are increasingly being used as an efficient reliable backend for frameworks targeted at completely different audiences.\n\nThere\'s going to be less of ""by Python developers for Python developers"" looking forward.\n\n# Nothing is forever\n\nSpark is here to stay for many years still, hey, Hive is still being used and maintained, but I belive that peak adoption has been reached, there\'s nowhere to go from here than downhill. Users don\'t have much to expect in terms of performance and usability looking forward.\n\nOn the other hand, frameworks like [Daft](https://www.getdaft.io) offer a completely different experience working with data, no strange JVM error messages, no waiting for things to boot, just bliss. Maybe it\'s not Daft that is going to be the next best thing, but it\'s inevitable that Spark will be overthroned.\n\n# Adapt\n\nDatabricks better be ahead of the curve on this one.  \nInstead of using scaremongering marketing gimmicks like labelling the use of engines other than Spark as *Allow External Data Access*, it better ride with the wave.']"
14,14,14_data_the_to_and,"['data', 'the', 'to', 'and', 'we', 'warehouse', 'of', 'for', 'from', 'be']","[""I might be asking naive question, but looking forward for some good discussion and experts opinion. Currently I'm working on a solution basically azure functions which extracts data from different sources and make the data available in snowflake warehouse for the users to write their own analytics model on top of it, currently both data model and users business model is sitting on top of same database and schema the downside of this is objects under schema started growing and also we started to see the responsibility of the user model started to be blurred like it is being pushed on to engineering team for maintaince which is creating kind of urgent user request to be addressed mid sprint. I'm sure we are not the only one had this issue just started this discussion on how others tackled this scenario and what are the pros and cons of each scenario. If we can separate both modellings it will be easy incase if other teams decide to use the data from warehouse."", ""I have about 20 years worth of flat files stored in a folder on a network drive as a result of lackluster data practices. Essentially, three different flat files get printed to this folder on a nightly bases that represent three different types of data (think: person, sales, products). Essentially this data could exist as three separate long tables with date as key. \n\nI'd like to establish a proper data warehouse, but am unsure of how to best handle the process of warehousing these flats. I have been interfacing with the data through Python Pandas so far, but the company has a SQL server...It would probably be best to place the warehouse as a database on the server, then pull/manipulate the data from there? But what is tripping me up is the order of operations to perform in the warehousing procedure. I don't believe I would be able to dump into SQL server without profiling the data first as number of columns and the type of data stored in the flat files may have changed throughout the years.\n\n\n\nI am essentially struggling with how to sequence the process of : network drive flats > sql server db:\n\n  \nMy concerns are:\n\nBest method to profile the data?\n\nBest way to store the metadata?\n\nThrow flats into SQL server and then query them from there to perform data transformations/validations? \n\n  \\-- It seems without knowing the meta data, I should perform this step in Pandas first before loading into SQL server? What is the best practice for that? perform operations on each flat file separately or combine first (e.g., should I clean the data during the loop or after combining tables)?\n\n   \\-- Right now, I am creating a list of flat files, using that list to create a dictionary of dataframes, and then using that dictionary to create a dataframe of dataframes to group and concatenate into 3 long tables -- am I convoluting this process?\n\nHow to approach data cleaning/validation/and additional column calculations? e.g. -- Should I perform these procedures on each file separately before concatenating into a long table or perform these procedures after concatenation?-- Should I even concatenate into longs or keep them separate and define a relationship to their keys stored in a separate table?\n\nHow many databases for this process? One for raws? One for staging? A third as the datawarehouse to be queried?\n\nWhen to stage and how much of the process to perform in RAM/behind the scenes before printing to a new table?\n\nShould I consider compressing the data at any point in the process? (e.g. store as Parquet)\n\n\n\nThe data gets used for data analytics and to assemble reports/dashboards. Ideally, I would like to eliminate having to perform as many joins as possible during the querying for analysis process. I'd also like to orchestrate the warehouse so that adjustments only need to happen in a single place and propagate throughout the pipeline with a history of adjustments stored as record."", ""I might be asking naive question, but looking forward for some good discussion and experts opinion. Currently I'm working on a solution basically azure functions which extracts data from different sources and make the data available in snowflake warehouse for the users to write their own analytics model on top of it, currently both data model and users business model is sitting on top of same database and schema the downside of this is objects under schema started growing and also we started to see the responsibility of the user model started to be blurred like it is being pushed on to engineering team for maintaince which is creating kind of urgent user request to be addressed mid sprint. I'm sure we are not the only one had this issue just started this discussion on how others tackled this scenario and what are the pros and cons of each scenario. If we can separate both modellings it will be easy incase if other teams decide to use the data from warehouse.""]"
15,11,15_to_app_api_it,"['to', 'app', 'api', 'it', 'can', 'we', 'in', 'you', 'and', 'the']","[""Hi guys,\n\nI’ve built a small tool called [**DataPrep**](https://data-prep.app) that lets you visually **explore and clean datasets** in your browser without any coding requirement.\n\nYou can try the live demo here (no signup required):  \n [demo.data-prep.app](https://demo.data-prep.app)\n\n  \nI work with data pipelines and I often needed a quick way to inspect raw files, test cleaning steps, and get some insights into my data without jumping into Python or SQL and for that I started working on **DataPrep**.  \nThe app is in its **MVP / Alpha** stage.\n\nIt'd be really helpful if you guys can try it out and provide some feedback on some topics like :\n\n* Would this save time in your workflows ?\n* What features would make it more useful ?\n* Any integrations or export options that should be added to it ?\n* How can the UI / UX be improved to make it more intuitive ?\n* Bugs encountered\n\nThanks in advance for giving it a look. Happy to answer any questions regarding this."", ""Hello, \n\nI am not really a dataengineer but after looking at what I'm going to be working on I may be one soon. \n\n  \nOk to start with the project, I work for a clinical research company and we currently are pulling reports manually and working with them in excel (occasionally making visualizaitons).  We pull from two sources, one we own but can't access (we could probably ask but we want a proof of concept first) but the other source we can use their API to access our data on their system.  \n\nI am looking for open-source (free) programs I can use to take the information given in the API break it into a full database (dataset tables) and keep in constantly updated in a gateway.  In this phase of the project I am more invested in being able to do an API call and automatically pulling the data to set it into the appropriate schema. \n\nI have a really good understanding of dataset creation put I am new to the scripting an API side.  \n\nI don't really know what else to add but if you have any follow up questions please comment. \n\n  \nI appreciate any help or advice you can give me. (I will be using our lord and savior youtube to learn as much as I can about whatever you suggest). "", ""Hello, \n\nI am not really a dataengineer but after looking at what I'm going to be working on I may be one soon. \n\n  \nOk to start with the project, I work for a clinical research company and we currently are pulling reports manually and working with them in excel (occasionally making visualizaitons).  We pull from two sources, one we own but can't access (we could probably ask but we want a proof of concept first) but the other source we can use their API to access our data on their system.  \n\nI am looking for open-source (free) programs I can use to take the information given in the API break it into a full database (dataset tables) and keep in constantly updated in a gateway.  In this phase of the project I am more invested in being able to do an API call and automatically pulling the data to set it into the appropriate schema. \n\nI have a really good understanding of dataset creation put I am new to the scripting an API side.  \n\nI don't really know what else to add but if you have any follow up questions please comment. \n\n  \nI appreciate any help or advice you can give me. (I will be using our lord and savior youtube to learn as much as I can about whatever you suggest). ""]"
16,11,16_or_engineering_resources_data,"['or', 'engineering', 'resources', 'data', 'and', 'you', 'everyone', 'thanks', 'survey', 'if']","['I’m a recent MSCS graduate trying to navigate this tough U.S. job market. I have around 2.5 years of prior experience in data engineering, and I’m currently preparing for data engineering interviews. One of the biggest challenges I’m facing is the lack of structured, comprehensive resources—everything I find feels scattered and incomplete.\n\nIf anyone could share resources or materials, especially around **data modeling case studies**, I’d be incredibly grateful. 🙏🏼😭', ""Hey everyone,  \nI’m working on a Data Engineering project and I want to make sure I’m organizing everything properly from the start. I'm looking for **best practices, lessons learned, or even examples of folder structures** used in real-world data engineering projects.\n\nWould really appreciate:\n\n* Any **advice or personal experience** on what worked well (or didn’t) for you\n* **Blog posts, GitHub repos, YouTube videos**, or other resources that walk through good project structure\n* Recommendations for organizing things like ETL pipelines, raw vs processed data, scripts, configs, notebooks, etc.\n\nThanks in advance — trying to avoid a mess later by doing things right early on!"", ""I'm diving deeper into Data Engineering and I’d love some help finding quality resources. I’m familiar with the basics of tools like SQL, PySpark, Redshift, Glue, ETL, Data Lakes, and Data Marts etc.\n\nI'm specifically looking for:\n\n* Platforms or websites that provide *real-world case studies*, *architecture breakdowns*, or *project-based learning*\n* Blogs, YouTube channels, or newsletters that cover *practical DE problems* and how they’re solved in production\n* Anything that can help me understand *how these tools are used together* in real scenarios\n\nWould appreciate any suggestions! Paid or free resources — all are welcome. Thanks in advance!""]"
